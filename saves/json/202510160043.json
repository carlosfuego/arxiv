[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.12705v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12705v1",
                "updated": "2025-10-14T16:39:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    39,
                    29,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T16:39:29Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    39,
                    29,
                    1,
                    287,
                    0
                ],
                "title": "A GPU-resident Memory-Aware Algorithm for Accelerating Bidiagonalization\n  of Banded Matrices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A GPU-resident Memory-Aware Algorithm for Accelerating Bidiagonalization\n  of Banded Matrices"
                },
                "summary": "The reduction of a banded matrix to a bidiagonal form is a crucial step in\nthe Singular Value Decomposition (SVD), a cornerstone of scientific computing\nand AI. Despite being a highly parallel algorithm, it was previously believed\nto be unsuitable for GPU computation because it is memory bandwidth-bound.\nRecent developments in GPU hardware, including larger L1 memory per Streaming\nMultiprocessor/Compute Unit, have changed that. We present the first GPU\nalgorithm for reducing a banded matrix to bidiagonal form as part of the\nNextLA$.$jl open-source software package. Our algorithm is based on previous\nCPU-based multicore parallel cache-efficient bulge chasing algorithms and\nadapted to optimize for GPU throughput. We leverage Julia Language's Array\nabstractions and KernelAbstractions to implement a single hardware- and data\nprecision-agnostic function on NVIDIA, AMD, Intel, and Apple Metal GPUs for\nhalf, single, and double precision, and examine performance optimization across\nhardware architectures and data precision. We also develop a hardware-aware\nperformance model and identify key hyperparameters, such as inner tilewidth and\nblock concurrency, that govern optimal GPU execution for bandwidth-bound\nworkloads. We demonstrate highly parallel bandwidth-bound algorithm on the GPU\ncan outperform CPU-based implementations: the GPU algorithm outperforms\nmultithreaded CPU High-Performance libraries PLASMA and SLATE as of matrix size\n1024 x 1024 and by a factor over 100 for matrices of 32k x 32k. In addition,\nthe performance of the algorithm increases linearly with matrix bandwidth size,\nmaking faster reduction of larger matrix bandwidths now also possible. With\nthis work, we break memory bandwidth barriers, as well as matrix bandwidth\nbarriers, resulting in orders-of-magnitude faster algorithms for the reduction\nof banded matrices to bidiagonal form on the GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reduction of a banded matrix to a bidiagonal form is a crucial step in\nthe Singular Value Decomposition (SVD), a cornerstone of scientific computing\nand AI. Despite being a highly parallel algorithm, it was previously believed\nto be unsuitable for GPU computation because it is memory bandwidth-bound.\nRecent developments in GPU hardware, including larger L1 memory per Streaming\nMultiprocessor/Compute Unit, have changed that. We present the first GPU\nalgorithm for reducing a banded matrix to bidiagonal form as part of the\nNextLA$.$jl open-source software package. Our algorithm is based on previous\nCPU-based multicore parallel cache-efficient bulge chasing algorithms and\nadapted to optimize for GPU throughput. We leverage Julia Language's Array\nabstractions and KernelAbstractions to implement a single hardware- and data\nprecision-agnostic function on NVIDIA, AMD, Intel, and Apple Metal GPUs for\nhalf, single, and double precision, and examine performance optimization across\nhardware architectures and data precision. We also develop a hardware-aware\nperformance model and identify key hyperparameters, such as inner tilewidth and\nblock concurrency, that govern optimal GPU execution for bandwidth-bound\nworkloads. We demonstrate highly parallel bandwidth-bound algorithm on the GPU\ncan outperform CPU-based implementations: the GPU algorithm outperforms\nmultithreaded CPU High-Performance libraries PLASMA and SLATE as of matrix size\n1024 x 1024 and by a factor over 100 for matrices of 32k x 32k. In addition,\nthe performance of the algorithm increases linearly with matrix bandwidth size,\nmaking faster reduction of larger matrix bandwidths now also possible. With\nthis work, we break memory bandwidth barriers, as well as matrix bandwidth\nbarriers, resulting in orders-of-magnitude faster algorithms for the reduction\nof banded matrices to bidiagonal form on the GPU."
                },
                "authors": [
                    {
                        "name": "Evelyne Ringoot"
                    },
                    {
                        "name": "Rabab Alomairy"
                    },
                    {
                        "name": "Alan Edelman"
                    }
                ],
                "author_detail": {
                    "name": "Alan Edelman"
                },
                "author": "Alan Edelman",
                "arxiv_comment": "13 pages, 7 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12705v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12705v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02659v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02659v3",
                "updated": "2025-10-14T16:05:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    5,
                    11,
                    1,
                    287,
                    0
                ],
                "published": "2025-07-03T14:20:41Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding"
                },
                "summary": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup."
                },
                "authors": [
                    {
                        "name": "Ramchalam Kinattinkara Ramakrishnan"
                    },
                    {
                        "name": "Zhaocong Yuan"
                    },
                    {
                        "name": "Shaojie Zhuo"
                    },
                    {
                        "name": "Chen Feng"
                    },
                    {
                        "name": "Yicheng Lin"
                    },
                    {
                        "name": "Chenzheng Su"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaopeng Zhang"
                },
                "author": "Xiaopeng Zhang",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02659v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02659v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00299v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00299v5",
                "updated": "2025-10-14T15:42:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    42,
                    41,
                    1,
                    287,
                    0
                ],
                "published": "2025-02-01T03:49:47Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem. The code is available at\n\\href{https://github.com/NVIDIA/kvpress}{link}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem. The code is available at\n\\href{https://github.com/NVIDIA/kvpress}{link}."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00299v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00299v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12642v1",
                "updated": "2025-10-14T15:34:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    34,
                    35,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T15:34:35Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    34,
                    35,
                    1,
                    287,
                    0
                ],
                "title": "Aixel: A Unified, Adaptive and Extensible System for AI-powered Data\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aixel: A Unified, Adaptive and Extensible System for AI-powered Data\n  Analysis"
                },
                "summary": "A growing trend in modern data analysis is the integration of data management\nwith learning, guided by accuracy, latency, and cost requirements. In practice,\napplications draw data of different formats from many sources. In the\nmeanwhile, the objectives and budgets change over time. Existing systems handle\nthese applications across databases, analysis libraries, and tuning services.\nSuch fragmentation leads to complex user interaction, limited adaptability,\nsuboptimal performance, and poor extensibility across components. To address\nthese challenges, we present Aixel, a unified, adaptive, and extensible system\nfor AI-powered data analysis. The system organizes work across four layers:\napplication, task, model, and data. The task layer provides a declarative\ninterface to capture user intent, which is parsed into an executable operator\nplan. An optimizer compiles and schedules this plan to meet specified goals in\naccuracy, latency, and cost. The task layer coordinates the execution of data\nand model operators, with built-in support for reuse and caching to improve\nefficiency. The model layer offers versioned storage for index, metadata,\ntensors, and model artifacts. It supports adaptive construction, task-aligned\ndrift detection, and safe updates that reuse shared components. The data layer\nprovides unified data management capabilities, including indexing,\nconstraint-aware discovery, task-aligned selection, and comprehensive feature\nmanagement. With the above designed layers, Aixel delivers a user friendly,\nadaptive, efficient, and extensible system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A growing trend in modern data analysis is the integration of data management\nwith learning, guided by accuracy, latency, and cost requirements. In practice,\napplications draw data of different formats from many sources. In the\nmeanwhile, the objectives and budgets change over time. Existing systems handle\nthese applications across databases, analysis libraries, and tuning services.\nSuch fragmentation leads to complex user interaction, limited adaptability,\nsuboptimal performance, and poor extensibility across components. To address\nthese challenges, we present Aixel, a unified, adaptive, and extensible system\nfor AI-powered data analysis. The system organizes work across four layers:\napplication, task, model, and data. The task layer provides a declarative\ninterface to capture user intent, which is parsed into an executable operator\nplan. An optimizer compiles and schedules this plan to meet specified goals in\naccuracy, latency, and cost. The task layer coordinates the execution of data\nand model operators, with built-in support for reuse and caching to improve\nefficiency. The model layer offers versioned storage for index, metadata,\ntensors, and model artifacts. It supports adaptive construction, task-aligned\ndrift detection, and safe updates that reuse shared components. The data layer\nprovides unified data management capabilities, including indexing,\nconstraint-aware discovery, task-aligned selection, and comprehensive feature\nmanagement. With the above designed layers, Aixel delivers a user friendly,\nadaptive, efficient, and extensible system."
                },
                "authors": [
                    {
                        "name": "Meihui Zhang"
                    },
                    {
                        "name": "Liming Wang"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Zhaojing Luo"
                    }
                ],
                "author_detail": {
                    "name": "Zhaojing Luo"
                },
                "author": "Zhaojing Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12629v1",
                "updated": "2025-10-14T15:26:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    26,
                    9,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T15:26:09Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    26,
                    9,
                    1,
                    287,
                    0
                ],
                "title": "Noisy Neighbor: Exploiting RDMA for Resource Exhaustion Attacks in\n  Containerized Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noisy Neighbor: Exploiting RDMA for Resource Exhaustion Attacks in\n  Containerized Clouds"
                },
                "summary": "In modern containerized cloud environments, the adoption of RDMA (Remote\nDirect Memory Access) has expanded to reduce CPU overhead and enable\nhigh-performance data exchange. Achieving this requires strong performance\nisolation to ensure that one container's RDMA workload does not degrade the\nperformance of others, thereby maintaining critical security assurances.\nHowever, existing isolation techniques are difficult to apply effectively due\nto the complexity of microarchitectural resource management within RDMA NICs\n(RNICs). This paper experimentally analyzes two types of resource exhaustion\nattacks on NVIDIA BlueField-3: (i) state saturation attacks and (ii) pipeline\nsaturation attacks. Our results show that state saturation attacks can cause up\nto a 93.9% loss in bandwidth, a 1,117x increase in latency, and a 115% rise in\ncache misses for victim containers, while pipeline saturation attacks lead to\nsevere link-level congestion and significant amplification, where small verb\nrequests result in disproportionately high resource consumption. To mitigate\nthese threats and restore predictable security assurances, we propose HT-Verbs,\na threshold-driven framework based on real-time per-container RDMA verb\ntelemetry and adaptive resource classification that partitions RNIC resources\ninto hot, warm, and cold tiers and throttles abusive workloads without\nrequiring hardware modifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern containerized cloud environments, the adoption of RDMA (Remote\nDirect Memory Access) has expanded to reduce CPU overhead and enable\nhigh-performance data exchange. Achieving this requires strong performance\nisolation to ensure that one container's RDMA workload does not degrade the\nperformance of others, thereby maintaining critical security assurances.\nHowever, existing isolation techniques are difficult to apply effectively due\nto the complexity of microarchitectural resource management within RDMA NICs\n(RNICs). This paper experimentally analyzes two types of resource exhaustion\nattacks on NVIDIA BlueField-3: (i) state saturation attacks and (ii) pipeline\nsaturation attacks. Our results show that state saturation attacks can cause up\nto a 93.9% loss in bandwidth, a 1,117x increase in latency, and a 115% rise in\ncache misses for victim containers, while pipeline saturation attacks lead to\nsevere link-level congestion and significant amplification, where small verb\nrequests result in disproportionately high resource consumption. To mitigate\nthese threats and restore predictable security assurances, we propose HT-Verbs,\na threshold-driven framework based on real-time per-container RDMA verb\ntelemetry and adaptive resource classification that partitions RNIC resources\ninto hot, warm, and cold tiers and throttles abusive workloads without\nrequiring hardware modifications."
                },
                "authors": [
                    {
                        "name": "Gunwoo Kim"
                    },
                    {
                        "name": "Taejune Park"
                    },
                    {
                        "name": "Jinwoo Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jinwoo Kim"
                },
                "author": "Jinwoo Kim",
                "arxiv_comment": "20 pages, 14 figures, presented at the 4th International Workshop on\n  System Security Assurance (SecAssure 2025), co-located with ESORICS 2025, to\n  appear in Springer LNCS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15969v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15969v3",
                "updated": "2025-10-15T01:55:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    1,
                    55,
                    31,
                    2,
                    288,
                    0
                ],
                "published": "2025-06-19T02:25:04Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    25,
                    4,
                    3,
                    170,
                    0
                ],
                "title": "LazyEviction: Lagged KV Eviction with Attention Pattern Observation for\n  Efficient Long Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyEviction: Lagged KV Eviction with Attention Pattern Observation for\n  Efficient Long Reasoning"
                },
                "summary": "Large Language Models (LLMs) exhibit enhanced capabilities by\nChain-of-Thought reasoning. However, the extended reasoning sequences introduce\nsignificant GPU memory overhead due to increased key-value (KV) cache. Existing\nKV cache compression methods mitigate memory bottlenecks but struggle in long\nreasoning tasks. In this paper, we analyze attention patterns in reasoning\ntasks and reveal a Token Importance Recurrence phenomenon: a large proportion\nof tokens regain high attention after multiple decoding steps, which is failed\nto capture by existing works and may lead to unpredictable eviction on such\nperiodically critical tokens. To address this, we propose LazyEviction, an\nobservation window-based lagged eviction framework retaining latent recurring\ntokens by prioritized eviction based on tokens' recurrence patterns. Extensive\nexperiments demonstrate that LazyEviction reduces KV cache by 50%~70% while\nmaintaining comparable accuracy, outperforming existing KV cache compression\nbaselines. Our implementation code can be found at\nhttps://github.com/Halo-949/LazyEviction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit enhanced capabilities by\nChain-of-Thought reasoning. However, the extended reasoning sequences introduce\nsignificant GPU memory overhead due to increased key-value (KV) cache. Existing\nKV cache compression methods mitigate memory bottlenecks but struggle in long\nreasoning tasks. In this paper, we analyze attention patterns in reasoning\ntasks and reveal a Token Importance Recurrence phenomenon: a large proportion\nof tokens regain high attention after multiple decoding steps, which is failed\nto capture by existing works and may lead to unpredictable eviction on such\nperiodically critical tokens. To address this, we propose LazyEviction, an\nobservation window-based lagged eviction framework retaining latent recurring\ntokens by prioritized eviction based on tokens' recurrence patterns. Extensive\nexperiments demonstrate that LazyEviction reduces KV cache by 50%~70% while\nmaintaining comparable accuracy, outperforming existing KV cache compression\nbaselines. Our implementation code can be found at\nhttps://github.com/Halo-949/LazyEviction."
                },
                "authors": [
                    {
                        "name": "Haoyue Zhang"
                    },
                    {
                        "name": "Hualei Zhang"
                    },
                    {
                        "name": "Xiaosong Ma"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Song Guo"
                    }
                ],
                "author_detail": {
                    "name": "Song Guo"
                },
                "author": "Song Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15969v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15969v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12280v1",
                "updated": "2025-10-14T08:34:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    8,
                    34,
                    9,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T08:34:09Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    8,
                    34,
                    9,
                    1,
                    287,
                    0
                ],
                "title": "Analysis and Evaluation of Using Microsecond-Latency Memory for\n  In-Memory Indices and Caches in SSD-Based Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis and Evaluation of Using Microsecond-Latency Memory for\n  In-Memory Indices and Caches in SSD-Based Key-Value Stores"
                },
                "summary": "When key-value (KV) stores use SSDs for storing a large number of items,\noftentimes they also require large in-memory data structures including indices\nand caches to be traversed to reduce IOs. This paper considers offloading most\nof such data structures from the costly host DRAM to secondary memory whose\nlatency is in the microsecond range, an order of magnitude longer than those of\ncurrently available DIMM-mounted or CXL memory devices. While emerging\nmicrosecond-latency memory is likely to cost much less than DRAM, it can\nsignificantly slow down SSD-based KV stores if naively employed. This paper\nanalyzes and evaluates the impact of microsecond-level memory latency on the KV\noperation throughput. Our analysis finds that a well-known latency-hiding\ntechnique of software prefetching for long-latency memory from user-level\nthreads is effective. The novelty of our analysis lies in modeling how the\ninterplay between prefetching and IO affects performance, from which we derive\nan equation that well explains the throughput degradation due to long memory\nlatency. The model tells us that the presence of IO significantly enhances the\ntolerance to memory latency, leading to a finding that SSD-based KV stores can\nbe made latency-tolerant without devising new techniques for\nmicrosecond-latency memory. To confirm this, we design a microbenchmark as well\nas modify existing SSD-based KV stores so that they issue prefetches from\nuser-level threads, and run them while placing most of in-memory data\nstructures on FPGA-based memory with adjustable microsecond latency. The\nresults demonstrate that their KV operation throughputs can be well explained\nby our model, and the modified KV stores achieve near-DRAM throughputs for up\nto a memory latency of 5 microseconds. This suggests the possibility that\nSSD-based KV stores can use microsecond-latency memory as a cost-effective\nalternative to the host DRAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When key-value (KV) stores use SSDs for storing a large number of items,\noftentimes they also require large in-memory data structures including indices\nand caches to be traversed to reduce IOs. This paper considers offloading most\nof such data structures from the costly host DRAM to secondary memory whose\nlatency is in the microsecond range, an order of magnitude longer than those of\ncurrently available DIMM-mounted or CXL memory devices. While emerging\nmicrosecond-latency memory is likely to cost much less than DRAM, it can\nsignificantly slow down SSD-based KV stores if naively employed. This paper\nanalyzes and evaluates the impact of microsecond-level memory latency on the KV\noperation throughput. Our analysis finds that a well-known latency-hiding\ntechnique of software prefetching for long-latency memory from user-level\nthreads is effective. The novelty of our analysis lies in modeling how the\ninterplay between prefetching and IO affects performance, from which we derive\nan equation that well explains the throughput degradation due to long memory\nlatency. The model tells us that the presence of IO significantly enhances the\ntolerance to memory latency, leading to a finding that SSD-based KV stores can\nbe made latency-tolerant without devising new techniques for\nmicrosecond-latency memory. To confirm this, we design a microbenchmark as well\nas modify existing SSD-based KV stores so that they issue prefetches from\nuser-level threads, and run them while placing most of in-memory data\nstructures on FPGA-based memory with adjustable microsecond latency. The\nresults demonstrate that their KV operation throughputs can be well explained\nby our model, and the modified KV stores achieve near-DRAM throughputs for up\nto a memory latency of 5 microseconds. This suggests the possibility that\nSSD-based KV stores can use microsecond-latency memory as a cost-effective\nalternative to the host DRAM."
                },
                "authors": [
                    {
                        "name": "Yosuke Bando"
                    },
                    {
                        "name": "Akinobu Mita"
                    },
                    {
                        "name": "Kazuhiro Hiwada"
                    },
                    {
                        "name": "Shintaro Sano"
                    },
                    {
                        "name": "Tomoya Suzuki"
                    },
                    {
                        "name": "Yu Nakanishi"
                    },
                    {
                        "name": "Kazutaka Tomida"
                    },
                    {
                        "name": "Hirotsugu Kajihara"
                    },
                    {
                        "name": "Akiyuki Kaneko"
                    },
                    {
                        "name": "Daisuke Taki"
                    },
                    {
                        "name": "Yukimasa Miyamoto"
                    },
                    {
                        "name": "Tomokazu Yoshida"
                    },
                    {
                        "name": "Tatsuo Shiozawa"
                    }
                ],
                "author_detail": {
                    "name": "Tatsuo Shiozawa"
                },
                "author": "Tatsuo Shiozawa",
                "arxiv_doi": "10.1145/3769759",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3769759",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.12280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proc. ACM Manag. Data 3, 6 (SIGMOD), Article 294 (December 2025),\n  28 pages",
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10975v2",
                "updated": "2025-10-14T07:41:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    7,
                    41,
                    47,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-13T03:26:14Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    3,
                    26,
                    14,
                    0,
                    286,
                    0
                ],
                "title": "RoVer: Robot Reward Model as Test-Time Verifier for\n  Vision-Language-Action Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoVer: Robot Reward Model as Test-Time Verifier for\n  Vision-Language-Action Model"
                },
                "summary": "Vision-Language-Action (VLA) models have become a prominent paradigm for\nembodied intelligence, yet further performance improvements typically rely on\nscaling up training data and model size -- an approach that is prohibitively\nexpensive for robotics and fundamentally limited by data collection costs. We\naddress this limitation with $\\mathbf{RoVer}$, an embodied test-time scaling\nframework that uses a $\\mathbf{Ro}$bot Process Reward Model (PRM) as a\nTest-Time $\\mathbf{Ver}$ifier to enhance the capabilities of existing VLA\nmodels without modifying their architectures or weights. Specifically, RoVer\n(i) assigns scalar-based process rewards to evaluate the reliability of\ncandidate actions, and (ii) predicts an action-space direction for candidate\nexpansion/refinement. During inference, RoVer generates multiple candidate\nactions concurrently from the base policy, expands them along PRM-predicted\ndirections, and then scores all candidates with PRM to select the optimal\naction for execution. Notably, by caching shared perception features, it can\namortize perception cost and evaluate more candidates under the same test-time\ncomputational budget. Essentially, our approach effectively transforms\navailable computing resources into better action decision-making, realizing the\nbenefits of test-time scaling without extra training overhead. Our\ncontributions are threefold: (1) a general, plug-and-play test-time scaling\nframework for VLAs; (2) a PRM that jointly provides scalar process rewards and\nan action-space direction to guide exploration; and (3) an efficient\ndirection-guided sampling strategy that leverages a shared perception cache to\nenable scalable candidate generation and selection during inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have become a prominent paradigm for\nembodied intelligence, yet further performance improvements typically rely on\nscaling up training data and model size -- an approach that is prohibitively\nexpensive for robotics and fundamentally limited by data collection costs. We\naddress this limitation with $\\mathbf{RoVer}$, an embodied test-time scaling\nframework that uses a $\\mathbf{Ro}$bot Process Reward Model (PRM) as a\nTest-Time $\\mathbf{Ver}$ifier to enhance the capabilities of existing VLA\nmodels without modifying their architectures or weights. Specifically, RoVer\n(i) assigns scalar-based process rewards to evaluate the reliability of\ncandidate actions, and (ii) predicts an action-space direction for candidate\nexpansion/refinement. During inference, RoVer generates multiple candidate\nactions concurrently from the base policy, expands them along PRM-predicted\ndirections, and then scores all candidates with PRM to select the optimal\naction for execution. Notably, by caching shared perception features, it can\namortize perception cost and evaluate more candidates under the same test-time\ncomputational budget. Essentially, our approach effectively transforms\navailable computing resources into better action decision-making, realizing the\nbenefits of test-time scaling without extra training overhead. Our\ncontributions are threefold: (1) a general, plug-and-play test-time scaling\nframework for VLAs; (2) a PRM that jointly provides scalar process rewards and\nan action-space direction to guide exploration; and (3) an efficient\ndirection-guided sampling strategy that leverages a shared perception cache to\nenable scalable candidate generation and selection during inference."
                },
                "authors": [
                    {
                        "name": "Mingtong Dai"
                    },
                    {
                        "name": "Lingbo Liu"
                    },
                    {
                        "name": "Yongjie Bai"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zhouxia Wang"
                    },
                    {
                        "name": "Rui SU"
                    },
                    {
                        "name": "Chunjie Chen"
                    },
                    {
                        "name": "Liang Lin"
                    },
                    {
                        "name": "Xinyu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xinyu Wu"
                },
                "author": "Xinyu Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11496v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11496v2",
                "updated": "2025-10-14T05:05:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    5,
                    5,
                    14,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-13T15:04:38Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    4,
                    38,
                    0,
                    286,
                    0
                ],
                "title": "AndesVL Technical Report: An Efficient Mobile-side Multimodal Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AndesVL Technical Report: An Efficient Mobile-side Multimodal Large\n  Language Model"
                },
                "summary": "In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o,\nGemini, and Claude Sonnet have demonstrated outstanding performance with\nenormous model sizes reaching hundreds of billions of parameters, they\nsignificantly surpass the limitations in memory, power consumption, and\ncomputing capacity of edge devices such as mobile phones. This paper introduces\nAndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on\nQwen3's LLM and various visual encoders. We comprehensively outline the model\narchitectures, training pipeline, and training data of AndesVL, which achieves\nfirst-tier performance across a wide range of open-source benchmarks, including\nfields such as text-rich image understanding, reasoning and math, multi-image\ncomprehension, general VQA, hallucination mitigation, multilingual\nunderstanding, and GUI-related tasks when compared with state-of-the-art models\nof a similar scale. Furthermore, we introduce a 1+N LoRA architecture alongside\na Quantization-Aware LoRA Fine-Tuning (QALFT) framework to facilitate efficient\ntask adaptation and model compression during mobile-side deployment of AndesVL.\nMoreover, utilizing our cache eviction algorithm -- OKV -- along with\ncustomized speculative decoding and compression strategies, we achieve a 6.7x\npeak decoding speedup ratio, up to 30.9% memory reduction, and 1.8\nbits-per-weight when deploying AndesVL-4B on MediaTek Dimensity 9500 chips. We\nrelease all models on https://huggingface.co/OPPOer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o,\nGemini, and Claude Sonnet have demonstrated outstanding performance with\nenormous model sizes reaching hundreds of billions of parameters, they\nsignificantly surpass the limitations in memory, power consumption, and\ncomputing capacity of edge devices such as mobile phones. This paper introduces\nAndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on\nQwen3's LLM and various visual encoders. We comprehensively outline the model\narchitectures, training pipeline, and training data of AndesVL, which achieves\nfirst-tier performance across a wide range of open-source benchmarks, including\nfields such as text-rich image understanding, reasoning and math, multi-image\ncomprehension, general VQA, hallucination mitigation, multilingual\nunderstanding, and GUI-related tasks when compared with state-of-the-art models\nof a similar scale. Furthermore, we introduce a 1+N LoRA architecture alongside\na Quantization-Aware LoRA Fine-Tuning (QALFT) framework to facilitate efficient\ntask adaptation and model compression during mobile-side deployment of AndesVL.\nMoreover, utilizing our cache eviction algorithm -- OKV -- along with\ncustomized speculative decoding and compression strategies, we achieve a 6.7x\npeak decoding speedup ratio, up to 30.9% memory reduction, and 1.8\nbits-per-weight when deploying AndesVL-4B on MediaTek Dimensity 9500 chips. We\nrelease all models on https://huggingface.co/OPPOer."
                },
                "authors": [
                    {
                        "name": "Zhiwei Jin"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Yafei Liu"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Ruichen Wang"
                    },
                    {
                        "name": "Zhihao Li"
                    },
                    {
                        "name": "Qi Qi"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Dongze Hao"
                    },
                    {
                        "name": "Quanlong Zheng"
                    },
                    {
                        "name": "Yanhao Zhang"
                    },
                    {
                        "name": "Haobo Ji"
                    },
                    {
                        "name": "Jian Ma"
                    },
                    {
                        "name": "Zhitong Zheng"
                    },
                    {
                        "name": "Zhenyi Lin"
                    },
                    {
                        "name": "Haolin Deng"
                    },
                    {
                        "name": "Xin Zou"
                    },
                    {
                        "name": "Xiaojie Yin"
                    },
                    {
                        "name": "Ruilin Wang"
                    },
                    {
                        "name": "Liankai Cai"
                    },
                    {
                        "name": "Haijing Liu"
                    },
                    {
                        "name": "Yuqing Qiu"
                    },
                    {
                        "name": "Ke Chen"
                    },
                    {
                        "name": "Zixian Li"
                    },
                    {
                        "name": "Chi Xie"
                    },
                    {
                        "name": "Huafei Li"
                    },
                    {
                        "name": "Chenxing Li"
                    },
                    {
                        "name": "Chuangchuang Wang"
                    },
                    {
                        "name": "Kai Tang"
                    },
                    {
                        "name": "Zhiguang Zhu"
                    },
                    {
                        "name": "Kai Tang"
                    },
                    {
                        "name": "Wenmei Gao"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Jun Wu"
                    },
                    {
                        "name": "Chao Liu"
                    },
                    {
                        "name": "Qin Xie"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Haonan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Haonan Lu"
                },
                "author": "Haonan Lu",
                "arxiv_comment": "Tech report of OPPO AndesVL Team",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11496v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11496v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12051v1",
                "updated": "2025-10-14T01:26:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    1,
                    26,
                    36,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T01:26:36Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    1,
                    26,
                    36,
                    1,
                    287,
                    0
                ],
                "title": "APCE: Adaptive Progressive Context Expansion for Long Context Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APCE: Adaptive Progressive Context Expansion for Long Context Processing"
                },
                "summary": "Deploying useful Long-Context Transformer Models (LCTMs) requires addressing\ntwo key challenges: (1) A growing memory footprint due to quadratic\nself-attention and linear KV-cache scaling in memory as sequence length\nincreases; (2) the ContextRot phenomena where empirical evidence suggests that\ntransformer architecture's performance degrades with increasing context length.\nGiven the shared dependency on the input, a natural question arises: Can we\nsurgically select the most important input chunks for processing to\nsynergistically (a) reduce the memory footprint, and (b) mitigate the\nContextRot effects? In this paper, we answer this question in the affirmative\nfor long-context summarization tasks. We propose APCE as a context-aware\nsolution to select the most important input chunks through low-dimensional\nsemantic similarity matching with the current query. By directly operating on\nthe input, APCE decouples from strict dependency on underlying hardware or CUDA\nenvironments, promising a compatible solution scalable to different deployment\nsystems. Our empirical evaluations have demonstrated superior or on-par\nsummarization performance for APCE compared to the full dense baseline using a\nfraction (50%-70%) of the input sequence resulting in KV-cache and\nself-attention memory efficiency improvements. We hope our findings inspire\nfurther research on context-aware efficiency solutions for LCTMs geared towards\nother relevant long-context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying useful Long-Context Transformer Models (LCTMs) requires addressing\ntwo key challenges: (1) A growing memory footprint due to quadratic\nself-attention and linear KV-cache scaling in memory as sequence length\nincreases; (2) the ContextRot phenomena where empirical evidence suggests that\ntransformer architecture's performance degrades with increasing context length.\nGiven the shared dependency on the input, a natural question arises: Can we\nsurgically select the most important input chunks for processing to\nsynergistically (a) reduce the memory footprint, and (b) mitigate the\nContextRot effects? In this paper, we answer this question in the affirmative\nfor long-context summarization tasks. We propose APCE as a context-aware\nsolution to select the most important input chunks through low-dimensional\nsemantic similarity matching with the current query. By directly operating on\nthe input, APCE decouples from strict dependency on underlying hardware or CUDA\nenvironments, promising a compatible solution scalable to different deployment\nsystems. Our empirical evaluations have demonstrated superior or on-par\nsummarization performance for APCE compared to the full dense baseline using a\nfraction (50%-70%) of the input sequence resulting in KV-cache and\nself-attention memory efficiency improvements. We hope our findings inspire\nfurther research on context-aware efficiency solutions for LCTMs geared towards\nother relevant long-context tasks."
                },
                "authors": [
                    {
                        "name": "Baisub Lee"
                    },
                    {
                        "name": "Sanghyun Byun"
                    },
                    {
                        "name": "Mohanad Odema"
                    },
                    {
                        "name": "Jung Guack"
                    },
                    {
                        "name": "Jacob Song"
                    },
                    {
                        "name": "Woo Seong Chung"
                    }
                ],
                "author_detail": {
                    "name": "Woo Seong Chung"
                },
                "author": "Woo Seong Chung",
                "arxiv_comment": "NeurIPS 2025 Workshop: ML For Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22622v2",
                "updated": "2025-10-13T22:41:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    22,
                    41,
                    26,
                    0,
                    286,
                    0
                ],
                "published": "2025-09-26T17:48:24Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    48,
                    24,
                    4,
                    269,
                    0
                ],
                "title": "LongLive: Real-time Interactive Long Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongLive: Real-time Interactive Long Video Generation"
                },
                "summary": "We present LongLive, a frame-level autoregressive (AR) framework for\nreal-time and interactive long video generation. Long video generation presents\nchallenges in both efficiency and quality. Diffusion and Diffusion-Forcing\nmodels can produce high-quality videos but suffer from low efficiency due to\nbidirectional attention. Causal attention AR models support KV caching for\nfaster inference, but often degrade in quality on long videos due to memory\nchallenges during long-video training. In addition, beyond static prompt-based\ngeneration, interactive capabilities, such as streaming prompt inputs, are\ncritical for dynamic content creation, enabling users to guide narratives in\nreal time. This interactive requirement significantly increases complexity,\nespecially in ensuring visual consistency and semantic coherence during prompt\ntransitions. To address these challenges, LongLive adopts a causal, frame-level\nAR design that integrates a KV-recache mechanism that refreshes cached states\nwith new prompts for smooth, adherent switches; streaming long tuning to enable\nlong video training and to align training and inference (train-long-test-long);\nand short window attention paired with a frame-level attention sink, shorten as\nframe sink, preserving long-range consistency while enabling faster generation.\nWith these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model\nto minute-long generation in just 32 GPU-days. At inference, LongLive sustains\n20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both\nshort and long videos. LongLive supports up to 240-second videos on a single\nH100 GPU. LongLive further supports INT8-quantized inference with only marginal\nquality loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LongLive, a frame-level autoregressive (AR) framework for\nreal-time and interactive long video generation. Long video generation presents\nchallenges in both efficiency and quality. Diffusion and Diffusion-Forcing\nmodels can produce high-quality videos but suffer from low efficiency due to\nbidirectional attention. Causal attention AR models support KV caching for\nfaster inference, but often degrade in quality on long videos due to memory\nchallenges during long-video training. In addition, beyond static prompt-based\ngeneration, interactive capabilities, such as streaming prompt inputs, are\ncritical for dynamic content creation, enabling users to guide narratives in\nreal time. This interactive requirement significantly increases complexity,\nespecially in ensuring visual consistency and semantic coherence during prompt\ntransitions. To address these challenges, LongLive adopts a causal, frame-level\nAR design that integrates a KV-recache mechanism that refreshes cached states\nwith new prompts for smooth, adherent switches; streaming long tuning to enable\nlong video training and to align training and inference (train-long-test-long);\nand short window attention paired with a frame-level attention sink, shorten as\nframe sink, preserving long-range consistency while enabling faster generation.\nWith these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model\nto minute-long generation in just 32 GPU-days. At inference, LongLive sustains\n20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both\nshort and long videos. LongLive supports up to 240-second videos on a single\nH100 GPU. LongLive further supports INT8-quantized inference with only marginal\nquality loss."
                },
                "authors": [
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Ruihang Chu"
                    },
                    {
                        "name": "Yicheng Xiao"
                    },
                    {
                        "name": "Yuyang Zhao"
                    },
                    {
                        "name": "Xianbang Wang"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Enze Xie"
                    },
                    {
                        "name": "Yingcong Chen"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Yukang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yukang Chen"
                },
                "author": "Yukang Chen",
                "arxiv_comment": "Code, model, and demos are available at\n  https://github.com/NVlabs/LongLive",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11938v1",
                "updated": "2025-10-13T21:01:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    21,
                    1,
                    40,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T21:01:40Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    21,
                    1,
                    40,
                    0,
                    286,
                    0
                ],
                "title": "FlexPipe: Adapting Dynamic LLM Serving Through Inflight Pipeline\n  Refactoring in Fragmented Serverless Clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexPipe: Adapting Dynamic LLM Serving Through Inflight Pipeline\n  Refactoring in Fragmented Serverless Clusters"
                },
                "summary": "Serving Large Language Models (LLMs) in production faces significant\nchallenges from highly variable request patterns and severe resource\nfragmentation in serverless clusters. Current systems rely on static pipeline\nconfigurations that struggle to adapt to dynamic workload conditions, leading\nto substantial inefficiencies. We present FlexPipe, a novel system that\ndynamically reconfigures pipeline architectures during runtime to address these\nfundamental limitations. FlexPipe decomposes models into fine-grained stages\nand intelligently adjusts pipeline granularity based on real-time request\npattern analysis, implementing three key innovations: fine-grained model\npartitioning with preserved computational graph constraints, inflight pipeline\nrefactoring with consistent cache transitions, and topology-aware resource\nallocation that navigates GPU fragmentation. Comprehensive evaluation on an\n82-GPU cluster demonstrates that FlexPipe achieves up to 8.5x better resource\nefficiency while maintaining 38.3% lower latency compared to state-of-the-art\nsystems, reducing GPU reservation requirements from 75% to 30% of peak\ncapacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models (LLMs) in production faces significant\nchallenges from highly variable request patterns and severe resource\nfragmentation in serverless clusters. Current systems rely on static pipeline\nconfigurations that struggle to adapt to dynamic workload conditions, leading\nto substantial inefficiencies. We present FlexPipe, a novel system that\ndynamically reconfigures pipeline architectures during runtime to address these\nfundamental limitations. FlexPipe decomposes models into fine-grained stages\nand intelligently adjusts pipeline granularity based on real-time request\npattern analysis, implementing three key innovations: fine-grained model\npartitioning with preserved computational graph constraints, inflight pipeline\nrefactoring with consistent cache transitions, and topology-aware resource\nallocation that navigates GPU fragmentation. Comprehensive evaluation on an\n82-GPU cluster demonstrates that FlexPipe achieves up to 8.5x better resource\nefficiency while maintaining 38.3% lower latency compared to state-of-the-art\nsystems, reducing GPU reservation requirements from 75% to 30% of peak\ncapacity."
                },
                "authors": [
                    {
                        "name": "Yanying Lin"
                    },
                    {
                        "name": "Shijie Peng"
                    },
                    {
                        "name": "Chengzhi Lu"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Kejiang Ye"
                    }
                ],
                "author_detail": {
                    "name": "Kejiang Ye"
                },
                "author": "Kejiang Ye",
                "arxiv_doi": "10.1145/3767295.3769316",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3767295.3769316",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.11938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "EuroSys 26",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11857v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11857v2",
                "updated": "2025-10-13T20:40:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    20,
                    40,
                    32,
                    0,
                    286,
                    0
                ],
                "published": "2024-10-04T15:23:28Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    15,
                    23,
                    28,
                    4,
                    278,
                    0
                ],
                "title": "LLMBridge: Reducing Costs in a Prompt-Centric Internet",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMBridge: Reducing Costs in a Prompt-Centric Internet"
                },
                "summary": "Today's Internet infrastructure is centered around content retrieval over\nHTTP, with middleboxes (e.g., HTTP proxies) playing a crucial role in\nperformance, security, and cost-effectiveness. We envision a future where\nInternet communication will be dominated by \"prompts\" sent to generative AI\nmodels. For this, we will need proxies that provide similar functions to HTTP\nproxies (e.g., caching, routing, compression) while dealing with unique\nchallenges and opportunities of prompt-based communication. As a first step\ntoward supporting prompt-based communication, we present LLMBridge, an LLM\nproxy designed for cost-conscious users, such as those in developing regions\nand education (e.g., students, instructors). LLMBridge supports three key\noptimizations: model selection (routing prompts to the most suitable model),\ncontext management (intelligently reducing the amount of context), and semantic\ncaching (serving prompts using local models and vector databases). These\noptimizations introduce trade-offs between cost and quality, which applications\nnavigate through a high-level, bidirectional interface. As case studies, we\ndeploy LLMBridge in two cost-sensitive settings: a WhatsApp-based Q&A service\nand a university classroom environment. The WhatsApp service has been live for\nover twelve months, serving 100+ users and handling more than 14.7K requests.\nIn parallel, we exposed LLMBridge to students across three computer science\ncourses over a semester, where it supported diverse LLM-powered applications -\nsuch as reasoning agents and chatbots - and handled an average of 500 requests\nper day. We report on deployment experiences across both settings and use the\ncollected workloads to benchmark the effectiveness of various cost-optimization\nstrategies, analyzing their trade-offs in cost, latency, and response quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Today's Internet infrastructure is centered around content retrieval over\nHTTP, with middleboxes (e.g., HTTP proxies) playing a crucial role in\nperformance, security, and cost-effectiveness. We envision a future where\nInternet communication will be dominated by \"prompts\" sent to generative AI\nmodels. For this, we will need proxies that provide similar functions to HTTP\nproxies (e.g., caching, routing, compression) while dealing with unique\nchallenges and opportunities of prompt-based communication. As a first step\ntoward supporting prompt-based communication, we present LLMBridge, an LLM\nproxy designed for cost-conscious users, such as those in developing regions\nand education (e.g., students, instructors). LLMBridge supports three key\noptimizations: model selection (routing prompts to the most suitable model),\ncontext management (intelligently reducing the amount of context), and semantic\ncaching (serving prompts using local models and vector databases). These\noptimizations introduce trade-offs between cost and quality, which applications\nnavigate through a high-level, bidirectional interface. As case studies, we\ndeploy LLMBridge in two cost-sensitive settings: a WhatsApp-based Q&A service\nand a university classroom environment. The WhatsApp service has been live for\nover twelve months, serving 100+ users and handling more than 14.7K requests.\nIn parallel, we exposed LLMBridge to students across three computer science\ncourses over a semester, where it supported diverse LLM-powered applications -\nsuch as reasoning agents and chatbots - and handled an average of 500 requests\nper day. We report on deployment experiences across both settings and use the\ncollected workloads to benchmark the effectiveness of various cost-optimization\nstrategies, analyzing their trade-offs in cost, latency, and response quality."
                },
                "authors": [
                    {
                        "name": "Noah Martin"
                    },
                    {
                        "name": "Abdullah Bin Faisal"
                    },
                    {
                        "name": "Hiba Eltigani"
                    },
                    {
                        "name": "Rukhshan Haroon"
                    },
                    {
                        "name": "Swaminathan Lamelas"
                    },
                    {
                        "name": "Fahad Dogar"
                    }
                ],
                "author_detail": {
                    "name": "Fahad Dogar"
                },
                "author": "Fahad Dogar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11857v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11857v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01875v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01875v3",
                "updated": "2025-10-13T17:15:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    15,
                    14,
                    0,
                    286,
                    0
                ],
                "published": "2025-08-03T18:15:42Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    18,
                    15,
                    42,
                    6,
                    215,
                    0
                ],
                "title": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding"
                },
                "summary": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios."
                },
                "authors": [
                    {
                        "name": "Haolin Yang"
                    },
                    {
                        "name": "Feilong Tang"
                    },
                    {
                        "name": "Lingxiao Zhao"
                    },
                    {
                        "name": "Xiang An"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Huifa Li"
                    },
                    {
                        "name": "Xinlin Zhuang"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Xiaofeng Zhang"
                    },
                    {
                        "name": "Abdalla Swikir"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Zongyuan Ge"
                    },
                    {
                        "name": "Imran Razzak"
                    }
                ],
                "author_detail": {
                    "name": "Imran Razzak"
                },
                "author": "Imran Razzak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01875v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01875v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17238v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17238v2",
                "updated": "2025-10-13T16:48:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    48,
                    37,
                    0,
                    286,
                    0
                ],
                "published": "2025-09-21T21:05:29Z",
                "published_parsed": [
                    2025,
                    9,
                    21,
                    21,
                    5,
                    29,
                    6,
                    264,
                    0
                ],
                "title": "MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with\n  RoE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with\n  RoE"
                },
                "summary": "The generation quality of large language models (LLMs) is often improved by\nutilizing inference-time sequence-level scaling methods (e.g.,\nChain-of-Thought). We introduce hyper-parallel scaling, a complementary\nframework that improves prediction quality at the token level. Hyper-parallel\nscaling computes and aggregates multiple output proposals for a single token\nfrom the model. We implement this concept in Mixture-of-Experts (MoE) models,\nwhich we refer to as Roster of Experts (RoE). RoE is a training-free inference\nalgorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects\ncontrolled stochasticity into the expert routing mechanism, enabling it to\nsample multiple diverse experts for each token and aggregate their outputs for\na more accurate final prediction. To overcome the computational cost, we\nintroduce an efficient batching strategy and a specialized KV-caching mechanism\nthat minimizes compute and memory overhead. For example, RoE enables a 7B MoE\nmodel to match the performance of a 10.5B MoE model while using 30% less\ncompute for inference. These gains are achieved without any fine-tuning of\nmodel parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generation quality of large language models (LLMs) is often improved by\nutilizing inference-time sequence-level scaling methods (e.g.,\nChain-of-Thought). We introduce hyper-parallel scaling, a complementary\nframework that improves prediction quality at the token level. Hyper-parallel\nscaling computes and aggregates multiple output proposals for a single token\nfrom the model. We implement this concept in Mixture-of-Experts (MoE) models,\nwhich we refer to as Roster of Experts (RoE). RoE is a training-free inference\nalgorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects\ncontrolled stochasticity into the expert routing mechanism, enabling it to\nsample multiple diverse experts for each token and aggregate their outputs for\na more accurate final prediction. To overcome the computational cost, we\nintroduce an efficient batching strategy and a specialized KV-caching mechanism\nthat minimizes compute and memory overhead. For example, RoE enables a 7B MoE\nmodel to match the performance of a 10.5B MoE model while using 30% less\ncompute for inference. These gains are achieved without any fine-tuning of\nmodel parameters."
                },
                "authors": [
                    {
                        "name": "Soheil Zibakhsh"
                    },
                    {
                        "name": "Mohammad Samragh"
                    },
                    {
                        "name": "Kumari Nishu"
                    },
                    {
                        "name": "Lauren Hannah"
                    },
                    {
                        "name": "Arnav Kundu"
                    },
                    {
                        "name": "Minsik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Minsik Cho"
                },
                "author": "Minsik Cho",
                "arxiv_comment": "Corrected typo in arxiv abstract",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17238v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17238v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11292v1",
                "updated": "2025-10-13T11:28:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    28,
                    30,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T11:28:30Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    28,
                    30,
                    0,
                    286,
                    0
                ],
                "title": "LouisKV: Efficient KV Cache Retrieval for Long Input-Output Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LouisKV: Efficient KV Cache Retrieval for Long Input-Output Sequences"
                },
                "summary": "While Key-Value (KV) cache succeeds in reducing redundant computations in\nauto-regressive models, it introduces significant memory overhead, limiting its\npractical deployment in long-sequence scenarios. Existing KV retrieval methods\nmitigate this by dynamically retaining only a subset of KV entries on the GPU.\nHowever, they still suffer from notable efficiency and accuracy bottlenecks due\nto per-token retrieval and coarse-grained page-level KV management, especially\nin long-output reasoning scenarios. With the emergence of large reasoning\nmodels, efficiently handling such scenarios has become increasingly important.\nTo address this issue, we present two key observations: (1) critical KVs\nexhibit strong temporal locality during decoding, and (2) these KVs exhibit\ndistinct distribution patterns across the input prompt and generated output.\nBuilding on these observations, we propose LouisKV, an efficient KV cache\nretrieval framework designed for various long-sequence scenarios. Specifically,\nLouisKV introduces a semantic-aware retrieval strategy leveraging temporal\nlocality to trigger retrieval only at semantic boundaries, drastically reducing\ncomputation and data transfer overhead. LouisKV also designs a decoupled,\nfine-grained management scheme that tailors differentiated strategies for input\nand output sequences to create retrieval units that better match the model's\nattention patterns, enabling precise identification of critical KVs.\nFurthermore, to boost efficiency, LouisKV incorporates several kernel-level\noptimizations, including custom Triton and CUDA kernels to accelerate the KV\nclustering and retrieval. Evaluations show that LouisKV achieves up to\n4.7$\\times$ speedup over state-of-the-art KV retrieval methods while\nmaintaining near-lossless accuracy across diverse long-sequence tasks,\nincluding long-input short-output, short-input long-output, and long-input\nlong-output scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Key-Value (KV) cache succeeds in reducing redundant computations in\nauto-regressive models, it introduces significant memory overhead, limiting its\npractical deployment in long-sequence scenarios. Existing KV retrieval methods\nmitigate this by dynamically retaining only a subset of KV entries on the GPU.\nHowever, they still suffer from notable efficiency and accuracy bottlenecks due\nto per-token retrieval and coarse-grained page-level KV management, especially\nin long-output reasoning scenarios. With the emergence of large reasoning\nmodels, efficiently handling such scenarios has become increasingly important.\nTo address this issue, we present two key observations: (1) critical KVs\nexhibit strong temporal locality during decoding, and (2) these KVs exhibit\ndistinct distribution patterns across the input prompt and generated output.\nBuilding on these observations, we propose LouisKV, an efficient KV cache\nretrieval framework designed for various long-sequence scenarios. Specifically,\nLouisKV introduces a semantic-aware retrieval strategy leveraging temporal\nlocality to trigger retrieval only at semantic boundaries, drastically reducing\ncomputation and data transfer overhead. LouisKV also designs a decoupled,\nfine-grained management scheme that tailors differentiated strategies for input\nand output sequences to create retrieval units that better match the model's\nattention patterns, enabling precise identification of critical KVs.\nFurthermore, to boost efficiency, LouisKV incorporates several kernel-level\noptimizations, including custom Triton and CUDA kernels to accelerate the KV\nclustering and retrieval. Evaluations show that LouisKV achieves up to\n4.7$\\times$ speedup over state-of-the-art KV retrieval methods while\nmaintaining near-lossless accuracy across diverse long-sequence tasks,\nincluding long-input short-output, short-input long-output, and long-input\nlong-output scenarios."
                },
                "authors": [
                    {
                        "name": "Wenbo Wu"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Xiurui Pan"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21725v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21725v2",
                "updated": "2025-10-13T11:21:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    21,
                    0,
                    0,
                    286,
                    0
                ],
                "published": "2025-03-27T17:37:12Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    37,
                    12,
                    3,
                    86,
                    0
                ],
                "title": "Low-noise environment for probing fundamental symmetries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-noise environment for probing fundamental symmetries"
                },
                "summary": "We present the design and characterization of a low-noise environment for\nmeasuring the electron's electric dipole moment (EDM) with a beam of molecules.\nTo minimize magnetic Johnson noise from metals, the design features ceramic\nelectric field plates housed in a glass vacuum chamber. To suppress external\nmagnetic noise the apparatus is enclosed within a cylindrical four-layer\nmu-metal shield with a shielding factor exceeding $10^6$ in one radial\ndirection and $10^5$ in the other. Finite element modelling shows that the\ndifference between these shielding factors is due to imperfect joints between\nsections of mu-metal. Using atomic magnetometers to monitor the magnetic field\ninside the shield, we measure noise below 40 fT/$\\sqrt{{\\rm Hz}}$ at 1 Hz and\nabove, rising to 500 fT/$\\sqrt{{\\rm Hz}}$ at 0.1 Hz. Analytical and numerical\nstudies show that residual magnetic Johnson noise contributes approximately 13\nfT/$\\sqrt{{\\rm Hz}}$. The background magnetic field averaged along the beamline\nis maintained below 3 pT, with typical gradients of a few nT/m. An electric\nfield of 20 kV/cm is applied without discharges and with leakage currents below\n1 nA. Each magnetometer measures the magnetic field correlated with the\ndirection of the applied electric field with a precision of 0.11 fT in 104\nhours of data. These results demonstrate that the apparatus is suitable for\nmeasuring the electron EDM with precision at the $10^{-31}$ e cm level. The\ndesign principles and characterization techniques presented here are broadly\napplicable to precision measurements probing fundamental symmetries in\nmolecules, atoms, and neutrons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the design and characterization of a low-noise environment for\nmeasuring the electron's electric dipole moment (EDM) with a beam of molecules.\nTo minimize magnetic Johnson noise from metals, the design features ceramic\nelectric field plates housed in a glass vacuum chamber. To suppress external\nmagnetic noise the apparatus is enclosed within a cylindrical four-layer\nmu-metal shield with a shielding factor exceeding $10^6$ in one radial\ndirection and $10^5$ in the other. Finite element modelling shows that the\ndifference between these shielding factors is due to imperfect joints between\nsections of mu-metal. Using atomic magnetometers to monitor the magnetic field\ninside the shield, we measure noise below 40 fT/$\\sqrt{{\\rm Hz}}$ at 1 Hz and\nabove, rising to 500 fT/$\\sqrt{{\\rm Hz}}$ at 0.1 Hz. Analytical and numerical\nstudies show that residual magnetic Johnson noise contributes approximately 13\nfT/$\\sqrt{{\\rm Hz}}$. The background magnetic field averaged along the beamline\nis maintained below 3 pT, with typical gradients of a few nT/m. An electric\nfield of 20 kV/cm is applied without discharges and with leakage currents below\n1 nA. Each magnetometer measures the magnetic field correlated with the\ndirection of the applied electric field with a precision of 0.11 fT in 104\nhours of data. These results demonstrate that the apparatus is suitable for\nmeasuring the electron EDM with precision at the $10^{-31}$ e cm level. The\ndesign principles and characterization techniques presented here are broadly\napplicable to precision measurements probing fundamental symmetries in\nmolecules, atoms, and neutrons."
                },
                "authors": [
                    {
                        "name": "F. J. Collings"
                    },
                    {
                        "name": "N. J. Fitch"
                    },
                    {
                        "name": "R. A. Jenkins"
                    },
                    {
                        "name": "J. M. Dyne"
                    },
                    {
                        "name": "E. Wursten"
                    },
                    {
                        "name": "M. T. Ziemba"
                    },
                    {
                        "name": "X. S. Zheng"
                    },
                    {
                        "name": "F. Castellini"
                    },
                    {
                        "name": "J. Lim"
                    },
                    {
                        "name": "B. E. Sauer"
                    },
                    {
                        "name": "M. R. Tarbutt"
                    }
                ],
                "author_detail": {
                    "name": "M. R. Tarbutt"
                },
                "author": "M. R. Tarbutt",
                "arxiv_doi": "10.1088/1367-2630/ae0ea7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1088/1367-2630/ae0ea7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.21725v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21725v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Added a figure, minor changes to text",
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08666v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08666v2",
                "updated": "2025-10-13T10:39:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    10,
                    39,
                    59,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-09T16:19:42Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    19,
                    42,
                    3,
                    282,
                    0
                ],
                "title": "dInfer: An Efficient Inference Framework for Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "dInfer: An Efficient Inference Framework for Diffusion Language Models"
                },
                "summary": "Diffusion-based large language models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs, leveraging denoising-based generation\nto enable inherent parallelism. Even more and more open-sourced dLLM models\nemerge, yet their widespread adoption remains constrained by the lack of a\nstandardized and efficient inference framework. We present dInfer, an efficient\nand extensible framework for dLLM inference. dInfer decomposes the inference\npipeline into four modular components--model, diffusion iteration manager,\ndecoding strategy, and KV-cache manager--and integrates novel algorithms for\neach component alongside system-level optimizations. Through this combination\nof algorithmic innovations and system enhancements, dInfer achieves substantial\nefficiency gains without compromising output quality on LLaDA-MoE. At batch\nsize 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800\ntokens per second across six benchmarks on $8\\times$ H800 GPUs. Compared to\nprior systems, dInfer delivers a $10\\times$ speedup over Fast-dLLM while\nmaintaining similar model performance. Even compared to the AR model (with a\ncomparable number of activation parameters and performance) QWen2.5-3B, which\nis highly optimized with the latest vLLM inference engine, dInfer still\ndelivers a $2$-$3\\times$ speedup. The implementation of dInfer is open-sourced\nat https://github.com/inclusionAI/dInfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs, leveraging denoising-based generation\nto enable inherent parallelism. Even more and more open-sourced dLLM models\nemerge, yet their widespread adoption remains constrained by the lack of a\nstandardized and efficient inference framework. We present dInfer, an efficient\nand extensible framework for dLLM inference. dInfer decomposes the inference\npipeline into four modular components--model, diffusion iteration manager,\ndecoding strategy, and KV-cache manager--and integrates novel algorithms for\neach component alongside system-level optimizations. Through this combination\nof algorithmic innovations and system enhancements, dInfer achieves substantial\nefficiency gains without compromising output quality on LLaDA-MoE. At batch\nsize 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800\ntokens per second across six benchmarks on $8\\times$ H800 GPUs. Compared to\nprior systems, dInfer delivers a $10\\times$ speedup over Fast-dLLM while\nmaintaining similar model performance. Even compared to the AR model (with a\ncomparable number of activation parameters and performance) QWen2.5-3B, which\nis highly optimized with the latest vLLM inference engine, dInfer still\ndelivers a $2$-$3\\times$ speedup. The implementation of dInfer is open-sourced\nat https://github.com/inclusionAI/dInfer."
                },
                "authors": [
                    {
                        "name": "Yuxin Ma"
                    },
                    {
                        "name": "Lun Du"
                    },
                    {
                        "name": "Lanning Wei"
                    },
                    {
                        "name": "Kun Chen"
                    },
                    {
                        "name": "Qian Xu"
                    },
                    {
                        "name": "Kangyu Wang"
                    },
                    {
                        "name": "Guofeng Feng"
                    },
                    {
                        "name": "Guoshan Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Xiaojing Qi"
                    },
                    {
                        "name": "Xinyuan Zhang"
                    },
                    {
                        "name": "Zhen Tao"
                    },
                    {
                        "name": "Haibo Feng"
                    },
                    {
                        "name": "Ziyun Jiang"
                    },
                    {
                        "name": "Ying Xu"
                    },
                    {
                        "name": "Zenan Huang"
                    },
                    {
                        "name": "Yihong Zhuang"
                    },
                    {
                        "name": "Haokai Xu"
                    },
                    {
                        "name": "Jiaqi Hu"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    },
                    {
                        "name": "Junbo Zhao"
                    },
                    {
                        "name": "Jianguo Li"
                    },
                    {
                        "name": "Da Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Da Zheng"
                },
                "author": "Da Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08666v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08666v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19257v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19257v2",
                "updated": "2025-10-13T10:18:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    10,
                    18,
                    34,
                    0,
                    286,
                    0
                ],
                "published": "2025-08-15T12:03:34Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    12,
                    3,
                    34,
                    4,
                    227,
                    0
                ],
                "title": "TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for\n  Vision-Language-Action Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for\n  Vision-Language-Action Models"
                },
                "summary": "Vision-Language-Action (VLA) models process visual inputs independently at\neach timestep, discarding valuable temporal information inherent in robotic\nmanipulation tasks. This frame-by-frame processing makes models vulnerable to\nvisual noise while ignoring the substantial coherence between consecutive\nframes in manipulation sequences. We propose Temporal Token Fusion (TTF), a\ntraining-free approach that intelligently integrates historical and current\nvisual representations to enhance VLA inference quality. Our method employs\ndual-dimension detection combining efficient grayscale pixel difference\nanalysis with attention-based semantic relevance assessment, enabling selective\ntemporal token fusion through hard fusion strategies and keyframe anchoring to\nprevent error accumulation. Comprehensive experiments across LIBERO,\nSimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0\npercentage points average on LIBERO (72.4\\% vs 68.4\\% baseline),\ncross-environment validation on SimplerEnv (4.8\\% relative improvement), and\n8.7\\% relative improvement on real robot tasks. Our approach proves\nmodel-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,\nTTF reveals that selective Query matrix reuse in attention mechanisms enhances\nrather than compromises performance, suggesting promising directions for direct\nKQV matrix reuse strategies that achieve computational acceleration while\nimproving task success rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models process visual inputs independently at\neach timestep, discarding valuable temporal information inherent in robotic\nmanipulation tasks. This frame-by-frame processing makes models vulnerable to\nvisual noise while ignoring the substantial coherence between consecutive\nframes in manipulation sequences. We propose Temporal Token Fusion (TTF), a\ntraining-free approach that intelligently integrates historical and current\nvisual representations to enhance VLA inference quality. Our method employs\ndual-dimension detection combining efficient grayscale pixel difference\nanalysis with attention-based semantic relevance assessment, enabling selective\ntemporal token fusion through hard fusion strategies and keyframe anchoring to\nprevent error accumulation. Comprehensive experiments across LIBERO,\nSimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0\npercentage points average on LIBERO (72.4\\% vs 68.4\\% baseline),\ncross-environment validation on SimplerEnv (4.8\\% relative improvement), and\n8.7\\% relative improvement on real robot tasks. Our approach proves\nmodel-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,\nTTF reveals that selective Query matrix reuse in attention mechanisms enhances\nrather than compromises performance, suggesting promising directions for direct\nKQV matrix reuse strategies that achieve computational acceleration while\nimproving task success rates."
                },
                "authors": [
                    {
                        "name": "Chenghao Liu"
                    },
                    {
                        "name": "Jiachen Zhang"
                    },
                    {
                        "name": "Chengxuan Li"
                    },
                    {
                        "name": "Zhimu Zhou"
                    },
                    {
                        "name": "Shixin Wu"
                    },
                    {
                        "name": "Songfang Huang"
                    },
                    {
                        "name": "Huiling Duan"
                    }
                ],
                "author_detail": {
                    "name": "Huiling Duan"
                },
                "author": "Huiling Duan",
                "arxiv_comment": "Manuscript submitted to AAAI 2026, currently under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19257v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19257v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11236v1",
                "updated": "2025-10-13T10:17:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    10,
                    17,
                    21,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T10:17:21Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    10,
                    17,
                    21,
                    0,
                    286,
                    0
                ],
                "title": "XQuant: Achieving Ultra-Low Bit KV Cache Quantization with Cross-Layer\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XQuant: Achieving Ultra-Low Bit KV Cache Quantization with Cross-Layer\n  Compression"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\ndiverse natural language processing tasks. However, their extensive memory\nrequirements, particularly due to KV cache growth during long-text\nunderstanding and generation, present significant challenges for deployment in\nresource-constrained environments. Quantization has emerged as a promising\nsolution to reduce memory consumption while preserving historical information.\nWe propose XQuant, a training-free and plug-and-play framework that achieves\nultra-low equivalent bit-width KV cache quantization. XQuant introduces two key\ninnovations: a computationally negligible data-free calibration method and\ncross-layer KV cache compression, enabling quantization to sub-1.4 bits.\nExtensive experiments on TruthfulQA and LongBench demonstrate that XQuant\noutperforms state-of-the-art methods (e.g., KIVI-2bit and AsymKV-1.5bit) by\nachieving lower bit-width while maintaining superior performance, establishing\na better trade-off between memory efficiency and model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\ndiverse natural language processing tasks. However, their extensive memory\nrequirements, particularly due to KV cache growth during long-text\nunderstanding and generation, present significant challenges for deployment in\nresource-constrained environments. Quantization has emerged as a promising\nsolution to reduce memory consumption while preserving historical information.\nWe propose XQuant, a training-free and plug-and-play framework that achieves\nultra-low equivalent bit-width KV cache quantization. XQuant introduces two key\ninnovations: a computationally negligible data-free calibration method and\ncross-layer KV cache compression, enabling quantization to sub-1.4 bits.\nExtensive experiments on TruthfulQA and LongBench demonstrate that XQuant\noutperforms state-of-the-art methods (e.g., KIVI-2bit and AsymKV-1.5bit) by\nachieving lower bit-width while maintaining superior performance, establishing\na better trade-off between memory efficiency and model accuracy."
                },
                "authors": [
                    {
                        "name": "Haoqi Yang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Baoyuan Qi"
                    },
                    {
                        "name": "Guoming Liu"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "To be published in The 2025 Conference on Empirical Methods in\n  Natural Language Processing (EMNLP 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24695v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24695v2",
                "updated": "2025-10-13T09:12:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    9,
                    12,
                    27,
                    0,
                    286,
                    0
                ],
                "published": "2025-09-29T12:28:09Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    12,
                    28,
                    9,
                    0,
                    272,
                    0
                ],
                "title": "SANA-Video: Efficient Video Generation with Block Linear Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SANA-Video: Efficient Video Generation with Block Linear Diffusion\n  Transformer"
                },
                "summary": "We introduce SANA-Video, a small diffusion model that can efficiently\ngenerate videos up to 720x1280 resolution and minute-length duration.\nSANA-Video synthesizes high-resolution, high-quality and long videos with\nstrong text-video alignment at a remarkably fast speed, deployable on RTX 5090\nGPU. Two core designs ensure our efficient, effective and long video\ngeneration: (1) Linear DiT: We leverage linear attention as the core operation,\nwhich is more efficient than vanilla attention given the large number of tokens\nprocessed in video generation. (2) Constant-Memory KV cache for Block Linear\nAttention: we design block-wise autoregressive approach for long video\ngeneration by employing a constant-memory state, derived from the cumulative\nproperties of linear attention. This KV cache provides the Linear DiT with\nglobal context at a fixed memory cost, eliminating the need for a traditional\nKV cache and enabling efficient, minute-long video generation. In addition, we\nexplore effective data filters and model training strategies, narrowing the\ntraining cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of\nMovieGen. Given its low cost, SANA-Video achieves competitive performance\ncompared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B\nand SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover,\nSANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating\nthe inference speed of generating a 5-second 720p video from 71s to 29s (2.4x\nspeedup). In summary, SANA-Video enables low-cost, high-quality video\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SANA-Video, a small diffusion model that can efficiently\ngenerate videos up to 720x1280 resolution and minute-length duration.\nSANA-Video synthesizes high-resolution, high-quality and long videos with\nstrong text-video alignment at a remarkably fast speed, deployable on RTX 5090\nGPU. Two core designs ensure our efficient, effective and long video\ngeneration: (1) Linear DiT: We leverage linear attention as the core operation,\nwhich is more efficient than vanilla attention given the large number of tokens\nprocessed in video generation. (2) Constant-Memory KV cache for Block Linear\nAttention: we design block-wise autoregressive approach for long video\ngeneration by employing a constant-memory state, derived from the cumulative\nproperties of linear attention. This KV cache provides the Linear DiT with\nglobal context at a fixed memory cost, eliminating the need for a traditional\nKV cache and enabling efficient, minute-long video generation. In addition, we\nexplore effective data filters and model training strategies, narrowing the\ntraining cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of\nMovieGen. Given its low cost, SANA-Video achieves competitive performance\ncompared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B\nand SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover,\nSANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating\nthe inference speed of generating a 5-second 720p video from 71s to 29s (2.4x\nspeedup). In summary, SANA-Video enables low-cost, high-quality video\ngeneration."
                },
                "authors": [
                    {
                        "name": "Junsong Chen"
                    },
                    {
                        "name": "Yuyang Zhao"
                    },
                    {
                        "name": "Jincheng Yu"
                    },
                    {
                        "name": "Ruihang Chu"
                    },
                    {
                        "name": "Junyu Chen"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Xianbang Wang"
                    },
                    {
                        "name": "Yicheng Pan"
                    },
                    {
                        "name": "Daquan Zhou"
                    },
                    {
                        "name": "Huan Ling"
                    },
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Hongwei Yi"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Han Cai"
                    },
                    {
                        "name": "Sanja Fidler"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "arxiv_comment": "21 pages, 15 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24695v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24695v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08907v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08907v2",
                "updated": "2025-10-13T08:26:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    8,
                    26,
                    21,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-10T01:42:14Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    1,
                    42,
                    14,
                    4,
                    283,
                    0
                ],
                "title": "Autoencoding-Free Context Compression for LLMs via Contextual Semantic\n  Anchors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoencoding-Free Context Compression for LLMs via Contextual Semantic\n  Anchors"
                },
                "summary": "Context compression presents a promising approach for accelerating large\nlanguage model (LLM) inference by compressing long contexts into compact\nrepresentations. Current context compression methods predominantly rely on\nautoencoding tasks to train context-agnostic compression tokens to compress\ncontextual semantics. While autoencoding tasks enable compression tokens to\nacquire compression capabilities, compression via autoencoding tasks creates a\nfundamental mismatch: the models are optimized for reconstruction that diverge\nfrom actual downstream tasks, thereby weakening the features more beneficial\nfor real-world usage. We propose Semantic-Anchor Compression (SAC), a novel\nmethod that shifts from autoencoding task based compression to an architecture\nthat is equipped with this compression capability \\textit{a priori}. Instead of\ntraining models to compress contexts through autoencoding tasks, SAC directly\nselects so-called anchor tokens from the original context and aggregates\ncontextual information into their key-value (KV) representations. By deriving\nrepresentations directly from the contextual tokens, SAC eliminates the need\nfor autoencoding training. To ensure compression performance while directly\nleveraging anchor tokens, SAC incorporates two key designs: (1) anchor\nembeddings that enable the compressor to identify critical tokens, and (2)\nbidirectional attention modification that allows anchor tokens to capture\ninformation from the entire context. Experimental results demonstrate that SAC\nconsistently outperforms existing context compression methods across various\ncompression ratios. On out-of-distribution evaluation using MRQA, SAC achieves\n1 EM improvement at 5x compression over strong baselines, with increasing\nadvantages at higher compression ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context compression presents a promising approach for accelerating large\nlanguage model (LLM) inference by compressing long contexts into compact\nrepresentations. Current context compression methods predominantly rely on\nautoencoding tasks to train context-agnostic compression tokens to compress\ncontextual semantics. While autoencoding tasks enable compression tokens to\nacquire compression capabilities, compression via autoencoding tasks creates a\nfundamental mismatch: the models are optimized for reconstruction that diverge\nfrom actual downstream tasks, thereby weakening the features more beneficial\nfor real-world usage. We propose Semantic-Anchor Compression (SAC), a novel\nmethod that shifts from autoencoding task based compression to an architecture\nthat is equipped with this compression capability \\textit{a priori}. Instead of\ntraining models to compress contexts through autoencoding tasks, SAC directly\nselects so-called anchor tokens from the original context and aggregates\ncontextual information into their key-value (KV) representations. By deriving\nrepresentations directly from the contextual tokens, SAC eliminates the need\nfor autoencoding training. To ensure compression performance while directly\nleveraging anchor tokens, SAC incorporates two key designs: (1) anchor\nembeddings that enable the compressor to identify critical tokens, and (2)\nbidirectional attention modification that allows anchor tokens to capture\ninformation from the entire context. Experimental results demonstrate that SAC\nconsistently outperforms existing context compression methods across various\ncompression ratios. On out-of-distribution evaluation using MRQA, SAC achieves\n1 EM improvement at 5x compression over strong baselines, with increasing\nadvantages at higher compression ratios."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Runsong Zhao"
                    },
                    {
                        "name": "Pengcheng Huang"
                    },
                    {
                        "name": "Xinyu Liu"
                    },
                    {
                        "name": "Junyi Xiao"
                    },
                    {
                        "name": "Chunyang Xiao"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Shengxiang Gao"
                    },
                    {
                        "name": "Zhengtao Yu"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "18 pages,9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08907v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08907v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11121v1",
                "updated": "2025-10-13T08:08:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    8,
                    8,
                    58,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T08:08:58Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    8,
                    8,
                    58,
                    0,
                    286,
                    0
                ],
                "title": "Refining Hybrid Genetic Search for CVRP via Reinforcement\n  Learning-Finetuned LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refining Hybrid Genetic Search for CVRP via Reinforcement\n  Learning-Finetuned LLM"
                },
                "summary": "While large language models (LLMs) are increasingly used as automated\nheuristic designers for vehicle routing problems (VRPs), current\nstate-of-the-art methods predominantly rely on prompting massive,\ngeneral-purpose models like GPT-4. This work challenges that paradigm by\ndemonstrating that a smaller, specialized LLM, when meticulously fine-tuned,\ncan generate components that surpass expert-crafted heuristics within advanced\nsolvers. We propose RFTHGS, a novel Reinforcement learning (RL) framework for\nFine-Tuning a small LLM to generate high-performance crossover operators for\nthe Hybrid Genetic Search (HGS) solver, applied to the Capacitated VRP (CVRP).\nOur method employs a multi-tiered, curriculum-based reward function that\nprogressively guides the LLM to master generating first compilable, then\nexecutable, and finally, superior-performing operators that exceed human expert\ndesigns. This is coupled with an operator caching mechanism that discourages\nplagiarism and promotes diversity during training. Comprehensive experiments\nshow that our fine-tuned LLM produces crossover operators which significantly\noutperform the expert-designed ones in HGS. The performance advantage remains\nconsistent, generalizing from small-scale instances to large-scale problems\nwith up to 1000 nodes. Furthermore, RFTHGS exceeds the performance of leading\nneuro-combinatorial baselines, prompt-based methods, and commercial LLMs such\nas GPT-4o and GPT-4o-mini.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) are increasingly used as automated\nheuristic designers for vehicle routing problems (VRPs), current\nstate-of-the-art methods predominantly rely on prompting massive,\ngeneral-purpose models like GPT-4. This work challenges that paradigm by\ndemonstrating that a smaller, specialized LLM, when meticulously fine-tuned,\ncan generate components that surpass expert-crafted heuristics within advanced\nsolvers. We propose RFTHGS, a novel Reinforcement learning (RL) framework for\nFine-Tuning a small LLM to generate high-performance crossover operators for\nthe Hybrid Genetic Search (HGS) solver, applied to the Capacitated VRP (CVRP).\nOur method employs a multi-tiered, curriculum-based reward function that\nprogressively guides the LLM to master generating first compilable, then\nexecutable, and finally, superior-performing operators that exceed human expert\ndesigns. This is coupled with an operator caching mechanism that discourages\nplagiarism and promotes diversity during training. Comprehensive experiments\nshow that our fine-tuned LLM produces crossover operators which significantly\noutperform the expert-designed ones in HGS. The performance advantage remains\nconsistent, generalizing from small-scale instances to large-scale problems\nwith up to 1000 nodes. Furthermore, RFTHGS exceeds the performance of leading\nneuro-combinatorial baselines, prompt-based methods, and commercial LLMs such\nas GPT-4o and GPT-4o-mini."
                },
                "authors": [
                    {
                        "name": "Rongjie Zhu"
                    },
                    {
                        "name": "Cong Zhang"
                    },
                    {
                        "name": "Zhiguang Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zhiguang Cao"
                },
                "author": "Zhiguang Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11011v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11011v1",
                "updated": "2025-10-13T05:03:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    5,
                    3,
                    23,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T05:03:23Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    5,
                    3,
                    23,
                    0,
                    286,
                    0
                ],
                "title": "GrASP: A Generalizable Address-based Semantic Prefetcher for Scalable\n  Transactional and Analytical Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GrASP: A Generalizable Address-based Semantic Prefetcher for Scalable\n  Transactional and Analytical Workloads"
                },
                "summary": "Data prefetching--loading data into the cache before it is requested--is\nessential for reducing I/O overhead and improving database performance. While\ntraditional prefetchers focus on sequential patterns, recent learning-based\napproaches, especially those leveraging data semantics, achieve higher accuracy\nfor complex access patterns. However, these methods often struggle with today's\ndynamic, ever-growing datasets and require frequent, timely fine-tuning.\nPrivacy constraints may also restrict access to complete datasets,\nnecessitating prefetchers that can learn effectively from samples. To address\nthese challenges, we present GrASP, a learning-based prefetcher designed for\nboth analytical and transactional workloads. GrASP enhances prefetching\naccuracy and scalability by leveraging logical block address deltas and\ncombining query representations with result encodings. It frames prefetching as\na context-aware multi-label classification task, using multi-layer LSTMs to\npredict delta patterns from embedded context. This delta modeling approach\nenables GrASP to generalize predictions from small samples to larger, dynamic\ndatasets without requiring extensive retraining. Experiments on real-world\ndatasets and industrial benchmarks demonstrate that GrASP generalizes to\ndatasets 250 times larger than the training data, achieving up to 45% higher\nhit ratios, 60% lower I/O time, and 55% lower end-to-end query execution\nlatency than existing baselines. On average, GrASP attains a 91.4% hit ratio, a\n90.8% I/O time reduction, and a 57.1% execution latency reduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data prefetching--loading data into the cache before it is requested--is\nessential for reducing I/O overhead and improving database performance. While\ntraditional prefetchers focus on sequential patterns, recent learning-based\napproaches, especially those leveraging data semantics, achieve higher accuracy\nfor complex access patterns. However, these methods often struggle with today's\ndynamic, ever-growing datasets and require frequent, timely fine-tuning.\nPrivacy constraints may also restrict access to complete datasets,\nnecessitating prefetchers that can learn effectively from samples. To address\nthese challenges, we present GrASP, a learning-based prefetcher designed for\nboth analytical and transactional workloads. GrASP enhances prefetching\naccuracy and scalability by leveraging logical block address deltas and\ncombining query representations with result encodings. It frames prefetching as\na context-aware multi-label classification task, using multi-layer LSTMs to\npredict delta patterns from embedded context. This delta modeling approach\nenables GrASP to generalize predictions from small samples to larger, dynamic\ndatasets without requiring extensive retraining. Experiments on real-world\ndatasets and industrial benchmarks demonstrate that GrASP generalizes to\ndatasets 250 times larger than the training data, achieving up to 45% higher\nhit ratios, 60% lower I/O time, and 55% lower end-to-end query execution\nlatency than existing baselines. On average, GrASP attains a 91.4% hit ratio, a\n90.8% I/O time reduction, and a 57.1% execution latency reduction."
                },
                "authors": [
                    {
                        "name": "Farzaneh Zirak"
                    },
                    {
                        "name": "Farhana Choudhury"
                    },
                    {
                        "name": "Renata Borovica-Gajic"
                    }
                ],
                "author_detail": {
                    "name": "Renata Borovica-Gajic"
                },
                "author": "Renata Borovica-Gajic",
                "arxiv_comment": "This is a preprint version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11011v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11011v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10964v1",
                "updated": "2025-10-13T03:14:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    3,
                    14,
                    28,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T03:14:28Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    3,
                    14,
                    28,
                    0,
                    286,
                    0
                ],
                "title": "Not All Bits Are Equal: Scale-Dependent Memory Optimization Strategies\n  for Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Bits Are Equal: Scale-Dependent Memory Optimization Strategies\n  for Reasoning Models"
                },
                "summary": "While 4-bit quantization has emerged as a memory-optimal choice for\nnon-reasoning models and zero-shot tasks across scales, we show that this\nuniversal prescription fails for reasoning models, where the KV cache rather\nthan model size can dominate memory. Through systematic experiments across\n1,700 inference scenarios on AIME25 and GPQA-Diamond, we find a scale-dependent\ntrade-off: models with an effective size below 8-bit 4B parameters achieve\nbetter accuracy by allocating memory to more weights rather than longer\ngeneration, while larger models achieve better accuracy by allocating memory to\nlonger generations. This scale threshold also determines when parallel scaling\nbecomes memory-efficient and whether KV cache eviction outperforms KV\nquantization. Our findings show that memory optimization for LLMs cannot be\nscale-agnostic, while providing principled guidelines: for small reasoning\nmodels, prioritize model capacity over test-time compute, while for larger\nones, maximize test-time compute. Our results suggest that optimizing reasoning\nmodels for deployment requires fundamentally different strategies from those\nestablished for non-reasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While 4-bit quantization has emerged as a memory-optimal choice for\nnon-reasoning models and zero-shot tasks across scales, we show that this\nuniversal prescription fails for reasoning models, where the KV cache rather\nthan model size can dominate memory. Through systematic experiments across\n1,700 inference scenarios on AIME25 and GPQA-Diamond, we find a scale-dependent\ntrade-off: models with an effective size below 8-bit 4B parameters achieve\nbetter accuracy by allocating memory to more weights rather than longer\ngeneration, while larger models achieve better accuracy by allocating memory to\nlonger generations. This scale threshold also determines when parallel scaling\nbecomes memory-efficient and whether KV cache eviction outperforms KV\nquantization. Our findings show that memory optimization for LLMs cannot be\nscale-agnostic, while providing principled guidelines: for small reasoning\nmodels, prioritize model capacity over test-time compute, while for larger\nones, maximize test-time compute. Our results suggest that optimizing reasoning\nmodels for deployment requires fundamentally different strategies from those\nestablished for non-reasoning models."
                },
                "authors": [
                    {
                        "name": "Junhyuck Kim"
                    },
                    {
                        "name": "Ethan Ewer"
                    },
                    {
                        "name": "Taehong Moon"
                    },
                    {
                        "name": "Jongho Park"
                    },
                    {
                        "name": "Dimitris Papailiopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris Papailiopoulos"
                },
                "author": "Dimitris Papailiopoulos",
                "arxiv_comment": "20 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10862v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10862v1",
                "updated": "2025-10-13T00:11:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    0,
                    11,
                    2,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T00:11:02Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    0,
                    11,
                    2,
                    0,
                    286,
                    0
                ],
                "title": "A Joint Learning Approach to Hardware Caching and Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Joint Learning Approach to Hardware Caching and Prefetching"
                },
                "summary": "Several learned policies have been proposed to replace heuristics for\nscheduling, caching, and other system components in modern systems. By\nleveraging diverse features, learning from historical trends, and predicting\nfuture behaviors, such models promise to keep pace with ever-increasing\nworkload dynamism and continuous hardware evolution. However, policies trained\nin isolation may still achieve suboptimal performance when placed together. In\nthis paper, we inspect one such instance in the domain of hardware caching --\nfor the policies of cache replacement and prefetching. We argue that these two\npolicies are bidirectionally interdependent and make the case for training the\ntwo jointly. We propose a joint learning approach based on developing shared\nrepresentations for the features used by the two policies. We present two\napproaches to develop these shared representations, one based on a joint\nencoder and another based on contrastive learning of the embeddings, and\ndemonstrate promising preliminary results for both of these. Finally, we lay\ndown an agenda for future research in this direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several learned policies have been proposed to replace heuristics for\nscheduling, caching, and other system components in modern systems. By\nleveraging diverse features, learning from historical trends, and predicting\nfuture behaviors, such models promise to keep pace with ever-increasing\nworkload dynamism and continuous hardware evolution. However, policies trained\nin isolation may still achieve suboptimal performance when placed together. In\nthis paper, we inspect one such instance in the domain of hardware caching --\nfor the policies of cache replacement and prefetching. We argue that these two\npolicies are bidirectionally interdependent and make the case for training the\ntwo jointly. We propose a joint learning approach based on developing shared\nrepresentations for the features used by the two policies. We present two\napproaches to develop these shared representations, one based on a joint\nencoder and another based on contrastive learning of the embeddings, and\ndemonstrate promising preliminary results for both of these. Finally, we lay\ndown an agenda for future research in this direction."
                },
                "authors": [
                    {
                        "name": "Samuel Yuan"
                    },
                    {
                        "name": "Divyanshu Saxena"
                    },
                    {
                        "name": "Jiayi Chen"
                    },
                    {
                        "name": "Nihal Sharma"
                    },
                    {
                        "name": "Aditya Akella"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Akella"
                },
                "author": "Aditya Akella",
                "arxiv_comment": "Accepted at ML for Systems Workshop at the 39th Conference on Neural\n  Information Processing Systems (NeurIPS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10862v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10858v1",
                "updated": "2025-10-12T23:46:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    23,
                    46,
                    4,
                    6,
                    285,
                    0
                ],
                "published": "2025-10-12T23:46:04Z",
                "published_parsed": [
                    2025,
                    10,
                    12,
                    23,
                    46,
                    4,
                    6,
                    285,
                    0
                ],
                "title": "DriftBench: Defining and Generating Data and Query Workload Drift for\n  Benchmarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DriftBench: Defining and Generating Data and Query Workload Drift for\n  Benchmarking"
                },
                "summary": "Data and workload drift are key to evaluating database components such as\ncaching, cardinality estimation, indexing, and query optimization. Yet,\nexisting benchmarks are static, offering little to no support for modeling\ndrift. This limitation stems from the lack of clear definitions and tools for\ngenerating data and workload drift. Motivated by this gap, we propose a unified\ntaxonomy for data and workload drift, grounded in observations from both\nacademia and industry. Building on this foundation, we introduce DriftBench, a\nlightweight and extensible framework for generating data and workload drift in\nbenchmark inputs. Together, the taxonomy and DriftBench provide a standardized\nvocabulary and mechanism for modeling and generating drift in benchmarking. We\ndemonstrate their effectiveness through case studies involving data drift,\nworkload drift, and drift-aware cardinality estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data and workload drift are key to evaluating database components such as\ncaching, cardinality estimation, indexing, and query optimization. Yet,\nexisting benchmarks are static, offering little to no support for modeling\ndrift. This limitation stems from the lack of clear definitions and tools for\ngenerating data and workload drift. Motivated by this gap, we propose a unified\ntaxonomy for data and workload drift, grounded in observations from both\nacademia and industry. Building on this foundation, we introduce DriftBench, a\nlightweight and extensible framework for generating data and workload drift in\nbenchmark inputs. Together, the taxonomy and DriftBench provide a standardized\nvocabulary and mechanism for modeling and generating drift in benchmarking. We\ndemonstrate their effectiveness through case studies involving data drift,\nworkload drift, and drift-aware cardinality estimation."
                },
                "authors": [
                    {
                        "name": "Guanli Liu"
                    },
                    {
                        "name": "Renata Borovica-Gajic"
                    }
                ],
                "author_detail": {
                    "name": "Renata Borovica-Gajic"
                },
                "author": "Renata Borovica-Gajic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00313v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00313v2",
                "updated": "2025-10-12T23:17:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    23,
                    17,
                    39,
                    6,
                    285,
                    0
                ],
                "published": "2024-05-01T04:30:03Z",
                "published_parsed": [
                    2024,
                    5,
                    1,
                    4,
                    30,
                    3,
                    2,
                    122,
                    0
                ],
                "title": "Streamlining Image Editing with Layered Diffusion Brushes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streamlining Image Editing with Layered Diffusion Brushes"
                },
                "summary": "Denoising diffusion models have emerged as powerful tools for image\nmanipulation, yet interactive, localized editing workflows remain\nunderdeveloped. We introduce Layered Diffusion Brushes (LDB), a novel\ntraining-free framework that enables interactive, layer-based editing using\nstandard diffusion models. LDB defines each \"layer\" as a self-contained set of\nparameters guiding the generative process, enabling independent,\nnon-destructive, and fine-grained prompt-guided edits, even in overlapping\nregions. LDB leverages a unique intermediate latent caching approach to reduce\neach edit to only a few denoising steps, achieving 140~ms per edit on consumer\nGPUs. An editor implementing LDB, incorporating familiar layer concepts, was\nevaluated via user study and quantitative metrics. Results demonstrate LDB's\nsuperior speed alongside comparable or improved image quality, background\npreservation, and edit fidelity relative to state-of-the-art methods across\nvarious sequential image manipulation tasks. The findings highlight LDB's\nability to significantly enhance creative workflows by providing an intuitive\nand efficient approach to diffusion-based image editing and its potential for\nexpansion into related subdomains, such as video editing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Denoising diffusion models have emerged as powerful tools for image\nmanipulation, yet interactive, localized editing workflows remain\nunderdeveloped. We introduce Layered Diffusion Brushes (LDB), a novel\ntraining-free framework that enables interactive, layer-based editing using\nstandard diffusion models. LDB defines each \"layer\" as a self-contained set of\nparameters guiding the generative process, enabling independent,\nnon-destructive, and fine-grained prompt-guided edits, even in overlapping\nregions. LDB leverages a unique intermediate latent caching approach to reduce\neach edit to only a few denoising steps, achieving 140~ms per edit on consumer\nGPUs. An editor implementing LDB, incorporating familiar layer concepts, was\nevaluated via user study and quantitative metrics. Results demonstrate LDB's\nsuperior speed alongside comparable or improved image quality, background\npreservation, and edit fidelity relative to state-of-the-art methods across\nvarious sequential image manipulation tasks. The findings highlight LDB's\nability to significantly enhance creative workflows by providing an intuitive\nand efficient approach to diffusion-based image editing and its potential for\nexpansion into related subdomains, such as video editing."
                },
                "authors": [
                    {
                        "name": "Peyman Gholami"
                    },
                    {
                        "name": "Robert Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Robert Xiao"
                },
                "author": "Robert Xiao",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2306.00219",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00313v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00313v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10587v1",
                "updated": "2025-10-12T13:06:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    13,
                    6,
                    59,
                    6,
                    285,
                    0
                ],
                "published": "2025-10-12T13:06:59Z",
                "published_parsed": [
                    2025,
                    10,
                    12,
                    13,
                    6,
                    59,
                    6,
                    285,
                    0
                ],
                "title": "A Simple and Better Baseline for Visual Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Better Baseline for Visual Grounding"
                },
                "summary": "Visual grounding aims to predict the locations of target objects specified by\ntextual descriptions. For this task with linguistic and visual modalities,\nthere is a latest research line that focuses on only selecting the\nlinguistic-relevant visual regions for object localization to reduce the\ncomputational overhead. Albeit achieving impressive performance, it is\niteratively performed on different image scales, and at every iteration,\nlinguistic features and visual features need to be stored in a cache, incurring\nextra overhead. To facilitate the implementation, in this paper, we propose a\nfeature selection-based simple yet effective baseline for visual grounding,\ncalled FSVG. Specifically, we directly encapsulate the linguistic and visual\nmodalities into an overall network architecture without complicated iterative\nprocedures, and utilize the language in parallel as guidance to facilitate the\ninteraction between linguistic modal and visual modal for extracting effective\nvisual features. Furthermore, to reduce the computational cost, during the\nvisual feature learning, we introduce a similarity-based feature selection\nmechanism to only exploit language-related visual features for faster\nprediction. Extensive experiments conducted on several benchmark datasets\ncomprehensively substantiate that the proposed FSVG achieves a better balance\nbetween accuracy and efficiency beyond the current state-of-the-art methods.\nCode is available at https://github.com/jcwang0602/FSVG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual grounding aims to predict the locations of target objects specified by\ntextual descriptions. For this task with linguistic and visual modalities,\nthere is a latest research line that focuses on only selecting the\nlinguistic-relevant visual regions for object localization to reduce the\ncomputational overhead. Albeit achieving impressive performance, it is\niteratively performed on different image scales, and at every iteration,\nlinguistic features and visual features need to be stored in a cache, incurring\nextra overhead. To facilitate the implementation, in this paper, we propose a\nfeature selection-based simple yet effective baseline for visual grounding,\ncalled FSVG. Specifically, we directly encapsulate the linguistic and visual\nmodalities into an overall network architecture without complicated iterative\nprocedures, and utilize the language in parallel as guidance to facilitate the\ninteraction between linguistic modal and visual modal for extracting effective\nvisual features. Furthermore, to reduce the computational cost, during the\nvisual feature learning, we introduce a similarity-based feature selection\nmechanism to only exploit language-related visual features for faster\nprediction. Extensive experiments conducted on several benchmark datasets\ncomprehensively substantiate that the proposed FSVG achieves a better balance\nbetween accuracy and efficiency beyond the current state-of-the-art methods.\nCode is available at https://github.com/jcwang0602/FSVG."
                },
                "authors": [
                    {
                        "name": "Jingchao Wang"
                    },
                    {
                        "name": "Wenlong Zhang"
                    },
                    {
                        "name": "Dingjiang Huang"
                    },
                    {
                        "name": "Hong Wang"
                    },
                    {
                        "name": "Yefeng Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Yefeng Zheng"
                },
                "author": "Yefeng Zheng",
                "arxiv_comment": "ICME2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18809v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18809v2",
                "updated": "2025-10-12T10:09:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    10,
                    9,
                    53,
                    6,
                    285,
                    0
                ],
                "published": "2025-05-24T17:46:47Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    17,
                    46,
                    47,
                    5,
                    144,
                    0
                ],
                "title": "VORTA: Efficient Video Diffusion via Routing Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VORTA: Efficient Video Diffusion via Routing Sparse Attention"
                },
                "summary": "Video diffusion transformers have achieved remarkable progress in\nhigh-quality video generation, but remain computationally expensive due to the\nquadratic complexity of attention over high-dimensional video sequences. Recent\nacceleration methods enhance the efficiency by exploiting the local sparsity of\nattention scores; yet they often struggle with accelerating the long-range\ncomputation. To address this problem, we propose VORTA, an acceleration\nframework with two novel components: 1) a sparse attention mechanism that\nefficiently captures long-range dependencies, and 2) a routing strategy that\nadaptively replaces full 3D attention with specialized sparse attention\nvariants. VORTA achieves an end-to-end speedup $1.76\\times$ without loss of\nquality on VBench. Furthermore, it can seamlessly integrate with various other\nacceleration methods, such as model caching and step distillation, reaching up\nto speedup $14.41\\times$ with negligible performance degradation. VORTA\ndemonstrates its efficiency and enhances the practicality of video diffusion\ntransformers in real-world settings. Codes and weights are available at\nhttps://github.com/wenhao728/VORTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video diffusion transformers have achieved remarkable progress in\nhigh-quality video generation, but remain computationally expensive due to the\nquadratic complexity of attention over high-dimensional video sequences. Recent\nacceleration methods enhance the efficiency by exploiting the local sparsity of\nattention scores; yet they often struggle with accelerating the long-range\ncomputation. To address this problem, we propose VORTA, an acceleration\nframework with two novel components: 1) a sparse attention mechanism that\nefficiently captures long-range dependencies, and 2) a routing strategy that\nadaptively replaces full 3D attention with specialized sparse attention\nvariants. VORTA achieves an end-to-end speedup $1.76\\times$ without loss of\nquality on VBench. Furthermore, it can seamlessly integrate with various other\nacceleration methods, such as model caching and step distillation, reaching up\nto speedup $14.41\\times$ with negligible performance degradation. VORTA\ndemonstrates its efficiency and enhances the practicality of video diffusion\ntransformers in real-world settings. Codes and weights are available at\nhttps://github.com/wenhao728/VORTA."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Yifu Ding"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "Accepted by NeurIPS 2025. The code is available at\n  https://github.com/wenhao728/VORTA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18809v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18809v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01092v2",
                "updated": "2025-10-12T04:46:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    4,
                    46,
                    48,
                    6,
                    285,
                    0
                ],
                "published": "2025-09-01T03:31:44Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    31,
                    44,
                    0,
                    244,
                    0
                ],
                "title": "REFRAG: Rethinking RAG based Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REFRAG: Rethinking RAG based Decoding"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes."
                },
                "authors": [
                    {
                        "name": "Xiaoqiang Lin"
                    },
                    {
                        "name": "Aritra Ghosh"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    },
                    {
                        "name": "Anshumali Shrivastava"
                    },
                    {
                        "name": "Vijai Mohan"
                    }
                ],
                "author_detail": {
                    "name": "Vijai Mohan"
                },
                "author": "Vijai Mohan",
                "arxiv_comment": "fix typo perplexity->log perplexity; added recent papers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19274v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19274v3",
                "updated": "2025-10-12T04:04:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    4,
                    4,
                    34,
                    6,
                    285,
                    0
                ],
                "published": "2024-10-25T03:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    3,
                    1,
                    19,
                    4,
                    299,
                    0
                ],
                "title": "Neuralink: Fast LLM Inference on Smartphones with Neuron Co-Activation\n  Linking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuralink: Fast LLM Inference on Smartphones with Neuron Co-Activation\n  Linking"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Neuralink, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory.\nNeuralink leverages the concept of Neuron Co-Activation, where neurons\nfrequently activated together are linked to facilitate continuous read access\nand optimize I/O efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Neuralink achieves on\naverage $1.49\\times$ improvements in end-to-end latency compared to the\nstate-of-the-art. As the first solution to optimize storage placement under\nsparsity, Neuralink explores a new optimization space at the intersection of\nsparsity-driven algorithm and storage-level system co-design for LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Neuralink, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory.\nNeuralink leverages the concept of Neuron Co-Activation, where neurons\nfrequently activated together are linked to facilitate continuous read access\nand optimize I/O efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Neuralink achieves on\naverage $1.49\\times$ improvements in end-to-end latency compared to the\nstate-of-the-art. As the first solution to optimize storage placement under\nsparsity, Neuralink explores a new optimization space at the intersection of\nsparsity-driven algorithm and storage-level system co-design for LLM inference."
                },
                "authors": [
                    {
                        "name": "Tuowei Wang"
                    },
                    {
                        "name": "Ruwen Fan"
                    },
                    {
                        "name": "Minxing Huang"
                    },
                    {
                        "name": "Zixu Hao"
                    },
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Youyou Lu"
                    },
                    {
                        "name": "Yaoxue Zhang"
                    },
                    {
                        "name": "Ju Ren"
                    }
                ],
                "author_detail": {
                    "name": "Ju Ren"
                },
                "author": "Ju Ren",
                "arxiv_doi": "10.1145/3676642.3736114",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676642.3736114",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.19274v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19274v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, Vol.\n  3, Rotterdam, Netherlands, 2025, pp. 147-162",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10290v1",
                "updated": "2025-10-11T17:08:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    17,
                    8,
                    45,
                    5,
                    284,
                    0
                ],
                "published": "2025-10-11T17:08:45Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    17,
                    8,
                    45,
                    5,
                    284,
                    0
                ],
                "title": "Grounded AI for Code Review: Resource-Efficient Large-Model Serving in\n  Enterprise Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounded AI for Code Review: Resource-Efficient Large-Model Serving in\n  Enterprise Pipelines"
                },
                "summary": "Automated code review adoption lags in compliance-heavy settings, where\nstatic analyzers produce high-volume, low-rationale outputs, and naive LLM use\nrisks hallucination and incurring cost overhead. We present a production system\nfor grounded, PR-native review that pairs static-analysis findings with\nAST-guided context extraction and a single-GPU, on-demand serving stack\n(quantized open-weight model, multi-tier caching) to deliver concise\nexplanations and remediation guidance. Evaluated on safety-oriented C/C++\nstandards, the approach achieves sub-minute median first-feedback (offline p50\nbuild+LLM 59.8s) while maintaining competitive violation reduction and lower\nviolation rates versus larger proprietary models. The architecture is\ndecoupled: teams can adopt the grounding/prompting layer or the serving layer\nindependently. A small internal survey (n=8) provides directional signals of\nreduced triage effort and moderate perceived grounding, with participants\nreporting fewer human review iterations. We outline operational lessons and\nlimitations, emphasizing reproducibility, auditability, and pathways to broader\nstandards and assisted patching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated code review adoption lags in compliance-heavy settings, where\nstatic analyzers produce high-volume, low-rationale outputs, and naive LLM use\nrisks hallucination and incurring cost overhead. We present a production system\nfor grounded, PR-native review that pairs static-analysis findings with\nAST-guided context extraction and a single-GPU, on-demand serving stack\n(quantized open-weight model, multi-tier caching) to deliver concise\nexplanations and remediation guidance. Evaluated on safety-oriented C/C++\nstandards, the approach achieves sub-minute median first-feedback (offline p50\nbuild+LLM 59.8s) while maintaining competitive violation reduction and lower\nviolation rates versus larger proprietary models. The architecture is\ndecoupled: teams can adopt the grounding/prompting layer or the serving layer\nindependently. A small internal survey (n=8) provides directional signals of\nreduced triage effort and moderate perceived grounding, with participants\nreporting fewer human review iterations. We outline operational lessons and\nlimitations, emphasizing reproducibility, auditability, and pathways to broader\nstandards and assisted patching."
                },
                "authors": [
                    {
                        "name": "Sayan Mandal"
                    },
                    {
                        "name": "Hua Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hua Jiang"
                },
                "author": "Hua Jiang",
                "arxiv_comment": "Submitted to MLSys 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10219v1",
                "updated": "2025-10-11T13:52:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    13,
                    52,
                    48,
                    5,
                    284,
                    0
                ],
                "published": "2025-10-11T13:52:48Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    13,
                    52,
                    48,
                    5,
                    284,
                    0
                ],
                "title": "Old is Gold: Optimizing Single-threaded Applications with Exgen-Malloc",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Old is Gold: Optimizing Single-threaded Applications with Exgen-Malloc"
                },
                "summary": "Memory allocators hide beneath nearly every application stack, yet their\nperformance footprint extends far beyond their code size. Even small\ninefficiencies in the allocators ripple through caches and the rest of the\nmemory hierarchy, collectively imposing what operators often call a \"datacenter\ntax\". At hyperscale, even a 1% improvement in allocator efficiency can unlock\nmillions of dollars in savings and measurable reductions in datacenter energy\nconsumption. Modern memory allocators are designed to optimize allocation speed\nand memory fragmentation in multi-threaded environments, relying on complex\nmetadata and control logic to achieve high performance. However, the overhead\nintroduced by this complexity prompts a reevaluation of allocator design.\nNotably, such overhead can be avoided in single-threaded scenarios, which\ncontinue to be widely used across diverse application domains.\n  In this paper, we introduce Exgen-Malloc, a memory allocator purpose-built\nfor single-threaded applications. By specializing for single-threaded\nexecution, Exgen-Malloc eliminates unnecessary metadata, simplifies the control\nflow, thereby reducing overhead and improving allocation efficiency. Its core\ndesign features include a centralized heap, a single free-block list, and a\nbalanced strategy for memory commitment and relocation. Additionally,\nExgen-Malloc incorporates design principles in modern multi-threaded\nallocators, which do not exist in legacy single-threaded allocators such as\ndlmalloc. We evaluate Exgen-Malloc on two Intel Xeon platforms. Across both\nsystems, Exgen-Malloc achieves a speedup of 1.17x, 1.10x, and 1.93x over\ndlmalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench, respectively. In\naddition to performance, Exgen-Malloc achieves 6.2%, 0.1%, and 25.2% memory\nsavings over mimalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory allocators hide beneath nearly every application stack, yet their\nperformance footprint extends far beyond their code size. Even small\ninefficiencies in the allocators ripple through caches and the rest of the\nmemory hierarchy, collectively imposing what operators often call a \"datacenter\ntax\". At hyperscale, even a 1% improvement in allocator efficiency can unlock\nmillions of dollars in savings and measurable reductions in datacenter energy\nconsumption. Modern memory allocators are designed to optimize allocation speed\nand memory fragmentation in multi-threaded environments, relying on complex\nmetadata and control logic to achieve high performance. However, the overhead\nintroduced by this complexity prompts a reevaluation of allocator design.\nNotably, such overhead can be avoided in single-threaded scenarios, which\ncontinue to be widely used across diverse application domains.\n  In this paper, we introduce Exgen-Malloc, a memory allocator purpose-built\nfor single-threaded applications. By specializing for single-threaded\nexecution, Exgen-Malloc eliminates unnecessary metadata, simplifies the control\nflow, thereby reducing overhead and improving allocation efficiency. Its core\ndesign features include a centralized heap, a single free-block list, and a\nbalanced strategy for memory commitment and relocation. Additionally,\nExgen-Malloc incorporates design principles in modern multi-threaded\nallocators, which do not exist in legacy single-threaded allocators such as\ndlmalloc. We evaluate Exgen-Malloc on two Intel Xeon platforms. Across both\nsystems, Exgen-Malloc achieves a speedup of 1.17x, 1.10x, and 1.93x over\ndlmalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench, respectively. In\naddition to performance, Exgen-Malloc achieves 6.2%, 0.1%, and 25.2% memory\nsavings over mimalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Ruihao Li"
                    },
                    {
                        "name": "Lizy K. John"
                    },
                    {
                        "name": "Neeraja J. Yadwadkar"
                    }
                ],
                "author_detail": {
                    "name": "Neeraja J. Yadwadkar"
                },
                "author": "Neeraja J. Yadwadkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10129v1",
                "updated": "2025-10-11T09:28:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    9,
                    28,
                    26,
                    5,
                    284,
                    0
                ],
                "published": "2025-10-11T09:28:26Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    9,
                    28,
                    26,
                    5,
                    284,
                    0
                ],
                "title": "CacheClip: Accelerating RAG with Effective KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheClip: Accelerating RAG with Effective KV Cache Reuse"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems suffer from severe\ntime-to-first-token (TTFT) bottlenecks due to long input sequences. Existing KV\ncache reuse methods face a fundamental trade-off: prefix caching requires\nidentical prefixes that rarely occur in RAG scenarios, while direct\nprecomputation sacrifices quality due to missing inter-chunk attention and\nrepeated attention sinks. Recent methods like APE and CacheBlend partially\naddress these issues but remain inadequate for robust RAG applications. This\npaper presents CacheClip, a novel framework that achieves both fast TTFT and\nhigh generation quality. Our key insight is that small auxiliary LLMs exhibit\nsimilar last-layer attention distributions to primary LLMs (the target model\nfor generation), enabling efficient identification of tokens critical for\nrestoring inter-chunk attention, thereby significantly improving response\nquality on cross-chunk reasoning tasks. CacheClip integrates three techniques:\n(1) auxiliary-model-guided token selection for selective KV cache\nrecomputation, where the auxiliary model is finetuned to improve selection\naccuracy, (2) shared prefixes to eliminate redundant attention sinks, and (3)\ngrouping strategy to maintain local coherence during partial KV cache updates.\nExperiments show CacheClip retains up to 94.8% and 85.0% of full-attention\nperformance on NIAH and LongBench, outperforming APE and CacheBlend by 25.2%\nand 35.1% on NIAH (with reomp% = 20%). Meanwhile, CacheClip accelerates LLM\ninference by up to 1.92x in prefill time, providing a practical solution to the\nefficiency-quality trade-off in RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems suffer from severe\ntime-to-first-token (TTFT) bottlenecks due to long input sequences. Existing KV\ncache reuse methods face a fundamental trade-off: prefix caching requires\nidentical prefixes that rarely occur in RAG scenarios, while direct\nprecomputation sacrifices quality due to missing inter-chunk attention and\nrepeated attention sinks. Recent methods like APE and CacheBlend partially\naddress these issues but remain inadequate for robust RAG applications. This\npaper presents CacheClip, a novel framework that achieves both fast TTFT and\nhigh generation quality. Our key insight is that small auxiliary LLMs exhibit\nsimilar last-layer attention distributions to primary LLMs (the target model\nfor generation), enabling efficient identification of tokens critical for\nrestoring inter-chunk attention, thereby significantly improving response\nquality on cross-chunk reasoning tasks. CacheClip integrates three techniques:\n(1) auxiliary-model-guided token selection for selective KV cache\nrecomputation, where the auxiliary model is finetuned to improve selection\naccuracy, (2) shared prefixes to eliminate redundant attention sinks, and (3)\ngrouping strategy to maintain local coherence during partial KV cache updates.\nExperiments show CacheClip retains up to 94.8% and 85.0% of full-attention\nperformance on NIAH and LongBench, outperforming APE and CacheBlend by 25.2%\nand 35.1% on NIAH (with reomp% = 20%). Meanwhile, CacheClip accelerates LLM\ninference by up to 1.92x in prefill time, providing a practical solution to the\nefficiency-quality trade-off in RAG systems."
                },
                "authors": [
                    {
                        "name": "Bin Yang"
                    },
                    {
                        "name": "Qiuyu Leng"
                    },
                    {
                        "name": "Jun Zeng"
                    },
                    {
                        "name": "Zhenhua Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhenhua Wu"
                },
                "author": "Zhenhua Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17396v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17396v3",
                "updated": "2025-10-11T09:04:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    9,
                    4,
                    23,
                    5,
                    284,
                    0
                ],
                "published": "2025-09-22T06:56:35Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    56,
                    35,
                    0,
                    265,
                    0
                ],
                "title": "EpiCache: Episodic KV Cache Management for Long Conversational Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EpiCache: Episodic KV Cache Management for Long Conversational Question\n  Answering"
                },
                "summary": "Modern large language models (LLMs) extend context lengths to millions of\ntokens, enabling coherent, personalized responses grounded in long\nconversational histories. This ability, however, hinges on Key-Value (KV)\ncaching, whose memory grows linearly with dialogue length and quickly becomes\nthe bottleneck in resource-constrained environments. An active line of research\nfor reducing memory bottleneck is KV cache compression, which seeks to limit\ncache size while preserving accuracy. Yet existing methods face two major\nlimitations: (i) evicting the KV cache after full-context prefill causes\nunbounded peak memory, and (ii) query-dependent eviction narrows the cache to a\nsingle query, leading to failure cases in multi-turn conversations. We\nintroduce EpiCache, a training-free KV cache management framework for long\nconversational question answering (LongConvQA) under fixed memory budgets.\nEpiCache bounds cache growth through block-wise prefill and preserves\ntopic-relevant context via episodic KV compression, which clusters conversation\nhistory into coherent episodes and applies episode-specific KV cache eviction.\nWe further design an adaptive layer-wise budget allocation strategy that\nmeasures each layer's sensitivity to eviction and distributes the memory budget\nacross layers accordingly. Across three LongConvQA benchmarks, EpiCache\nimproves accuracy by up to 40%, maintains near-full KV accuracy under 4-6x\ncompression, and reduces latency/memory by up to 2.4x/3.5x, enabling efficient\nmulti-turn interaction under strict resource limits. Our code is available at\nhttps://github.com/apple/ml-epicache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) extend context lengths to millions of\ntokens, enabling coherent, personalized responses grounded in long\nconversational histories. This ability, however, hinges on Key-Value (KV)\ncaching, whose memory grows linearly with dialogue length and quickly becomes\nthe bottleneck in resource-constrained environments. An active line of research\nfor reducing memory bottleneck is KV cache compression, which seeks to limit\ncache size while preserving accuracy. Yet existing methods face two major\nlimitations: (i) evicting the KV cache after full-context prefill causes\nunbounded peak memory, and (ii) query-dependent eviction narrows the cache to a\nsingle query, leading to failure cases in multi-turn conversations. We\nintroduce EpiCache, a training-free KV cache management framework for long\nconversational question answering (LongConvQA) under fixed memory budgets.\nEpiCache bounds cache growth through block-wise prefill and preserves\ntopic-relevant context via episodic KV compression, which clusters conversation\nhistory into coherent episodes and applies episode-specific KV cache eviction.\nWe further design an adaptive layer-wise budget allocation strategy that\nmeasures each layer's sensitivity to eviction and distributes the memory budget\nacross layers accordingly. Across three LongConvQA benchmarks, EpiCache\nimproves accuracy by up to 40%, maintains near-full KV accuracy under 4-6x\ncompression, and reduces latency/memory by up to 2.4x/3.5x, enabling efficient\nmulti-turn interaction under strict resource limits. Our code is available at\nhttps://github.com/apple/ml-epicache."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Arnav Kundu"
                    },
                    {
                        "name": "Han-Byul Kim"
                    },
                    {
                        "name": "Richa Dixit"
                    },
                    {
                        "name": "Minsik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Minsik Cho"
                },
                "author": "Minsik Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17396v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17396v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10102v1",
                "updated": "2025-10-11T08:24:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    8,
                    24,
                    19,
                    5,
                    284,
                    0
                ],
                "published": "2025-10-11T08:24:19Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    8,
                    24,
                    19,
                    5,
                    284,
                    0
                ],
                "title": "PANTHER: Generative Pretraining Beyond Language for Sequential User\n  Behavior Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PANTHER: Generative Pretraining Beyond Language for Sequential User\n  Behavior Modeling"
                },
                "summary": "Large language models (LLMs) have shown that generative pretraining can\ndistill vast world knowledge into compact token representations. While LLMs\nencapsulate extensive world knowledge, they remain limited in modeling the\nbehavioral knowledge contained within user interaction histories. User behavior\nforms a distinct modality, where each action, defined by multi-dimensional\nattributes such as time, context, and transaction type, constitutes a\nbehavioral token. Modeling these high-cardinality sequences is challenging, and\ndiscriminative models often falter under limited supervision. To bridge this\ngap, we extend generative pretraining to user behavior, learning transferable\nrepresentations from unlabeled behavioral data analogous to how LLMs learn from\ntext. We present PANTHER, a hybrid generative-discriminative framework that\nunifies user behavior pretraining and downstream adaptation, enabling\nlarge-scale sequential user representation learning and real-time inference.\nPANTHER introduces: (1) Structured Tokenization to compress multi-dimensional\ntransaction attributes into an interpretable vocabulary; (2) Sequence Pattern\nRecognition Module (SPRM) for modeling periodic transaction motifs; (3) a\nUnified User-Profile Embedding that fuses static demographics with dynamic\ntransaction histories; and (4) Real-time scalability enabled by offline caching\nof pretrained embeddings for millisecond-level inference. Fully deployed and\noperational online at WeChat Pay, PANTHER delivers a 25.6 percent boost in\nnext-transaction prediction HitRate@1 and a 38.6 percent relative improvement\nin fraud detection recall over baselines. Cross-domain evaluations on public\nbenchmarks show strong generalization, achieving up to 21 percent HitRate@1\ngains over transformer baselines, establishing PANTHER as a scalable,\nhigh-performance framework for industrial sequential user behavior modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown that generative pretraining can\ndistill vast world knowledge into compact token representations. While LLMs\nencapsulate extensive world knowledge, they remain limited in modeling the\nbehavioral knowledge contained within user interaction histories. User behavior\nforms a distinct modality, where each action, defined by multi-dimensional\nattributes such as time, context, and transaction type, constitutes a\nbehavioral token. Modeling these high-cardinality sequences is challenging, and\ndiscriminative models often falter under limited supervision. To bridge this\ngap, we extend generative pretraining to user behavior, learning transferable\nrepresentations from unlabeled behavioral data analogous to how LLMs learn from\ntext. We present PANTHER, a hybrid generative-discriminative framework that\nunifies user behavior pretraining and downstream adaptation, enabling\nlarge-scale sequential user representation learning and real-time inference.\nPANTHER introduces: (1) Structured Tokenization to compress multi-dimensional\ntransaction attributes into an interpretable vocabulary; (2) Sequence Pattern\nRecognition Module (SPRM) for modeling periodic transaction motifs; (3) a\nUnified User-Profile Embedding that fuses static demographics with dynamic\ntransaction histories; and (4) Real-time scalability enabled by offline caching\nof pretrained embeddings for millisecond-level inference. Fully deployed and\noperational online at WeChat Pay, PANTHER delivers a 25.6 percent boost in\nnext-transaction prediction HitRate@1 and a 38.6 percent relative improvement\nin fraud detection recall over baselines. Cross-domain evaluations on public\nbenchmarks show strong generalization, achieving up to 21 percent HitRate@1\ngains over transformer baselines, establishing PANTHER as a scalable,\nhigh-performance framework for industrial sequential user behavior modeling."
                },
                "authors": [
                    {
                        "name": "Guilin Li"
                    },
                    {
                        "name": "Yun Zhang"
                    },
                    {
                        "name": "Xiuyuan Chen"
                    },
                    {
                        "name": "Chengqi Li"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Wenjia Wang"
                    },
                    {
                        "name": "Weiran Huang"
                    },
                    {
                        "name": "Matthias Hwai Yong Tan"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Hwai Yong Tan"
                },
                "author": "Matthias Hwai Yong Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09952v1",
                "updated": "2025-10-11T01:42:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    1,
                    42,
                    38,
                    5,
                    284,
                    0
                ],
                "published": "2025-10-11T01:42:38Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    1,
                    42,
                    38,
                    5,
                    284,
                    0
                ],
                "title": "HTTP Request Synchronization Defeats Discrepancy Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HTTP Request Synchronization Defeats Discrepancy Attacks"
                },
                "summary": "Contemporary web application architectures involve many layers of proxy\nservices that process traffic. Due to the complexity of HTTP and vendor design\ndecisions, these proxies sometimes process a given request in different ways.\nAttackers can exploit these processing discrepancies to launch damaging attacks\nincluding web cache poisoning and request smuggling. Discrepancy attacks are\nsurging, yet, there exists no systemic defense.\n  In this work, we propose the first comprehensive defense to address this\nproblem, called HTTP Request Synchronization. Our scheme uses standard HTTP\nextension mechanisms to augment each request with a complete processing\nhistory. It propagates this context through the traffic path detailing how each\nserver hop has processed said request. Using this history, every proxy server\ncan validate that their processing is consistent with all previous hops,\neliminating discrepancy attacks. We implement our scheme for 5 popular proxy\ntechnologies, Apache, NGINX, HAProxy, Varnish, and Cloudflare, demonstrating\nits practical impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary web application architectures involve many layers of proxy\nservices that process traffic. Due to the complexity of HTTP and vendor design\ndecisions, these proxies sometimes process a given request in different ways.\nAttackers can exploit these processing discrepancies to launch damaging attacks\nincluding web cache poisoning and request smuggling. Discrepancy attacks are\nsurging, yet, there exists no systemic defense.\n  In this work, we propose the first comprehensive defense to address this\nproblem, called HTTP Request Synchronization. Our scheme uses standard HTTP\nextension mechanisms to augment each request with a complete processing\nhistory. It propagates this context through the traffic path detailing how each\nserver hop has processed said request. Using this history, every proxy server\ncan validate that their processing is consistent with all previous hops,\neliminating discrepancy attacks. We implement our scheme for 5 popular proxy\ntechnologies, Apache, NGINX, HAProxy, Varnish, and Cloudflare, demonstrating\nits practical impact."
                },
                "authors": [
                    {
                        "name": "Cem Topcuoglu"
                    },
                    {
                        "name": "Kaan Onarlioglu"
                    },
                    {
                        "name": "Steven Sprecher"
                    },
                    {
                        "name": "Engin Kirda"
                    }
                ],
                "author_detail": {
                    "name": "Engin Kirda"
                },
                "author": "Engin Kirda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09907v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09907v1",
                "updated": "2025-10-10T22:43:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    22,
                    43,
                    54,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T22:43:54Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    22,
                    43,
                    54,
                    4,
                    283,
                    0
                ],
                "title": "Agentic Property-Based Testing: Finding Bugs Across the Python Ecosystem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Property-Based Testing: Finding Bugs Across the Python Ecosystem"
                },
                "summary": "Property-based testing (PBT) is a lightweight formal method, typically\nimplemented as a randomized testing framework. Users specify the input domain\nfor their test using combinators supplied by the PBT framework, and the\nexpected properties or invariants as a unit-test function. The framework then\nsearches for a counterexample, e.g. by generating inputs and calling the test\nfunction. In this work, we demonstrate an LLM-based agent which analyzes Python\nmodules, infers function-specific and cross-function properties from code and\ndocumentation, synthesizes and executes PBTs, reflects on outputs of these\ntests to confirm true bugs, and finally outputs actionable bug reports for the\ndeveloper. We perform an extensive evaluation of our agent across 100 popular\nPython packages. Of the bug reports generated by the agent, we found after\nmanual review that 56\\% were valid bugs and 32\\% were valid bugs that we would\nreport to maintainers. We then developed a ranking rubric to surface\nhigh-priority valid bugs to developers, and found that of the 21 top-scoring\nbugs, 86\\% were valid and 81\\% we would report. The bugs span diverse failure\nmodes from serialization failures to numerical precision errors to flawed cache\nimplementations. We reported 5 bugs, 4 with patches, including to NumPy and\ncloud computing SDKs, with 3 patches merged successfully. Our results suggest\nthat LLMs with PBT provides a rigorous and scalable method for autonomously\ntesting software. Our code and artifacts are available at:\nhttps://github.com/mmaaz-git/agentic-pbt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Property-based testing (PBT) is a lightweight formal method, typically\nimplemented as a randomized testing framework. Users specify the input domain\nfor their test using combinators supplied by the PBT framework, and the\nexpected properties or invariants as a unit-test function. The framework then\nsearches for a counterexample, e.g. by generating inputs and calling the test\nfunction. In this work, we demonstrate an LLM-based agent which analyzes Python\nmodules, infers function-specific and cross-function properties from code and\ndocumentation, synthesizes and executes PBTs, reflects on outputs of these\ntests to confirm true bugs, and finally outputs actionable bug reports for the\ndeveloper. We perform an extensive evaluation of our agent across 100 popular\nPython packages. Of the bug reports generated by the agent, we found after\nmanual review that 56\\% were valid bugs and 32\\% were valid bugs that we would\nreport to maintainers. We then developed a ranking rubric to surface\nhigh-priority valid bugs to developers, and found that of the 21 top-scoring\nbugs, 86\\% were valid and 81\\% we would report. The bugs span diverse failure\nmodes from serialization failures to numerical precision errors to flawed cache\nimplementations. We reported 5 bugs, 4 with patches, including to NumPy and\ncloud computing SDKs, with 3 patches merged successfully. Our results suggest\nthat LLMs with PBT provides a rigorous and scalable method for autonomously\ntesting software. Our code and artifacts are available at:\nhttps://github.com/mmaaz-git/agentic-pbt."
                },
                "authors": [
                    {
                        "name": "Muhammad Maaz"
                    },
                    {
                        "name": "Liam DeVoe"
                    },
                    {
                        "name": "Zac Hatfield-Dodds"
                    },
                    {
                        "name": "Nicholas Carlini"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Carlini"
                },
                "author": "Nicholas Carlini",
                "arxiv_comment": "4 pages (main), NeurIPS 2025, The 4th Deep Learning for Code Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09907v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09907v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09883v1",
                "updated": "2025-10-10T21:37:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    21,
                    37,
                    49,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T21:37:49Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    21,
                    37,
                    49,
                    4,
                    283,
                    0
                ],
                "title": "DELTA: Dynamic Layer-Aware Token Attention for Efficient Long-Context\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DELTA: Dynamic Layer-Aware Token Attention for Efficient Long-Context\n  Reasoning"
                },
                "summary": "Large reasoning models (LRMs) achieve state-of-the-art performance on\nchallenging benchmarks by generating long chains of intermediate steps, but\ntheir inference cost is dominated by decoding, where each new token must attend\nto the entire growing sequence. Existing sparse attention methods reduce\ncomputation by pruning the key-value (KV) cache, yet they suffer from severe\naccuracy degradation on reasoning tasks due to cumulative selection errors and\nthe dynamic importance of tokens over long derivations. We present\n\\textbf{DELTA}, a training-free sparse attention mechanism that achieves\ncomputational efficiency without sacrificing model accuracy. DELTA partitions\ntransformer layers into three groups: initial layers that use full attention, a\nsmall set of \\emph{selection layers} that identify salient tokens via\naggregated head-level attention scores, and subsequent \\emph{sparse-attention\nlayers} that attend only to the selected subset. This design preserves the full\nKV cache in GPU memory for accuracy, while avoiding expensive full-attention\ncomputation over many layers. On reasoning benchmarks such as AIME and\nGPQA-Diamond, DELTA matches or surpasses full attention in accuracy, while\nreducing the number of attended tokens by up to $5\\times$ and delivering\n$1.5\\times$ end-to-end speedup. Our results show that selective reuse of\nintermediate attention maps offers a robust path toward efficient long-context\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large reasoning models (LRMs) achieve state-of-the-art performance on\nchallenging benchmarks by generating long chains of intermediate steps, but\ntheir inference cost is dominated by decoding, where each new token must attend\nto the entire growing sequence. Existing sparse attention methods reduce\ncomputation by pruning the key-value (KV) cache, yet they suffer from severe\naccuracy degradation on reasoning tasks due to cumulative selection errors and\nthe dynamic importance of tokens over long derivations. We present\n\\textbf{DELTA}, a training-free sparse attention mechanism that achieves\ncomputational efficiency without sacrificing model accuracy. DELTA partitions\ntransformer layers into three groups: initial layers that use full attention, a\nsmall set of \\emph{selection layers} that identify salient tokens via\naggregated head-level attention scores, and subsequent \\emph{sparse-attention\nlayers} that attend only to the selected subset. This design preserves the full\nKV cache in GPU memory for accuracy, while avoiding expensive full-attention\ncomputation over many layers. On reasoning benchmarks such as AIME and\nGPQA-Diamond, DELTA matches or surpasses full attention in accuracy, while\nreducing the number of attended tokens by up to $5\\times$ and delivering\n$1.5\\times$ end-to-end speedup. Our results show that selective reuse of\nintermediate attention maps offers a robust path toward efficient long-context\nreasoning."
                },
                "authors": [
                    {
                        "name": "Hossein Entezari Zarch"
                    },
                    {
                        "name": "Lei Gao"
                    },
                    {
                        "name": "Chaoyi Jiang"
                    },
                    {
                        "name": "Murali Annavarm"
                    }
                ],
                "author_detail": {
                    "name": "Murali Annavarm"
                },
                "author": "Murali Annavarm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09847v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09847v1",
                "updated": "2025-10-10T20:19:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    20,
                    19,
                    44,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T20:19:44Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    20,
                    19,
                    44,
                    4,
                    283,
                    0
                ],
                "title": "THEAS: Efficient Power Management in Multi-Core CPUs via Cache-Aware\n  Resource Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "THEAS: Efficient Power Management in Multi-Core CPUs via Cache-Aware\n  Resource Scheduling"
                },
                "summary": "The dynamic adaptation of resource levels enables the system to enhance\nenergy efficiency while maintaining the necessary computational resources,\nparticularly in scenarios where workloads fluctuate significantly over time.\nThe proposed approach can play a crucial role in heterogeneous systems where\nworkload characteristics are not uniformly distributed, such as non-pinning\ntasks. The deployed THEAS algorithm in this research work ensures a balance\nbetween performance and power consumption, making it suitable for a wide range\nof real-time applications. A comparative analysis of the proposed THEAS\nalgorithm with well-known scheduling techniques such as Completely Fair\nScheduler (CFS), Energy-Aware Scheduling (EAS), Heterogeneous Scheduling\n(HeteroSched), and Utility-Based Scheduling is presented in Table III. Each\nscheme is compared based on adaptability, core selection criteria, performance\nscaling, cache awareness, overhead, and real-time suitability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dynamic adaptation of resource levels enables the system to enhance\nenergy efficiency while maintaining the necessary computational resources,\nparticularly in scenarios where workloads fluctuate significantly over time.\nThe proposed approach can play a crucial role in heterogeneous systems where\nworkload characteristics are not uniformly distributed, such as non-pinning\ntasks. The deployed THEAS algorithm in this research work ensures a balance\nbetween performance and power consumption, making it suitable for a wide range\nof real-time applications. A comparative analysis of the proposed THEAS\nalgorithm with well-known scheduling techniques such as Completely Fair\nScheduler (CFS), Energy-Aware Scheduling (EAS), Heterogeneous Scheduling\n(HeteroSched), and Utility-Based Scheduling is presented in Table III. Each\nscheme is compared based on adaptability, core selection criteria, performance\nscaling, cache awareness, overhead, and real-time suitability."
                },
                "authors": [
                    {
                        "name": "Said Muhammad"
                    },
                    {
                        "name": "Lahlou Laaziz"
                    },
                    {
                        "name": "Nadjia Kara"
                    },
                    {
                        "name": "Phat Tan Nguyen"
                    },
                    {
                        "name": "Timothy Murphy"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Murphy"
                },
                "author": "Timothy Murphy",
                "arxiv_comment": "Accepted and presented at the 13th IEEE International Conference on\n  Intelligent Mobile Computing 2025 (IMC), CISOSE 2025 in Tucson, Arizona, USA.\n  This is the author's accepted manuscript (AAM). The final published version\n  will appear in the IEEE conference proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09847v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09847v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09608v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09608v1",
                "updated": "2025-10-10T17:59:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    17,
                    59,
                    58,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T17:59:58Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    17,
                    59,
                    58,
                    4,
                    283,
                    0
                ],
                "title": "StreamingVLM: Real-Time Understanding for Infinite Video Streams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamingVLM: Real-Time Understanding for Infinite Video Streams"
                },
                "summary": "Vision-language models (VLMs) could power real-time assistants and autonomous\nagents, but they face a critical challenge: understanding near-infinite video\nstreams without escalating latency and memory usage. Processing entire videos\nwith full attention leads to quadratic computational costs and poor performance\non long videos. Meanwhile, simple sliding window methods are also flawed, as\nthey either break coherence or suffer from high latency due to redundant\nrecomputation. In this paper, we introduce StreamingVLM, a model designed for\nreal-time, stable understanding of infinite visual input. Our approach is a\nunified framework that aligns training with streaming inference. During\ninference, we maintain a compact KV cache by reusing states of attention sinks,\na short window of recent vision tokens, and a long window of recent text\ntokens. This streaming ability is instilled via a simple supervised fine-tuning\n(SFT) strategy that applies full attention on short, overlapped video chunks,\nwhich effectively mimics the inference-time attention pattern without training\non prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a\nnew benchmark with videos averaging over two hours that requires dense,\nper-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM\nachieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time\nperformance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy\nalso enhances general VQA abilities without any VQA-specific fine-tuning,\nimproving performance on LongVideoBench by +4.30 and OVOBench Realtime by\n+5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) could power real-time assistants and autonomous\nagents, but they face a critical challenge: understanding near-infinite video\nstreams without escalating latency and memory usage. Processing entire videos\nwith full attention leads to quadratic computational costs and poor performance\non long videos. Meanwhile, simple sliding window methods are also flawed, as\nthey either break coherence or suffer from high latency due to redundant\nrecomputation. In this paper, we introduce StreamingVLM, a model designed for\nreal-time, stable understanding of infinite visual input. Our approach is a\nunified framework that aligns training with streaming inference. During\ninference, we maintain a compact KV cache by reusing states of attention sinks,\na short window of recent vision tokens, and a long window of recent text\ntokens. This streaming ability is instilled via a simple supervised fine-tuning\n(SFT) strategy that applies full attention on short, overlapped video chunks,\nwhich effectively mimics the inference-time attention pattern without training\non prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a\nnew benchmark with videos averaging over two hours that requires dense,\nper-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM\nachieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time\nperformance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy\nalso enhances general VQA abilities without any VQA-specific fine-tuning,\nimproving performance on LongVideoBench by +4.30 and OVOBench Realtime by\n+5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm."
                },
                "authors": [
                    {
                        "name": "Ruyi Xu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Liuning He"
                    },
                    {
                        "name": "Kelly Peng"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "The first two authors contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09608v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18494v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18494v2",
                "updated": "2025-10-10T16:56:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    16,
                    56,
                    23,
                    4,
                    283,
                    0
                ],
                "published": "2025-08-25T21:07:52Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    21,
                    7,
                    52,
                    0,
                    237,
                    0
                ],
                "title": "DiskJoin: Large-scale Vector Similarity Join with SSD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiskJoin: Large-scale Vector Similarity Join with SSD"
                },
                "summary": "Similarity join--a widely used operation in data science--finds all pairs of\nitems that have distance smaller than a threshold. Prior work has explored\ndistributed computation methods to scale similarity join to large data volumes\nbut these methods require a cluster deployment, and efficiency suffers from\nexpensive inter-machine communication. On the other hand, disk-based solutions\nare more cost-effective by using a single machine and storing the large dataset\non high-performance external storage, such as NVMe SSDs, but in these methods\nthe disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin,\nthe first disk-based similarity join algorithm that can process billion-scale\nvector datasets efficiently on a single machine. DiskJoin improves disk I/O by\ntailoring the data access patterns to avoid repetitive accesses and read\namplification. It also uses main memory as a dynamic cache and carefully\nmanages cache eviction to improve cache hit rate and reduce disk retrieval\ntime. For further acceleration, we adopt a probabilistic pruning technique that\ncan effectively prune a large number of vector pairs from computation. Our\nevaluation on real-world, large-scale datasets shows that DiskJoin\nsignificantly outperforms alternatives, achieving speedups from 50x to 1000x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Similarity join--a widely used operation in data science--finds all pairs of\nitems that have distance smaller than a threshold. Prior work has explored\ndistributed computation methods to scale similarity join to large data volumes\nbut these methods require a cluster deployment, and efficiency suffers from\nexpensive inter-machine communication. On the other hand, disk-based solutions\nare more cost-effective by using a single machine and storing the large dataset\non high-performance external storage, such as NVMe SSDs, but in these methods\nthe disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin,\nthe first disk-based similarity join algorithm that can process billion-scale\nvector datasets efficiently on a single machine. DiskJoin improves disk I/O by\ntailoring the data access patterns to avoid repetitive accesses and read\namplification. It also uses main memory as a dynamic cache and carefully\nmanages cache eviction to improve cache hit rate and reduce disk retrieval\ntime. For further acceleration, we adopt a probabilistic pruning technique that\ncan effectively prune a large number of vector pairs from computation. Our\nevaluation on real-world, large-scale datasets shows that DiskJoin\nsignificantly outperforms alternatives, achieving speedups from 50x to 1000x."
                },
                "authors": [
                    {
                        "name": "Yanqi Chen"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Alexandra Meliou"
                    },
                    {
                        "name": "Eric Lo"
                    }
                ],
                "author_detail": {
                    "name": "Eric Lo"
                },
                "author": "Eric Lo",
                "arxiv_doi": "10.1145/3769780",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3769780",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.18494v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18494v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at SIGMOD 2026",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03812v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03812v2",
                "updated": "2025-10-10T16:08:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    16,
                    8,
                    26,
                    4,
                    283,
                    0
                ],
                "published": "2025-07-04T21:09:51Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    21,
                    9,
                    51,
                    4,
                    185,
                    0
                ],
                "title": "Memory- and compute-optimized geometric multigrid GMGPolar for\n  curvilinear coordinate representations -- Applications to fusion plasma",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory- and compute-optimized geometric multigrid GMGPolar for\n  curvilinear coordinate representations -- Applications to fusion plasma"
                },
                "summary": "Tokamak fusion reactors are actively studied as a means of realizing energy\nproduction from plasma fusion. However, due to the substantial cost and time\nrequired to construct fusion reactors and run physical experiments, numerical\nexperiments are indispensable for understanding plasma physics inside tokamaks,\nsupporting the design and engineering phase, and optimizing future reactor\ndesigns. Geometric multigrid methods are optimal solvers for many problems that\narise from the discretization of partial differential equations. It has been\nshown that the multigrid solver GMGPolar solves the 2D gyrokinetic Poisson\nequation in linear complexity and with only small memory requirements compared\nto other state-of-the-art solvers. In this paper, we present a completely\nrefactored and object-oriented version of GMGPolar which offers two different\nmatrix-free implementations. Among other things, we leverage the\nSherman-Morrison formula to solve cyclic tridiagonal systems from circular line\nsolvers without additional fill-in and we apply reordering to optimize cache\naccess of circular and radial smoothing operations. With the Give approach,\nmemory requirements are further reduced and speedups of four to seven are\nobtained for usual test cases. For the Take approach, speedups of 16 to 18 can\nbe attained. In an additionally experimental setup of using GMGPolar as a\npreconditioner for conjugate gradients, this speedup could even be increased to\nfactors between 25 and 37.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokamak fusion reactors are actively studied as a means of realizing energy\nproduction from plasma fusion. However, due to the substantial cost and time\nrequired to construct fusion reactors and run physical experiments, numerical\nexperiments are indispensable for understanding plasma physics inside tokamaks,\nsupporting the design and engineering phase, and optimizing future reactor\ndesigns. Geometric multigrid methods are optimal solvers for many problems that\narise from the discretization of partial differential equations. It has been\nshown that the multigrid solver GMGPolar solves the 2D gyrokinetic Poisson\nequation in linear complexity and with only small memory requirements compared\nto other state-of-the-art solvers. In this paper, we present a completely\nrefactored and object-oriented version of GMGPolar which offers two different\nmatrix-free implementations. Among other things, we leverage the\nSherman-Morrison formula to solve cyclic tridiagonal systems from circular line\nsolvers without additional fill-in and we apply reordering to optimize cache\naccess of circular and radial smoothing operations. With the Give approach,\nmemory requirements are further reduced and speedups of four to seven are\nobtained for usual test cases. For the Take approach, speedups of 16 to 18 can\nbe attained. In an additionally experimental setup of using GMGPolar as a\npreconditioner for conjugate gradients, this speedup could even be increased to\nfactors between 25 and 37."
                },
                "authors": [
                    {
                        "name": "Julian Litz"
                    },
                    {
                        "name": "Philippe Leleux"
                    },
                    {
                        "name": "Carola Kruse"
                    },
                    {
                        "name": "Joscha Gedicke"
                    },
                    {
                        "name": "Martin J. Khn"
                    }
                ],
                "author_detail": {
                    "name": "Martin J. Khn"
                },
                "author": "Martin J. Khn",
                "arxiv_comment": "29 pages, 11 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03812v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03812v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68Q25, 65Y20, 65Y05, 65N55, 65N06, 65B99",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09477v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09477v1",
                "updated": "2025-10-10T15:32:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    15,
                    32,
                    58,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T15:32:58Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    15,
                    32,
                    58,
                    4,
                    283,
                    0
                ],
                "title": "Efficient Autoregressive Inference for Transformer Probabilistic Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Autoregressive Inference for Transformer Probabilistic Models"
                },
                "summary": "Transformer-based models for amortized probabilistic inference, such as\nneural processes, prior-fitted networks, and tabular foundation models, excel\nat single-pass marginal prediction. However, many real-world applications, from\nsignal interpolation to multi-column tabular predictions, require coherent\njoint distributions that capture dependencies between predictions. While purely\nautoregressive architectures efficiently generate such distributions, they\nsacrifice the flexible set-conditioning that makes these models powerful for\nmeta-learning. Conversely, the standard approach to obtain joint distributions\nfrom set-based models requires expensive re-encoding of the entire augmented\nconditioning set at each autoregressive step. We introduce a causal\nautoregressive buffer that preserves the advantages of both paradigms. Our\napproach decouples context encoding from updating the conditioning set. The\nmodel processes the context once and caches it. A dynamic buffer then captures\ntarget dependencies: as targets are incorporated, they enter the buffer and\nattend to both the cached context and previously buffered targets. This enables\nefficient batched autoregressive generation and one-pass joint log-likelihood\nevaluation. A unified training strategy allows seamless integration of\nset-based and autoregressive modes at minimal additional cost. Across synthetic\nfunctions, EEG signals, cognitive models, and tabular data, our method matches\npredictive accuracy of strong baselines while delivering up to 20 times faster\njoint sampling. Our approach combines the efficiency of autoregressive\ngenerative models with the representational power of set-based conditioning,\nmaking joint prediction practical for transformer-based probabilistic models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models for amortized probabilistic inference, such as\nneural processes, prior-fitted networks, and tabular foundation models, excel\nat single-pass marginal prediction. However, many real-world applications, from\nsignal interpolation to multi-column tabular predictions, require coherent\njoint distributions that capture dependencies between predictions. While purely\nautoregressive architectures efficiently generate such distributions, they\nsacrifice the flexible set-conditioning that makes these models powerful for\nmeta-learning. Conversely, the standard approach to obtain joint distributions\nfrom set-based models requires expensive re-encoding of the entire augmented\nconditioning set at each autoregressive step. We introduce a causal\nautoregressive buffer that preserves the advantages of both paradigms. Our\napproach decouples context encoding from updating the conditioning set. The\nmodel processes the context once and caches it. A dynamic buffer then captures\ntarget dependencies: as targets are incorporated, they enter the buffer and\nattend to both the cached context and previously buffered targets. This enables\nefficient batched autoregressive generation and one-pass joint log-likelihood\nevaluation. A unified training strategy allows seamless integration of\nset-based and autoregressive modes at minimal additional cost. Across synthetic\nfunctions, EEG signals, cognitive models, and tabular data, our method matches\npredictive accuracy of strong baselines while delivering up to 20 times faster\njoint sampling. Our approach combines the efficiency of autoregressive\ngenerative models with the representational power of set-based conditioning,\nmaking joint prediction practical for transformer-based probabilistic models."
                },
                "authors": [
                    {
                        "name": "Conor Hassan"
                    },
                    {
                        "name": "Nasrulloh Loka"
                    },
                    {
                        "name": "Cen-You Li"
                    },
                    {
                        "name": "Daolang Huang"
                    },
                    {
                        "name": "Paul E. Chang"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Francesco Silvestrin"
                    },
                    {
                        "name": "Samuel Kaski"
                    },
                    {
                        "name": "Luigi Acerbi"
                    }
                ],
                "author_detail": {
                    "name": "Luigi Acerbi"
                },
                "author": "Luigi Acerbi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09477v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09477v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09409v1",
                "updated": "2025-10-10T14:03:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    14,
                    3,
                    42,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T14:03:42Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    14,
                    3,
                    42,
                    4,
                    283,
                    0
                ],
                "title": "3C Resources Joint Allocation for Time-Deterministic Remote Sensing\n  Image Backhaul in the Space-Ground Integrated Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3C Resources Joint Allocation for Time-Deterministic Remote Sensing\n  Image Backhaul in the Space-Ground Integrated Network"
                },
                "summary": "Low-Earth-orbit (LEO) satellites assist observation satellites (OSs) to\ncompress and backhaul more time-determined images (TDI) has become a new\nparadigm, which is used to enhance the timeout caused by the limited computing\nresources of OSs. However, how to capture the time-varying and dynamic\ncharacteristics of multi-dimensional resources is challenging for efficient\ncollaborative scheduling. Motivated by this factor, we design a highly succinct\nmulti-dimensional resource time-expanded graph (MDR-TEG) modell. Specifically,\nby employing a slots division mechanism and introducing an external virtual\nnode, the time-varying communication, caching, and computing (3C) resources are\ndepicted in low complexity by the link weights within, between, and outside the\nslots. Based on the MDR-TEG, the maximizing successful transmission ratio of\nTDI (MSTR-TDI) is modeled as a mixed integer linear programming (MILP) problem.\nWhich further relaxed decomposed into two tractable sub-problems: maximizing\nthe successful transmission rate of images (MSTRI) and ensuring the timeliness\nproblem (ETP). Subsequently, an efficient subgradient of relaxation computing\nconstraint (SRCC) algorithm is proposed. The upper and lower bounds of MSTR-TDI\nare obtained by solving the two subproblems and the dual problem (DP), and the\ndirection of the next iteration is obtained by feedback. Furthermore, arranging\nthe sending sequences of images to improve the quality of the solution. The\napproximate optimal solution of MSTR-TDI is eventually obtained through\nrepeated iterations. The simulation results verify the superiority of the\nproposed MDR-TEG model and the effectiveness of the SRCC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Earth-orbit (LEO) satellites assist observation satellites (OSs) to\ncompress and backhaul more time-determined images (TDI) has become a new\nparadigm, which is used to enhance the timeout caused by the limited computing\nresources of OSs. However, how to capture the time-varying and dynamic\ncharacteristics of multi-dimensional resources is challenging for efficient\ncollaborative scheduling. Motivated by this factor, we design a highly succinct\nmulti-dimensional resource time-expanded graph (MDR-TEG) modell. Specifically,\nby employing a slots division mechanism and introducing an external virtual\nnode, the time-varying communication, caching, and computing (3C) resources are\ndepicted in low complexity by the link weights within, between, and outside the\nslots. Based on the MDR-TEG, the maximizing successful transmission ratio of\nTDI (MSTR-TDI) is modeled as a mixed integer linear programming (MILP) problem.\nWhich further relaxed decomposed into two tractable sub-problems: maximizing\nthe successful transmission rate of images (MSTRI) and ensuring the timeliness\nproblem (ETP). Subsequently, an efficient subgradient of relaxation computing\nconstraint (SRCC) algorithm is proposed. The upper and lower bounds of MSTR-TDI\nare obtained by solving the two subproblems and the dual problem (DP), and the\ndirection of the next iteration is obtained by feedback. Furthermore, arranging\nthe sending sequences of images to improve the quality of the solution. The\napproximate optimal solution of MSTR-TDI is eventually obtained through\nrepeated iterations. The simulation results verify the superiority of the\nproposed MDR-TEG model and the effectiveness of the SRCC."
                },
                "authors": [
                    {
                        "name": "Chongxiao Cai"
                    },
                    {
                        "name": "Yan Zhu"
                    },
                    {
                        "name": "Min Sheng"
                    },
                    {
                        "name": "Jiandong Li"
                    },
                    {
                        "name": "Yan Shi"
                    },
                    {
                        "name": "Di Zhou"
                    },
                    {
                        "name": "Ziwen Xie"
                    },
                    {
                        "name": "Chen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chen Zhang"
                },
                "author": "Chen Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08272v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08272v2",
                "updated": "2025-10-10T13:15:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    13,
                    15,
                    40,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-09T14:29:54Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    29,
                    54,
                    3,
                    282,
                    0
                ],
                "title": "Systematic Assessment of Cache Timing Vulnerabilities on RISC-V\n  Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Assessment of Cache Timing Vulnerabilities on RISC-V\n  Processors"
                },
                "summary": "While interest in the open RISC-V instruction set architecture is growing,\ntools to assess the security of concrete processor implementations are lacking.\nThere are dedicated tools and benchmarks for common microarchitectural\nside-channel vulnerabilities for popular processor families such as Intel\nx86-64 or ARM, but not for RISC-V. In this paper we describe our efforts in\nporting an Intel x86-64 benchmark suite for cache-based timing vulnerabilities\nto RISC-V. We then use this benchmark to evaluate the security of three\ncommercially available RISC-V processors, the T-Head C910 and the SiFive U54\nand U74 cores. We observe that the C910 processor exhibits more distinct timing\ntypes than the other processors, leading to the assumption that code running on\nthe C910 would be exposed to more microarchitectural vulnerability sources. In\naddition, our evaluation reveals that $65.9\\%$ of the vulnerabilities covered\nby the benchmark exist in all processors, while only $6.8\\%$ are absent from\nall cores. Our work, in particular the ported benchmark, aims to support RISC-V\nprocessor designers to identify leakage sources early in their designs and to\nsupport the development of countermeasures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While interest in the open RISC-V instruction set architecture is growing,\ntools to assess the security of concrete processor implementations are lacking.\nThere are dedicated tools and benchmarks for common microarchitectural\nside-channel vulnerabilities for popular processor families such as Intel\nx86-64 or ARM, but not for RISC-V. In this paper we describe our efforts in\nporting an Intel x86-64 benchmark suite for cache-based timing vulnerabilities\nto RISC-V. We then use this benchmark to evaluate the security of three\ncommercially available RISC-V processors, the T-Head C910 and the SiFive U54\nand U74 cores. We observe that the C910 processor exhibits more distinct timing\ntypes than the other processors, leading to the assumption that code running on\nthe C910 would be exposed to more microarchitectural vulnerability sources. In\naddition, our evaluation reveals that $65.9\\%$ of the vulnerabilities covered\nby the benchmark exist in all processors, while only $6.8\\%$ are absent from\nall cores. Our work, in particular the ported benchmark, aims to support RISC-V\nprocessor designers to identify leakage sources early in their designs and to\nsupport the development of countermeasures."
                },
                "authors": [
                    {
                        "name": "Cdrick Austa"
                    },
                    {
                        "name": "Jan Tobias Mhlberg"
                    },
                    {
                        "name": "Jean-Michel Dricot"
                    }
                ],
                "author_detail": {
                    "name": "Jean-Michel Dricot"
                },
                "author": "Jean-Michel Dricot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08272v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08272v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19433v2",
                "updated": "2025-10-10T13:08:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    13,
                    8,
                    39,
                    4,
                    283,
                    0
                ],
                "published": "2025-06-24T09:00:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    0,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments\n  with a Hierarchical Spatial-Cognition Long-Short Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments\n  with a Hierarchical Spatial-Cognition Long-Short Memory System"
                },
                "summary": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav."
                },
                "authors": [
                    {
                        "name": "Lixuan He"
                    },
                    {
                        "name": "Haoyu Dong"
                    },
                    {
                        "name": "Zhenxing Chen"
                    },
                    {
                        "name": "Yangcheng Yu"
                    },
                    {
                        "name": "Jie Feng"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "arxiv_comment": "The paper is currently under investigation regarding concerns of\n  potential academic misconduct. While the investigation is ongoing, the\n  authors have voluntarily requested to withdraw the manuscript",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09309v1",
                "updated": "2025-10-10T12:01:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    12,
                    1,
                    16,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T12:01:16Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    12,
                    1,
                    16,
                    4,
                    283,
                    0
                ],
                "title": "Mask Tokens as Prophet: Fine-Grained Cache Eviction for Efficient dLLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mask Tokens as Prophet: Fine-Grained Cache Eviction for Efficient dLLM\n  Inference"
                },
                "summary": "Diffusion large language models (dLLMs) present a promising alternative to\ndominant autoregressive models (ARMs) by the ability of parallel decoding at\nthe expense of substantial computation and memory costs. Specifically, the\ncache mechanism for bidirectional attention in dLLMs demands large memory\nfootprint, restricting their ability to handle long contexts under\nresource-limited settings. Existing cache eviction strategies are designed for\nARMs and ignore the unique characteristics of dLLMs, thus leading to\nunsatisfactory performance. To address these challenges, we introduce MaskKV, a\ntraining-free cache eviction framework tailored to dLLMs, focusing on the\neffect of mask tokens in dLLMs. MaskKV is built on two key innovations: (1) a\nmask-query guided scoring mechanism that leverages attention weights to\nidentify and evict less critical prompt tokens for each head; (2) an adaptive\ncache budgeting strategy that improves efficiency by reducing allocation in\nintermediate layers and concentrating resources on prompt-preferring heads. On\nLLaDA with MaskKV, compressing the KV cache to only 256 pairs (less than 5% of\ntokens) retains 94% of the full-cache performance on LongBench and achieves up\nto 31x acceleration at 32k prompt length. The code is publicly available at:\nhttps://github.com/jianuo-huang/MaskKV",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion large language models (dLLMs) present a promising alternative to\ndominant autoregressive models (ARMs) by the ability of parallel decoding at\nthe expense of substantial computation and memory costs. Specifically, the\ncache mechanism for bidirectional attention in dLLMs demands large memory\nfootprint, restricting their ability to handle long contexts under\nresource-limited settings. Existing cache eviction strategies are designed for\nARMs and ignore the unique characteristics of dLLMs, thus leading to\nunsatisfactory performance. To address these challenges, we introduce MaskKV, a\ntraining-free cache eviction framework tailored to dLLMs, focusing on the\neffect of mask tokens in dLLMs. MaskKV is built on two key innovations: (1) a\nmask-query guided scoring mechanism that leverages attention weights to\nidentify and evict less critical prompt tokens for each head; (2) an adaptive\ncache budgeting strategy that improves efficiency by reducing allocation in\nintermediate layers and concentrating resources on prompt-preferring heads. On\nLLaDA with MaskKV, compressing the KV cache to only 256 pairs (less than 5% of\ntokens) retains 94% of the full-cache performance on LongBench and achieves up\nto 31x acceleration at 32k prompt length. The code is publicly available at:\nhttps://github.com/jianuo-huang/MaskKV"
                },
                "authors": [
                    {
                        "name": "Jianuo Huang"
                    },
                    {
                        "name": "Yaojie Zhang"
                    },
                    {
                        "name": "Yicun Yang"
                    },
                    {
                        "name": "Benhao Huang"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09182v1",
                "updated": "2025-10-10T09:24:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    9,
                    24,
                    53,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T09:24:53Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    9,
                    24,
                    53,
                    4,
                    283,
                    0
                ],
                "title": "Online Video Depth Anything: Temporally-Consistent Depth Prediction with\n  Low Memory Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Video Depth Anything: Temporally-Consistent Depth Prediction with\n  Low Memory Consumption"
                },
                "summary": "Depth estimation from monocular video has become a key component of many\nreal-world computer vision systems. Recently, Video Depth Anything (VDA) has\ndemonstrated strong performance on long video sequences. However, it relies on\nbatch-processing which prohibits its use in an online setting. In this work, we\novercome this limitation and introduce online VDA (oVDA). The key innovation is\nto employ techniques from Large Language Models (LLMs), namely, caching latent\nfeatures during inference and masking frames at training. Our oVDA method\noutperforms all competing online video depth estimation methods in both\naccuracy and VRAM usage. Low VRAM usage is particularly important for\ndeployment on edge devices. We demonstrate that oVDA runs at 42 FPS on an\nNVIDIA A100 and at 20 FPS on an NVIDIA Jetson edge device. We will release\nboth, code and compilation scripts, making oVDA easy to deploy on low-power\nhardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depth estimation from monocular video has become a key component of many\nreal-world computer vision systems. Recently, Video Depth Anything (VDA) has\ndemonstrated strong performance on long video sequences. However, it relies on\nbatch-processing which prohibits its use in an online setting. In this work, we\novercome this limitation and introduce online VDA (oVDA). The key innovation is\nto employ techniques from Large Language Models (LLMs), namely, caching latent\nfeatures during inference and masking frames at training. Our oVDA method\noutperforms all competing online video depth estimation methods in both\naccuracy and VRAM usage. Low VRAM usage is particularly important for\ndeployment on edge devices. We demonstrate that oVDA runs at 42 FPS on an\nNVIDIA A100 and at 20 FPS on an NVIDIA Jetson edge device. We will release\nboth, code and compilation scripts, making oVDA easy to deploy on low-power\nhardware."
                },
                "authors": [
                    {
                        "name": "Johann-Friedrich Feiden"
                    },
                    {
                        "name": "Tim Kchler"
                    },
                    {
                        "name": "Denis Zavadski"
                    },
                    {
                        "name": "Bogdan Savchynskyy"
                    },
                    {
                        "name": "Carsten Rother"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Rother"
                },
                "author": "Carsten Rother",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09154v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09154v1",
                "updated": "2025-10-10T08:57:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    8,
                    57,
                    16,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T08:57:16Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    8,
                    57,
                    16,
                    4,
                    283,
                    0
                ],
                "title": "Enhanced Breakdown and RF Performance in Field-Plated AlGaN/GaN HEMT for\n  High-Power Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Breakdown and RF Performance in Field-Plated AlGaN/GaN HEMT for\n  High-Power Applications"
                },
                "summary": "High Electron Mobility Transistors (HEMTs) are most suitable for harsh\nenvironments as they operate reliably under extreme conditions such as high\nvoltages, high temperatures, radiation exposure and corrosive atmospheres. In\nthis article, gate field-plated engineering Al0.295GaN/GaN HEMT is proposed for\nachieving high breakdown voltage to reliably operate in harsh environments. The\nAl0.295GaN/GaN heterointerface results in a 2DEG (two-dimensional electron gas)\ndensity of the order of 1013 cm-2 obtained from the self-consistent solution of\nSchr\\\"odinger and Poisson equations. The device has undergone DC and breakdown\nsimulations which result in threshold voltage of -5.5 V, drain saturation\ncurrent of 3000 mA, and breakdown voltage of 1 kV. The HEMT also shows\nexcellent RF characteristics which include cut-off frequency (ft) of 28 GHz and\nmaximum frequency of oscillation (fmax) of 38 GHz. The proposed gate\nfield-plated HEMT is stable up to 40 GHz and suitable for high-voltage and\nhigh-power RF operation during harsh environment applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Electron Mobility Transistors (HEMTs) are most suitable for harsh\nenvironments as they operate reliably under extreme conditions such as high\nvoltages, high temperatures, radiation exposure and corrosive atmospheres. In\nthis article, gate field-plated engineering Al0.295GaN/GaN HEMT is proposed for\nachieving high breakdown voltage to reliably operate in harsh environments. The\nAl0.295GaN/GaN heterointerface results in a 2DEG (two-dimensional electron gas)\ndensity of the order of 1013 cm-2 obtained from the self-consistent solution of\nSchr\\\"odinger and Poisson equations. The device has undergone DC and breakdown\nsimulations which result in threshold voltage of -5.5 V, drain saturation\ncurrent of 3000 mA, and breakdown voltage of 1 kV. The HEMT also shows\nexcellent RF characteristics which include cut-off frequency (ft) of 28 GHz and\nmaximum frequency of oscillation (fmax) of 38 GHz. The proposed gate\nfield-plated HEMT is stable up to 40 GHz and suitable for high-voltage and\nhigh-power RF operation during harsh environment applications."
                },
                "authors": [
                    {
                        "name": "Tanjim Rahman"
                    },
                    {
                        "name": "Trupti Ranjan Lenka"
                    }
                ],
                "author_detail": {
                    "name": "Trupti Ranjan Lenka"
                },
                "author": "Trupti Ranjan Lenka",
                "arxiv_comment": "13 pages, 13 figures including DC, RF, and breakdown analysis of\n  field-plated AlGaN/GaN HEMT using TCAD simulations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09154v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09025v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09025v3",
                "updated": "2025-10-09T20:37:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    20,
                    37,
                    43,
                    3,
                    282,
                    0
                ],
                "published": "2025-07-11T21:19:18Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    21,
                    19,
                    18,
                    4,
                    192,
                    0
                ],
                "title": "Lizard: An Efficient Linearization Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lizard: An Efficient Linearization Framework for Large Language Models"
                },
                "summary": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into subquadratic architectures.\nTransformers faces severe computational and memory bottlenecks with long\nsequences due to the quadratic complexity of softmax attention and the growing\nKey-Value (KV) cache that makes inference memory-bound by context length.\nLizard addresses these limitations by introducing a subquadratic attention\nmechanism that closely approximates softmax attention while preserving model\nquality. Unlike prior linearization methods constrained by fixed, non-adaptive\nstructures, Lizard augments the architecture with compact, learnable modules\nthat enable adaptive memory control and robust length generalization. Moreover,\nwe introduce a hardwareaware algorithm that solves numerical instability in\ngated attention to accelerate training. Extensive experiments show that Lizard\nachieves near-lossless recovery of its teacher model's performance,\nsignificantly outperforming previous methods by up to 9.4 - 24.5 points on the\n5-shot MMLU benchmark and demonstrating superior associative recall.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into subquadratic architectures.\nTransformers faces severe computational and memory bottlenecks with long\nsequences due to the quadratic complexity of softmax attention and the growing\nKey-Value (KV) cache that makes inference memory-bound by context length.\nLizard addresses these limitations by introducing a subquadratic attention\nmechanism that closely approximates softmax attention while preserving model\nquality. Unlike prior linearization methods constrained by fixed, non-adaptive\nstructures, Lizard augments the architecture with compact, learnable modules\nthat enable adaptive memory control and robust length generalization. Moreover,\nwe introduce a hardwareaware algorithm that solves numerical instability in\ngated attention to accelerate training. Extensive experiments show that Lizard\nachieves near-lossless recovery of its teacher model's performance,\nsignificantly outperforming previous methods by up to 9.4 - 24.5 points on the\n5-shot MMLU benchmark and demonstrating superior associative recall."
                },
                "authors": [
                    {
                        "name": "Chien Van Nguyen"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Hanieh Deilamsalehy"
                    },
                    {
                        "name": "Puneet Mathur"
                    },
                    {
                        "name": "Viet Dac Lai"
                    },
                    {
                        "name": "Haoliang Wang"
                    },
                    {
                        "name": "Jayakumar Subramanian"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Trung Bui"
                    },
                    {
                        "name": "Nikos Vlassis"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Thien Huu Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Thien Huu Nguyen"
                },
                "author": "Thien Huu Nguyen",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09025v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09025v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08803v1",
                "updated": "2025-10-09T20:35:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    20,
                    35,
                    0,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T20:35:00Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    20,
                    35,
                    0,
                    3,
                    282,
                    0
                ],
                "title": "Man-Made Heuristics Are Dead. Long Live Code Generators!",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Man-Made Heuristics Are Dead. Long Live Code Generators!"
                },
                "summary": "Policy design for various systems controllers has conventionally been a\nmanual process, with domain experts carefully tailoring heuristics for the\nspecific instance in which the policy will be deployed. In this paper, we\nre-imagine policy design via a novel automated search technique fueled by\nrecent advances in generative models, specifically Large Language Model\n(LLM)-driven code generation. We outline the design and implementation of\nPolicySmith, a framework that applies LLMs to synthesize instance-optimal\nheuristics. We apply PolicySmith to two long-standing systems policies - web\ncaching and congestion control, highlighting the opportunities unraveled by\nthis LLM-driven heuristic search. For caching, PolicySmith discovers heuristics\nthat outperform established baselines on standard open-source traces. For\ncongestion control, we show that PolicySmith can generate safe policies that\nintegrate directly into the Linux kernel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Policy design for various systems controllers has conventionally been a\nmanual process, with domain experts carefully tailoring heuristics for the\nspecific instance in which the policy will be deployed. In this paper, we\nre-imagine policy design via a novel automated search technique fueled by\nrecent advances in generative models, specifically Large Language Model\n(LLM)-driven code generation. We outline the design and implementation of\nPolicySmith, a framework that applies LLMs to synthesize instance-optimal\nheuristics. We apply PolicySmith to two long-standing systems policies - web\ncaching and congestion control, highlighting the opportunities unraveled by\nthis LLM-driven heuristic search. For caching, PolicySmith discovers heuristics\nthat outperform established baselines on standard open-source traces. For\ncongestion control, we show that PolicySmith can generate safe policies that\nintegrate directly into the Linux kernel."
                },
                "authors": [
                    {
                        "name": "Rohit Dwivedula"
                    },
                    {
                        "name": "Divyanshu Saxena"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Swarat Chaudhuri"
                    },
                    {
                        "name": "Daehyeok Kim"
                    }
                ],
                "author_detail": {
                    "name": "Daehyeok Kim"
                },
                "author": "Daehyeok Kim",
                "arxiv_comment": "10 pages, 2 figures, 2 tables. To be presented at HotNets 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08774v1",
                "updated": "2025-10-09T19:45:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    19,
                    45,
                    54,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T19:45:54Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    19,
                    45,
                    54,
                    3,
                    282,
                    0
                ],
                "title": "Struc-EMB: The Potential of Structure-Aware Encoding in Language\n  Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Struc-EMB: The Potential of Structure-Aware Encoding in Language\n  Embeddings"
                },
                "summary": "Text embeddings from Large Language Models (LLMs) have become foundational\nfor numerous applications. However, these models typically operate on raw text,\noverlooking the rich structural information, such as hyperlinks or citations,\nthat provides crucial context in many real-world datasets. This paper\nintroduces and systematically evaluates a new paradigm for generating\nstructure-aware text embeddings by integrating these structural relations\ndirectly into the LLM's internal encoding process, rather than relying on\ntraditional post-hoc aggregation. We investigate two primary in-process\nmethods: sequential concatenation and parallel caching. Through extensive\nzero-shot experiments across retrieval, clustering, classification, and\nrecommendation tasks, we demonstrate that our structure-aware approaches\nconsistently outperform both text-only and post-hoc baselines. Our analysis\nreveals critical trade-offs: sequential concatenation excels with noisy,\nmoderate-length contexts, while parallel caching scales more effectively to\nlong, high-signal contexts but is more susceptible to distractors. To address\nthe challenge of noisy structural data, we also introduce and validate two\neffective techniques: Context Distillation and Semantic Balancing. This work\nprovides the first comprehensive analysis of in-process structure-aware\nencoding, offering a blueprint for building more powerful and contextually\naware embedding models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text embeddings from Large Language Models (LLMs) have become foundational\nfor numerous applications. However, these models typically operate on raw text,\noverlooking the rich structural information, such as hyperlinks or citations,\nthat provides crucial context in many real-world datasets. This paper\nintroduces and systematically evaluates a new paradigm for generating\nstructure-aware text embeddings by integrating these structural relations\ndirectly into the LLM's internal encoding process, rather than relying on\ntraditional post-hoc aggregation. We investigate two primary in-process\nmethods: sequential concatenation and parallel caching. Through extensive\nzero-shot experiments across retrieval, clustering, classification, and\nrecommendation tasks, we demonstrate that our structure-aware approaches\nconsistently outperform both text-only and post-hoc baselines. Our analysis\nreveals critical trade-offs: sequential concatenation excels with noisy,\nmoderate-length contexts, while parallel caching scales more effectively to\nlong, high-signal contexts but is more susceptible to distractors. To address\nthe challenge of noisy structural data, we also introduce and validate two\neffective techniques: Context Distillation and Semantic Balancing. This work\nprovides the first comprehensive analysis of in-process structure-aware\nencoding, offering a blueprint for building more powerful and contextually\naware embedding models."
                },
                "authors": [
                    {
                        "name": "Shikun Liu"
                    },
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Mufei Li"
                    },
                    {
                        "name": "Pan Li"
                    }
                ],
                "author_detail": {
                    "name": "Pan Li"
                },
                "author": "Pan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08525v1",
                "updated": "2025-10-09T17:50:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    50,
                    0,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:50:00Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    50,
                    0,
                    3,
                    282,
                    0
                ],
                "title": "Which Heads Matter for Reasoning? RL-Guided KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which Heads Matter for Reasoning? RL-Guided KV Cache Compression"
                },
                "summary": "Reasoning large language models exhibit complex reasoning behaviors through\nthe extended chain-of-thought generation, creating unprecedented Key-Value (KV)\ncache overhead during the decoding phase. Existing KV cache compression methods\nunderperform on reasoning models: token-dropping methods break reasoning\nintegrity by discarding critical information, while head-reallocating methods\nmistakenly compress reasoning-critical heads since they are designed for\nretrieval tasks, resulting in significant performance degradation as\ncompression rates increase. We hypothesize that KV heads exhibit functional\nheterogeneity in reasoning models-some heads are critical for chain-of-thought\nconsistency while others are compressible. To validate and exploit this\ninsight, we propose RLKV, a novel reasoning-critical head identification\nframework, which uses reinforcement learning to directly optimize the\nrelationship between each head's cache usage and reasoning quality. As RLKV\nproduces rewards from actual generated samples during training, it naturally\nidentifies heads relevant to reasoning behaviors. We then allocate full KV\ncache to these heads while applying compressed constant KV cache to others for\nefficient inference. Our experiments reveal that only a small fraction of\nattention heads is essential for reasoning, enabling our KV compression\napproach to outperform baseline methods while achieving 20-50% cache reduction\nwith near lossless performance compared to uncompressed results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning large language models exhibit complex reasoning behaviors through\nthe extended chain-of-thought generation, creating unprecedented Key-Value (KV)\ncache overhead during the decoding phase. Existing KV cache compression methods\nunderperform on reasoning models: token-dropping methods break reasoning\nintegrity by discarding critical information, while head-reallocating methods\nmistakenly compress reasoning-critical heads since they are designed for\nretrieval tasks, resulting in significant performance degradation as\ncompression rates increase. We hypothesize that KV heads exhibit functional\nheterogeneity in reasoning models-some heads are critical for chain-of-thought\nconsistency while others are compressible. To validate and exploit this\ninsight, we propose RLKV, a novel reasoning-critical head identification\nframework, which uses reinforcement learning to directly optimize the\nrelationship between each head's cache usage and reasoning quality. As RLKV\nproduces rewards from actual generated samples during training, it naturally\nidentifies heads relevant to reasoning behaviors. We then allocate full KV\ncache to these heads while applying compressed constant KV cache to others for\nefficient inference. Our experiments reveal that only a small fraction of\nattention heads is essential for reasoning, enabling our KV compression\napproach to outperform baseline methods while achieving 20-50% cache reduction\nwith near lossless performance compared to uncompressed results."
                },
                "authors": [
                    {
                        "name": "Wenjie Du"
                    },
                    {
                        "name": "Li Jiang"
                    },
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Xue Liu"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06493v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06493v2",
                "updated": "2025-10-09T17:45:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    45,
                    50,
                    3,
                    282,
                    0
                ],
                "published": "2025-09-08T09:54:18Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    54,
                    18,
                    0,
                    251,
                    0
                ],
                "title": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers"
                },
                "summary": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search."
                },
                "authors": [
                    {
                        "name": "Ran Xin"
                    },
                    {
                        "name": "Zeyu Zheng"
                    },
                    {
                        "name": "Yanchen Nie"
                    },
                    {
                        "name": "Kun Yuan"
                    },
                    {
                        "name": "Xia Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Xia Xiao"
                },
                "author": "Xia Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06493v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06493v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18085v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18085v2",
                "updated": "2025-10-09T17:38:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    38,
                    52,
                    3,
                    282,
                    0
                ],
                "published": "2025-09-22T17:58:21Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    58,
                    21,
                    0,
                    265,
                    0
                ],
                "title": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative\n  Decoding"
                },
                "summary": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$."
                },
                "authors": [
                    {
                        "name": "Sudhanshu Agrawal"
                    },
                    {
                        "name": "Risheek Garrepalli"
                    },
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Christopher Lott"
                    },
                    {
                        "name": "Fatih Porikli"
                    }
                ],
                "author_detail": {
                    "name": "Fatih Porikli"
                },
                "author": "Fatih Porikli",
                "arxiv_comment": "Original version uploaded on Sep 22, 2025. (v2): Extended Table 2\n  with additional analysis and referenced it in Sec 5.2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18085v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18085v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08669v1",
                "updated": "2025-10-09T17:22:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    22,
                    23,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:22:23Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    22,
                    23,
                    3,
                    282,
                    0
                ],
                "title": "FreqCa: Accelerating Diffusion Models via Frequency-Aware Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreqCa: Accelerating Diffusion Models via Frequency-Aware Caching"
                },
                "summary": "The application of diffusion transformers is suffering from their significant\ninference costs. Recently, feature caching has been proposed to solve this\nproblem by reusing features from previous timesteps, thereby skipping\ncomputation in future timesteps. However, previous feature caching assumes that\nfeatures in adjacent timesteps are similar or continuous, which does not always\nhold in all settings. To investigate this, this paper begins with an analysis\nfrom the frequency domain, which reveal that different frequency bands in the\nfeatures of diffusion models exhibit different dynamics across timesteps.\nConcretely, low-frequency components, which decide the structure of images,\nexhibit higher similarity but poor continuity. In contrast, the high-frequency\nbands, which decode the details of images, show significant continuity but poor\nsimilarity. These interesting observations motivate us to propose\nFrequency-aware Caching (FreqCa)\n  which directly reuses features of low-frequency components based on their\nsimilarity, while using a second-order Hermite interpolator to predict the\nvolatile high-frequency ones based on its continuity.\n  Besides, we further propose to cache Cumulative Residual Feature (CRF)\ninstead of the features in all the layers, which reduces the memory footprint\nof feature caching by 99%.\n  Extensive experiments on FLUX.1-dev, FLUX.1-Kontext-dev, Qwen-Image, and\nQwen-Image-Edit demonstrate its effectiveness in both generation and editing.\nCodes are available in the supplementary materials and will be released on\nGitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The application of diffusion transformers is suffering from their significant\ninference costs. Recently, feature caching has been proposed to solve this\nproblem by reusing features from previous timesteps, thereby skipping\ncomputation in future timesteps. However, previous feature caching assumes that\nfeatures in adjacent timesteps are similar or continuous, which does not always\nhold in all settings. To investigate this, this paper begins with an analysis\nfrom the frequency domain, which reveal that different frequency bands in the\nfeatures of diffusion models exhibit different dynamics across timesteps.\nConcretely, low-frequency components, which decide the structure of images,\nexhibit higher similarity but poor continuity. In contrast, the high-frequency\nbands, which decode the details of images, show significant continuity but poor\nsimilarity. These interesting observations motivate us to propose\nFrequency-aware Caching (FreqCa)\n  which directly reuses features of low-frequency components based on their\nsimilarity, while using a second-order Hermite interpolator to predict the\nvolatile high-frequency ones based on its continuity.\n  Besides, we further propose to cache Cumulative Residual Feature (CRF)\ninstead of the features in all the layers, which reduces the memory footprint\nof feature caching by 99%.\n  Extensive experiments on FLUX.1-dev, FLUX.1-Kontext-dev, Qwen-Image, and\nQwen-Image-Edit demonstrate its effectiveness in both generation and editing.\nCodes are available in the supplementary materials and will be released on\nGitHub."
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Peiliang Cai"
                    },
                    {
                        "name": "Qinming Zhou"
                    },
                    {
                        "name": "Yuqi Lin"
                    },
                    {
                        "name": "Deyang Kong"
                    },
                    {
                        "name": "Benhao Huang"
                    },
                    {
                        "name": "Yupei Pan"
                    },
                    {
                        "name": "Haowen Xu"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Junshu Tang"
                    },
                    {
                        "name": "Shikang Zheng"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "15 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08351v1",
                "updated": "2025-10-09T15:38:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    38,
                    13,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T15:38:13Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    38,
                    13,
                    3,
                    282,
                    0
                ],
                "title": "FMCache: File-System Metadata Caching in Programmable Switches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FMCache: File-System Metadata Caching in Programmable Switches"
                },
                "summary": "Fast and scalable metadata management across multiple metadata servers is\ncrucial for distributed file systems to handle numerous files and directories.\nClient-side caching of frequently accessed metadata can mitigate server loads,\nbut incurs significant overhead and complexity in maintaining cache consistency\nwhen the number of clients increases. We propose FMCache, an in-switch\nfile-system metadata caching framework that leverages programmable switches to\nserve file-system metadata requests from multiple clients directly in the\nswitch data plane. Unlike prior in-switch key-value caching approaches, FMCache\naddresses file-system-specific path dependencies under stringent switch\nresource constraints. We implement FMCache atop Hadoop HDFS and evaluate it on\na Tofino-switch testbed using real-world file-system metadata workloads.\nFMCache achieves up to 181.6% higher throughput than vanilla HDFS and\ncomplements client-side caching with additional throughput gains of up to\n139.6%. It also incurs low latencies and limited switch resource usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and scalable metadata management across multiple metadata servers is\ncrucial for distributed file systems to handle numerous files and directories.\nClient-side caching of frequently accessed metadata can mitigate server loads,\nbut incurs significant overhead and complexity in maintaining cache consistency\nwhen the number of clients increases. We propose FMCache, an in-switch\nfile-system metadata caching framework that leverages programmable switches to\nserve file-system metadata requests from multiple clients directly in the\nswitch data plane. Unlike prior in-switch key-value caching approaches, FMCache\naddresses file-system-specific path dependencies under stringent switch\nresource constraints. We implement FMCache atop Hadoop HDFS and evaluate it on\na Tofino-switch testbed using real-world file-system metadata workloads.\nFMCache achieves up to 181.6% higher throughput than vanilla HDFS and\ncomplements client-side caching with additional throughput gains of up to\n139.6%. It also incurs low latencies and limited switch resource usage."
                },
                "authors": [
                    {
                        "name": "Qingxiu Liu"
                    },
                    {
                        "name": "Jiazhen Cai"
                    },
                    {
                        "name": "Siyuan Sheng"
                    },
                    {
                        "name": "Yuhui Chen"
                    },
                    {
                        "name": "Lu Tang"
                    },
                    {
                        "name": "Zhirong Shen"
                    },
                    {
                        "name": "Patrick P. C. Lee"
                    }
                ],
                "author_detail": {
                    "name": "Patrick P. C. Lee"
                },
                "arxiv_affiliation": "The Chinese University of Hong Kong",
                "author": "Patrick P. C. Lee",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08180v1",
                "updated": "2025-10-09T13:06:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    13,
                    6,
                    16,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T13:06:16Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    13,
                    6,
                    16,
                    3,
                    282,
                    0
                ],
                "title": "Towards Energy-Efficient Serverless Computing with Hardware Isolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Energy-Efficient Serverless Computing with Hardware Isolation"
                },
                "summary": "Serverless computing provides just-in-time infrastructure provisioning with\nrapid elasticity and a finely-grained pricing model. As full control of\nresource allocation is in the hands of the cloud provider and applications only\nconsume resources when they actually perform work, we believe that serverless\ncomputing is uniquely positioned to maximize energy efficiency.\n  However, the focus of current serverless platforms is to run hundreds or\nthousands of serverless functions from different tenants on traditional server\nhardware, requiring expensive software isolation mechanisms and a high degree\nof overprovisioning, i.e., idle servers, to anticipate load spikes. With shared\ncaches, high clock frequencies, and many-core architectures, servers today are\noptimized for large, singular workloads but not to run thousands of isolated\nfunctions.\n  We propose rethinking the serverless hardware architecture to align it with\nthe requirements of serverless software. Specifically, we propose using\nhardware isolation with individual processors per function instead of software\nisolation resulting in a serverless hardware stack that consumes energy only\nwhen an application actually performs work. In preliminary evaluation with real\nhardware and a typical serverless workload we find that this could reduce\nenergy consumption overheads by 90.63% or an average 70.8MW.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing provides just-in-time infrastructure provisioning with\nrapid elasticity and a finely-grained pricing model. As full control of\nresource allocation is in the hands of the cloud provider and applications only\nconsume resources when they actually perform work, we believe that serverless\ncomputing is uniquely positioned to maximize energy efficiency.\n  However, the focus of current serverless platforms is to run hundreds or\nthousands of serverless functions from different tenants on traditional server\nhardware, requiring expensive software isolation mechanisms and a high degree\nof overprovisioning, i.e., idle servers, to anticipate load spikes. With shared\ncaches, high clock frequencies, and many-core architectures, servers today are\noptimized for large, singular workloads but not to run thousands of isolated\nfunctions.\n  We propose rethinking the serverless hardware architecture to align it with\nthe requirements of serverless software. Specifically, we propose using\nhardware isolation with individual processors per function instead of software\nisolation resulting in a serverless hardware stack that consumes energy only\nwhen an application actually performs work. In preliminary evaluation with real\nhardware and a typical serverless workload we find that this could reduce\nenergy consumption overheads by 90.63% or an average 70.8MW."
                },
                "authors": [
                    {
                        "name": "Natalie Carl"
                    },
                    {
                        "name": "Tobias Pfandzelter"
                    },
                    {
                        "name": "David Bermbach"
                    }
                ],
                "author_detail": {
                    "name": "David Bermbach"
                },
                "author": "David Bermbach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26541v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26541v2",
                "updated": "2025-10-09T13:03:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    13,
                    3,
                    29,
                    3,
                    282,
                    0
                ],
                "published": "2025-09-30T17:15:27Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    15,
                    27,
                    1,
                    273,
                    0
                ],
                "title": "TASP: Topology-aware Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TASP: Topology-aware Sequence Parallelism"
                },
                "summary": "Long-context large language models (LLMs) face constraints due to the\nquadratic complexity of the self-attention mechanism. The mainstream sequence\nparallelism (SP) method, Ring Attention, attempts to solve this by distributing\nthe query into multiple query chunks across accelerators and enable each Q\ntensor to access all KV tensors from other accelerators via the Ring AllGather\ncommunication primitive. However, it exhibits low communication efficiency,\nrestricting its practical applicability. This inefficiency stems from the\nmismatch between the Ring AllGather communication primitive it adopts and the\nAlltoAll topology of modern accelerators. A Ring AllGather primitive is\ncomposed of iterations of ring-styled data transfer, which can only utilize a\nvery limited fraction of an AlltoAll topology.\n  Inspired by the Hamiltonian decomposition of complete directed graphs, we\nidentify that modern accelerator topology can be decomposed into multiple\northogonal ring datapaths which can concurrently transfer data without\ninterference. Based on this, we further observe that the Ring AllGather\nprimitive can also be decomposed into the same number of concurrent ring-styled\ndata transfer at every iteration. Based on these insights, we propose TASP, a\ntopology-aware SP method for long-context LLMs that fully utilizes the\ncommunication capacity of modern accelerators via topology decomposition and\nprimitive decomposition. Experimental results on both single-node and\nmulti-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate\nthat TASP achieves higher communication efficiency than Ring Attention on these\nmodern accelerator topologies and achieves up to 3.58 speedup than Ring\nAttention and its variant Zigzag-Ring Attention. The code is available at\nhttps://github.com/infinigence/HamiltonAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context large language models (LLMs) face constraints due to the\nquadratic complexity of the self-attention mechanism. The mainstream sequence\nparallelism (SP) method, Ring Attention, attempts to solve this by distributing\nthe query into multiple query chunks across accelerators and enable each Q\ntensor to access all KV tensors from other accelerators via the Ring AllGather\ncommunication primitive. However, it exhibits low communication efficiency,\nrestricting its practical applicability. This inefficiency stems from the\nmismatch between the Ring AllGather communication primitive it adopts and the\nAlltoAll topology of modern accelerators. A Ring AllGather primitive is\ncomposed of iterations of ring-styled data transfer, which can only utilize a\nvery limited fraction of an AlltoAll topology.\n  Inspired by the Hamiltonian decomposition of complete directed graphs, we\nidentify that modern accelerator topology can be decomposed into multiple\northogonal ring datapaths which can concurrently transfer data without\ninterference. Based on this, we further observe that the Ring AllGather\nprimitive can also be decomposed into the same number of concurrent ring-styled\ndata transfer at every iteration. Based on these insights, we propose TASP, a\ntopology-aware SP method for long-context LLMs that fully utilizes the\ncommunication capacity of modern accelerators via topology decomposition and\nprimitive decomposition. Experimental results on both single-node and\nmulti-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate\nthat TASP achieves higher communication efficiency than Ring Attention on these\nmodern accelerator topologies and achieves up to 3.58 speedup than Ring\nAttention and its variant Zigzag-Ring Attention. The code is available at\nhttps://github.com/infinigence/HamiltonAttention."
                },
                "authors": [
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ke Hong"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Wenxun Wang"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "arxiv_affiliation": "Tsinghua University",
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26541v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26541v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v4",
                "updated": "2025-10-09T12:05:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    12,
                    5,
                    4,
                    3,
                    282,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Tianfu Wang"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "arxiv_comment": "Accepted by EMNLP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00566v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00566v2",
                "updated": "2025-10-09T12:01:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    12,
                    1,
                    20,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-01T06:38:45Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    6,
                    38,
                    45,
                    2,
                    274,
                    0
                ],
                "title": "Panorama: Fast-Track Nearest Neighbors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panorama: Fast-Track Nearest Neighbors"
                },
                "summary": "Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose\nembeddings are close to that of a given query in a high-dimensional space,\naiming to balance accuracy with speed. Used in recommendation systems, image\nand video retrieval, natural language processing, and retrieval-augmented\ngeneration (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT\nutilize graph, tree, clustering, and quantization techniques to navigate large\nvector spaces. Despite this progress, ANNS systems spend up to 99\\% of query\ntime to compute distances in their final refinement phase. In this paper, we\npresent PANORAMA, a machine learning-driven approach that tackles the ANNS\nverification bottleneck through data-adaptive learned orthogonal transforms\nthat facilitate the accretive refinement of distance bounds. Such transforms\ncompact over 90\\% of signal energy into the first half of dimensions, enabling\nearly candidate pruning with partial distance computations. We integrate\nPANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and\nAnnoy, without index modification, using level-major memory layouts,\nSIMD-vectorized partial distance computations, and cache-aware access patterns.\nExperiments across diverse datasets -- from image-based CIFAR-10 and GIST to\nmodern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate\nthat PANORAMA affords a 2--30$\\times$ end-to-end speedup with no recall loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose\nembeddings are close to that of a given query in a high-dimensional space,\naiming to balance accuracy with speed. Used in recommendation systems, image\nand video retrieval, natural language processing, and retrieval-augmented\ngeneration (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT\nutilize graph, tree, clustering, and quantization techniques to navigate large\nvector spaces. Despite this progress, ANNS systems spend up to 99\\% of query\ntime to compute distances in their final refinement phase. In this paper, we\npresent PANORAMA, a machine learning-driven approach that tackles the ANNS\nverification bottleneck through data-adaptive learned orthogonal transforms\nthat facilitate the accretive refinement of distance bounds. Such transforms\ncompact over 90\\% of signal energy into the first half of dimensions, enabling\nearly candidate pruning with partial distance computations. We integrate\nPANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and\nAnnoy, without index modification, using level-major memory layouts,\nSIMD-vectorized partial distance computations, and cache-aware access patterns.\nExperiments across diverse datasets -- from image-based CIFAR-10 and GIST to\nmodern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate\nthat PANORAMA affords a 2--30$\\times$ end-to-end speedup with no recall loss."
                },
                "authors": [
                    {
                        "name": "Vansh Ramani"
                    },
                    {
                        "name": "Alexis Schlomer"
                    },
                    {
                        "name": "Akash Nayar"
                    },
                    {
                        "name": "Panagiotis Karras"
                    },
                    {
                        "name": "Sayan Ranu"
                    },
                    {
                        "name": "Jignesh M. Patel"
                    }
                ],
                "author_detail": {
                    "name": "Jignesh M. Patel"
                },
                "author": "Jignesh M. Patel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00566v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00566v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19740v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19740v4",
                "updated": "2025-10-09T09:33:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    9,
                    33,
                    47,
                    3,
                    282,
                    0
                ],
                "published": "2025-08-27T10:11:27Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    11,
                    27,
                    2,
                    239,
                    0
                ],
                "title": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval"
                },
                "summary": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding."
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Haiyuan Wan"
                    },
                    {
                        "name": "Ziyang Gong"
                    },
                    {
                        "name": "Fei Chao"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19740v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19740v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23488v2",
                "updated": "2025-10-09T09:14:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    9,
                    14,
                    43,
                    3,
                    282,
                    0
                ],
                "published": "2025-06-30T03:22:32Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    3,
                    22,
                    32,
                    0,
                    181,
                    0
                ],
                "title": "Generative AI-enhanced Low-Altitude UAV-Mounted Stacked Intelligent\n  Metasurfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI-enhanced Low-Altitude UAV-Mounted Stacked Intelligent\n  Metasurfaces"
                },
                "summary": "Wireless communication systems face challenges in meeting the demand for\nhigher data rates and reliable connectivity in complex environments. Stacked\nintelligent metasurfaces (SIMs) have emerged as a promising technology for\nadvanced wave-domain signal processing, where mobile SIMs can outperform fixed\ncounterparts. In this paper, we propose a novel unmanned aerial vehicle\n(UAV)-mounted SIM (UAV-SIM) assisted communication system within low-altitude\neconomy (LAE) networks, where UAVs act as both cache-enabled base stations and\nmobile SIM carriers to enhance uplink transmissions. To maximize network\ncapacity, we formulate a UAV-SIM-based joint optimization problem (USBJOP) that\nintegrates user association, UAV-SIM three-dimensional positioning, and\nmulti-layer SIM phase shift design. Due to the non-convexity and NP-hardness of\nUSBJOP, we decompose it into three subproblems, which are the association\nbetween UAV-SIMs and users optimization problem (AUUOP), the UAV location\noptimization problem (ULOP), and the UAV-SIM phase shifts optimization problem\n(USPSOP). Then, we solve them through an alternating optimization strategy.\nSpecifically, AUUOP and ULOP are transformed into convex forms solvable via the\nCVX tool, while USPSOP is addressed by a generative artificial intelligence\n(GAI)-based hybrid optimization algorithm. Simulation results show that the\nproposed approach achieves approximately 1.5 times higher network capacity\ncompared with suboptimal schemes, effectively mitigates multi-user interference\nwith increasing SIM layers and meta-atoms, and reduces runtime by 10\\% while\nmaintaining solution quality, thereby demonstrating its practicality for\nreal-world deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless communication systems face challenges in meeting the demand for\nhigher data rates and reliable connectivity in complex environments. Stacked\nintelligent metasurfaces (SIMs) have emerged as a promising technology for\nadvanced wave-domain signal processing, where mobile SIMs can outperform fixed\ncounterparts. In this paper, we propose a novel unmanned aerial vehicle\n(UAV)-mounted SIM (UAV-SIM) assisted communication system within low-altitude\neconomy (LAE) networks, where UAVs act as both cache-enabled base stations and\nmobile SIM carriers to enhance uplink transmissions. To maximize network\ncapacity, we formulate a UAV-SIM-based joint optimization problem (USBJOP) that\nintegrates user association, UAV-SIM three-dimensional positioning, and\nmulti-layer SIM phase shift design. Due to the non-convexity and NP-hardness of\nUSBJOP, we decompose it into three subproblems, which are the association\nbetween UAV-SIMs and users optimization problem (AUUOP), the UAV location\noptimization problem (ULOP), and the UAV-SIM phase shifts optimization problem\n(USPSOP). Then, we solve them through an alternating optimization strategy.\nSpecifically, AUUOP and ULOP are transformed into convex forms solvable via the\nCVX tool, while USPSOP is addressed by a generative artificial intelligence\n(GAI)-based hybrid optimization algorithm. Simulation results show that the\nproposed approach achieves approximately 1.5 times higher network capacity\ncompared with suboptimal schemes, effectively mitigates multi-user interference\nwith increasing SIM layers and meta-atoms, and reduces runtime by 10\\% while\nmaintaining solution quality, thereby demonstrating its practicality for\nreal-world deployments."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Mingzhe Fan"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Hongyang Pan"
                    },
                    {
                        "name": "Jiahui Li"
                    },
                    {
                        "name": "Chuang Zhang"
                    },
                    {
                        "name": "Linyao Li"
                    },
                    {
                        "name": "Changyuan Zhao"
                    },
                    {
                        "name": "Chau Yuen"
                    }
                ],
                "author_detail": {
                    "name": "Chau Yuen"
                },
                "author": "Chau Yuen",
                "arxiv_comment": "This paper has been already submitted to TCCN",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07467v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07467v4",
                "updated": "2025-10-09T02:37:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    2,
                    37,
                    26,
                    3,
                    282,
                    0
                ],
                "published": "2024-06-11T17:13:18Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    17,
                    13,
                    18,
                    1,
                    163,
                    0
                ],
                "title": "LLM meets ML: Data-efficient Anomaly Detection on Unstable Logs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM meets ML: Data-efficient Anomaly Detection on Unstable Logs"
                },
                "summary": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for \\task, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\n(pp) in F1 score while using much less labeled data (62.87 pp reduction). When\ntrained on the same amount of data as the baselines, FlexLog achieves up to a\n13 pp increase in F1 score on ADFA-U across varying training dataset sizes.\nAdditionally, FlexLog maintains inference time under one second per log\nsequence, making it suitable for most applications, except latency-sensitive\nsystems. Further analysis reveals the positive impact of FlexLog's key\ncomponents: cache, RAG and ensemble learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for \\task, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\n(pp) in F1 score while using much less labeled data (62.87 pp reduction). When\ntrained on the same amount of data as the baselines, FlexLog achieves up to a\n13 pp increase in F1 score on ADFA-U across varying training dataset sizes.\nAdditionally, FlexLog maintains inference time under one second per log\nsequence, making it suitable for most applications, except latency-sensitive\nsystems. Further analysis reveals the positive impact of FlexLog's key\ncomponents: cache, RAG and ensemble learning."
                },
                "authors": [
                    {
                        "name": "Fatemeh Hadadi"
                    },
                    {
                        "name": "Qinghua Xu"
                    },
                    {
                        "name": "Domenico Bianculli"
                    },
                    {
                        "name": "Lionel Briand"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Briand"
                },
                "author": "Lionel Briand",
                "arxiv_doi": "10.1145/3771283",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3771283",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.07467v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07467v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ACM Transactions on Software Engineering and Methodology\n  (TOSEM) 2025",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21467v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21467v2",
                "updated": "2025-10-09T01:43:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    1,
                    43,
                    2,
                    3,
                    282,
                    0
                ],
                "published": "2025-05-27T17:39:39Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    39,
                    39,
                    1,
                    147,
                    0
                ],
                "title": "FlashDLM: Accelerating Diffusion Language Model Inference via Efficient\n  KV Caching and Guided Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashDLM: Accelerating Diffusion Language Model Inference via Efficient\n  KV Caching and Guided Diffusion"
                },
                "summary": "Diffusion language models offer parallel token generation and inherent\nbidirectionality, promising more efficient and powerful sequence modeling\ncompared to autoregressive approaches. However, state-of-the-art diffusion\nmodels (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match\nthe quality of similarly sized autoregressive (AR) models (e.g., Qwen2.5 7B,\nLlama3 8B), their iterative denoising requires multiple full-sequence forward\npasses, resulting in high computational costs and latency, particularly for\nlong input prompts and long-context scenarios. Furthermore, parallel token\ngeneration introduces token incoherence problems, and current sampling\nheuristics suffer from significant quality drops with decreasing denoising\nsteps. We address these limitations with two training-free techniques. First,\nwe propose FreeCache, a Key-Value (KV) approximation caching technique that\nreuses stable KV projections across denoising steps, effectively reducing the\ncomputational cost of DLM inference. Second, we introduce Guided Diffusion, a\ntraining-free method that uses a lightweight pretrained autoregressive model to\nsupervise token unmasking, dramatically reducing the total number of denoising\niterations without sacrificing quality. We conduct extensive evaluations on\nopen-source reasoning benchmarks, and our combined methods deliver an average\nof 12.14x end-to-end speedup across various tasks with negligible accuracy\ndegradation. For the first time, diffusion language models achieve a comparable\nand even faster latency as the widely adopted autoregressive models. Our work\nsuccessfully paved the way for scaling up the diffusion language model to a\nbroader scope of applications across different domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models offer parallel token generation and inherent\nbidirectionality, promising more efficient and powerful sequence modeling\ncompared to autoregressive approaches. However, state-of-the-art diffusion\nmodels (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match\nthe quality of similarly sized autoregressive (AR) models (e.g., Qwen2.5 7B,\nLlama3 8B), their iterative denoising requires multiple full-sequence forward\npasses, resulting in high computational costs and latency, particularly for\nlong input prompts and long-context scenarios. Furthermore, parallel token\ngeneration introduces token incoherence problems, and current sampling\nheuristics suffer from significant quality drops with decreasing denoising\nsteps. We address these limitations with two training-free techniques. First,\nwe propose FreeCache, a Key-Value (KV) approximation caching technique that\nreuses stable KV projections across denoising steps, effectively reducing the\ncomputational cost of DLM inference. Second, we introduce Guided Diffusion, a\ntraining-free method that uses a lightweight pretrained autoregressive model to\nsupervise token unmasking, dramatically reducing the total number of denoising\niterations without sacrificing quality. We conduct extensive evaluations on\nopen-source reasoning benchmarks, and our combined methods deliver an average\nof 12.14x end-to-end speedup across various tasks with negligible accuracy\ndegradation. For the first time, diffusion language models achieve a comparable\nand even faster latency as the widely adopted autoregressive models. Our work\nsuccessfully paved the way for scaling up the diffusion language model to a\nbroader scope of applications across different domains."
                },
                "authors": [
                    {
                        "name": "Zhanqiu Hu"
                    },
                    {
                        "name": "Jian Meng"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Jae-sun Seo"
                    },
                    {
                        "name": "Zhiru Zhang"
                    },
                    {
                        "name": "Udit Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Udit Gupta"
                },
                "author": "Udit Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21467v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21467v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07667v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07667v1",
                "updated": "2025-10-09T01:40:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    1,
                    40,
                    39,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T01:40:39Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    1,
                    40,
                    39,
                    3,
                    282,
                    0
                ],
                "title": "An Energy-Efficient Edge Coprocessor for Neural Rendering with Explicit\n  Data Reuse Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Energy-Efficient Edge Coprocessor for Neural Rendering with Explicit\n  Data Reuse Strategies"
                },
                "summary": "Neural radiance fields (NeRF) have transformed 3D reconstruction and\nrendering, facilitating photorealistic image synthesis from sparse viewpoints.\nThis work introduces an explicit data reuse neural rendering (EDR-NR)\narchitecture, which reduces frequent external memory accesses (EMAs) and cache\nmisses by exploiting the spatial locality from three phases, including rays,\nray packets (RPs), and samples. The EDR-NR architecture features a four-stage\nscheduler that clusters rays on the basis of Z-order, prioritize lagging rays\nwhen ray divergence happens, reorders RPs based on spatial proximity, and\nissues samples out-of-orderly (OoO) according to the availability of on-chip\nfeature data. In addition, a four-tier hierarchical RP marching (HRM) technique\nis integrated with an axis-aligned bounding box (AABB) to facilitate spatial\nskipping (SS), reducing redundant computations and improving throughput.\nMoreover, a balanced allocation strategy for feature storage is proposed to\nmitigate SRAM bank conflicts. Fabricated using a 40 nm process with a die area\nof 10.5 mmX, the EDR-NR chip demonstrates a 2.41X enhancement in normalized\nenergy efficiency, a 1.21X improvement in normalized area efficiency, a 1.20X\nincrease in normalized throughput, and a 53.42% reduction in on-chip SRAM\nconsumption compared to state-of-the-art accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural radiance fields (NeRF) have transformed 3D reconstruction and\nrendering, facilitating photorealistic image synthesis from sparse viewpoints.\nThis work introduces an explicit data reuse neural rendering (EDR-NR)\narchitecture, which reduces frequent external memory accesses (EMAs) and cache\nmisses by exploiting the spatial locality from three phases, including rays,\nray packets (RPs), and samples. The EDR-NR architecture features a four-stage\nscheduler that clusters rays on the basis of Z-order, prioritize lagging rays\nwhen ray divergence happens, reorders RPs based on spatial proximity, and\nissues samples out-of-orderly (OoO) according to the availability of on-chip\nfeature data. In addition, a four-tier hierarchical RP marching (HRM) technique\nis integrated with an axis-aligned bounding box (AABB) to facilitate spatial\nskipping (SS), reducing redundant computations and improving throughput.\nMoreover, a balanced allocation strategy for feature storage is proposed to\nmitigate SRAM bank conflicts. Fabricated using a 40 nm process with a die area\nof 10.5 mmX, the EDR-NR chip demonstrates a 2.41X enhancement in normalized\nenergy efficiency, a 1.21X improvement in normalized area efficiency, a 1.20X\nincrease in normalized throughput, and a 53.42% reduction in on-chip SRAM\nconsumption compared to state-of-the-art accelerators."
                },
                "authors": [
                    {
                        "name": "Binzhe Yuan"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Zeyu Zheng"
                    },
                    {
                        "name": "Yuefeng Zhang"
                    },
                    {
                        "name": "Haochuan Wan"
                    },
                    {
                        "name": "Zhechen Yuan"
                    },
                    {
                        "name": "Junsheng Chen"
                    },
                    {
                        "name": "Yunxiang He"
                    },
                    {
                        "name": "Junran Ding"
                    },
                    {
                        "name": "Xiaoming Zhang"
                    },
                    {
                        "name": "Chaolin Rao"
                    },
                    {
                        "name": "Wenyan Su"
                    },
                    {
                        "name": "Pingqiang Zhou"
                    },
                    {
                        "name": "Jingyi Yu"
                    },
                    {
                        "name": "Xin Lou"
                    }
                ],
                "author_detail": {
                    "name": "Xin Lou"
                },
                "author": "Xin Lou",
                "arxiv_comment": "11 pages, 17 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07667v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07667v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07651v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07651v1",
                "updated": "2025-10-09T00:58:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    0,
                    58,
                    28,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T00:58:28Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    0,
                    58,
                    28,
                    3,
                    282,
                    0
                ],
                "title": "OBCache: Optimal Brain KV Cache Pruning for Efficient Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OBCache: Optimal Brain KV Cache Pruning for Efficient Long-Context LLM\n  Inference"
                },
                "summary": "Large language models (LLMs) with extended context windows enable powerful\ndownstream applications but impose significant memory overhead, as caching all\nkey-value (KV) states scales linearly with sequence length and batch size.\nExisting cache eviction methods address this by exploiting attention sparsity,\nyet they typically rank tokens heuristically using accumulated attention\nweights without considering their true impact on attention outputs. We propose\nOptimal Brain Cache (OBCache), a principled framework that formulates cache\neviction as a layer-wise structured pruning problem. Building upon the Optimal\nBrain Damage (OBD) theory, OBCache quantifies token saliency by measuring the\nperturbation in attention outputs induced by pruning tokens, with closed-form\nscores derived for isolated keys, isolated values, and joint key-value pairs.\nOur scores account not only for attention weights but also for information from\nvalue states and attention outputs, thereby enhancing existing eviction\nstrategies with output-aware signals. Experiments on LLaMA and Qwen models\ndemonstrate that replacing the heuristic scores in existing works, which\nestimate token saliency across different query positions, with OBCache's\noutput-aware scores consistently improves long-context accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with extended context windows enable powerful\ndownstream applications but impose significant memory overhead, as caching all\nkey-value (KV) states scales linearly with sequence length and batch size.\nExisting cache eviction methods address this by exploiting attention sparsity,\nyet they typically rank tokens heuristically using accumulated attention\nweights without considering their true impact on attention outputs. We propose\nOptimal Brain Cache (OBCache), a principled framework that formulates cache\neviction as a layer-wise structured pruning problem. Building upon the Optimal\nBrain Damage (OBD) theory, OBCache quantifies token saliency by measuring the\nperturbation in attention outputs induced by pruning tokens, with closed-form\nscores derived for isolated keys, isolated values, and joint key-value pairs.\nOur scores account not only for attention weights but also for information from\nvalue states and attention outputs, thereby enhancing existing eviction\nstrategies with output-aware signals. Experiments on LLaMA and Qwen models\ndemonstrate that replacing the heuristic scores in existing works, which\nestimate token saliency across different query positions, with OBCache's\noutput-aware scores consistently improves long-context accuracy."
                },
                "authors": [
                    {
                        "name": "Yuzhe Gu"
                    },
                    {
                        "name": "Xiyu Liang"
                    },
                    {
                        "name": "Jiaojiao Zhao"
                    },
                    {
                        "name": "Enmao Diao"
                    }
                ],
                "author_detail": {
                    "name": "Enmao Diao"
                },
                "author": "Enmao Diao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07651v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07651v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07499v1",
                "updated": "2025-10-08T19:52:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    19,
                    52,
                    35,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T19:52:35Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    19,
                    52,
                    35,
                    2,
                    281,
                    0
                ],
                "title": "When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs"
                },
                "summary": "Recent Long-Context Language Models (LCLMs) can process hundreds of thousands\nof tokens in a single prompt, enabling new opportunities for\nknowledge-intensive multi-hop reasoning by integrating large sets of retrieved\ndocuments or, in some cases, directly all necessary information. However,\nsimply feeding more documents into the context window fails to capture how\nevidence should be connected. We address this gap with thought templates, which\nrecast reasoning as reusable thought caches, derived from prior problem solving\ntraces, structuring how evidence is combined and guiding multi-hop inference\nwith factual documents. To keep these templates effective, we propose an update\nstrategy that iteratively refines templates derived from training data through\nnatural-language feedback. Across diverse benchmarks and LCLM families, our\napproach delivers consistent gains over strong baselines in both\nretrieval-based and retrieval-free settings. Furthermore, we show that\noptimized templates can be distilled into smaller open-source models,\ndemonstrating its broad applicability and transparent reasoning reuse. We refer\nto our framework as Thought Template Augmented LCLMs (ToTAL).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Long-Context Language Models (LCLMs) can process hundreds of thousands\nof tokens in a single prompt, enabling new opportunities for\nknowledge-intensive multi-hop reasoning by integrating large sets of retrieved\ndocuments or, in some cases, directly all necessary information. However,\nsimply feeding more documents into the context window fails to capture how\nevidence should be connected. We address this gap with thought templates, which\nrecast reasoning as reusable thought caches, derived from prior problem solving\ntraces, structuring how evidence is combined and guiding multi-hop inference\nwith factual documents. To keep these templates effective, we propose an update\nstrategy that iteratively refines templates derived from training data through\nnatural-language feedback. Across diverse benchmarks and LCLM families, our\napproach delivers consistent gains over strong baselines in both\nretrieval-based and retrieval-free settings. Furthermore, we show that\noptimized templates can be distilled into smaller open-source models,\ndemonstrating its broad applicability and transparent reasoning reuse. We refer\nto our framework as Thought Template Augmented LCLMs (ToTAL)."
                },
                "authors": [
                    {
                        "name": "Soyeong Jeong"
                    },
                    {
                        "name": "Taehee Jung"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    },
                    {
                        "name": "Joo-Kyung Kim"
                    },
                    {
                        "name": "Dongyeop Kang"
                    }
                ],
                "author_detail": {
                    "name": "Dongyeop Kang"
                },
                "author": "Dongyeop Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07486v1",
                "updated": "2025-10-08T19:36:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    19,
                    36,
                    11,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T19:36:11Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    19,
                    36,
                    11,
                    2,
                    281,
                    0
                ],
                "title": "AsyncSpade: Efficient Test-Time Scaling with Asynchronous Sparse\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsyncSpade: Efficient Test-Time Scaling with Asynchronous Sparse\n  Decoding"
                },
                "summary": "Test-time scaling (TTS) boosts LLM reasoning via long chain-of-thought (CoT),\nbut the linear KV-cache growth amplifies the memory-bound bottleneck of LLM\ndecoding. Query-aware page-level sparse decoding can achieve state-of-the-art\nperformance under constrained FLOPs budgets, but is limited by both\nsequential-dependent page filtering and coarse-grained token selection,\nhampering serving efficiency and model performance on TTS tasks under high\nconcurrency and long CoT scenarios (consuming even higher runtime than the\nforward pipeline itself). In this paper, we first find that the current-step\nquery state can be accurately approximated in a unified manner from a short\nwindow of recent queries, enabling training-free query-aware sparsity without\nwaiting in the decoding loop. We propose AsyncSpade, an asynchronous framework\nfor efficient TTS built on two core components: (1) a novel light-weight\ntemporal-regressive module that predicts the next-token query state; (2) an\nasynchronous and disaggregated framework that decouples the KV cache filtering\nfrom the auto-regressive decoding loop, overlapping the token-level KV\nselection with the forward inference computation through asynchronism. To our\nknowledge, AsyncSpade is the first to eliminate the sequential dependence\nwithout sacrificing model performance. We validate the effectiveness of\nAsyncSpade on common LLM serving setups with an A100 node, where AsyncSpade\nfully overlaps KV-cache operations with the inference pipeline, achieving\ntheoretical optimal time-per-output-token (TPOT). Specifically, AsyncSpade\ndelivers over 20% reduction on TPOT compared to SoTA baseline (i.e. Quest) and\nat least 50% TPOT reduction compared to full attention on Qwen3-8B and\nQwen3-32B models, while matching or surpassing their accuracy on various TTS\nbenchmarks (AIME-24/25, GPQA-Diamond, MATH-500).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling (TTS) boosts LLM reasoning via long chain-of-thought (CoT),\nbut the linear KV-cache growth amplifies the memory-bound bottleneck of LLM\ndecoding. Query-aware page-level sparse decoding can achieve state-of-the-art\nperformance under constrained FLOPs budgets, but is limited by both\nsequential-dependent page filtering and coarse-grained token selection,\nhampering serving efficiency and model performance on TTS tasks under high\nconcurrency and long CoT scenarios (consuming even higher runtime than the\nforward pipeline itself). In this paper, we first find that the current-step\nquery state can be accurately approximated in a unified manner from a short\nwindow of recent queries, enabling training-free query-aware sparsity without\nwaiting in the decoding loop. We propose AsyncSpade, an asynchronous framework\nfor efficient TTS built on two core components: (1) a novel light-weight\ntemporal-regressive module that predicts the next-token query state; (2) an\nasynchronous and disaggregated framework that decouples the KV cache filtering\nfrom the auto-regressive decoding loop, overlapping the token-level KV\nselection with the forward inference computation through asynchronism. To our\nknowledge, AsyncSpade is the first to eliminate the sequential dependence\nwithout sacrificing model performance. We validate the effectiveness of\nAsyncSpade on common LLM serving setups with an A100 node, where AsyncSpade\nfully overlaps KV-cache operations with the inference pipeline, achieving\ntheoretical optimal time-per-output-token (TPOT). Specifically, AsyncSpade\ndelivers over 20% reduction on TPOT compared to SoTA baseline (i.e. Quest) and\nat least 50% TPOT reduction compared to full attention on Qwen3-8B and\nQwen3-32B models, while matching or surpassing their accuracy on various TTS\nbenchmarks (AIME-24/25, GPQA-Diamond, MATH-500)."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Yilin Guan"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Hanrui Wang"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "14 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18344v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18344v2",
                "updated": "2025-10-08T18:16:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    18,
                    16,
                    4,
                    2,
                    281,
                    0
                ],
                "published": "2025-09-22T19:08:57Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    19,
                    8,
                    57,
                    0,
                    265,
                    0
                ],
                "title": "Speculate Deep and Accurate: Lossless and Training-Free Acceleration for\n  Offloaded LLMs via Substitute Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculate Deep and Accurate: Lossless and Training-Free Acceleration for\n  Offloaded LLMs via Substitute Speculative Decoding"
                },
                "summary": "The immense model sizes of large language models (LLMs) challenge deployment\non memory-limited consumer GPUs. Although model compression and parameter\noffloading are common strategies to address memory limitations, compression can\ndegrade quality, and offloading maintains quality but suffers from slow\ninference. Speculative decoding presents a promising avenue to accelerate\nparameter offloading, utilizing a fast draft model to propose multiple draft\ntokens, which are then verified by the target LLM in parallel with a single\nforward pass. This method reduces the time-consuming data transfers in forward\npasses that involve offloaded weight transfers. Existing methods often rely on\npretrained weights of the same family, but require additional training to align\nwith custom-trained models. Moreover, approaches that involve draft model\ntraining usually yield only modest speedups. This limitation arises from\ninsufficient alignment with the target model, preventing higher token\nacceptance lengths. To address these challenges and achieve greater speedups,\nwe propose SubSpec, a plug-and-play method to accelerate parameter offloading\nthat is lossless and training-free. SubSpec constructs a highly aligned draft\nmodel by generating low-bit quantized substitute layers from offloaded target\nLLM portions. Additionally, our method shares the remaining GPU-resident layers\nand the KV-Cache, further reducing memory overhead and enhance alignment.\nSubSpec achieves a high average acceptance length, delivering 9.1x speedup for\nQwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for\nQwen2.5 32B on popular generation benchmarks (24GB VRAM limit).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The immense model sizes of large language models (LLMs) challenge deployment\non memory-limited consumer GPUs. Although model compression and parameter\noffloading are common strategies to address memory limitations, compression can\ndegrade quality, and offloading maintains quality but suffers from slow\ninference. Speculative decoding presents a promising avenue to accelerate\nparameter offloading, utilizing a fast draft model to propose multiple draft\ntokens, which are then verified by the target LLM in parallel with a single\nforward pass. This method reduces the time-consuming data transfers in forward\npasses that involve offloaded weight transfers. Existing methods often rely on\npretrained weights of the same family, but require additional training to align\nwith custom-trained models. Moreover, approaches that involve draft model\ntraining usually yield only modest speedups. This limitation arises from\ninsufficient alignment with the target model, preventing higher token\nacceptance lengths. To address these challenges and achieve greater speedups,\nwe propose SubSpec, a plug-and-play method to accelerate parameter offloading\nthat is lossless and training-free. SubSpec constructs a highly aligned draft\nmodel by generating low-bit quantized substitute layers from offloaded target\nLLM portions. Additionally, our method shares the remaining GPU-resident layers\nand the KV-Cache, further reducing memory overhead and enhance alignment.\nSubSpec achieves a high average acceptance length, delivering 9.1x speedup for\nQwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for\nQwen2.5 32B on popular generation benchmarks (24GB VRAM limit)."
                },
                "authors": [
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Jian-Jia Chen"
                    },
                    {
                        "name": "Chun-Che Yang"
                    },
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18344v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18344v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07318v1",
                "updated": "2025-10-08T17:59:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    59,
                    55,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:59:55Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    59,
                    55,
                    2,
                    281,
                    0
                ],
                "title": "Artificial Hippocampus Networks for Efficient Long-Context Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Hippocampus Networks for Efficient Long-Context Modeling"
                },
                "summary": "Long-sequence modeling faces a fundamental trade-off between the efficiency\nof compressive fixed-size memory in RNN-like models and the fidelity of\nlossless growing memory in attention-based Transformers. Inspired by the\nMulti-Store Model in cognitive science, we introduce a memory framework of\nartificial neural networks. Our method maintains a sliding window of the\nTransformer's KV cache as lossless short-term memory, while a learnable module\ntermed Artificial Hippocampus Network (AHN) recurrently compresses\nout-of-window information into a fixed-size compact long-term memory. To\nvalidate this framework, we instantiate AHNs using modern RNN-like\narchitectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive\nexperiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate\nthat AHN-augmented models consistently outperform sliding window baselines and\nachieve performance comparable or even superior to full-attention models, while\nsubstantially reducing computational and memory requirements. For instance,\naugmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5%\nand memory cache by 74.0%, while improving its average score on LV-Eval (128k\nsequence length) from 4.41 to 5.88. Code is available at:\nhttps://github.com/ByteDance-Seed/AHN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-sequence modeling faces a fundamental trade-off between the efficiency\nof compressive fixed-size memory in RNN-like models and the fidelity of\nlossless growing memory in attention-based Transformers. Inspired by the\nMulti-Store Model in cognitive science, we introduce a memory framework of\nartificial neural networks. Our method maintains a sliding window of the\nTransformer's KV cache as lossless short-term memory, while a learnable module\ntermed Artificial Hippocampus Network (AHN) recurrently compresses\nout-of-window information into a fixed-size compact long-term memory. To\nvalidate this framework, we instantiate AHNs using modern RNN-like\narchitectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive\nexperiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate\nthat AHN-augmented models consistently outperform sliding window baselines and\nachieve performance comparable or even superior to full-attention models, while\nsubstantially reducing computational and memory requirements. For instance,\naugmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5%\nand memory cache by 74.0%, while improving its average score on LV-Eval (128k\nsequence length) from 4.41 to 5.88. Code is available at:\nhttps://github.com/ByteDance-Seed/AHN."
                },
                "authors": [
                    {
                        "name": "Yunhao Fang"
                    },
                    {
                        "name": "Weihao Yu"
                    },
                    {
                        "name": "Shu Zhong"
                    },
                    {
                        "name": "Qinghao Ye"
                    },
                    {
                        "name": "Xuehan Xiong"
                    },
                    {
                        "name": "Lai Wei"
                    }
                ],
                "author_detail": {
                    "name": "Lai Wei"
                },
                "author": "Lai Wei",
                "arxiv_comment": "Code: https://github.com/ByteDance-Seed/AHN",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07297v1",
                "updated": "2025-10-08T17:51:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    51,
                    34,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:51:34Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    51,
                    34,
                    2,
                    281,
                    0
                ],
                "title": "Agentic generative AI for media content discovery at the national\n  football league",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic generative AI for media content discovery at the national\n  football league"
                },
                "summary": "Generative AI has unlocked new possibilities in content discovery and\nmanagement. Through collaboration with the National Football League (NFL), we\ndemonstrate how a generative-AI based workflow enables media researchers and\nanalysts to query relevant historical plays using natural language rather than\ntraditional filter-and-click interfaces. The agentic workflow takes a user\nquery as input, breaks it into elements, and translates them into the\nunderlying database query language. Accuracy and latency are further improved\nthrough carefully designed semantic caching. The solution achieves over 95\npercent accuracy and reduces the average time to find relevant videos from 10\nminutes to 30 seconds, significantly increasing the NFL's operational\nefficiency and allowing users to focus on producing creative content and\nengaging storylines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI has unlocked new possibilities in content discovery and\nmanagement. Through collaboration with the National Football League (NFL), we\ndemonstrate how a generative-AI based workflow enables media researchers and\nanalysts to query relevant historical plays using natural language rather than\ntraditional filter-and-click interfaces. The agentic workflow takes a user\nquery as input, breaks it into elements, and translates them into the\nunderlying database query language. Accuracy and latency are further improved\nthrough carefully designed semantic caching. The solution achieves over 95\npercent accuracy and reduces the average time to find relevant videos from 10\nminutes to 30 seconds, significantly increasing the NFL's operational\nefficiency and allowing users to focus on producing creative content and\nengaging storylines."
                },
                "authors": [
                    {
                        "name": "Henry Wang"
                    },
                    {
                        "name": "Md Sirajus Salekin"
                    },
                    {
                        "name": "Jake Lee"
                    },
                    {
                        "name": "Ross Claytor"
                    },
                    {
                        "name": "Shinan Zhang"
                    },
                    {
                        "name": "Michael Chi"
                    }
                ],
                "author_detail": {
                    "name": "Michael Chi"
                },
                "author": "Michael Chi",
                "arxiv_comment": "13 pages, 7 figures, International Sports Analytics Conference and\n  Exhibition",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07293v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07293v1",
                "updated": "2025-10-08T17:50:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    50,
                    16,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:50:16Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    50,
                    16,
                    2,
                    281,
                    0
                ],
                "title": "AudioMarathon: A Comprehensive Benchmark for Long-Context Audio\n  Understanding and Efficiency in Audio LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AudioMarathon: A Comprehensive Benchmark for Long-Context Audio\n  Understanding and Efficiency in Audio LLMs"
                },
                "summary": "Processing long-form audio is a major challenge for Large Audio Language\nmodels (LALMs). These models struggle with the quadratic cost of attention\n($O(N^2)$) and with modeling long-range temporal dependencies. Existing audio\nbenchmarks are built mostly from short clips and do not evaluate models in\nrealistic long context settings. To address this gap, we introduce\nAudioMarathon, a benchmark designed to evaluate both understanding and\ninference efficiency on long-form audio. AudioMarathon provides a diverse set\nof tasks built upon three pillars: long-context audio inputs with durations\nranging from 90.0 to 300.0 seconds, which correspond to encoded sequences of\n2,250 to 7,500 audio tokens, respectively, full domain coverage across speech,\nsound, and music, and complex reasoning that requires multi-hop inference. We\nevaluate state-of-the-art LALMs and observe clear performance drops as audio\nlength grows. We also study acceleration techniques and analyze the trade-offs\nof token pruning and KV cache eviction. The results show large gaps across\ncurrent LALMs and highlight the need for better temporal reasoning and\nmemory-efficient architectures. We believe AudioMarathon will drive the audio\nand multimodal research community to develop more advanced audio understanding\nmodels capable of solving complex audio tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long-form audio is a major challenge for Large Audio Language\nmodels (LALMs). These models struggle with the quadratic cost of attention\n($O(N^2)$) and with modeling long-range temporal dependencies. Existing audio\nbenchmarks are built mostly from short clips and do not evaluate models in\nrealistic long context settings. To address this gap, we introduce\nAudioMarathon, a benchmark designed to evaluate both understanding and\ninference efficiency on long-form audio. AudioMarathon provides a diverse set\nof tasks built upon three pillars: long-context audio inputs with durations\nranging from 90.0 to 300.0 seconds, which correspond to encoded sequences of\n2,250 to 7,500 audio tokens, respectively, full domain coverage across speech,\nsound, and music, and complex reasoning that requires multi-hop inference. We\nevaluate state-of-the-art LALMs and observe clear performance drops as audio\nlength grows. We also study acceleration techniques and analyze the trade-offs\nof token pruning and KV cache eviction. The results show large gaps across\ncurrent LALMs and highlight the need for better temporal reasoning and\nmemory-efficient architectures. We believe AudioMarathon will drive the audio\nand multimodal research community to develop more advanced audio understanding\nmodels capable of solving complex audio tasks."
                },
                "authors": [
                    {
                        "name": "Peize He"
                    },
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Yubo Wang"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Xiaoqian Liu"
                    },
                    {
                        "name": "Jiajie Huang"
                    },
                    {
                        "name": "Zehui Lei"
                    },
                    {
                        "name": "Zhuangcheng Gu"
                    },
                    {
                        "name": "Xiangqi Jin"
                    },
                    {
                        "name": "Jiabing Yang"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Zhifei Liu"
                    },
                    {
                        "name": "Weijia Li"
                    },
                    {
                        "name": "Cunxiang Wang"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "26 pages, 23 figures, the code is available at\n  \\url{https://github.com/DabDans/AudioMarathon}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07293v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07293v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09665v1",
                "updated": "2025-10-08T00:15:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    0,
                    15,
                    4,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T00:15:04Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    0,
                    15,
                    4,
                    2,
                    281,
                    0
                ],
                "title": "LMCache: An Efficient KV Cache Layer for Enterprise-Scale LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LMCache: An Efficient KV Cache Layer for Enterprise-Scale LLM Inference"
                },
                "summary": "Today's LLM inference systems treat individual engines and queries\nindependently for simplicity, but this causes significant resource\ninefficiencies. While there are proposals to avoid redundant computation by\nreusing KV caches across queries and to increase GPU utilization by\ndisaggregating a single query to different engines, their promises cannot be\nrealized without efficiently offloading and communicating KV cache across LLM\ninference engines and queries.\n  We present LMCache, the first and so far the most efficient open-source KV\ncaching solution, which extracts and stores KV caches generated by modern LLM\nengines (vLLM and SGLang) and shares the KV caches across engines and queries.\nLMCache exposes KV caches in the LLM engine interface, effectively transforming\nLLM engines from individual token processors to a collection of engines with KV\ncache as the storage and communication medium. In particular, it supports both\ncache offloading (prefix reuse across queries) and prefill-decode\ndisaggregation (cross-engine cache transfer). LMCache's high performance and\nwide adoption stem from the following contributions: highly optimized KV cache\ndata movement with performance optimizations including batched data movement\noperations, compute and I/O pipelining; a modular KV cache connector component,\ndecoupling LMCache from the rapid evolution of inference engines; a first-class\ncontrol API, such as pinning, lookup, cleanup, movement, and compression, for\nflexible cache orchestration across GPU, CPU, storage, and network layers.\nEvaluation shows that combining LMCache with vLLM achieves up to 15x\nimprovement in throughput across diverse workloads. With a growing community,\nLMCache has seen dramatic growth in adoption by enterprise inference systems,\nwhich provides valuable lessons for future KV caching solutions. The source\ncode of LMCache is at: https://github.com/LMCache/LMCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Today's LLM inference systems treat individual engines and queries\nindependently for simplicity, but this causes significant resource\ninefficiencies. While there are proposals to avoid redundant computation by\nreusing KV caches across queries and to increase GPU utilization by\ndisaggregating a single query to different engines, their promises cannot be\nrealized without efficiently offloading and communicating KV cache across LLM\ninference engines and queries.\n  We present LMCache, the first and so far the most efficient open-source KV\ncaching solution, which extracts and stores KV caches generated by modern LLM\nengines (vLLM and SGLang) and shares the KV caches across engines and queries.\nLMCache exposes KV caches in the LLM engine interface, effectively transforming\nLLM engines from individual token processors to a collection of engines with KV\ncache as the storage and communication medium. In particular, it supports both\ncache offloading (prefix reuse across queries) and prefill-decode\ndisaggregation (cross-engine cache transfer). LMCache's high performance and\nwide adoption stem from the following contributions: highly optimized KV cache\ndata movement with performance optimizations including batched data movement\noperations, compute and I/O pipelining; a modular KV cache connector component,\ndecoupling LMCache from the rapid evolution of inference engines; a first-class\ncontrol API, such as pinning, lookup, cleanup, movement, and compression, for\nflexible cache orchestration across GPU, CPU, storage, and network layers.\nEvaluation shows that combining LMCache with vLLM achieves up to 15x\nimprovement in throughput across diverse workloads. With a growing community,\nLMCache has seen dramatic growth in adoption by enterprise inference systems,\nwhich provides valuable lessons for future KV caching solutions. The source\ncode of LMCache is at: https://github.com/LMCache/LMCache."
                },
                "authors": [
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Yuwei An"
                    },
                    {
                        "name": "Xiaokun Chen"
                    },
                    {
                        "name": "Shaoting Feng"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Samuel Shen"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15347v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15347v2",
                "updated": "2025-10-08T00:06:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    0,
                    6,
                    52,
                    2,
                    281,
                    0
                ],
                "published": "2025-05-21T10:20:46Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    10,
                    20,
                    46,
                    2,
                    141,
                    0
                ],
                "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via\n  Isolated Key-Value Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via\n  Isolated Key-Value Cache Management"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in multi-turn\nconversational applications, where the management of the Key-Value (KV) Cache\npresents a significant bottleneck. The linear growth of the KV Cache with\ndialogue history imposes substantial computational costs, and existing eviction\nstrategies often degrade performance by repeatedly compressing early\nconversational context, leading to information loss and context forgetting.\nThis paper introduces FlowKV, a novel \\textbf{multi-turn isolation mechanism}\nfor KV Cache management, which can be applied to any KV Cache compression\nmethod without training. FlowKV's core innovation is a multi-turn isolation\nmechanism that preserves the accumulated compressed KV cache from past turns.\nCompression is then strategically applied only to the newly generated KV pairs\nof the latest completed turn, effectively preventing the re-compression of\nolder context and thereby mitigating catastrophic forgetting. Our results\ndemonstrate that FlowKV consistently and significantly outperforms baseline\nstrategies in maintaining instruction-following accuracy and user preference\nretention from 10.90\\% to 75.40\\%, particularly in later conversational turns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in multi-turn\nconversational applications, where the management of the Key-Value (KV) Cache\npresents a significant bottleneck. The linear growth of the KV Cache with\ndialogue history imposes substantial computational costs, and existing eviction\nstrategies often degrade performance by repeatedly compressing early\nconversational context, leading to information loss and context forgetting.\nThis paper introduces FlowKV, a novel \\textbf{multi-turn isolation mechanism}\nfor KV Cache management, which can be applied to any KV Cache compression\nmethod without training. FlowKV's core innovation is a multi-turn isolation\nmechanism that preserves the accumulated compressed KV cache from past turns.\nCompression is then strategically applied only to the newly generated KV pairs\nof the latest completed turn, effectively preventing the re-compression of\nolder context and thereby mitigating catastrophic forgetting. Our results\ndemonstrate that FlowKV consistently and significantly outperforms baseline\nstrategies in maintaining instruction-following accuracy and user preference\nretention from 10.90\\% to 75.40\\%, particularly in later conversational turns."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "NeurIPS 2025 Workshop on Multi-Turn Interactions in Large Language\n  Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15347v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15347v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04975v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04975v3",
                "updated": "2025-10-07T22:07:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    22,
                    7,
                    44,
                    1,
                    280,
                    0
                ],
                "published": "2024-11-07T18:49:33Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    49,
                    33,
                    3,
                    312,
                    0
                ],
                "title": "SuffixDecoding: Extreme Speculative Decoding for Emerging AI\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuffixDecoding: Extreme Speculative Decoding for Emerging AI\n  Applications"
                },
                "summary": "Speculative decoding is widely adopted to reduce latency in large language\nmodel (LLM) inference by leveraging smaller draft models capable of handling\ndiverse user tasks. However, emerging AI applications, such as LLM-based\nagents, present unique workload characteristics: instead of diverse independent\nrequests, agentic frameworks typically submit repetitive inference requests,\nsuch as multi-agent pipelines performing similar subtasks or self-refinement\nloops iteratively enhancing outputs. These workloads result in long and highly\npredictable sequences, which current speculative decoding methods do not\neffectively exploit. To address this gap, we introduce \\emph{SuffixDecoding}, a\nnovel method that utilizes efficient suffix trees to cache long token sequences\nfrom prompts and previous outputs. By adaptively speculating more tokens when\nacceptance likelihood is high and fewer when it is low, SuffixDecoding\neffectively exploits opportunities for longer speculations while conserving\ncomputation when those opportunities are limited. Evaluations on agentic\nbenchmarks, including SWE-Bench and Text-to-SQL, demonstrate that\nSuffixDecoding achieves speedups of up to 5.3$\\times$, outperforming\nstate-of-the-art methods -- 2.8$\\times$ faster than model-based approaches like\nEAGLE-2/3 and 1.9$\\times$ faster than model-free approaches such as Token\nRecycling. SuffixDecoding is open-sourced at\nhttps://github.com/snowflakedb/ArcticInference",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is widely adopted to reduce latency in large language\nmodel (LLM) inference by leveraging smaller draft models capable of handling\ndiverse user tasks. However, emerging AI applications, such as LLM-based\nagents, present unique workload characteristics: instead of diverse independent\nrequests, agentic frameworks typically submit repetitive inference requests,\nsuch as multi-agent pipelines performing similar subtasks or self-refinement\nloops iteratively enhancing outputs. These workloads result in long and highly\npredictable sequences, which current speculative decoding methods do not\neffectively exploit. To address this gap, we introduce \\emph{SuffixDecoding}, a\nnovel method that utilizes efficient suffix trees to cache long token sequences\nfrom prompts and previous outputs. By adaptively speculating more tokens when\nacceptance likelihood is high and fewer when it is low, SuffixDecoding\neffectively exploits opportunities for longer speculations while conserving\ncomputation when those opportunities are limited. Evaluations on agentic\nbenchmarks, including SWE-Bench and Text-to-SQL, demonstrate that\nSuffixDecoding achieves speedups of up to 5.3$\\times$, outperforming\nstate-of-the-art methods -- 2.8$\\times$ faster than model-based approaches like\nEAGLE-2/3 and 1.9$\\times$ faster than model-free approaches such as Token\nRecycling. SuffixDecoding is open-sourced at\nhttps://github.com/snowflakedb/ArcticInference"
                },
                "authors": [
                    {
                        "name": "Gabriele Oliaro"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Daniel Campos"
                    },
                    {
                        "name": "Aurick Qiao"
                    }
                ],
                "author_detail": {
                    "name": "Aurick Qiao"
                },
                "author": "Aurick Qiao",
                "arxiv_comment": "NeurIPS 2025 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04975v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04975v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06415v1",
                "updated": "2025-10-07T19:50:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    19,
                    50,
                    52,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T19:50:52Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    19,
                    50,
                    52,
                    1,
                    280,
                    0
                ],
                "title": "Enhanced Breakdown Voltage in $$-Ga$_2$O$_3$ Schottky Barrier\n  Diodes via Fast Neutron Irradiation and Electrothermal Annealing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Breakdown Voltage in $$-Ga$_2$O$_3$ Schottky Barrier\n  Diodes via Fast Neutron Irradiation and Electrothermal Annealing"
                },
                "summary": "This study investigates the impact of fast neutron irradiation and\npost-radiation electro-thermal annealing on the electrical performance of\n$\\beta$-Ga$_2$O$_3$ Schottky barrier diodes. Devices irradiated with 1 MeV\nneutrons at a high fluence of 1E15 n/cm^2 exhibited substantial degradation,\nincluding a drastic reduction in on-current and an increase in on-resistance.\nElectrothermal testing, conducted through simultaneous current-voltage (J-V)\nmeasurements and thermal annealing, resulted in significant recovery. After\nfour cycles of electro-thermal testing, the devices demonstrated significant\nimprovements in performance, with a substantial recovery of on-current and a\nreduction in on-resistance compared to the post-radiation condition,\napproaching pre-radiation levels. Most recovery occurred during the first two\ncycles, with diminishing improvements in later cycles, indicating that most\nthermally recoverable traps were mitigated early. Capacitance-voltage (C-V)\nmeasurements revealed a substantial reduction in carrier concentration,\ndecreasing from 3.2E16 cm^-3 pre-radiation to 5.5E15 cm^-3 after the first\nelectro-thermal testing cycle, indicating an over 82% reduction. Following the\nthird cycle, the carrier concentration partially recovered to 9.9E15 cm^-3,\nreflecting a carrier removal rate of ~22 cm^-1. The breakdown voltage exhibited\na remarkable enhancement, increasing from approximately 300 V to 1.28 kV (a\n~325% improvement) after the first electro-thermal testing, attributed to the\nreduction in carrier concentration by compensating radiation-induced traps.\nSubsequent testing reduced breakdown voltage slightly to 940 V due to partial\nrecovery of carrier concentration, but it remained significantly higher than\npre-radiation levels, highlighting the promise of $\\beta$-Ga$_2$O$_3$ power\ndevices for high-power applications in radiation-intense environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the impact of fast neutron irradiation and\npost-radiation electro-thermal annealing on the electrical performance of\n$\\beta$-Ga$_2$O$_3$ Schottky barrier diodes. Devices irradiated with 1 MeV\nneutrons at a high fluence of 1E15 n/cm^2 exhibited substantial degradation,\nincluding a drastic reduction in on-current and an increase in on-resistance.\nElectrothermal testing, conducted through simultaneous current-voltage (J-V)\nmeasurements and thermal annealing, resulted in significant recovery. After\nfour cycles of electro-thermal testing, the devices demonstrated significant\nimprovements in performance, with a substantial recovery of on-current and a\nreduction in on-resistance compared to the post-radiation condition,\napproaching pre-radiation levels. Most recovery occurred during the first two\ncycles, with diminishing improvements in later cycles, indicating that most\nthermally recoverable traps were mitigated early. Capacitance-voltage (C-V)\nmeasurements revealed a substantial reduction in carrier concentration,\ndecreasing from 3.2E16 cm^-3 pre-radiation to 5.5E15 cm^-3 after the first\nelectro-thermal testing cycle, indicating an over 82% reduction. Following the\nthird cycle, the carrier concentration partially recovered to 9.9E15 cm^-3,\nreflecting a carrier removal rate of ~22 cm^-1. The breakdown voltage exhibited\na remarkable enhancement, increasing from approximately 300 V to 1.28 kV (a\n~325% improvement) after the first electro-thermal testing, attributed to the\nreduction in carrier concentration by compensating radiation-induced traps.\nSubsequent testing reduced breakdown voltage slightly to 940 V due to partial\nrecovery of carrier concentration, but it remained significantly higher than\npre-radiation levels, highlighting the promise of $\\beta$-Ga$_2$O$_3$ power\ndevices for high-power applications in radiation-intense environments."
                },
                "authors": [
                    {
                        "name": "Saleh Ahmed Khan"
                    },
                    {
                        "name": "Sudipto Saha"
                    },
                    {
                        "name": "Ahmed Ibreljic"
                    },
                    {
                        "name": "Stephen Margiotta"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Walid Amir"
                    },
                    {
                        "name": "Surajit Chakraborty"
                    },
                    {
                        "name": "Uttam Singisetti"
                    },
                    {
                        "name": "A F M Anhar Uddin Bhuiyan"
                    }
                ],
                "author_detail": {
                    "name": "A F M Anhar Uddin Bhuiyan"
                },
                "author": "A F M Anhar Uddin Bhuiyan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06175v1",
                "updated": "2025-10-07T17:35:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    35,
                    28,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T17:35:28Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    35,
                    28,
                    1,
                    280,
                    0
                ],
                "title": "VecInfer: Efficient LLM Inference with Low-Bit KV Cache via\n  Outlier-Suppressed Vector Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VecInfer: Efficient LLM Inference with Low-Bit KV Cache via\n  Outlier-Suppressed Vector Quantization"
                },
                "summary": "The Key-Value (KV) cache introduces substantial memory overhead during large\nlanguage model (LLM) inference. Although existing vector quantization (VQ)\nmethods reduce KV cache usage and provide flexible representational capacity\nacross bit-widths, they suffer severe performance degradation at ultra-low\nbit-widths due to key cache outliers that hinder effective codebook\nutilization. To address this challenge, we propose VecInfer, a novel VQ method\nfor aggressive KV cache compression while enabling efficient inference. By\napplying smooth and Hadamard transformations, VecInfer suppresses outliers in\nthe key cache, enabling the codebook to comprehensively cover the original data\ndistribution and thereby reducing quantization difficulty. To facilitate\nefficient deployment, we design an optimized CUDA kernel that fuses computation\nwith dequantization to minimize memory access overhead. Extensive evaluations\ndemonstrate that VecInfer consistently outperforms existing quantization\nbaselines across both long-context understanding and mathematical reasoning\ntasks. With only 2-bit quantization, VecInfer achieves performance comparable\nto full precision, while delivering up to $\\mathbf{2.7\\times}$ speedup in\nlarge-batch self-attention computation and $\\mathbf{8.3\\times}$ reduction in\nsingle-batch end-to-end latency on Llama-3.1-8B with a 196k sequence length.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache introduces substantial memory overhead during large\nlanguage model (LLM) inference. Although existing vector quantization (VQ)\nmethods reduce KV cache usage and provide flexible representational capacity\nacross bit-widths, they suffer severe performance degradation at ultra-low\nbit-widths due to key cache outliers that hinder effective codebook\nutilization. To address this challenge, we propose VecInfer, a novel VQ method\nfor aggressive KV cache compression while enabling efficient inference. By\napplying smooth and Hadamard transformations, VecInfer suppresses outliers in\nthe key cache, enabling the codebook to comprehensively cover the original data\ndistribution and thereby reducing quantization difficulty. To facilitate\nefficient deployment, we design an optimized CUDA kernel that fuses computation\nwith dequantization to minimize memory access overhead. Extensive evaluations\ndemonstrate that VecInfer consistently outperforms existing quantization\nbaselines across both long-context understanding and mathematical reasoning\ntasks. With only 2-bit quantization, VecInfer achieves performance comparable\nto full precision, while delivering up to $\\mathbf{2.7\\times}$ speedup in\nlarge-batch self-attention computation and $\\mathbf{8.3\\times}$ reduction in\nsingle-batch end-to-end latency on Llama-3.1-8B with a 196k sequence length."
                },
                "authors": [
                    {
                        "name": "Dingyu Yao"
                    },
                    {
                        "name": "Chenxu Yang"
                    },
                    {
                        "name": "Zhengyang Tong"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Weiping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Wang"
                },
                "author": "Weiping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05686v1",
                "updated": "2025-10-07T08:43:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    8,
                    43,
                    7,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T08:43:07Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    8,
                    43,
                    7,
                    1,
                    280,
                    0
                ],
                "title": "On Enhancing Delay SLAs in TCP Networks through Joint Routing and\n  Transport Assistant Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Enhancing Delay SLAs in TCP Networks through Joint Routing and\n  Transport Assistant Deployment"
                },
                "summary": "The Transport Control Protocol has long been the primary transport protocol\nfor applications requiring performance and reliability over the Internet.\nUnfortunately, due its retransmission mechanism, TCP incurs high packet\ndelivery delays when segments are lost. To address this issue, previous\nresearch proposed to use a novel network function, namely Transport Assistant,\ndeployed within the network to cache and retransmit lost packets, thus reducing\nretransmission delays. In this paper, we propose to jointly route the flows and\ndeploy TAs in order to minimize packet delivery delays in best-effort networks\n(scenario 1) or to satisfy delay-based Service Level Agreements in QoS-based\nnetworks (scenario 2). We hence formulate the joint routing and TA deployment\nproblem as Integer Linear Program for the two scenarios and propose a heuristic\nsolution for large-scale instances of the problem. Through extensive\nsimulations, we demonstrate the benefits of performing joint routing flows and\nTA deployment in reducing packet delivery delays (up to 16.4%) while minimizing\ndeployment costs (up to 60.98%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Transport Control Protocol has long been the primary transport protocol\nfor applications requiring performance and reliability over the Internet.\nUnfortunately, due its retransmission mechanism, TCP incurs high packet\ndelivery delays when segments are lost. To address this issue, previous\nresearch proposed to use a novel network function, namely Transport Assistant,\ndeployed within the network to cache and retransmit lost packets, thus reducing\nretransmission delays. In this paper, we propose to jointly route the flows and\ndeploy TAs in order to minimize packet delivery delays in best-effort networks\n(scenario 1) or to satisfy delay-based Service Level Agreements in QoS-based\nnetworks (scenario 2). We hence formulate the joint routing and TA deployment\nproblem as Integer Linear Program for the two scenarios and propose a heuristic\nsolution for large-scale instances of the problem. Through extensive\nsimulations, we demonstrate the benefits of performing joint routing flows and\nTA deployment in reducing packet delivery delays (up to 16.4%) while minimizing\ndeployment costs (up to 60.98%)."
                },
                "authors": [
                    {
                        "name": "Jos Gmez-delaHiz"
                    },
                    {
                        "name": "Mohamed Faten Zhani"
                    },
                    {
                        "name": "Jaime Galn-Jimnez"
                    },
                    {
                        "name": "John Kaippallimalil"
                    }
                ],
                "author_detail": {
                    "name": "John Kaippallimalil"
                },
                "author": "John Kaippallimalil",
                "arxiv_comment": "10 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05529v1",
                "updated": "2025-10-07T02:39:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    2,
                    39,
                    35,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T02:39:35Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    2,
                    39,
                    35,
                    1,
                    280,
                    0
                ],
                "title": "H1B-KV: Hybrid One-Bit Caches for Memory-Efficient Large Language Model\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "H1B-KV: Hybrid One-Bit Caches for Memory-Efficient Large Language Model\n  Inference"
                },
                "summary": "Autoregressive decoding in large language models (LLMs) requires caching a\ngrowing list of past key-value (KV) pairs, making long-context inference a\nmemory-bound problem. While recent methods have explored quantizing the cache,\nevicting tokens, or using binary sketches for keys (e.g., Loki), these\napproaches often provide an incomplete solution by leaving one component (like\nvalues) uncompressed or by discarding context information. This paper\nintroduces the Hybrid One-Bit KV Cache (H1B-KV), a comprehensive compression\nscheme that radically reduces memory usage without sacrificing context. H1B-KV\nrepresents each key vector using a 1-bit binary sketch, enabling\nhardware-friendly bitwise attention, and further compresses value vectors using\n4-bit quantization. This holistic, hybrid approach allows a 7-billion parameter\nLLM to handle an 8k-token context with under 60 MB of cache memory - a 70x\nreduction. We demonstrate that after a lightweight finetuning, H1B-KV matches\nfull-precision performance not only on perplexity benchmarks but also on\ncomplex downstream tasks like mathematical reasoning (GSM8K), multi-task\nunderstanding (MMLU), and code generation (HumanEval). Our results show H1B-KV\nsignificantly outperforms leading quantization (KIVI), token eviction\n(SparseLLM), and key-only sketching (Loki) methods in quality-per-byte,\nestablishing it as a robust solution for deploying LLMs in memory-constrained\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive decoding in large language models (LLMs) requires caching a\ngrowing list of past key-value (KV) pairs, making long-context inference a\nmemory-bound problem. While recent methods have explored quantizing the cache,\nevicting tokens, or using binary sketches for keys (e.g., Loki), these\napproaches often provide an incomplete solution by leaving one component (like\nvalues) uncompressed or by discarding context information. This paper\nintroduces the Hybrid One-Bit KV Cache (H1B-KV), a comprehensive compression\nscheme that radically reduces memory usage without sacrificing context. H1B-KV\nrepresents each key vector using a 1-bit binary sketch, enabling\nhardware-friendly bitwise attention, and further compresses value vectors using\n4-bit quantization. This holistic, hybrid approach allows a 7-billion parameter\nLLM to handle an 8k-token context with under 60 MB of cache memory - a 70x\nreduction. We demonstrate that after a lightweight finetuning, H1B-KV matches\nfull-precision performance not only on perplexity benchmarks but also on\ncomplex downstream tasks like mathematical reasoning (GSM8K), multi-task\nunderstanding (MMLU), and code generation (HumanEval). Our results show H1B-KV\nsignificantly outperforms leading quantization (KIVI), token eviction\n(SparseLLM), and key-only sketching (Loki) methods in quality-per-byte,\nestablishing it as a robust solution for deploying LLMs in memory-constrained\nenvironments."
                },
                "authors": [
                    {
                        "name": "Harshil Vejendla"
                    }
                ],
                "author_detail": {
                    "name": "Harshil Vejendla"
                },
                "author": "Harshil Vejendla",
                "arxiv_comment": "MIT URTC 2025 Technical Paper (Oral), 5 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05476v1",
                "updated": "2025-10-07T00:32:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    0,
                    32,
                    45,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T00:32:45Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    0,
                    32,
                    45,
                    1,
                    280,
                    0
                ],
                "title": "cMPI: Using CXL Memory Sharing for MPI One-Sided and Two-Sided\n  Inter-Node Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cMPI: Using CXL Memory Sharing for MPI One-Sided and Two-Sided\n  Inter-Node Communications"
                },
                "summary": "Message Passing Interface (MPI) is a foundational programming model for\nhigh-performance computing. MPI libraries traditionally employ network\ninterconnects (e.g., Ethernet and InfiniBand) and network protocols (e.g., TCP\nand RoCE) with complex software stacks for cross-node communication. We present\ncMPI, the first work to optimize MPI point-to-point communication (both\none-sided and two-sided) using CXL memory sharing on a real CXL platform,\ntransforming cross-node communication into memory transactions and data copies\nwithin CXL memory, bypassing traditional network protocols. We analyze\nperformance across various interconnects and find that CXL memory sharing\nachieves 7.2x-8.1x lower latency than TCP-based interconnects deployed in\nsmall- and medium-scale clusters. We address challenges of CXL memory sharing\nfor MPI communication, including data object management over the dax\nrepresentation [50], cache coherence, and atomic operations. Overall, cMPI\noutperforms TCP over standard Ethernet NIC and high-end SmartNIC by up to 49x\nand 72x in latency and bandwidth, respectively, for small messages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Message Passing Interface (MPI) is a foundational programming model for\nhigh-performance computing. MPI libraries traditionally employ network\ninterconnects (e.g., Ethernet and InfiniBand) and network protocols (e.g., TCP\nand RoCE) with complex software stacks for cross-node communication. We present\ncMPI, the first work to optimize MPI point-to-point communication (both\none-sided and two-sided) using CXL memory sharing on a real CXL platform,\ntransforming cross-node communication into memory transactions and data copies\nwithin CXL memory, bypassing traditional network protocols. We analyze\nperformance across various interconnects and find that CXL memory sharing\nachieves 7.2x-8.1x lower latency than TCP-based interconnects deployed in\nsmall- and medium-scale clusters. We address challenges of CXL memory sharing\nfor MPI communication, including data object management over the dax\nrepresentation [50], cache coherence, and atomic operations. Overall, cMPI\noutperforms TCP over standard Ethernet NIC and high-end SmartNIC by up to 49x\nand 72x in latency and bandwidth, respectively, for small messages."
                },
                "authors": [
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Bin Ma"
                    },
                    {
                        "name": "Jongryool Kim"
                    },
                    {
                        "name": "Byungil Koh"
                    },
                    {
                        "name": "Hoshik Kim"
                    },
                    {
                        "name": "Dong Li"
                    }
                ],
                "author_detail": {
                    "name": "Dong Li"
                },
                "author": "Dong Li",
                "arxiv_doi": "10.1145/3712285.3759816",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3712285.3759816",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.05476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05373v1",
                "updated": "2025-10-06T21:08:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    21,
                    8,
                    11,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T21:08:11Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    21,
                    8,
                    11,
                    0,
                    279,
                    0
                ],
                "title": "KVLinC : KV Cache Quantization with Hadamard Rotation and Linear\n  Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVLinC : KV Cache Quantization with Hadamard Rotation and Linear\n  Correction"
                },
                "summary": "Quantizing the key-value (KV) cache is a promising strategy for improving the\ninference efficiency of large language models (LLMs). However, aggressive\nquantization to very low precision (e.g., 2 bits) introduces significant errors\nin the stored key and value tensors, which propagate through the dot-product\nattention mechanism and ultimately degrade generation quality. To address this,\nwe propose KVLinC, a framework to mitigate attention errors introduced by KV\ncache quantization in the extreme low-precision regime. KVLinC combines a\nHadamard rotation, which reduces quantization error in values, with lightweight\nlinear correction adapters that explicitly compensate for errors introduced by\nquantized keys. Across extensive evaluations on the LLaMA, Qwen2.5, and Qwen3\nmodel families, KVLinC consistently matches or surpasses strong baselines while\nachieving higher KV-cache compression. Furthermore, we implement a custom\nattention kernel that results in upto 2.55x faster inference compared to Flash\nAttention baseline, enabling efficient long-context LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantizing the key-value (KV) cache is a promising strategy for improving the\ninference efficiency of large language models (LLMs). However, aggressive\nquantization to very low precision (e.g., 2 bits) introduces significant errors\nin the stored key and value tensors, which propagate through the dot-product\nattention mechanism and ultimately degrade generation quality. To address this,\nwe propose KVLinC, a framework to mitigate attention errors introduced by KV\ncache quantization in the extreme low-precision regime. KVLinC combines a\nHadamard rotation, which reduces quantization error in values, with lightweight\nlinear correction adapters that explicitly compensate for errors introduced by\nquantized keys. Across extensive evaluations on the LLaMA, Qwen2.5, and Qwen3\nmodel families, KVLinC consistently matches or surpasses strong baselines while\nachieving higher KV-cache compression. Furthermore, we implement a custom\nattention kernel that results in upto 2.55x faster inference compared to Flash\nAttention baseline, enabling efficient long-context LLM inference."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "14 pages, 7 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05367v1",
                "updated": "2025-10-06T20:54:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    20,
                    54,
                    44,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T20:54:44Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    20,
                    54,
                    44,
                    0,
                    279,
                    0
                ],
                "title": "LightCache: Memory-Efficient, Training-Free Acceleration for Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightCache: Memory-Efficient, Training-Free Acceleration for Video\n  Generation"
                },
                "summary": "Training-free acceleration has emerged as an advanced research area in video\ngeneration based on diffusion models. The redundancy of latents in diffusion\nmodel inference provides a natural entry point for acceleration. In this paper,\nwe decompose the inference process into the encoding, denoising, and decoding\nstages, and observe that cache-based acceleration methods often lead to\nsubstantial memory surges in the latter two stages. To address this problem, we\nanalyze the characteristics of inference across different stages and propose\nstage-specific strategies for reducing memory consumption: 1) Asynchronous\nCache Swapping. 2) Feature chunk. 3) Slicing latents to decode. At the same\ntime, we ensure that the time overhead introduced by these three strategies\nremains lower than the acceleration gains themselves. Compared with the\nbaseline, our approach achieves faster inference speed and lower memory usage,\nwhile maintaining quality degradation within an acceptable range. The Code is\navailable at https://github.com/NKUShaw/LightCache .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-free acceleration has emerged as an advanced research area in video\ngeneration based on diffusion models. The redundancy of latents in diffusion\nmodel inference provides a natural entry point for acceleration. In this paper,\nwe decompose the inference process into the encoding, denoising, and decoding\nstages, and observe that cache-based acceleration methods often lead to\nsubstantial memory surges in the latter two stages. To address this problem, we\nanalyze the characteristics of inference across different stages and propose\nstage-specific strategies for reducing memory consumption: 1) Asynchronous\nCache Swapping. 2) Feature chunk. 3) Slicing latents to decode. At the same\ntime, we ensure that the time overhead introduced by these three strategies\nremains lower than the acceleration gains themselves. Compared with the\nbaseline, our approach achieves faster inference speed and lower memory usage,\nwhile maintaining quality degradation within an acceptable range. The Code is\navailable at https://github.com/NKUShaw/LightCache ."
                },
                "authors": [
                    {
                        "name": "Yang Xiao"
                    },
                    {
                        "name": "Gen Li"
                    },
                    {
                        "name": "Kaiyuan Deng"
                    },
                    {
                        "name": "Yushu Wu"
                    },
                    {
                        "name": "Zheng Zhan"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Xiaolong Ma"
                    },
                    {
                        "name": "Bo Hui"
                    }
                ],
                "author_detail": {
                    "name": "Bo Hui"
                },
                "author": "Bo Hui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06416v2",
                "updated": "2025-10-06T17:09:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    9,
                    39,
                    0,
                    279,
                    0
                ],
                "published": "2025-04-08T20:32:10Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    32,
                    10,
                    1,
                    98,
                    0
                ],
                "title": "Unifying Autoregressive and Diffusion-Based Sequence Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Autoregressive and Diffusion-Based Sequence Generation"
                },
                "summary": "We present significant extensions to diffusion-based sequence generation\nmodels, blurring the line with autoregressive language models. We introduce\nhyperschedules, which assign distinct noise schedules to individual token\npositions, generalizing both autoregressive models (e.g., GPT) and conventional\ndiffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two\nhybrid token-wise noising processes that interpolate between absorbing and\nuniform processes, enabling the model to fix past mistakes, and we introduce a\nnovel inference algorithm that leverages this new feature in a simplified\ncontext inspired from MDLM. To support efficient training and inference, we\ndesign attention masks compatible with KV-caching. Our methods achieve\nstate-of-the-art perplexity and generate diverse, high-quality sequences across\nstandard benchmarks, suggesting a promising path for autoregressive\ndiffusion-based sequence generation. See code and resources at\nhttps://hdlm-colm.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present significant extensions to diffusion-based sequence generation\nmodels, blurring the line with autoregressive language models. We introduce\nhyperschedules, which assign distinct noise schedules to individual token\npositions, generalizing both autoregressive models (e.g., GPT) and conventional\ndiffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two\nhybrid token-wise noising processes that interpolate between absorbing and\nuniform processes, enabling the model to fix past mistakes, and we introduce a\nnovel inference algorithm that leverages this new feature in a simplified\ncontext inspired from MDLM. To support efficient training and inference, we\ndesign attention masks compatible with KV-caching. Our methods achieve\nstate-of-the-art perplexity and generate diverse, high-quality sequences across\nstandard benchmarks, suggesting a promising path for autoregressive\ndiffusion-based sequence generation. See code and resources at\nhttps://hdlm-colm.github.io/"
                },
                "authors": [
                    {
                        "name": "Nima Fathi"
                    },
                    {
                        "name": "Torsten Scholak"
                    },
                    {
                        "name": "Pierre-Andr Nol"
                    }
                ],
                "author_detail": {
                    "name": "Pierre-Andr Nol"
                },
                "author": "Pierre-Andr Nol",
                "arxiv_comment": "Published as a conference paper at COLM 2025 Website:\n  https://hdlm-colm.github.io/",
                "arxiv_journal_ref": "Second Conference on Language Modeling,\n  https://openreview.net/forum?id=rgq9BFXSFl (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19341v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19341v2",
                "updated": "2025-10-06T13:23:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    13,
                    23,
                    4,
                    0,
                    279,
                    0
                ],
                "published": "2025-09-16T09:14:15Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    14,
                    15,
                    1,
                    259,
                    0
                ],
                "title": "Fine-Grained AI Model Caching and Downloading With Coordinated\n  Multipoint Broadcasting in Multi-Cell Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained AI Model Caching and Downloading With Coordinated\n  Multipoint Broadcasting in Multi-Cell Edge Networks"
                },
                "summary": "6G networks are envisioned to support on-demand AI model downloading to\naccommodate diverse inference requirements of end users. By proactively caching\nmodels at edge nodes, users can retrieve the requested models with low latency\nfor on-device AI inference. However, the substantial size of contemporary AI\nmodels poses significant challenges for edge caching under limited storage\ncapacity, as well as for the concurrent delivery of heterogeneous models over\nwireless channels. To address these challenges, we propose a fine-grained AI\nmodel caching and downloading system that exploits parameter reusability,\nstemming from the common practice of fine-tuning task-specific models from a\nshared pre-trained model with frozen parameters. This system selectively caches\nmodel parameter blocks (PBs) at edge nodes, eliminating redundant storage of\nreusable parameters across different cached models. Additionally, it\nincorporates coordinated multipoint (CoMP) broadcasting to simultaneously\ndeliver reusable PBs to multiple users, thereby enhancing downlink spectrum\nutilization. Under this arrangement, we formulate a model downloading delay\nminimization problem to jointly optimize PB caching, migration (among edge\nnodes), and broadcasting beamforming. To tackle this intractable problem, we\ndevelop a distributed multi-agent learning framework that enables edge nodes to\nexplicitly learn mutual influence among their actions, thereby facilitating\ncooperation. Furthermore, a data augmentation approach is proposed to\nadaptively generate synthetic training samples through a predictive model,\nboosting sample efficiency and accelerating policy learning. Both theoretical\nanalysis and simulation experiments validate the superior convergence\nperformance of the proposed learning framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6G networks are envisioned to support on-demand AI model downloading to\naccommodate diverse inference requirements of end users. By proactively caching\nmodels at edge nodes, users can retrieve the requested models with low latency\nfor on-device AI inference. However, the substantial size of contemporary AI\nmodels poses significant challenges for edge caching under limited storage\ncapacity, as well as for the concurrent delivery of heterogeneous models over\nwireless channels. To address these challenges, we propose a fine-grained AI\nmodel caching and downloading system that exploits parameter reusability,\nstemming from the common practice of fine-tuning task-specific models from a\nshared pre-trained model with frozen parameters. This system selectively caches\nmodel parameter blocks (PBs) at edge nodes, eliminating redundant storage of\nreusable parameters across different cached models. Additionally, it\nincorporates coordinated multipoint (CoMP) broadcasting to simultaneously\ndeliver reusable PBs to multiple users, thereby enhancing downlink spectrum\nutilization. Under this arrangement, we formulate a model downloading delay\nminimization problem to jointly optimize PB caching, migration (among edge\nnodes), and broadcasting beamforming. To tackle this intractable problem, we\ndevelop a distributed multi-agent learning framework that enables edge nodes to\nexplicitly learn mutual influence among their actions, thereby facilitating\ncooperation. Furthermore, a data augmentation approach is proposed to\nadaptively generate synthetic training samples through a predictive model,\nboosting sample efficiency and accelerating policy learning. Both theoretical\nanalysis and simulation experiments validate the superior convergence\nperformance of the proposed learning framework."
                },
                "authors": [
                    {
                        "name": "Yang Fu"
                    },
                    {
                        "name": "Peng Qin"
                    },
                    {
                        "name": "Yueyue Zhang"
                    },
                    {
                        "name": "Pao Cheng"
                    },
                    {
                        "name": "Jun Lu"
                    },
                    {
                        "name": "Yifei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yifei Wang"
                },
                "author": "Yifei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19341v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19341v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04646v1",
                "updated": "2025-10-06T09:49:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    9,
                    49,
                    14,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T09:49:14Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    9,
                    49,
                    14,
                    0,
                    279,
                    0
                ],
                "title": "Predictive Feature Caching for Training-free Acceleration of Molecular\n  Geometry Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictive Feature Caching for Training-free Acceleration of Molecular\n  Geometry Generation"
                },
                "summary": "Flow matching models generate high-fidelity molecular geometries but incur\nsignificant computational costs during inference, requiring hundreds of network\nevaluations. This inference overhead becomes the primary bottleneck when such\nmodels are employed in practice to sample large numbers of molecular\ncandidates. This work discusses a training-free caching strategy that\naccelerates molecular geometry generation by predicting intermediate hidden\nstates across solver steps. The proposed method operates directly on the\nSE(3)-equivariant backbone, is compatible with pretrained models, and is\northogonal to existing training-based accelerations and system-level\noptimizations. Experiments on the GEOM-Drugs dataset demonstrate that caching\nachieves a twofold reduction in wall-clock inference time at matched sample\nquality and a speedup of up to 3x compared to the base model with minimal\nsample quality degradation. Because these gains compound with other\noptimizations, applying caching alongside other general, lossless optimizations\nyield as much as a 7x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow matching models generate high-fidelity molecular geometries but incur\nsignificant computational costs during inference, requiring hundreds of network\nevaluations. This inference overhead becomes the primary bottleneck when such\nmodels are employed in practice to sample large numbers of molecular\ncandidates. This work discusses a training-free caching strategy that\naccelerates molecular geometry generation by predicting intermediate hidden\nstates across solver steps. The proposed method operates directly on the\nSE(3)-equivariant backbone, is compatible with pretrained models, and is\northogonal to existing training-based accelerations and system-level\noptimizations. Experiments on the GEOM-Drugs dataset demonstrate that caching\nachieves a twofold reduction in wall-clock inference time at matched sample\nquality and a speedup of up to 3x compared to the base model with minimal\nsample quality degradation. Because these gains compound with other\noptimizations, applying caching alongside other general, lossless optimizations\nyield as much as a 7x speedup."
                },
                "authors": [
                    {
                        "name": "Johanna Sommer"
                    },
                    {
                        "name": "John Rachwan"
                    },
                    {
                        "name": "Nils Fleischmann"
                    },
                    {
                        "name": "Stephan Gnnemann"
                    },
                    {
                        "name": "Bertrand Charpentier"
                    }
                ],
                "author_detail": {
                    "name": "Bertrand Charpentier"
                },
                "author": "Bertrand Charpentier",
                "arxiv_comment": "Accepted at the AI for Science Workshop @ NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04525v1",
                "updated": "2025-10-06T06:30:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    6,
                    30,
                    22,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T06:30:22Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    6,
                    30,
                    22,
                    0,
                    279,
                    0
                ],
                "title": "Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in\n  Masked Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in\n  Masked Diffusion"
                },
                "summary": "Masked diffusion models have shown promising performance in generating\nhigh-quality samples in a wide range of domains, but accelerating their\nsampling process remains relatively underexplored. To investigate efficient\nsamplers for masked diffusion, this paper theoretically analyzes the MaskGIT\nsampler for image modeling, revealing its implicit temperature sampling\nmechanism. Through this analysis, we introduce the \"moment sampler,\" an\nasymptotically equivalent but more tractable and interpretable alternative to\nMaskGIT, which employs a \"choose-then-sample\" approach by selecting unmasking\npositions before sampling tokens. In addition, we improve the efficiency of\nchoose-then-sample algorithms through two key innovations: a partial caching\ntechnique for transformers that approximates longer sampling trajectories\nwithout proportional computational cost, and a hybrid approach formalizing the\nexploration-exploitation trade-off in adaptive unmasking. Experiments in image\nand text domains demonstrate our theory as well as the efficiency of our\nproposed methods, advancing both theoretical understanding and practical\nimplementation of masked diffusion samplers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked diffusion models have shown promising performance in generating\nhigh-quality samples in a wide range of domains, but accelerating their\nsampling process remains relatively underexplored. To investigate efficient\nsamplers for masked diffusion, this paper theoretically analyzes the MaskGIT\nsampler for image modeling, revealing its implicit temperature sampling\nmechanism. Through this analysis, we introduce the \"moment sampler,\" an\nasymptotically equivalent but more tractable and interpretable alternative to\nMaskGIT, which employs a \"choose-then-sample\" approach by selecting unmasking\npositions before sampling tokens. In addition, we improve the efficiency of\nchoose-then-sample algorithms through two key innovations: a partial caching\ntechnique for transformers that approximates longer sampling trajectories\nwithout proportional computational cost, and a hybrid approach formalizing the\nexploration-exploitation trade-off in adaptive unmasking. Experiments in image\nand text domains demonstrate our theory as well as the efficiency of our\nproposed methods, advancing both theoretical understanding and practical\nimplementation of masked diffusion samplers."
                },
                "authors": [
                    {
                        "name": "Satoshi Hayakawa"
                    },
                    {
                        "name": "Yuhta Takida"
                    },
                    {
                        "name": "Masaaki Imaizumi"
                    },
                    {
                        "name": "Hiromi Wakaki"
                    },
                    {
                        "name": "Yuki Mitsufuji"
                    }
                ],
                "author_detail": {
                    "name": "Yuki Mitsufuji"
                },
                "author": "Yuki Mitsufuji",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04492v1",
                "updated": "2025-10-06T05:04:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    5,
                    4,
                    57,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T05:04:57Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    5,
                    4,
                    57,
                    0,
                    279,
                    0
                ],
                "title": "Joint Probing and Scheduling for Cache-Aided Hybrid\n  Satellite-Terrestrial Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Probing and Scheduling for Cache-Aided Hybrid\n  Satellite-Terrestrial Networks"
                },
                "summary": "Caching is crucial in hybrid satellite-terrestrial networks to reduce\nlatency, optimize throughput, and improve data availability by storing\nfrequently accessed content closer to users, especially in bandwidth-limited\nsatellite systems, requiring strategic Medium Access Control (MAC) layer. This\npaper addresses throughput optimization in satellite-terrestrial integrated\nnetworks through opportunistic cooperative caching. We propose a joint probing\nand scheduling strategy to enhance content retrieval efficiency. The strategy\nleverages the LEO satellite to probe satellite-to-ground links and cache states\nof multiple cooperative terrestrial stations, enabling dynamic user scheduling\nfor content delivery. Using an optimal stopping theoretic approach with two\nlevels of incomplete information, we make real-time decisions on\nsatellite-terrestrial hybrid links and caching probing. Our threshold-based\nstrategy optimizes probing and scheduling, significantly improving average\nsystem throughput by exploiting cooperative caching, satellite-terrestrial link\ntransmission, and time diversity from dynamic user requests. Simulation results\nvalidate the effectiveness and practicality of the proposed strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching is crucial in hybrid satellite-terrestrial networks to reduce\nlatency, optimize throughput, and improve data availability by storing\nfrequently accessed content closer to users, especially in bandwidth-limited\nsatellite systems, requiring strategic Medium Access Control (MAC) layer. This\npaper addresses throughput optimization in satellite-terrestrial integrated\nnetworks through opportunistic cooperative caching. We propose a joint probing\nand scheduling strategy to enhance content retrieval efficiency. The strategy\nleverages the LEO satellite to probe satellite-to-ground links and cache states\nof multiple cooperative terrestrial stations, enabling dynamic user scheduling\nfor content delivery. Using an optimal stopping theoretic approach with two\nlevels of incomplete information, we make real-time decisions on\nsatellite-terrestrial hybrid links and caching probing. Our threshold-based\nstrategy optimizes probing and scheduling, significantly improving average\nsystem throughput by exploiting cooperative caching, satellite-terrestrial link\ntransmission, and time diversity from dynamic user requests. Simulation results\nvalidate the effectiveness and practicality of the proposed strategies."
                },
                "authors": [
                    {
                        "name": "Zhou Zhang"
                    },
                    {
                        "name": "Yizhu Wang"
                    },
                    {
                        "name": "Saman Atapattu"
                    },
                    {
                        "name": "Sumei Sun"
                    }
                ],
                "author_detail": {
                    "name": "Sumei Sun"
                },
                "author": "Sumei Sun",
                "arxiv_comment": "6 pages, IEEE Global Communications Conference (GLOBECOM), December\n  2025, Taipei, Taiwan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07120v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07120v3",
                "updated": "2025-10-06T04:28:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    4,
                    28,
                    5,
                    0,
                    279,
                    0
                ],
                "published": "2025-03-10T09:49:18Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    49,
                    18,
                    0,
                    69,
                    0
                ],
                "title": "FEB-Cache: Frequency-Guided Exposure Bias Reduction for Enhancing\n  Diffusion Transformer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FEB-Cache: Frequency-Guided Exposure Bias Reduction for Enhancing\n  Diffusion Transformer Caching"
                },
                "summary": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis issue, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing why caching damage the generation processes. In this paper, we first\nconfirm that the cache greatly amplifies the exposure bias, resulting in a\ndecline in the generation quality. However, directly applying noise scaling is\nchallenging for this issue due to the non-smoothness of exposure bias. We found\nthat this phenomenon stems from the mismatch between its frequency response\ncharacteristics and the simple cache of Attention and MLP. Since these two\ncomponents exhibit unique preferences for frequency signals, which provides us\nwith a caching strategy to separate Attention and MLP to achieve an enhanced\nfit of exposure bias and reduce it. Based on this, we introduced FEB-Cache, a\njoint caching strategy that aligns with the non-exposed bias diffusion process\n(which gives us a higher performance cap) of caching Attention and MLP based on\nthe frequency-guided cache table. Our approach combines a comprehensive\nunderstanding of the caching mechanism and offers a new perspective on\nleveraging caching to accelerate the diffusion process. Empirical results\nindicate that FEB-Cache optimizes model performance while concurrently\nfacilitating acceleration. Code is available at\nhttps://github.com/aSleepyTree/EB-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis issue, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing why caching damage the generation processes. In this paper, we first\nconfirm that the cache greatly amplifies the exposure bias, resulting in a\ndecline in the generation quality. However, directly applying noise scaling is\nchallenging for this issue due to the non-smoothness of exposure bias. We found\nthat this phenomenon stems from the mismatch between its frequency response\ncharacteristics and the simple cache of Attention and MLP. Since these two\ncomponents exhibit unique preferences for frequency signals, which provides us\nwith a caching strategy to separate Attention and MLP to achieve an enhanced\nfit of exposure bias and reduce it. Based on this, we introduced FEB-Cache, a\njoint caching strategy that aligns with the non-exposed bias diffusion process\n(which gives us a higher performance cap) of caching Attention and MLP based on\nthe frequency-guided cache table. Our approach combines a comprehensive\nunderstanding of the caching mechanism and offers a new perspective on\nleveraging caching to accelerate the diffusion process. Empirical results\nindicate that FEB-Cache optimizes model performance while concurrently\nfacilitating acceleration. Code is available at\nhttps://github.com/aSleepyTree/EB-Cache."
                },
                "authors": [
                    {
                        "name": "Zhen Zou"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07120v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07120v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04476v1",
                "updated": "2025-10-06T04:24:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    4,
                    24,
                    23,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T04:24:23Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    4,
                    24,
                    23,
                    0,
                    279,
                    0
                ],
                "title": "Compressed Convolutional Attention: Efficient Attention in a Compressed\n  Latent Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressed Convolutional Attention: Efficient Attention in a Compressed\n  Latent Space"
                },
                "summary": "Multi-headed Attention's (MHA) quadratic compute and linearly growing\nKV-cache make long-context transformers expensive to train and serve. Prior\nworks such as Grouped Query Attention (GQA) and Multi-Latent Attention (MLA)\nshrink the cache, speeding decode, but leave compute, which determines prefill\nand training speed, largely unchanged. We introduce Compressed Convolutional\nAttention (CCA), a novel attention method which down-projects queries, keys,\nand values and performs the entire attention operation inside the shared latent\nspace. This simple design dramatically cuts parameters, KV-cache, and FLOPs all\nat once by the desired compression factor. Because CCA is orthogonal to\nhead-sharing, we combine the two to form Compressed Convolutional Grouped Query\nAttention (CCGQA), which further tightens the compute-bandwidth Pareto frontier\nso that users can tune compression toward either FLOP or memory limits without\nsacrificing quality. Experiments show that CCGQA consistently outperforms both\nGQA and MLA at equal KV-cache compression on dense and MoE models.\nAdditionally, we show that CCGQA outperforms all other attention methods on MoE\nmodels with half the KV-cache of GQA and MLA, achieving an 8x KV-cache\ncompression with no drop in performance compared to standard MHA. CCA and CCGQA\nalso dramatically reduce the FLOP cost of attention which leads to\nsubstantially faster training and prefill than existing methods. On H100 GPUs,\nour fused CCA/CCGQA kernel reduces prefill latency by about 1.7x at a sequence\nlength of 16k relative to MHA, and accelerates backward by about 1.3x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-headed Attention's (MHA) quadratic compute and linearly growing\nKV-cache make long-context transformers expensive to train and serve. Prior\nworks such as Grouped Query Attention (GQA) and Multi-Latent Attention (MLA)\nshrink the cache, speeding decode, but leave compute, which determines prefill\nand training speed, largely unchanged. We introduce Compressed Convolutional\nAttention (CCA), a novel attention method which down-projects queries, keys,\nand values and performs the entire attention operation inside the shared latent\nspace. This simple design dramatically cuts parameters, KV-cache, and FLOPs all\nat once by the desired compression factor. Because CCA is orthogonal to\nhead-sharing, we combine the two to form Compressed Convolutional Grouped Query\nAttention (CCGQA), which further tightens the compute-bandwidth Pareto frontier\nso that users can tune compression toward either FLOP or memory limits without\nsacrificing quality. Experiments show that CCGQA consistently outperforms both\nGQA and MLA at equal KV-cache compression on dense and MoE models.\nAdditionally, we show that CCGQA outperforms all other attention methods on MoE\nmodels with half the KV-cache of GQA and MLA, achieving an 8x KV-cache\ncompression with no drop in performance compared to standard MHA. CCA and CCGQA\nalso dramatically reduce the FLOP cost of attention which leads to\nsubstantially faster training and prefill than existing methods. On H100 GPUs,\nour fused CCA/CCGQA kernel reduces prefill latency by about 1.7x at a sequence\nlength of 16k relative to MHA, and accelerates backward by about 1.3x."
                },
                "authors": [
                    {
                        "name": "Tomas Figliolia"
                    },
                    {
                        "name": "Nicholas Alonso"
                    },
                    {
                        "name": "Rishi Iyer"
                    },
                    {
                        "name": "Quentin Anthony"
                    },
                    {
                        "name": "Beren Millidge"
                    }
                ],
                "author_detail": {
                    "name": "Beren Millidge"
                },
                "author": "Beren Millidge",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.18149v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.18149v2",
                "updated": "2025-10-06T02:46:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    2,
                    46,
                    1,
                    0,
                    279,
                    0
                ],
                "published": "2024-03-26T23:17:05Z",
                "published_parsed": [
                    2024,
                    3,
                    26,
                    23,
                    17,
                    5,
                    1,
                    86,
                    0
                ],
                "title": "Code Generation and Conic Constraints for Model-Predictive Control on\n  Microcontrollers with Conic-TinyMPC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Generation and Conic Constraints for Model-Predictive Control on\n  Microcontrollers with Conic-TinyMPC"
                },
                "summary": "Model-predictive control (MPC) is a powerful framework for controlling\ndynamic systems under constraints, but it remains challenging to deploy on\nresource-constrained platforms, especially for problems involving conic\nconstraints. To address this, we extend recent work developing fast,\nstructure-exploiting, cached ADMM solvers for embedded applications, to provide\nsupport for second-order cones, as well as C++ code generation from Python,\nMATLAB, and Julia for easy deployment. Microcontroller benchmarks show that our\nsolver provides up to a two-order-of-magnitude speedup, ranging from 10.6x to\n142.7x, over state-of-the-art embedded solvers on QP and SOCP problems, and\nenables us to fit order-of-magnitude larger problems in memory. We validate our\nsolver's deployed performance through simulation and hardware experiments,\nincluding conically-constrained trajectory tracking on a 27g Crazyflie\nquadrotor. To get started with Conic-TinyMPC, visit our documentation,\nexamples, and the open-source codebase at https://tinympc.org.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-predictive control (MPC) is a powerful framework for controlling\ndynamic systems under constraints, but it remains challenging to deploy on\nresource-constrained platforms, especially for problems involving conic\nconstraints. To address this, we extend recent work developing fast,\nstructure-exploiting, cached ADMM solvers for embedded applications, to provide\nsupport for second-order cones, as well as C++ code generation from Python,\nMATLAB, and Julia for easy deployment. Microcontroller benchmarks show that our\nsolver provides up to a two-order-of-magnitude speedup, ranging from 10.6x to\n142.7x, over state-of-the-art embedded solvers on QP and SOCP problems, and\nenables us to fit order-of-magnitude larger problems in memory. We validate our\nsolver's deployed performance through simulation and hardware experiments,\nincluding conically-constrained trajectory tracking on a 27g Crazyflie\nquadrotor. To get started with Conic-TinyMPC, visit our documentation,\nexamples, and the open-source codebase at https://tinympc.org."
                },
                "authors": [
                    {
                        "name": "Ishaan Mahajan"
                    },
                    {
                        "name": "Khai Nguyen"
                    },
                    {
                        "name": "Sam Schoedel"
                    },
                    {
                        "name": "Elakhya Nedumaran"
                    },
                    {
                        "name": "Moises Mata"
                    },
                    {
                        "name": "Brian Plancher"
                    },
                    {
                        "name": "Zachary Manchester"
                    }
                ],
                "author_detail": {
                    "name": "Zachary Manchester"
                },
                "author": "Zachary Manchester",
                "arxiv_comment": "First three authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.18149v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.18149v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v6",
                "updated": "2025-10-05T22:17:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    22,
                    17,
                    34,
                    6,
                    278,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Cache Selection for LLMs via Attention Output Error-Based\n  Token Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Cache Selection for LLMs via Attention Output Error-Based\n  Token Eviction"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value tokens on top of\nattention-based eviction scores in closed-form. Additionally, CAOTE can act as\na meta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value tokens on top of\nattention-based eviction scores in closed-form. Additionally, CAOTE can act as\na meta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "15 pages, 3 figures, 13 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00384v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00384v2",
                "updated": "2025-10-05T21:29:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    21,
                    29,
                    28,
                    6,
                    278,
                    0
                ],
                "published": "2025-05-31T04:27:22Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    4,
                    27,
                    22,
                    5,
                    151,
                    0
                ],
                "title": "Learning Semantics, Not Addresses: Runtime Neural Prefetching for Far\n  Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Semantics, Not Addresses: Runtime Neural Prefetching for Far\n  Memory"
                },
                "summary": "Memory prefetching has long boosted CPU caches and is increasingly vital for\nfar-memory systems, where large portions of memory are offloaded to cheaper,\nremote tiers. While effective prefetching requires accurate prediction of\nfuture accesses, prior ML approaches have been limited to simulation or\nsmall-scale hardware. We introduce FarSight, the first Linux-based far-memory\nsystem to leverage deep learning by decoupling application semantics from\nruntime memory layout. This separation enables offline-trained models to\npredict access patterns over a compact ordinal vocabulary, which are resolved\nat runtime through lightweight mappings. Across four data-intensive workloads,\nFarSight delivers up to 3.6x higher performance than the state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory prefetching has long boosted CPU caches and is increasingly vital for\nfar-memory systems, where large portions of memory are offloaded to cheaper,\nremote tiers. While effective prefetching requires accurate prediction of\nfuture accesses, prior ML approaches have been limited to simulation or\nsmall-scale hardware. We introduce FarSight, the first Linux-based far-memory\nsystem to leverage deep learning by decoupling application semantics from\nruntime memory layout. This separation enables offline-trained models to\npredict access patterns over a compact ordinal vocabulary, which are resolved\nat runtime through lightweight mappings. Across four data-intensive workloads,\nFarSight delivers up to 3.6x higher performance than the state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Yutong Huang"
                    },
                    {
                        "name": "Zhiyuan Guo"
                    },
                    {
                        "name": "Yiying Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiying Zhang"
                },
                "author": "Yiying Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00384v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00384v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09253v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09253v2",
                "updated": "2025-10-05T18:13:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    18,
                    13,
                    39,
                    6,
                    278,
                    0
                ],
                "published": "2025-01-16T02:40:07Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    2,
                    40,
                    7,
                    3,
                    16,
                    0
                ],
                "title": "PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid\n  Resolution Diffusion Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid\n  Resolution Diffusion Serving"
                },
                "summary": "The Text-to-Image (T2I) diffusion model has emerged as one of the most widely\nadopted generative models. However, serving diffusion models at the granularity\nof entire images introduces significant challenges, particularly under\nmulti-resolution workloads. First, image-level serving obstructs batching\nacross requests. Second, heterogeneous resolutions exhibit distinct locality\ncharacteristics, making it difficult to apply a uniform cache policy\neffectively.\n  To address these challenges, we present PatchedServe, a Patch Management\nFramework for SLO-Optimized Hybrid-Resolution Diffusion Serving. PatchedServe\nis the first SLO-optimized T2I diffusion serving framework designed to handle\nheterogeneous resolutions. Specifically, it incorporates a novel patch-based\nprocessing workflow that substantially improves throughput for\nhybrid-resolution inputs. Moreover, PatchedServe devises a patch-level cache\nreuse policy to fully exploit diffusion redundancies and integrates an\nSLO-aware scheduling algorithm with lightweight online latency prediction to\nimprove responsiveness. Our evaluation demonstrates that PatchedServe achieves\n30.1 % higher SLO satisfaction than the state-of-the-art diffusion serving\nsystem, while preserving image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Text-to-Image (T2I) diffusion model has emerged as one of the most widely\nadopted generative models. However, serving diffusion models at the granularity\nof entire images introduces significant challenges, particularly under\nmulti-resolution workloads. First, image-level serving obstructs batching\nacross requests. Second, heterogeneous resolutions exhibit distinct locality\ncharacteristics, making it difficult to apply a uniform cache policy\neffectively.\n  To address these challenges, we present PatchedServe, a Patch Management\nFramework for SLO-Optimized Hybrid-Resolution Diffusion Serving. PatchedServe\nis the first SLO-optimized T2I diffusion serving framework designed to handle\nheterogeneous resolutions. Specifically, it incorporates a novel patch-based\nprocessing workflow that substantially improves throughput for\nhybrid-resolution inputs. Moreover, PatchedServe devises a patch-level cache\nreuse policy to fully exploit diffusion redundancies and integrates an\nSLO-aware scheduling algorithm with lightweight online latency prediction to\nimprove responsiveness. Our evaluation demonstrates that PatchedServe achieves\n30.1 % higher SLO satisfaction than the state-of-the-art diffusion serving\nsystem, while preserving image quality."
                },
                "authors": [
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Zepeng Zhao"
                    },
                    {
                        "name": "Yuke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuke Wang"
                },
                "author": "Yuke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09253v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09253v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04188v1",
                "updated": "2025-10-05T13:01:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    13,
                    1,
                    8,
                    6,
                    278,
                    0
                ],
                "published": "2025-10-05T13:01:08Z",
                "published_parsed": [
                    2025,
                    10,
                    5,
                    13,
                    1,
                    8,
                    6,
                    278,
                    0
                ],
                "title": "Let Features Decide Their Own Solvers: Hybrid Feature Caching for\n  Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let Features Decide Their Own Solvers: Hybrid Feature Caching for\n  Diffusion Transformers"
                },
                "summary": "Diffusion Transformers offer state-of-the-art fidelity in image and video\nsynthesis, but their iterative sampling process remains a major bottleneck due\nto the high cost of transformer forward passes at each timestep. To mitigate\nthis, feature caching has emerged as a training-free acceleration technique\nthat reuses or forecasts hidden representations. However, existing methods\noften apply a uniform caching strategy across all feature dimensions, ignoring\ntheir heterogeneous dynamic behaviors. Therefore, we adopt a new perspective by\nmodeling hidden feature evolution as a mixture of ODEs across dimensions, and\nintroduce HyCa, a Hybrid ODE solver inspired caching framework that applies\ndimension-wise caching strategies. HyCa achieves near-lossless acceleration\nacross diverse domains and models, including 5.55 times speedup on FLUX, 5.56\ntimes speedup on HunyuanVideo, 6.24 times speedup on Qwen-Image and\nQwen-Image-Edit without retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers offer state-of-the-art fidelity in image and video\nsynthesis, but their iterative sampling process remains a major bottleneck due\nto the high cost of transformer forward passes at each timestep. To mitigate\nthis, feature caching has emerged as a training-free acceleration technique\nthat reuses or forecasts hidden representations. However, existing methods\noften apply a uniform caching strategy across all feature dimensions, ignoring\ntheir heterogeneous dynamic behaviors. Therefore, we adopt a new perspective by\nmodeling hidden feature evolution as a mixture of ODEs across dimensions, and\nintroduce HyCa, a Hybrid ODE solver inspired caching framework that applies\ndimension-wise caching strategies. HyCa achieves near-lossless acceleration\nacross diverse domains and models, including 5.55 times speedup on FLUX, 5.56\ntimes speedup on HunyuanVideo, 6.24 times speedup on Qwen-Image and\nQwen-Image-Edit without retraining."
                },
                "authors": [
                    {
                        "name": "Shikang Zheng"
                    },
                    {
                        "name": "Guantao Chen"
                    },
                    {
                        "name": "Qinming Zhou"
                    },
                    {
                        "name": "Yuqi Lin"
                    },
                    {
                        "name": "Lixuan He"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Peiliang Cai"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05176v1",
                "updated": "2025-10-05T12:09:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    12,
                    9,
                    14,
                    6,
                    278,
                    0
                ],
                "published": "2025-10-05T12:09:14Z",
                "published_parsed": [
                    2025,
                    10,
                    5,
                    12,
                    9,
                    14,
                    6,
                    278,
                    0
                ],
                "title": "PatternKV: Flattening KV Representation Expands Quantization Headroom",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PatternKV: Flattening KV Representation Expands Quantization Headroom"
                },
                "summary": "KV cache in autoregressive LLMs eliminates redundant recomputation but has\nemerged as the dominant memory and bandwidth bottleneck during inference,\nnotably with long contexts and test-time scaling. KV quantization is a key\nlever for reducing cache cost, but accuracy drops sharply as the native KV\ndistribution lacks flatness and thus maintains a wide quantization range. Prior\nwork focuses on isolating outliers, which caps their error but fails to flatten\nthe overall distribution, leaving performance fragile under low-bit settings.\nIn this work, we show that the K cache maintains a stable structure that\nevolves gradually with context, while the V cache carries latent semantic\nregularities. Building on these insights, we propose PatternKV, a\npattern-aligned residual quantization scheme. It mines representative pattern\nvectors online, aligns each KV vector to its nearest pattern, and quantizes\nonly the residual. This reshaping of the KV distribution flattens the\nquantization target and narrows its range, thereby improving the fidelity of\nlow-bit KV quantization. Across long-context and test-time scaling settings on\nmultiple backbones, PatternKV delivers consistent 2-bit gains, with a 0.08%\naverage 4-bit drop relative to FP16, improves test-time scaling accuracy by 10%\non average, and raises throughput by 1.4x while supporting 1.25x larger\nbatches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache in autoregressive LLMs eliminates redundant recomputation but has\nemerged as the dominant memory and bandwidth bottleneck during inference,\nnotably with long contexts and test-time scaling. KV quantization is a key\nlever for reducing cache cost, but accuracy drops sharply as the native KV\ndistribution lacks flatness and thus maintains a wide quantization range. Prior\nwork focuses on isolating outliers, which caps their error but fails to flatten\nthe overall distribution, leaving performance fragile under low-bit settings.\nIn this work, we show that the K cache maintains a stable structure that\nevolves gradually with context, while the V cache carries latent semantic\nregularities. Building on these insights, we propose PatternKV, a\npattern-aligned residual quantization scheme. It mines representative pattern\nvectors online, aligns each KV vector to its nearest pattern, and quantizes\nonly the residual. This reshaping of the KV distribution flattens the\nquantization target and narrows its range, thereby improving the fidelity of\nlow-bit KV quantization. Across long-context and test-time scaling settings on\nmultiple backbones, PatternKV delivers consistent 2-bit gains, with a 0.08%\naverage 4-bit drop relative to FP16, improves test-time scaling accuracy by 10%\non average, and raises throughput by 1.4x while supporting 1.25x larger\nbatches."
                },
                "authors": [
                    {
                        "name": "Ji Zhang"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Shaoxiong Feng"
                    },
                    {
                        "name": "Peiwen Yuan"
                    },
                    {
                        "name": "Xinglin Wang"
                    },
                    {
                        "name": "Jiayi Shi"
                    },
                    {
                        "name": "Yueqi Zhang"
                    },
                    {
                        "name": "Chuyi Tan"
                    },
                    {
                        "name": "Boyuan Pan"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Kan Li"
                    }
                ],
                "author_detail": {
                    "name": "Kan Li"
                },
                "author": "Kan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04153v1",
                "updated": "2025-10-05T11:09:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    11,
                    9,
                    10,
                    6,
                    278,
                    0
                ],
                "published": "2025-10-05T11:09:10Z",
                "published_parsed": [
                    2025,
                    10,
                    5,
                    11,
                    9,
                    10,
                    6,
                    278,
                    0
                ],
                "title": "ObCLIP: Oblivious CLoud-Device Hybrid Image Generation with Privacy\n  Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ObCLIP: Oblivious CLoud-Device Hybrid Image Generation with Privacy\n  Preservation"
                },
                "summary": "Diffusion Models have gained significant popularity due to their remarkable\ncapabilities in image generation, albeit at the cost of intensive computation\nrequirement. Meanwhile, despite their widespread deployment in inference\nservices such as Midjourney, concerns about the potential leakage of sensitive\ninformation in uploaded user prompts have arisen. Existing solutions either\nlack rigorous privacy guarantees or fail to strike an effective balance between\nutility and efficiency. To bridge this gap, we propose ObCLIP, a plug-and-play\nsafeguard that enables oblivious cloud-device hybrid generation. By oblivious,\neach input prompt is transformed into a set of semantically similar candidate\nprompts that differ only in sensitive attributes (e.g., gender, ethnicity). The\ncloud server processes all candidate prompts without knowing which one is the\nreal one, thus preventing any prompt leakage. To mitigate server cost, only a\nsmall portion of denoising steps is performed upon the large cloud model. The\nintermediate latents are then sent back to the client, which selects the\ntargeted latent and completes the remaining denoising using a small device\nmodel. Additionally, we analyze and incorporate several cache-based\naccelerations that leverage temporal and batch redundancy, effectively reducing\ncomputation cost with minimal utility degradation. Extensive experiments across\nmultiple datasets demonstrate that ObCLIP provides rigorous privacy and\ncomparable utility to cloud models with slightly increased server cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Models have gained significant popularity due to their remarkable\ncapabilities in image generation, albeit at the cost of intensive computation\nrequirement. Meanwhile, despite their widespread deployment in inference\nservices such as Midjourney, concerns about the potential leakage of sensitive\ninformation in uploaded user prompts have arisen. Existing solutions either\nlack rigorous privacy guarantees or fail to strike an effective balance between\nutility and efficiency. To bridge this gap, we propose ObCLIP, a plug-and-play\nsafeguard that enables oblivious cloud-device hybrid generation. By oblivious,\neach input prompt is transformed into a set of semantically similar candidate\nprompts that differ only in sensitive attributes (e.g., gender, ethnicity). The\ncloud server processes all candidate prompts without knowing which one is the\nreal one, thus preventing any prompt leakage. To mitigate server cost, only a\nsmall portion of denoising steps is performed upon the large cloud model. The\nintermediate latents are then sent back to the client, which selects the\ntargeted latent and completes the remaining denoising using a small device\nmodel. Additionally, we analyze and incorporate several cache-based\naccelerations that leverage temporal and batch redundancy, effectively reducing\ncomputation cost with minimal utility degradation. Extensive experiments across\nmultiple datasets demonstrate that ObCLIP provides rigorous privacy and\ncomparable utility to cloud models with slightly increased server cost."
                },
                "authors": [
                    {
                        "name": "Haoqi Wu"
                    },
                    {
                        "name": "Wei Dai"
                    },
                    {
                        "name": "Ming Xu"
                    },
                    {
                        "name": "Li Wang"
                    },
                    {
                        "name": "Qiang Yan"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Yan"
                },
                "author": "Qiang Yan",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10714v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10714v3",
                "updated": "2025-10-05T08:34:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    8,
                    34,
                    30,
                    6,
                    278,
                    0
                ],
                "published": "2025-03-13T03:36:03Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    36,
                    3,
                    3,
                    72,
                    0
                ],
                "title": "ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient\n  Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient\n  Long-Context LLMs"
                },
                "summary": "The linear growth of key-value (KV) cache memory and quadratic computational\nin attention mechanisms complexity pose significant bottlenecks for large\nlanguage models (LLMs) in long-context processing. While existing KV cache\noptimization methods address these challenges through token pruning or feature\nmerging, they often incur irreversible information loss or require costly\nparameter retraining. To this end, we propose ZSMerge, a dynamic KV cache\ncompression framework designed for efficient cache management, featuring three\nkey operations: (1) fine-grained memory allocation guided by multi-dimensional\ntoken importance metrics at head-level granularity, (2) a residual merging\nmechanism that preserves critical context through compensated attention\nscoring, and (3) a zero-shot adaptation mechanism compatible with diverse LLM\narchitectures without requiring retraining. ZSMerge significantly enhances\nmemory efficiency and inference speed with negligible performance degradation\nacross LLMs. When applied to LLaMA2-7B, it demonstrates a 20:1 compression\nratio for key-value cache retention (reducing memory footprint to 5\\% of\nbaseline) while sustaining comparable generation quality, coupled with triple\nthroughput gains at extreme 54k-token contexts that eliminate out-of-memory\nfailures. The code is available at https://github.com/SusCom-Lab/ZSMerge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The linear growth of key-value (KV) cache memory and quadratic computational\nin attention mechanisms complexity pose significant bottlenecks for large\nlanguage models (LLMs) in long-context processing. While existing KV cache\noptimization methods address these challenges through token pruning or feature\nmerging, they often incur irreversible information loss or require costly\nparameter retraining. To this end, we propose ZSMerge, a dynamic KV cache\ncompression framework designed for efficient cache management, featuring three\nkey operations: (1) fine-grained memory allocation guided by multi-dimensional\ntoken importance metrics at head-level granularity, (2) a residual merging\nmechanism that preserves critical context through compensated attention\nscoring, and (3) a zero-shot adaptation mechanism compatible with diverse LLM\narchitectures without requiring retraining. ZSMerge significantly enhances\nmemory efficiency and inference speed with negligible performance degradation\nacross LLMs. When applied to LLaMA2-7B, it demonstrates a 20:1 compression\nratio for key-value cache retention (reducing memory footprint to 5\\% of\nbaseline) while sustaining comparable generation quality, coupled with triple\nthroughput gains at extreme 54k-token contexts that eliminate out-of-memory\nfailures. The code is available at https://github.com/SusCom-Lab/ZSMerge."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Xudong Wang"
                    },
                    {
                        "name": "Pei Liu"
                    },
                    {
                        "name": "Guoming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Guoming Tang"
                },
                "author": "Guoming Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10714v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10714v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.12801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12801v1",
                "updated": "2025-10-14T17:59:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    59,
                    58,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T17:59:58Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    59,
                    58,
                    1,
                    287,
                    0
                ],
                "title": "DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search"
                },
                "summary": "Multimodal Large Language Models (MLLMs) in real-world applications require\naccess to external knowledge sources and must remain responsive to the dynamic\nand ever-changing real-world information in order to address\ninformation-seeking and knowledge-intensive user queries. Existing approaches,\nsuch as retrieval augmented generation (RAG) methods, search agents, and search\nequipped MLLMs, often suffer from rigid pipelines, excessive search calls, and\npoorly constructed search queries, which result in inefficiencies and\nsuboptimal outcomes. To address these limitations, we present DeepMMSearch-R1,\nthe first multimodal LLM capable of performing on-demand, multi-turn web\nsearches and dynamically crafting queries for both image and text search tools.\nSpecifically, DeepMMSearch-R1 can initiate web searches based on relevant crops\nof the input image making the image search more effective, and can iteratively\nadapt text search queries based on retrieved information, thereby enabling\nself-reflection and self-correction. Our approach relies on a two-stage\ntraining pipeline: a cold start supervised finetuning phase followed by an\nonline reinforcement learning optimization. For training, we introduce\nDeepMMSearchVQA, a novel multimodal VQA dataset created through an automated\npipeline intermixed with real-world information from web search tools. This\ndataset contains diverse, multi-hop queries that integrate textual and visual\ninformation, teaching the model when to search, what to search for, which\nsearch tool to use and how to reason over the retrieved information. We conduct\nextensive experiments across a range of knowledge-intensive benchmarks to\ndemonstrate the superiority of our approach. Finally, we analyze the results\nand provide insights that are valuable for advancing multimodal web-search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) in real-world applications require\naccess to external knowledge sources and must remain responsive to the dynamic\nand ever-changing real-world information in order to address\ninformation-seeking and knowledge-intensive user queries. Existing approaches,\nsuch as retrieval augmented generation (RAG) methods, search agents, and search\nequipped MLLMs, often suffer from rigid pipelines, excessive search calls, and\npoorly constructed search queries, which result in inefficiencies and\nsuboptimal outcomes. To address these limitations, we present DeepMMSearch-R1,\nthe first multimodal LLM capable of performing on-demand, multi-turn web\nsearches and dynamically crafting queries for both image and text search tools.\nSpecifically, DeepMMSearch-R1 can initiate web searches based on relevant crops\nof the input image making the image search more effective, and can iteratively\nadapt text search queries based on retrieved information, thereby enabling\nself-reflection and self-correction. Our approach relies on a two-stage\ntraining pipeline: a cold start supervised finetuning phase followed by an\nonline reinforcement learning optimization. For training, we introduce\nDeepMMSearchVQA, a novel multimodal VQA dataset created through an automated\npipeline intermixed with real-world information from web search tools. This\ndataset contains diverse, multi-hop queries that integrate textual and visual\ninformation, teaching the model when to search, what to search for, which\nsearch tool to use and how to reason over the retrieved information. We conduct\nextensive experiments across a range of knowledge-intensive benchmarks to\ndemonstrate the superiority of our approach. Finally, we analyze the results\nand provide insights that are valuable for advancing multimodal web-search."
                },
                "authors": [
                    {
                        "name": "Kartik Narayan"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Tian Cao"
                    },
                    {
                        "name": "Kavya Nerella"
                    },
                    {
                        "name": "Vishal M. Patel"
                    },
                    {
                        "name": "Navid Shiee"
                    },
                    {
                        "name": "Peter Grasch"
                    },
                    {
                        "name": "Chao Jia"
                    },
                    {
                        "name": "Yinfei Yang"
                    },
                    {
                        "name": "Zhe Gan"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Gan"
                },
                "author": "Zhe Gan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12796v1",
                "updated": "2025-10-14T17:59:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    59,
                    47,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T17:59:47Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    59,
                    47,
                    1,
                    287,
                    0
                ],
                "title": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving"
                },
                "summary": "Scaling Vision-Language-Action (VLA) models on large-scale data offers a\npromising path to achieving a more generalized driving intelligence. However,\nVLA models are limited by a ``supervision deficit'': the vast model capacity is\nsupervised by sparse, low-dimensional actions, leaving much of their\nrepresentational power underutilized. To remedy this, we propose\n\\textbf{DriveVLA-W0}, a training paradigm that employs world modeling to\npredict future images. This task generates a dense, self-supervised signal that\ncompels the model to learn the underlying dynamics of the driving environment.\nWe showcase the paradigm's versatility by instantiating it for two dominant VLA\narchetypes: an autoregressive world model for VLAs that use discrete visual\ntokens, and a diffusion world model for those operating on continuous visual\nfeatures. Building on the rich representations learned from world modeling, we\nintroduce a lightweight action expert to address the inference latency for\nreal-time deployment. Extensive experiments on the NAVSIM v1/v2 benchmark and a\n680x larger in-house dataset demonstrate that DriveVLA-W0 significantly\noutperforms BEV and VLA baselines. Crucially, it amplifies the data scaling\nlaw, showing that performance gains accelerate as the training dataset size\nincreases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Vision-Language-Action (VLA) models on large-scale data offers a\npromising path to achieving a more generalized driving intelligence. However,\nVLA models are limited by a ``supervision deficit'': the vast model capacity is\nsupervised by sparse, low-dimensional actions, leaving much of their\nrepresentational power underutilized. To remedy this, we propose\n\\textbf{DriveVLA-W0}, a training paradigm that employs world modeling to\npredict future images. This task generates a dense, self-supervised signal that\ncompels the model to learn the underlying dynamics of the driving environment.\nWe showcase the paradigm's versatility by instantiating it for two dominant VLA\narchetypes: an autoregressive world model for VLAs that use discrete visual\ntokens, and a diffusion world model for those operating on continuous visual\nfeatures. Building on the rich representations learned from world modeling, we\nintroduce a lightweight action expert to address the inference latency for\nreal-time deployment. Extensive experiments on the NAVSIM v1/v2 benchmark and a\n680x larger in-house dataset demonstrate that DriveVLA-W0 significantly\noutperforms BEV and VLA baselines. Crucially, it amplifies the data scaling\nlaw, showing that performance gains accelerate as the training dataset size\nincreases."
                },
                "authors": [
                    {
                        "name": "Yingyan Li"
                    },
                    {
                        "name": "Shuyao Shang"
                    },
                    {
                        "name": "Weisong Liu"
                    },
                    {
                        "name": "Bing Zhan"
                    },
                    {
                        "name": "Haochen Wang"
                    },
                    {
                        "name": "Yuqi Wang"
                    },
                    {
                        "name": "Yuntao Chen"
                    },
                    {
                        "name": "Xiaoman Wang"
                    },
                    {
                        "name": "Yasong An"
                    },
                    {
                        "name": "Chufeng Tang"
                    },
                    {
                        "name": "Lu Hou"
                    },
                    {
                        "name": "Lue Fan"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoxiang Zhang"
                },
                "author": "Zhaoxiang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12793v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12793v1",
                "updated": "2025-10-14T17:58:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    58,
                    10,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T17:58:10Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    58,
                    10,
                    1,
                    287,
                    0
                ],
                "title": "ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution"
                },
                "summary": "Existing Multimodal Large Language Models (MLLMs) suffer from increased\ninference costs due to the additional vision tokens introduced by image inputs.\nIn this work, we propose Visual Consistency Learning (ViCO), a novel training\nalgorithm that enables the model to represent images of varying semantic\ncomplexities using different numbers of vision tokens. The key idea behind our\nmethod is to employ multiple MLP connectors, each with a different image\ncompression ratio, to downsample the vision tokens based on the semantic\ncomplexity of the image. During training, we minimize the KL divergence between\nthe responses conditioned on different MLP connectors. At inference time, we\nintroduce an image router, termed Visual Resolution Router (ViR), that\nautomatically selects the appropriate compression rate for each image patch.\nCompared with existing dynamic high-resolution strategies, which adjust the\nnumber of visual tokens based on image resolutions, our method dynamically\nadapts the number of visual tokens according to semantic complexity.\nExperimental results demonstrate that our method can reduce the number of\nvision tokens by up to 50% while maintaining the model's perception, reasoning,\nand OCR capabilities. We hope this work will contribute to the development of\nmore efficient MLLMs. The code and models will be released to facilitate future\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing Multimodal Large Language Models (MLLMs) suffer from increased\ninference costs due to the additional vision tokens introduced by image inputs.\nIn this work, we propose Visual Consistency Learning (ViCO), a novel training\nalgorithm that enables the model to represent images of varying semantic\ncomplexities using different numbers of vision tokens. The key idea behind our\nmethod is to employ multiple MLP connectors, each with a different image\ncompression ratio, to downsample the vision tokens based on the semantic\ncomplexity of the image. During training, we minimize the KL divergence between\nthe responses conditioned on different MLP connectors. At inference time, we\nintroduce an image router, termed Visual Resolution Router (ViR), that\nautomatically selects the appropriate compression rate for each image patch.\nCompared with existing dynamic high-resolution strategies, which adjust the\nnumber of visual tokens based on image resolutions, our method dynamically\nadapts the number of visual tokens according to semantic complexity.\nExperimental results demonstrate that our method can reduce the number of\nvision tokens by up to 50% while maintaining the model's perception, reasoning,\nand OCR capabilities. We hope this work will contribute to the development of\nmore efficient MLLMs. The code and models will be released to facilitate future\nresearch."
                },
                "authors": [
                    {
                        "name": "Long Cui"
                    },
                    {
                        "name": "Weiyun Wang"
                    },
                    {
                        "name": "Jie Shao"
                    },
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Yanting Zhang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Wenhai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenhai Wang"
                },
                "author": "Wenhai Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12793v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12793v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12789v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12789v1",
                "updated": "2025-10-14T17:57:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    57,
                    56,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T17:57:56Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    57,
                    56,
                    1,
                    287,
                    0
                ],
                "title": "UniFusion: Vision-Language Model as Unified Encoder in Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniFusion: Vision-Language Model as Unified Encoder in Image Generation"
                },
                "summary": "Although recent advances in visual generation have been remarkable, most\nexisting architectures still depend on distinct encoders for images and text.\nThis separation constrains diffusion models' ability to perform cross-modal\nreasoning and knowledge transfer. Prior attempts to bridge this gap often use\nthe last layer information from VLM, employ multiple visual encoders, or train\nlarge unified models jointly for text and image generation, which demands\nsubstantial computational resources and large-scale data, limiting its\naccessibility.We present UniFusion, a diffusion-based generative model\nconditioned on a frozen large vision-language model (VLM) that serves as a\nunified multimodal encoder. At the core of UniFusion is the Layerwise Attention\nPooling (LAP) mechanism that extracts both high level semantics and low level\ndetails from text and visual tokens of a frozen VLM to condition a diffusion\ngenerative model. We demonstrate that LAP outperforms other shallow fusion\narchitectures on text-image alignment for generation and faithful transfer of\nvisual information from VLM to the diffusion model which is key for editing. We\npropose VLM-Enabled Rewriting Injection with Flexibile Inference (VERIFI),\nwhich conditions a diffusion transformer (DiT) only on the text tokens\ngenerated by the VLM during in-model prompt rewriting. VERIFI combines the\nalignment of the conditioning distribution with the VLM's reasoning\ncapabilities for increased capabilities and flexibility at inference. In\naddition, finetuning on editing task not only improves text-image alignment for\ngeneration, indicative of cross-modality knowledge transfer, but also exhibits\ntremendous generalization capabilities. Our model when trained on single image\nediting, zero-shot generalizes to multiple image references further motivating\nthe unified encoder design of UniFusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although recent advances in visual generation have been remarkable, most\nexisting architectures still depend on distinct encoders for images and text.\nThis separation constrains diffusion models' ability to perform cross-modal\nreasoning and knowledge transfer. Prior attempts to bridge this gap often use\nthe last layer information from VLM, employ multiple visual encoders, or train\nlarge unified models jointly for text and image generation, which demands\nsubstantial computational resources and large-scale data, limiting its\naccessibility.We present UniFusion, a diffusion-based generative model\nconditioned on a frozen large vision-language model (VLM) that serves as a\nunified multimodal encoder. At the core of UniFusion is the Layerwise Attention\nPooling (LAP) mechanism that extracts both high level semantics and low level\ndetails from text and visual tokens of a frozen VLM to condition a diffusion\ngenerative model. We demonstrate that LAP outperforms other shallow fusion\narchitectures on text-image alignment for generation and faithful transfer of\nvisual information from VLM to the diffusion model which is key for editing. We\npropose VLM-Enabled Rewriting Injection with Flexibile Inference (VERIFI),\nwhich conditions a diffusion transformer (DiT) only on the text tokens\ngenerated by the VLM during in-model prompt rewriting. VERIFI combines the\nalignment of the conditioning distribution with the VLM's reasoning\ncapabilities for increased capabilities and flexibility at inference. In\naddition, finetuning on editing task not only improves text-image alignment for\ngeneration, indicative of cross-modality knowledge transfer, but also exhibits\ntremendous generalization capabilities. Our model when trained on single image\nediting, zero-shot generalizes to multiple image references further motivating\nthe unified encoder design of UniFusion."
                },
                "authors": [
                    {
                        "name": "Kevin Li"
                    },
                    {
                        "name": "Manuel Brack"
                    },
                    {
                        "name": "Sudeep Katakol"
                    },
                    {
                        "name": "Hareesh Ravi"
                    },
                    {
                        "name": "Ajinkya Kale"
                    }
                ],
                "author_detail": {
                    "name": "Ajinkya Kale"
                },
                "author": "Ajinkya Kale",
                "arxiv_comment": "Project page at https://thekevinli.github.io/unifusion/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12789v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12789v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12787v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12787v1",
                "updated": "2025-10-14T17:57:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    57,
                    4,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T17:57:04Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    57,
                    4,
                    1,
                    287,
                    0
                ],
                "title": "Ax-Prover: A Deep Reasoning Agentic Framework for Theorem Proving in\n  Mathematics and Quantum Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ax-Prover: A Deep Reasoning Agentic Framework for Theorem Proving in\n  Mathematics and Quantum Physics"
                },
                "summary": "We present Ax-Prover, a multi-agent system for automated theorem proving in\nLean that can solve problems across diverse scientific domains and operate\neither autonomously or collaboratively with human experts. To achieve this,\nAx-Prover approaches scientific problem solving through formal proof\ngeneration, a process that demands both creative reasoning and strict syntactic\nrigor. Ax-Prover meets this challenge by equipping Large Language Models\n(LLMs), which provide knowledge and reasoning, with Lean tools via the Model\nContext Protocol (MCP), which ensure formal correctness. To evaluate its\nperformance as an autonomous prover, we benchmark our approach against frontier\nLLMs and specialized prover models on two public math benchmarks and on two\nLean benchmarks we introduce in the fields of abstract algebra and quantum\ntheory. On public datasets, Ax-Prover is competitive with state-of-the-art\nprovers, while it largely outperform them on the new benchmarks. This shows\nthat, unlike specialized systems that struggle to generalize, our tool-based\nagentic theorem prover approach offers a generalizable methodology for formal\nverification across diverse scientific domains. Furthermore, we demonstrate\nAx-Prover's assistant capabilities in a practical use case, showing how it\nenabled an expert mathematician to formalize the proof of a complex\ncryptography theorem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Ax-Prover, a multi-agent system for automated theorem proving in\nLean that can solve problems across diverse scientific domains and operate\neither autonomously or collaboratively with human experts. To achieve this,\nAx-Prover approaches scientific problem solving through formal proof\ngeneration, a process that demands both creative reasoning and strict syntactic\nrigor. Ax-Prover meets this challenge by equipping Large Language Models\n(LLMs), which provide knowledge and reasoning, with Lean tools via the Model\nContext Protocol (MCP), which ensure formal correctness. To evaluate its\nperformance as an autonomous prover, we benchmark our approach against frontier\nLLMs and specialized prover models on two public math benchmarks and on two\nLean benchmarks we introduce in the fields of abstract algebra and quantum\ntheory. On public datasets, Ax-Prover is competitive with state-of-the-art\nprovers, while it largely outperform them on the new benchmarks. This shows\nthat, unlike specialized systems that struggle to generalize, our tool-based\nagentic theorem prover approach offers a generalizable methodology for formal\nverification across diverse scientific domains. Furthermore, we demonstrate\nAx-Prover's assistant capabilities in a practical use case, showing how it\nenabled an expert mathematician to formalize the proof of a complex\ncryptography theorem."
                },
                "authors": [
                    {
                        "name": "Marco Del Tredici"
                    },
                    {
                        "name": "Jacob McCarran"
                    },
                    {
                        "name": "Benjamin Breen"
                    },
                    {
                        "name": "Javier Aspuru Mijares"
                    },
                    {
                        "name": "Weichen Winston Yin"
                    },
                    {
                        "name": "Jacob M. Taylor"
                    },
                    {
                        "name": "Frank Koppens"
                    },
                    {
                        "name": "Dirk Englund"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Englund"
                },
                "author": "Dirk Englund",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12787v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12787v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12776v1",
                "updated": "2025-10-14T17:51:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    51,
                    48,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T17:51:48Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    51,
                    48,
                    1,
                    287,
                    0
                ],
                "title": "A Quantum Generative Framework for Modeling Single-Cell Transcriptomes\n  with Gene-Gene and Cell-Cell Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Quantum Generative Framework for Modeling Single-Cell Transcriptomes\n  with Gene-Gene and Cell-Cell Interactions"
                },
                "summary": "Single-cell RNA sequencing (scRNA-seq) data simulation is limited by\nclassical methods that rely on linear correlations, failing to capture the\nintrinsic, nonlinear dependencies and the simultaneous gene-gene and cell-cell\ninteractions. We introduce qSimCells, a novel hybrid quantum-classical\nsimulator that leverages quantum entanglement to model single-cell\ntranscriptomes. The core innovation is a quantum kernel that uses a\nparameterized quantum circuit with CNOT gates to encode complex, nonlinear gene\nregulatory network (GRN) and cell-cell communication topologies with explicit\ndirectionality (causality). The synthetic data exhibits non-classical\ndependencies that challenge standard analysis. We demonstrated that classical\ncorrelation methods (Pearson and Spearman) failed to reconstruct the complete\nprogrammed quantum causal paths, instead reporting spurious statistical\nartifacts driven by high base-gene expression probabilities. Applying\nCellChat2.0 to the simulated cell-cell communication validated the true\nmechanistic links by showing a robust, relative increase in communication\nprobability (up to 75-fold) only when the quantum entanglement was active. This\nwork confirms that the quantum kernel is essential for creating high-fidelity\nground truth data, highlighting the need for advanced inference techniques to\ncapture the complex, non-classical dependencies inherent in gene regulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single-cell RNA sequencing (scRNA-seq) data simulation is limited by\nclassical methods that rely on linear correlations, failing to capture the\nintrinsic, nonlinear dependencies and the simultaneous gene-gene and cell-cell\ninteractions. We introduce qSimCells, a novel hybrid quantum-classical\nsimulator that leverages quantum entanglement to model single-cell\ntranscriptomes. The core innovation is a quantum kernel that uses a\nparameterized quantum circuit with CNOT gates to encode complex, nonlinear gene\nregulatory network (GRN) and cell-cell communication topologies with explicit\ndirectionality (causality). The synthetic data exhibits non-classical\ndependencies that challenge standard analysis. We demonstrated that classical\ncorrelation methods (Pearson and Spearman) failed to reconstruct the complete\nprogrammed quantum causal paths, instead reporting spurious statistical\nartifacts driven by high base-gene expression probabilities. Applying\nCellChat2.0 to the simulated cell-cell communication validated the true\nmechanistic links by showing a robust, relative increase in communication\nprobability (up to 75-fold) only when the quantum entanglement was active. This\nwork confirms that the quantum kernel is essential for creating high-fidelity\nground truth data, highlighting the need for advanced inference techniques to\ncapture the complex, non-classical dependencies inherent in gene regulation."
                },
                "authors": [
                    {
                        "name": "Selim Romero"
                    },
                    {
                        "name": "Vignesh Kumar"
                    },
                    {
                        "name": "Robert S. Chapkin"
                    },
                    {
                        "name": "James J. Cai"
                    }
                ],
                "author_detail": {
                    "name": "James J. Cai"
                },
                "author": "James J. Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12773v1",
                "updated": "2025-10-14T17:51:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    51,
                    26,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T17:51:26Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    51,
                    26,
                    1,
                    287,
                    0
                ],
                "title": "Dr.LLM: Dynamic Layer Routing in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dr.LLM: Dynamic Layer Routing in LLMs"
                },
                "summary": "Large Language Models (LLMs) process every token through all layers of a\ntransformer stack, causing wasted computation on simple queries and\ninsufficient flexibility for harder ones that need deeper reasoning.\nAdaptive-depth methods can improve efficiency, but prior approaches rely on\ncostly inference-time search, architectural changes, or large-scale retraining,\nand in practice often degrade accuracy despite efficiency gains. We introduce\nDr.LLM, Dynamic routing of Layers for LLMs, a retrofittable framework that\nequips pretrained models with lightweight per-layer routers deciding to skip,\nexecute, or repeat a block. Routers are trained with explicit supervision:\nusing Monte Carlo Tree Search (MCTS), we derive high-quality layer\nconfigurations that preserve or improve accuracy under a compute budget. Our\ndesign, windowed pooling for stable routing, focal loss with class balancing,\nand bottleneck MLP routers, ensures robustness under class imbalance and long\nsequences. On ARC (logic) and DART (math), Dr.LLM improves accuracy by up to\n+3.4%p while saving 5 layers per example on average. Routers generalize to\nout-of-domain tasks (MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA,\nAGIEval) with only 0.85% accuracy drop while retaining efficiency, and\noutperform prior routing methods by up to +7.7%p. Overall, Dr.LLM shows that\nexplicitly supervised routers retrofit frozen LLMs for budget-aware,\naccuracy-driven inference without altering base weights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) process every token through all layers of a\ntransformer stack, causing wasted computation on simple queries and\ninsufficient flexibility for harder ones that need deeper reasoning.\nAdaptive-depth methods can improve efficiency, but prior approaches rely on\ncostly inference-time search, architectural changes, or large-scale retraining,\nand in practice often degrade accuracy despite efficiency gains. We introduce\nDr.LLM, Dynamic routing of Layers for LLMs, a retrofittable framework that\nequips pretrained models with lightweight per-layer routers deciding to skip,\nexecute, or repeat a block. Routers are trained with explicit supervision:\nusing Monte Carlo Tree Search (MCTS), we derive high-quality layer\nconfigurations that preserve or improve accuracy under a compute budget. Our\ndesign, windowed pooling for stable routing, focal loss with class balancing,\nand bottleneck MLP routers, ensures robustness under class imbalance and long\nsequences. On ARC (logic) and DART (math), Dr.LLM improves accuracy by up to\n+3.4%p while saving 5 layers per example on average. Routers generalize to\nout-of-domain tasks (MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA,\nAGIEval) with only 0.85% accuracy drop while retaining efficiency, and\noutperform prior routing methods by up to +7.7%p. Overall, Dr.LLM shows that\nexplicitly supervised routers retrofit frozen LLMs for budget-aware,\naccuracy-driven inference without altering base weights."
                },
                "authors": [
                    {
                        "name": "Ahmed Heakl"
                    },
                    {
                        "name": "Martin Gubri"
                    },
                    {
                        "name": "Salman Khan"
                    },
                    {
                        "name": "Sangdoo Yun"
                    },
                    {
                        "name": "Seong Joon Oh"
                    }
                ],
                "author_detail": {
                    "name": "Seong Joon Oh"
                },
                "author": "Seong Joon Oh",
                "arxiv_comment": "17 pages, Under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12766v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12766v1",
                "updated": "2025-10-14T17:45:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    45,
                    31,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T17:45:31Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    45,
                    31,
                    1,
                    287,
                    0
                ],
                "title": "Language Models Model Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models Model Language"
                },
                "summary": "Linguistic commentary on LLMs, heavily influenced by the theoretical\nframeworks of de Saussure and Chomsky, is often speculative and unproductive.\nCritics challenge whether LLMs can legitimately model language, citing the need\nfor \"deep structure\" or \"grounding\" to achieve an idealized linguistic\n\"competence.\" We argue for a radical shift in perspective towards the\nempiricist principles of Witold Ma\\'nczak, a prominent general and historical\nlinguist. He defines language not as a \"system of signs\" or a \"computational\nsystem of the brain\" but as the totality of all that is said and written. Above\nall, he identifies frequency of use of particular language elements as\nlanguage's primary governing principle. Using his framework, we challenge prior\ncritiques of LLMs and provide a constructive guide for designing, evaluating,\nand interpreting language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linguistic commentary on LLMs, heavily influenced by the theoretical\nframeworks of de Saussure and Chomsky, is often speculative and unproductive.\nCritics challenge whether LLMs can legitimately model language, citing the need\nfor \"deep structure\" or \"grounding\" to achieve an idealized linguistic\n\"competence.\" We argue for a radical shift in perspective towards the\nempiricist principles of Witold Ma\\'nczak, a prominent general and historical\nlinguist. He defines language not as a \"system of signs\" or a \"computational\nsystem of the brain\" but as the totality of all that is said and written. Above\nall, he identifies frequency of use of particular language elements as\nlanguage's primary governing principle. Using his framework, we challenge prior\ncritiques of LLMs and provide a constructive guide for designing, evaluating,\nand interpreting language models."
                },
                "authors": [
                    {
                        "name": "ukasz Borchmann"
                    }
                ],
                "author_detail": {
                    "name": "ukasz Borchmann"
                },
                "author": "ukasz Borchmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12766v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12766v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12764v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12764v1",
                "updated": "2025-10-14T17:45:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    45,
                    17,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T17:45:17Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    45,
                    17,
                    1,
                    287,
                    0
                ],
                "title": "AnyUp: Universal Feature Upsampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnyUp: Universal Feature Upsampling"
                },
                "summary": "We introduce AnyUp, a method for feature upsampling that can be applied to\nany vision feature at any resolution, without encoder-specific training.\nExisting learning-based upsamplers for features like DINO or CLIP need to be\nre-trained for every feature extractor and thus do not generalize to different\nfeature types at inference time. In this work, we propose an inference-time\nfeature-agnostic upsampling architecture to alleviate this limitation and\nimprove upsampling quality. In our experiments, AnyUp sets a new state of the\nart for upsampled features, generalizes to different feature types, and\npreserves feature semantics while being efficient and easy to apply to a wide\nrange of downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce AnyUp, a method for feature upsampling that can be applied to\nany vision feature at any resolution, without encoder-specific training.\nExisting learning-based upsamplers for features like DINO or CLIP need to be\nre-trained for every feature extractor and thus do not generalize to different\nfeature types at inference time. In this work, we propose an inference-time\nfeature-agnostic upsampling architecture to alleviate this limitation and\nimprove upsampling quality. In our experiments, AnyUp sets a new state of the\nart for upsampled features, generalizes to different feature types, and\npreserves feature semantics while being efficient and easy to apply to a wide\nrange of downstream tasks."
                },
                "authors": [
                    {
                        "name": "Thomas Wimmer"
                    },
                    {
                        "name": "Prune Truong"
                    },
                    {
                        "name": "Marie-Julie Rakotosaona"
                    },
                    {
                        "name": "Michael Oechsle"
                    },
                    {
                        "name": "Federico Tombari"
                    },
                    {
                        "name": "Bernt Schiele"
                    },
                    {
                        "name": "Jan Eric Lenssen"
                    }
                ],
                "author_detail": {
                    "name": "Jan Eric Lenssen"
                },
                "author": "Jan Eric Lenssen",
                "arxiv_comment": "Project Website: https://wimmerth.github.io/anyup/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12764v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12764v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12742v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12742v1",
                "updated": "2025-10-14T17:20:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    20,
                    4,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T17:20:04Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    20,
                    4,
                    1,
                    287,
                    0
                ],
                "title": "CTRL-Rec: Controlling Recommender Systems With Natural Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CTRL-Rec: Controlling Recommender Systems With Natural Language"
                },
                "summary": "When users are dissatisfied with recommendations from a recommender system,\nthey often lack fine-grained controls for changing them. Large language models\n(LLMs) offer a solution by allowing users to guide their recommendations\nthrough natural language requests (e.g., \"I want to see respectful posts with a\ndifferent perspective than mine\"). We propose a method, CTRL-Rec, that allows\nfor natural language control of traditional recommender systems in real-time\nwith computational efficiency. Specifically, at training time, we use an LLM to\nsimulate whether users would approve of items based on their language requests,\nand we train embedding models that approximate such simulated judgments. We\nthen integrate these user-request-based predictions into the standard weighting\nof signals that traditional recommender systems optimize. At deployment time,\nwe require only a single LLM embedding computation per user request, allowing\nfor real-time control of recommendations. In experiments with the MovieLens\ndataset, our method consistently allows for fine-grained control across a\ndiversity of requests. In a study with 19 Letterboxd users, we find that\nCTRL-Rec was positively received by users and significantly enhanced users'\nsense of control and satisfaction with recommendations compared to traditional\ncontrols.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When users are dissatisfied with recommendations from a recommender system,\nthey often lack fine-grained controls for changing them. Large language models\n(LLMs) offer a solution by allowing users to guide their recommendations\nthrough natural language requests (e.g., \"I want to see respectful posts with a\ndifferent perspective than mine\"). We propose a method, CTRL-Rec, that allows\nfor natural language control of traditional recommender systems in real-time\nwith computational efficiency. Specifically, at training time, we use an LLM to\nsimulate whether users would approve of items based on their language requests,\nand we train embedding models that approximate such simulated judgments. We\nthen integrate these user-request-based predictions into the standard weighting\nof signals that traditional recommender systems optimize. At deployment time,\nwe require only a single LLM embedding computation per user request, allowing\nfor real-time control of recommendations. In experiments with the MovieLens\ndataset, our method consistently allows for fine-grained control across a\ndiversity of requests. In a study with 19 Letterboxd users, we find that\nCTRL-Rec was positively received by users and significantly enhanced users'\nsense of control and satisfaction with recommendations compared to traditional\ncontrols."
                },
                "authors": [
                    {
                        "name": "Micah Carroll"
                    },
                    {
                        "name": "Adeline Foote"
                    },
                    {
                        "name": "Kevin Feng"
                    },
                    {
                        "name": "Marcus Williams"
                    },
                    {
                        "name": "Anca Dragan"
                    },
                    {
                        "name": "W. Bradley Knox"
                    },
                    {
                        "name": "Smitha Milli"
                    }
                ],
                "author_detail": {
                    "name": "Smitha Milli"
                },
                "author": "Smitha Milli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12742v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12742v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09823v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09823v2",
                "updated": "2025-10-14T17:17:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    17,
                    59,
                    1,
                    287,
                    0
                ],
                "published": "2025-08-13T13:55:43Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    55,
                    43,
                    2,
                    225,
                    0
                ],
                "title": "KonfAI: A Modular and Fully Configurable Framework for Deep Learning in\n  Medical Imaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KonfAI: A Modular and Fully Configurable Framework for Deep Learning in\n  Medical Imaging"
                },
                "summary": "KonfAI is a modular, extensible, and fully configurable deep learning\nframework specifically designed for medical imaging tasks. It enables users to\ndefine complete training, inference, and evaluation workflows through\nstructured YAML configuration files, without modifying the underlying code.\nThis declarative approach enhances reproducibility, transparency, and\nexperimental traceability while reducing development time. Beyond the\ncapabilities of standard pipelines, KonfAI provides native abstractions for\nadvanced strategies including patch-based learning, test-time augmentation,\nmodel ensembling, and direct access to intermediate feature representations for\ndeep supervision. It also supports complex multi-model training setups such as\ngenerative adversarial architectures. Thanks to its modular and extensible\narchitecture, KonfAI can easily accommodate custom models, loss functions, and\ndata processing components. The framework has been successfully applied to\nsegmentation, registration, and image synthesis tasks, and has contributed to\ntop-ranking results in several international medical imaging challenges. KonfAI\nis open source and available at https://github.com/vboussot/KonfAI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KonfAI is a modular, extensible, and fully configurable deep learning\nframework specifically designed for medical imaging tasks. It enables users to\ndefine complete training, inference, and evaluation workflows through\nstructured YAML configuration files, without modifying the underlying code.\nThis declarative approach enhances reproducibility, transparency, and\nexperimental traceability while reducing development time. Beyond the\ncapabilities of standard pipelines, KonfAI provides native abstractions for\nadvanced strategies including patch-based learning, test-time augmentation,\nmodel ensembling, and direct access to intermediate feature representations for\ndeep supervision. It also supports complex multi-model training setups such as\ngenerative adversarial architectures. Thanks to its modular and extensible\narchitecture, KonfAI can easily accommodate custom models, loss functions, and\ndata processing components. The framework has been successfully applied to\nsegmentation, registration, and image synthesis tasks, and has contributed to\ntop-ranking results in several international medical imaging challenges. KonfAI\nis open source and available at https://github.com/vboussot/KonfAI."
                },
                "authors": [
                    {
                        "name": "Valentin Boussot"
                    },
                    {
                        "name": "Jean-Louis Dillenseger"
                    }
                ],
                "author_detail": {
                    "name": "Jean-Louis Dillenseger"
                },
                "author": "Jean-Louis Dillenseger",
                "arxiv_comment": "https://github.com/vboussot/KonfAI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09823v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09823v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12739v1",
                "updated": "2025-10-14T17:17:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    17,
                    14,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T17:17:14Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    17,
                    14,
                    1,
                    287,
                    0
                ],
                "title": "CoNet-Rx: Collaborative Neural Networks for OFDM Receivers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoNet-Rx: Collaborative Neural Networks for OFDM Receivers"
                },
                "summary": "Deep learning (DL) based methods for orthogonal frequency division\nmultiplexing (OFDM) radio receivers demonstrated higher signal detection\nperformance compared to the traditional receivers. However, the existing\nDL-based models, usually adapted from computer vision, aren't well suited for\nwireless communications. These models require high computational resources and\nmemory, and have significant inference delays, limiting their use in\nresource-constrained settings. Additionally, reducing network size to ease\nresource demands often leads to notable performance degradation. This paper\nintroduces collaborative networks (CoNet), a novel neural network (NN)\narchitecture designed for OFDM receivers. CoNet uses multiple small ResNet or\nCNN subnetworks to simultaneously process signal features from different\nperspectives like capturing channel correlations and interference patterns.\nThese subnetworks fuse their outputs through interaction operations (e.g.,\nelement-wise multiplication), significantly enhancing detection performance.\nSimulation results show CoNet significantly outperforms traditional\narchitectures like residual networks (ResNets) in bit error rate (BER) and\nreduces inference delay when both nets have the same size and the same\ncomputational complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning (DL) based methods for orthogonal frequency division\nmultiplexing (OFDM) radio receivers demonstrated higher signal detection\nperformance compared to the traditional receivers. However, the existing\nDL-based models, usually adapted from computer vision, aren't well suited for\nwireless communications. These models require high computational resources and\nmemory, and have significant inference delays, limiting their use in\nresource-constrained settings. Additionally, reducing network size to ease\nresource demands often leads to notable performance degradation. This paper\nintroduces collaborative networks (CoNet), a novel neural network (NN)\narchitecture designed for OFDM receivers. CoNet uses multiple small ResNet or\nCNN subnetworks to simultaneously process signal features from different\nperspectives like capturing channel correlations and interference patterns.\nThese subnetworks fuse their outputs through interaction operations (e.g.,\nelement-wise multiplication), significantly enhancing detection performance.\nSimulation results show CoNet significantly outperforms traditional\narchitectures like residual networks (ResNets) in bit error rate (BER) and\nreduces inference delay when both nets have the same size and the same\ncomputational complexity."
                },
                "authors": [
                    {
                        "name": "Mohanad Obeed"
                    },
                    {
                        "name": "Ming Jian"
                    }
                ],
                "author_detail": {
                    "name": "Ming Jian"
                },
                "author": "Ming Jian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12734v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12734v1",
                "updated": "2025-10-14T17:12:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    12,
                    28,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T17:12:28Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    12,
                    28,
                    1,
                    287,
                    0
                ],
                "title": "Doctor Rashomon and the UNIVERSE of Madness: Variable Importance with\n  Unobserved Confounding and the Rashomon Effect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doctor Rashomon and the UNIVERSE of Madness: Variable Importance with\n  Unobserved Confounding and the Rashomon Effect"
                },
                "summary": "Variable importance (VI) methods are often used for hypothesis generation,\nfeature selection, and scientific validation. In the standard VI pipeline, an\nanalyst estimates VI for a single predictive model with only the observed\nfeatures. However, the importance of a feature depends heavily on which other\nvariables are included in the model, and essential variables are often omitted\nfrom observational datasets. Moreover, the VI estimated for one model is often\nnot the same as the VI estimated for another equally-good model - a phenomenon\nknown as the Rashomon Effect. We address these gaps by introducing\nUNobservables and Inference for Variable importancE using Rashomon SEts\n(UNIVERSE). Our approach adapts Rashomon sets - the sets of near-optimal models\nin a dataset - to produce bounds on the true VI even with missing features. We\ntheoretically guarantee the robustness of our approach, show strong performance\non semi-synthetic simulations, and demonstrate its utility in a credit risk\ntask.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variable importance (VI) methods are often used for hypothesis generation,\nfeature selection, and scientific validation. In the standard VI pipeline, an\nanalyst estimates VI for a single predictive model with only the observed\nfeatures. However, the importance of a feature depends heavily on which other\nvariables are included in the model, and essential variables are often omitted\nfrom observational datasets. Moreover, the VI estimated for one model is often\nnot the same as the VI estimated for another equally-good model - a phenomenon\nknown as the Rashomon Effect. We address these gaps by introducing\nUNobservables and Inference for Variable importancE using Rashomon SEts\n(UNIVERSE). Our approach adapts Rashomon sets - the sets of near-optimal models\nin a dataset - to produce bounds on the true VI even with missing features. We\ntheoretically guarantee the robustness of our approach, show strong performance\non semi-synthetic simulations, and demonstrate its utility in a credit risk\ntask."
                },
                "authors": [
                    {
                        "name": "Jon Donnelly"
                    },
                    {
                        "name": "Srikar Katta"
                    },
                    {
                        "name": "Emanuele Borgonovo"
                    },
                    {
                        "name": "Cynthia Rudin"
                    }
                ],
                "author_detail": {
                    "name": "Cynthia Rudin"
                },
                "author": "Cynthia Rudin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12734v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12734v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10341v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10341v2",
                "updated": "2025-10-14T17:11:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    11,
                    26,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-11T20:57:03Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    20,
                    57,
                    3,
                    5,
                    284,
                    0
                ],
                "title": "Multi-View Graph Learning with Graph-Tuple",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-View Graph Learning with Graph-Tuple"
                },
                "summary": "Graph Neural Networks (GNNs) typically scale with the number of graph edges,\nmaking them well suited for sparse graphs but less efficient on dense graphs,\nsuch as point clouds or molecular interactions. A common remedy is to sparsify\nthe graph via similarity thresholding or distance pruning, but this forces an\narbitrary choice of a single interaction scale and discards crucial information\nfrom other scales. To overcome this limitation, we introduce a multi-view\ngraph-tuple framework. Instead of a single graph, our graph-tuple framework\npartitions the graph into disjoint subgraphs, capturing primary local\ninteractions and weaker, long-range connections. We then learn multi-view\nrepresentations from the graph-tuple via a heterogeneous message-passing\narchitecture inspired by the theory of non-commuting operators, which we\nformally prove is strictly more expressive and guarantees a lower oracle risk\ncompared to single-graph message-passing models. We instantiate our framework\non two scientific domains: molecular property prediction from feature-scarce\nCoulomb matrices and cosmological parameter inference from geometric point\nclouds. On both applications, our multi-view graph-tuple models demonstrate\nbetter performance than single-graph baselines, highlighting the power and\nversatility of our multi-view approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) typically scale with the number of graph edges,\nmaking them well suited for sparse graphs but less efficient on dense graphs,\nsuch as point clouds or molecular interactions. A common remedy is to sparsify\nthe graph via similarity thresholding or distance pruning, but this forces an\narbitrary choice of a single interaction scale and discards crucial information\nfrom other scales. To overcome this limitation, we introduce a multi-view\ngraph-tuple framework. Instead of a single graph, our graph-tuple framework\npartitions the graph into disjoint subgraphs, capturing primary local\ninteractions and weaker, long-range connections. We then learn multi-view\nrepresentations from the graph-tuple via a heterogeneous message-passing\narchitecture inspired by the theory of non-commuting operators, which we\nformally prove is strictly more expressive and guarantees a lower oracle risk\ncompared to single-graph message-passing models. We instantiate our framework\non two scientific domains: molecular property prediction from feature-scarce\nCoulomb matrices and cosmological parameter inference from geometric point\nclouds. On both applications, our multi-view graph-tuple models demonstrate\nbetter performance than single-graph baselines, highlighting the power and\nversatility of our multi-view approach."
                },
                "authors": [
                    {
                        "name": "Shiyu Chen"
                    },
                    {
                        "name": "Ningyuan Huang"
                    },
                    {
                        "name": "Soledad Villar"
                    }
                ],
                "author_detail": {
                    "name": "Soledad Villar"
                },
                "author": "Soledad Villar",
                "arxiv_comment": "Submitted to TAG workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10341v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10341v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12728v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12728v1",
                "updated": "2025-10-14T17:07:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    7,
                    37,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T17:07:37Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    7,
                    37,
                    1,
                    287,
                    0
                ],
                "title": "Data-Model Co-Evolution: Growing Test Sets to Refine LLM Behavior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Model Co-Evolution: Growing Test Sets to Refine LLM Behavior"
                },
                "summary": "A long-standing challenge in machine learning has been the rigid separation\nbetween data work and model refinement, enforced by slow fine-tuning cycles.\nThe rise of Large Language Models (LLMs) overcomes this historical barrier,\nallowing applications developers to instantly govern model behavior by editing\nprompt instructions. This shift enables a new paradigm: data-model\nco-evolution, where a living test set and a model's instructions evolve in\ntandem. We operationalize this paradigm in an interactive system designed to\naddress the critical challenge of encoding subtle, domain-specific policies\ninto prompt instructions. The system's structured workflow guides people to\ndiscover edge cases, articulate rationales for desired behavior, and\niteratively evaluate instruction revisions against a growing test set. A user\nstudy shows our workflow helps participants refine instructions systematically\nand specify ambiguous policies more concretely. This work points toward more\nrobust and responsible LLM applications through human-in-the-loop development\naligned with local preferences and policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A long-standing challenge in machine learning has been the rigid separation\nbetween data work and model refinement, enforced by slow fine-tuning cycles.\nThe rise of Large Language Models (LLMs) overcomes this historical barrier,\nallowing applications developers to instantly govern model behavior by editing\nprompt instructions. This shift enables a new paradigm: data-model\nco-evolution, where a living test set and a model's instructions evolve in\ntandem. We operationalize this paradigm in an interactive system designed to\naddress the critical challenge of encoding subtle, domain-specific policies\ninto prompt instructions. The system's structured workflow guides people to\ndiscover edge cases, articulate rationales for desired behavior, and\niteratively evaluate instruction revisions against a growing test set. A user\nstudy shows our workflow helps participants refine instructions systematically\nand specify ambiguous policies more concretely. This work points toward more\nrobust and responsible LLM applications through human-in-the-loop development\naligned with local preferences and policies."
                },
                "authors": [
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Minsuk Kahng"
                    }
                ],
                "author_detail": {
                    "name": "Minsuk Kahng"
                },
                "author": "Minsuk Kahng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12728v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12728v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12724v1",
                "updated": "2025-10-14T17:06:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    6,
                    0,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T17:06:00Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    6,
                    0,
                    1,
                    287,
                    0
                ],
                "title": "T(R,O) Grasp: Efficient Graph Diffusion of Robot-Object Spatial\n  Transformation for Cross-Embodiment Dexterous Grasping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T(R,O) Grasp: Efficient Graph Diffusion of Robot-Object Spatial\n  Transformation for Cross-Embodiment Dexterous Grasping"
                },
                "summary": "Dexterous grasping remains a central challenge in robotics due to the\ncomplexity of its high-dimensional state and action space. We introduce T(R,O)\nGrasp, a diffusion-based framework that efficiently generates accurate and\ndiverse grasps across multiple robotic hands. At its core is the T(R,O) Graph,\na unified representation that models spatial transformations between robotic\nhands and objects while encoding their geometric properties. A graph diffusion\nmodel, coupled with an efficient inverse kinematics solver, supports both\nunconditioned and conditioned grasp synthesis. Extensive experiments on a\ndiverse set of dexterous hands show that T(R,O) Grasp achieves average success\nrate of 94.83%, inference speed of 0.21s, and throughput of 41 grasps per\nsecond on an NVIDIA A100 40GB GPU, substantially outperforming existing\nbaselines. In addition, our approach is robust and generalizable across\nembodiments while significantly reducing memory consumption. More importantly,\nthe high inference speed enables closed-loop dexterous manipulation,\nunderscoring the potential of T(R,O) Grasp to scale into a foundation model for\ndexterous grasping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dexterous grasping remains a central challenge in robotics due to the\ncomplexity of its high-dimensional state and action space. We introduce T(R,O)\nGrasp, a diffusion-based framework that efficiently generates accurate and\ndiverse grasps across multiple robotic hands. At its core is the T(R,O) Graph,\na unified representation that models spatial transformations between robotic\nhands and objects while encoding their geometric properties. A graph diffusion\nmodel, coupled with an efficient inverse kinematics solver, supports both\nunconditioned and conditioned grasp synthesis. Extensive experiments on a\ndiverse set of dexterous hands show that T(R,O) Grasp achieves average success\nrate of 94.83%, inference speed of 0.21s, and throughput of 41 grasps per\nsecond on an NVIDIA A100 40GB GPU, substantially outperforming existing\nbaselines. In addition, our approach is robust and generalizable across\nembodiments while significantly reducing memory consumption. More importantly,\nthe high inference speed enables closed-loop dexterous manipulation,\nunderscoring the potential of T(R,O) Grasp to scale into a foundation model for\ndexterous grasping."
                },
                "authors": [
                    {
                        "name": "Xin Fei"
                    },
                    {
                        "name": "Zhixuan Xu"
                    },
                    {
                        "name": "Huaicong Fang"
                    },
                    {
                        "name": "Tianrui Zhang"
                    },
                    {
                        "name": "Lin Shao"
                    }
                ],
                "author_detail": {
                    "name": "Lin Shao"
                },
                "author": "Lin Shao",
                "arxiv_comment": "12 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12721v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12721v1",
                "updated": "2025-10-14T17:00:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    0,
                    13,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T17:00:13Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    0,
                    13,
                    1,
                    287,
                    0
                ],
                "title": "CARVQ: Corrective Adaptor with Group Residual Vector Quantization for\n  LLM Embedding Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CARVQ: Corrective Adaptor with Group Residual Vector Quantization for\n  LLM Embedding Compression"
                },
                "summary": "Large Language Models (LLMs) typically rely on a large number of parameters\nfor token embedding, leading to substantial storage requirements and memory\nfootprints. In particular, LLMs deployed on edge devices are memory-bound, and\nreducing the memory footprint by compressing the embedding layer not only frees\nup the memory bandwidth but also speeds up inference. To address this, we\nintroduce CARVQ, a post-training novel Corrective Adaptor combined with group\nResidual Vector Quantization. CARVQ relies on the composition of both linear\nand non-linear maps and mimics the original model embedding to compress to\napproximately 1.6 bits without requiring specialized hardware to support\nlower-bit storage. We test our method on pre-trained LLMs such as LLaMA-3.2-1B,\nLLaMA-3.2-3B, LLaMA-3.2-3B-Instruct, LLaMA-3.1-8B, Qwen2.5-7B, Qwen2.5-Math-7B\nand Phi-4, evaluating on common generative, discriminative, math and reasoning\ntasks. We show that in most cases, CARVQ can achieve lower average\nbitwidth-per-parameter while maintaining reasonable perplexity and accuracy\ncompared to scalar quantization. Our contributions include a novel compression\ntechnique that is compatible with state-of-the-art transformer quantization\nmethods and can be seamlessly integrated into any hardware supporting 4-bit\nmemory to reduce the model's memory footprint in memory-constrained devices.\nThis work demonstrates a crucial step toward the efficient deployment of LLMs\non edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) typically rely on a large number of parameters\nfor token embedding, leading to substantial storage requirements and memory\nfootprints. In particular, LLMs deployed on edge devices are memory-bound, and\nreducing the memory footprint by compressing the embedding layer not only frees\nup the memory bandwidth but also speeds up inference. To address this, we\nintroduce CARVQ, a post-training novel Corrective Adaptor combined with group\nResidual Vector Quantization. CARVQ relies on the composition of both linear\nand non-linear maps and mimics the original model embedding to compress to\napproximately 1.6 bits without requiring specialized hardware to support\nlower-bit storage. We test our method on pre-trained LLMs such as LLaMA-3.2-1B,\nLLaMA-3.2-3B, LLaMA-3.2-3B-Instruct, LLaMA-3.1-8B, Qwen2.5-7B, Qwen2.5-Math-7B\nand Phi-4, evaluating on common generative, discriminative, math and reasoning\ntasks. We show that in most cases, CARVQ can achieve lower average\nbitwidth-per-parameter while maintaining reasonable perplexity and accuracy\ncompared to scalar quantization. Our contributions include a novel compression\ntechnique that is compatible with state-of-the-art transformer quantization\nmethods and can be seamlessly integrated into any hardware supporting 4-bit\nmemory to reduce the model's memory footprint in memory-constrained devices.\nThis work demonstrates a crucial step toward the efficient deployment of LLMs\non edge devices."
                },
                "authors": [
                    {
                        "name": "Dayin Gou"
                    },
                    {
                        "name": "Sanghyun Byun"
                    },
                    {
                        "name": "Nilesh Malpeddi"
                    },
                    {
                        "name": "Gabrielle De Micheli"
                    },
                    {
                        "name": "Prathamesh Vaste"
                    },
                    {
                        "name": "Jacob Song"
                    },
                    {
                        "name": "Woo Seong Chung"
                    }
                ],
                "author_detail": {
                    "name": "Woo Seong Chung"
                },
                "author": "Woo Seong Chung",
                "arxiv_comment": "Accepted at EMNLP Findings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12721v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12721v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12719v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12719v1",
                "updated": "2025-10-14T16:58:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    58,
                    39,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T16:58:39Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    58,
                    39,
                    1,
                    287,
                    0
                ],
                "title": "Multitask finetuning and acceleration of chemical pretrained models for\n  small molecule drug property prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multitask finetuning and acceleration of chemical pretrained models for\n  small molecule drug property prediction"
                },
                "summary": "Chemical pretrained models, sometimes referred to as foundation models, are\nreceiving considerable interest for drug discovery applications. The general\nchemical knowledge extracted from self-supervised training has the potential to\nimprove predictions for critical drug discovery endpoints, including on-target\npotency and ADMET properties. Multi-task learning has previously been\nsuccessfully leveraged to improve predictive models. Here, we show that\nenabling multitasking in finetuning of chemical pretrained graph neural network\nmodels such as Kinetic GROVER Multi-Task (KERMT), an enhanced version of the\nGROVER model, and Knowledge-guided Pre-training of Graph Transformer (KGPT)\nsignificantly improves performance over non-pretrained graph neural network\nmodels. Surprisingly, we find that the performance improvement from finetuning\nKERMT in a multitask manner is most significant at larger data sizes.\nAdditionally, we publish two multitask ADMET data splits to enable more\naccurate benchmarking of multitask deep learning methods for drug property\nprediction. Finally, we provide an accelerated implementation of the KERMT\nmodel on GitHub, unlocking large-scale pretraining, finetuning, and inference\nin industrial drug discovery workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chemical pretrained models, sometimes referred to as foundation models, are\nreceiving considerable interest for drug discovery applications. The general\nchemical knowledge extracted from self-supervised training has the potential to\nimprove predictions for critical drug discovery endpoints, including on-target\npotency and ADMET properties. Multi-task learning has previously been\nsuccessfully leveraged to improve predictive models. Here, we show that\nenabling multitasking in finetuning of chemical pretrained graph neural network\nmodels such as Kinetic GROVER Multi-Task (KERMT), an enhanced version of the\nGROVER model, and Knowledge-guided Pre-training of Graph Transformer (KGPT)\nsignificantly improves performance over non-pretrained graph neural network\nmodels. Surprisingly, we find that the performance improvement from finetuning\nKERMT in a multitask manner is most significant at larger data sizes.\nAdditionally, we publish two multitask ADMET data splits to enable more\naccurate benchmarking of multitask deep learning methods for drug property\nprediction. Finally, we provide an accelerated implementation of the KERMT\nmodel on GitHub, unlocking large-scale pretraining, finetuning, and inference\nin industrial drug discovery workflows."
                },
                "authors": [
                    {
                        "name": "Matthew Adrian"
                    },
                    {
                        "name": "Yunsie Chung"
                    },
                    {
                        "name": "Kevin Boyd"
                    },
                    {
                        "name": "Saee Paliwal"
                    },
                    {
                        "name": "Srimukh Prasad Veccham"
                    },
                    {
                        "name": "Alan C. Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Alan C. Cheng"
                },
                "author": "Alan C. Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12719v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12719v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16463v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16463v2",
                "updated": "2025-10-14T16:54:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    54,
                    27,
                    1,
                    287,
                    0
                ],
                "published": "2025-08-22T15:25:40Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    15,
                    25,
                    40,
                    4,
                    234,
                    0
                ],
                "title": "Modular Embedding Recomposition for Incremental Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modular Embedding Recomposition for Incremental Learning"
                },
                "summary": "The advent of pre-trained Vision-Language Models (VLMs) has significantly\ntransformed Continual Learning (CL), mainly due to their zero-shot\nclassification abilities. Such proficiency makes VLMs well-suited for\nreal-world applications, enabling robust performance on novel unseen classes\nwithout requiring adaptation. However, fine-tuning remains essential when\ndownstream tasks deviate significantly from the pre-training domain. Prior CL\napproaches primarily focus on preserving the zero-shot capabilities of VLMs\nduring incremental fine-tuning on a downstream task. We take a step further by\ndevising an approach that transforms preservation into enhancement of the\nzero-shot capabilities of VLMs. Our approach, named MoDular Embedding\nRecomposition (MoDER), introduces a modular framework that trains multiple\ntextual experts, each specialized in a single seen class, and stores them in a\nfoundational hub. At inference time, for each unseen class, we query the hub\nand compose the retrieved experts to synthesize a refined prototype that\nimproves classification. We show the effectiveness of our method across two\npopular zero-shot incremental protocols, Class-IL and MTIL, comprising a total\nof 14 datasets. The codebase is available at\nhttps://github.com/aimagelab/mammoth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of pre-trained Vision-Language Models (VLMs) has significantly\ntransformed Continual Learning (CL), mainly due to their zero-shot\nclassification abilities. Such proficiency makes VLMs well-suited for\nreal-world applications, enabling robust performance on novel unseen classes\nwithout requiring adaptation. However, fine-tuning remains essential when\ndownstream tasks deviate significantly from the pre-training domain. Prior CL\napproaches primarily focus on preserving the zero-shot capabilities of VLMs\nduring incremental fine-tuning on a downstream task. We take a step further by\ndevising an approach that transforms preservation into enhancement of the\nzero-shot capabilities of VLMs. Our approach, named MoDular Embedding\nRecomposition (MoDER), introduces a modular framework that trains multiple\ntextual experts, each specialized in a single seen class, and stores them in a\nfoundational hub. At inference time, for each unseen class, we query the hub\nand compose the retrieved experts to synthesize a refined prototype that\nimproves classification. We show the effectiveness of our method across two\npopular zero-shot incremental protocols, Class-IL and MTIL, comprising a total\nof 14 datasets. The codebase is available at\nhttps://github.com/aimagelab/mammoth."
                },
                "authors": [
                    {
                        "name": "Aniello Panariello"
                    },
                    {
                        "name": "Emanuele Frascaroli"
                    },
                    {
                        "name": "Pietro Buzzega"
                    },
                    {
                        "name": "Lorenzo Bonicelli"
                    },
                    {
                        "name": "Angelo Porrello"
                    },
                    {
                        "name": "Simone Calderara"
                    }
                ],
                "author_detail": {
                    "name": "Simone Calderara"
                },
                "author": "Simone Calderara",
                "arxiv_comment": "Accepted to the 36th British Machine Vision Conference (BMVC 2025),\n  Sheffield, UK",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16463v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16463v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12712v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12712v1",
                "updated": "2025-10-14T16:50:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    50,
                    49,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T16:50:49Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    50,
                    49,
                    1,
                    287,
                    0
                ],
                "title": "Beyond Seeing: Evaluating Multimodal LLMs on Tool-Enabled Image\n  Perception, Transformation, and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Seeing: Evaluating Multimodal LLMs on Tool-Enabled Image\n  Perception, Transformation, and Reasoning"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are increasingly applied in\nreal-world scenarios where user-provided images are often imperfect, requiring\nactive image manipulations such as cropping, editing, or enhancement to uncover\nsalient visual cues. Beyond static visual perception, MLLMs must also think\nwith images: dynamically transforming visual content and integrating it with\nother tools to solve complex tasks. However, this shift from treating vision as\npassive context to a manipulable cognitive workspace remains underexplored.\nMost existing benchmarks still follow a think about images paradigm, where\nimages are regarded as static inputs. To address this gap, we introduce IRIS,\nan Interactive Reasoning with Images and Systems that evaluates MLLMs' ability\nto perceive, transform, and reason across complex visual-textual tasks under\nthe think with images paradigm. IRIS comprises 1,204 challenging, open-ended\nvision tasks (603 single-turn, 601 multi-turn) spanning across five diverse\ndomains, each paired with detailed rubrics to enable systematic evaluation. Our\nevaluation shows that current MLLMs struggle with tasks requiring effective\nintegration of vision and general-purpose tools. Even the strongest model\n(GPT-5-think) reaches only 18.68% pass rate. We further observe divergent\ntool-use behaviors, with OpenAI models benefiting from diverse image\nmanipulations while Gemini-2.5-pro shows no improvement. By introducing the\nfirst benchmark centered on think with images, IRIS offers critical insights\nfor advancing visual intelligence in MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are increasingly applied in\nreal-world scenarios where user-provided images are often imperfect, requiring\nactive image manipulations such as cropping, editing, or enhancement to uncover\nsalient visual cues. Beyond static visual perception, MLLMs must also think\nwith images: dynamically transforming visual content and integrating it with\nother tools to solve complex tasks. However, this shift from treating vision as\npassive context to a manipulable cognitive workspace remains underexplored.\nMost existing benchmarks still follow a think about images paradigm, where\nimages are regarded as static inputs. To address this gap, we introduce IRIS,\nan Interactive Reasoning with Images and Systems that evaluates MLLMs' ability\nto perceive, transform, and reason across complex visual-textual tasks under\nthe think with images paradigm. IRIS comprises 1,204 challenging, open-ended\nvision tasks (603 single-turn, 601 multi-turn) spanning across five diverse\ndomains, each paired with detailed rubrics to enable systematic evaluation. Our\nevaluation shows that current MLLMs struggle with tasks requiring effective\nintegration of vision and general-purpose tools. Even the strongest model\n(GPT-5-think) reaches only 18.68% pass rate. We further observe divergent\ntool-use behaviors, with OpenAI models benefiting from diverse image\nmanipulations while Gemini-2.5-pro shows no improvement. By introducing the\nfirst benchmark centered on think with images, IRIS offers critical insights\nfor advancing visual intelligence in MLLMs."
                },
                "authors": [
                    {
                        "name": "Xingang Guo"
                    },
                    {
                        "name": "Utkarsh Tyagi"
                    },
                    {
                        "name": "Advait Gosai"
                    },
                    {
                        "name": "Paula Vergara"
                    },
                    {
                        "name": "Ernesto Gabriel Hernndez Montoya"
                    },
                    {
                        "name": "Chen Bo Calvin Zhang"
                    },
                    {
                        "name": "Bin Hu"
                    },
                    {
                        "name": "Yunzhong He"
                    },
                    {
                        "name": "Bing Liu"
                    },
                    {
                        "name": "Rakshith Sharma Srinivasa"
                    }
                ],
                "author_detail": {
                    "name": "Rakshith Sharma Srinivasa"
                },
                "author": "Rakshith Sharma Srinivasa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12712v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12712v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12421v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12421v3",
                "updated": "2025-10-14T16:45:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    45,
                    31,
                    1,
                    287,
                    0
                ],
                "published": "2025-05-18T13:43:25Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    13,
                    43,
                    25,
                    6,
                    138,
                    0
                ],
                "title": "Fixed Point Explainability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fixed Point Explainability"
                },
                "summary": "This paper introduces a formal notion of fixed point explanations, inspired\nby the \"why regress\" principle, to assess, through recursive applications, the\nstability of the interplay between a model and its explainer. Fixed point\nexplanations satisfy properties like minimality, stability, and faithfulness,\nrevealing hidden model behaviours and explanatory weaknesses. We define\nconvergence conditions for several classes of explainers, from feature-based to\nmechanistic tools like Sparse AutoEncoders, and we report quantitative and\nqualitative results for several datasets and models, including LLMs such as\nLlama-3.3-70B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a formal notion of fixed point explanations, inspired\nby the \"why regress\" principle, to assess, through recursive applications, the\nstability of the interplay between a model and its explainer. Fixed point\nexplanations satisfy properties like minimality, stability, and faithfulness,\nrevealing hidden model behaviours and explanatory weaknesses. We define\nconvergence conditions for several classes of explainers, from feature-based to\nmechanistic tools like Sparse AutoEncoders, and we report quantitative and\nqualitative results for several datasets and models, including LLMs such as\nLlama-3.3-70B."
                },
                "authors": [
                    {
                        "name": "Emanuele La Malfa"
                    },
                    {
                        "name": "Jon Vadillo"
                    },
                    {
                        "name": "Marco Molinari"
                    },
                    {
                        "name": "Michael Wooldridge"
                    }
                ],
                "author_detail": {
                    "name": "Michael Wooldridge"
                },
                "author": "Michael Wooldridge",
                "arxiv_comment": "The code is available here:\n  https://anonymous.4open.science/r/fixed_point_explainability_iclr2026-D188",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12421v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12421v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12702v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12702v1",
                "updated": "2025-10-14T16:37:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    37,
                    39,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T16:37:39Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    37,
                    39,
                    1,
                    287,
                    0
                ],
                "title": "Beyond Postconditions: Can Large Language Models infer Formal Contracts\n  for Automatic Software Verification?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Postconditions: Can Large Language Models infer Formal Contracts\n  for Automatic Software Verification?"
                },
                "summary": "Automatic software verifiers have become increasingly effective at the task\nof checking software against (formal) specifications. Yet, their adoption in\npractice has been hampered by the lack of such specifications in real world\ncode. Large Language Models (LLMs) have shown promise in inferring formal\npostconditions from natural language hints embedded in code such as function\nnames, comments or documentation. Using the generated postconditions as\nspecifications in a subsequent verification, however, often leads verifiers to\nsuggest invalid inputs, hinting at potential issues that ultimately turn out to\nbe false alarms.\n  To address this, we revisit the problem of specification inference from\nnatural language in the context of automatic software verification. In the\nprocess, we introduce NL2Contract, the task of employing LLMs to translate\ninformal natural language into formal functional contracts, consisting of\npostconditions as well as preconditions. We introduce metrics to validate and\ncompare different NL2Contract approaches, using soundness, bug discriminative\npower of the generated contracts and their usability in the context of\nautomatic software verification as key metrics. We evaluate NL2Contract with\ndifferent LLMs and compare it to the task of postcondition generation\nnl2postcond. Our evaluation shows that (1) LLMs are generally effective at\ngenerating functional contracts sound for all possible inputs, (2) the\ngenerated contracts are sufficiently expressive for discriminating buggy from\ncorrect behavior, and (3) verifiers supplied with LLM inferred functional\ncontracts produce fewer false alarms than when provided with postconditions\nalone. Further investigations show that LLM inferred preconditions generally\nalign well with developers intentions which allows us to use automatic software\nverifiers to catch real-world bugs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic software verifiers have become increasingly effective at the task\nof checking software against (formal) specifications. Yet, their adoption in\npractice has been hampered by the lack of such specifications in real world\ncode. Large Language Models (LLMs) have shown promise in inferring formal\npostconditions from natural language hints embedded in code such as function\nnames, comments or documentation. Using the generated postconditions as\nspecifications in a subsequent verification, however, often leads verifiers to\nsuggest invalid inputs, hinting at potential issues that ultimately turn out to\nbe false alarms.\n  To address this, we revisit the problem of specification inference from\nnatural language in the context of automatic software verification. In the\nprocess, we introduce NL2Contract, the task of employing LLMs to translate\ninformal natural language into formal functional contracts, consisting of\npostconditions as well as preconditions. We introduce metrics to validate and\ncompare different NL2Contract approaches, using soundness, bug discriminative\npower of the generated contracts and their usability in the context of\nautomatic software verification as key metrics. We evaluate NL2Contract with\ndifferent LLMs and compare it to the task of postcondition generation\nnl2postcond. Our evaluation shows that (1) LLMs are generally effective at\ngenerating functional contracts sound for all possible inputs, (2) the\ngenerated contracts are sufficiently expressive for discriminating buggy from\ncorrect behavior, and (3) verifiers supplied with LLM inferred functional\ncontracts produce fewer false alarms than when provided with postconditions\nalone. Further investigations show that LLM inferred preconditions generally\nalign well with developers intentions which allows us to use automatic software\nverifiers to catch real-world bugs."
                },
                "authors": [
                    {
                        "name": "Cedric Richter"
                    },
                    {
                        "name": "Heike Wehrheim"
                    }
                ],
                "author_detail": {
                    "name": "Heike Wehrheim"
                },
                "author": "Heike Wehrheim",
                "arxiv_comment": "under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12702v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12699v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12699v1",
                "updated": "2025-10-14T16:31:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    31,
                    34,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T16:31:34Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    31,
                    34,
                    1,
                    287,
                    0
                ],
                "title": "Generation Space Size: Understanding and Calibrating Open-Endedness of\n  LLM Generations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generation Space Size: Understanding and Calibrating Open-Endedness of\n  LLM Generations"
                },
                "summary": "Different open-ended generation tasks require different degrees of output\ndiversity. However, current LLMs are often miscalibrated. They collapse to\noverly homogeneous outputs for creative tasks and hallucinate diverse but\nincorrect responses for factual tasks. We argue that these two failure modes\nare unified by, and can both be addressed by, the notion of effective\ngeneration space size (GSS) -- the set of semantically distinct outputs a model\nconsiders for a prompt. We present GSSBench, a task suite of prompt pairs with\nground-truth GSS relationships to assess different metrics and understand where\nmodels diverge from desired behavior. We find that hallucination detection\nmetrics, particularly EigenScore, consistently outperform standard diversity\nand uncertainty quantification metrics, while using only model internals,\nproviding interpretable insights into a model's internal task representations.\nWe demonstrate three applications of GSS: (1) detecting prompt ambiguity and\npredicting clarification questions for better grounding, (2) interpreting\noverthinking and underthinking in reasoning models, and (3) steering models to\nexpand their generation space to yield high-quality and diverse outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Different open-ended generation tasks require different degrees of output\ndiversity. However, current LLMs are often miscalibrated. They collapse to\noverly homogeneous outputs for creative tasks and hallucinate diverse but\nincorrect responses for factual tasks. We argue that these two failure modes\nare unified by, and can both be addressed by, the notion of effective\ngeneration space size (GSS) -- the set of semantically distinct outputs a model\nconsiders for a prompt. We present GSSBench, a task suite of prompt pairs with\nground-truth GSS relationships to assess different metrics and understand where\nmodels diverge from desired behavior. We find that hallucination detection\nmetrics, particularly EigenScore, consistently outperform standard diversity\nand uncertainty quantification metrics, while using only model internals,\nproviding interpretable insights into a model's internal task representations.\nWe demonstrate three applications of GSS: (1) detecting prompt ambiguity and\npredicting clarification questions for better grounding, (2) interpreting\noverthinking and underthinking in reasoning models, and (3) steering models to\nexpand their generation space to yield high-quality and diverse outputs."
                },
                "authors": [
                    {
                        "name": "Sunny Yu"
                    },
                    {
                        "name": "Ahmad Jabbar"
                    },
                    {
                        "name": "Robert Hawkins"
                    },
                    {
                        "name": "Dan Jurafsky"
                    },
                    {
                        "name": "Myra Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Myra Cheng"
                },
                "author": "Myra Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12699v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12697v1",
                "updated": "2025-10-14T16:30:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    30,
                    30,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T16:30:30Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    30,
                    30,
                    1,
                    287,
                    0
                ],
                "title": "Multi-Agent Debate for LLM Judges with Adaptive Stability Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Debate for LLM Judges with Adaptive Stability Detection"
                },
                "summary": "With advancements in reasoning capabilities, Large Language Models (LLMs) are\nincreasingly employed for automated judgment tasks. While LLMs-as-Judges offer\npromise in automating evaluations, current approaches often rely on simplistic\naggregation methods (e.g., majority voting), which can fail even when\nindividual agents provide correct answers. To address this, we propose a\nmulti-agent debate judge framework where agents collaboratively reason and\niteratively refine their responses. We formalize the debate process\nmathematically, analyzing agent interactions and proving that debate amplifies\ncorrectness compared to static ensembles. To enhance efficiency, we introduce a\nstability detection mechanism that models judge consensus dynamics via a\ntime-varying Beta-Binomial mixture, with adaptive stopping based on\ndistributional similarity (Kolmogorov-Smirnov test). This mechanism models the\njudges' collective correct rate dynamics using a time-varying mixture of\nBeta-Binomial distributions and employs an adaptive stopping criterion based on\ndistributional similarity (Kolmogorov-Smirnov statistic). Experiments across\nmultiple benchmarks and models demonstrate that our framework improves judgment\naccuracy over majority voting while maintaining computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With advancements in reasoning capabilities, Large Language Models (LLMs) are\nincreasingly employed for automated judgment tasks. While LLMs-as-Judges offer\npromise in automating evaluations, current approaches often rely on simplistic\naggregation methods (e.g., majority voting), which can fail even when\nindividual agents provide correct answers. To address this, we propose a\nmulti-agent debate judge framework where agents collaboratively reason and\niteratively refine their responses. We formalize the debate process\nmathematically, analyzing agent interactions and proving that debate amplifies\ncorrectness compared to static ensembles. To enhance efficiency, we introduce a\nstability detection mechanism that models judge consensus dynamics via a\ntime-varying Beta-Binomial mixture, with adaptive stopping based on\ndistributional similarity (Kolmogorov-Smirnov test). This mechanism models the\njudges' collective correct rate dynamics using a time-varying mixture of\nBeta-Binomial distributions and employs an adaptive stopping criterion based on\ndistributional similarity (Kolmogorov-Smirnov statistic). Experiments across\nmultiple benchmarks and models demonstrate that our framework improves judgment\naccuracy over majority voting while maintaining computational efficiency."
                },
                "authors": [
                    {
                        "name": "Tianyu Hu"
                    },
                    {
                        "name": "Zhen Tan"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Huaizhi Qu"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22363v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22363v2",
                "updated": "2025-10-14T16:24:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    24,
                    33,
                    1,
                    287,
                    0
                ],
                "published": "2025-09-26T13:58:22Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    58,
                    22,
                    4,
                    269,
                    0
                ],
                "title": "Investigating Faithfulness in Large Audio Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Faithfulness in Large Audio Language Models"
                },
                "summary": "Faithfulness measures whether chain-of-thought (CoT) representations\naccurately reflect a model's decision process and can be used as reliable\nexplanations. Prior work has shown that CoTs from text-based LLMs are often\nunfaithful. This question has not been explored for large audio-language models\n(LALMs), where faithfulness is critical for safety-sensitive applications.\nReasoning in LALMs is also more challenging, as models must first extract\nrelevant clues from audio before reasoning over them. In this paper, we\ninvestigate the faithfulness of CoTs produced by several LALMs by applying\ntargeted interventions, including paraphrasing, filler token injection, early\nanswering, and introducing mistakes, on two challenging reasoning datasets:\nSAKURA and MMAR. After going through the aforementioned interventions across\nseveral datasets and tasks, our experiments suggest that, LALMs generally\nproduce CoTs that appear to be faithful to their underlying decision processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faithfulness measures whether chain-of-thought (CoT) representations\naccurately reflect a model's decision process and can be used as reliable\nexplanations. Prior work has shown that CoTs from text-based LLMs are often\nunfaithful. This question has not been explored for large audio-language models\n(LALMs), where faithfulness is critical for safety-sensitive applications.\nReasoning in LALMs is also more challenging, as models must first extract\nrelevant clues from audio before reasoning over them. In this paper, we\ninvestigate the faithfulness of CoTs produced by several LALMs by applying\ntargeted interventions, including paraphrasing, filler token injection, early\nanswering, and introducing mistakes, on two challenging reasoning datasets:\nSAKURA and MMAR. After going through the aforementioned interventions across\nseveral datasets and tasks, our experiments suggest that, LALMs generally\nproduce CoTs that appear to be faithful to their underlying decision processes."
                },
                "authors": [
                    {
                        "name": "Lovenya Jain"
                    },
                    {
                        "name": "Pooneh Mousavi"
                    },
                    {
                        "name": "Mirco Ravanelli"
                    },
                    {
                        "name": "Cem Subakan"
                    }
                ],
                "author_detail": {
                    "name": "Cem Subakan"
                },
                "author": "Cem Subakan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22363v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22363v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12689v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12689v1",
                "updated": "2025-10-14T16:24:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    24,
                    19,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T16:24:19Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    24,
                    19,
                    1,
                    287,
                    0
                ],
                "title": "From Delegates to Trustees: How Optimizing for Long-Term Interests\n  Shapes Bias and Alignment in LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Delegates to Trustees: How Optimizing for Long-Term Interests\n  Shapes Bias and Alignment in LLM"
                },
                "summary": "Large language models (LLMs) have shown promising accuracy in predicting\nsurvey responses and policy preferences, which has increased interest in their\npotential to represent human interests in various domains. Most existing\nresearch has focused on behavioral cloning, effectively evaluating how well\nmodels reproduce individuals' expressed preferences. Drawing on theories of\npolitical representation, we highlight an underexplored design trade-off:\nwhether AI systems should act as delegates, mirroring expressed preferences, or\nas trustees, exercising judgment about what best serves an individual's\ninterests. This trade-off is closely related to issues of LLM sycophancy, where\nmodels can encourage behavior or validate beliefs that may be aligned with a\nuser's short-term preferences, but is detrimental to their long-term interests.\nThrough a series of experiments simulating votes on various policy issues in\nthe U.S. context, we apply a temporal utility framework that weighs short and\nlong-term interests (simulating a trustee role) and compare voting outcomes to\nbehavior-cloning models (simulating a delegate). We find that trustee-style\npredictions weighted toward long-term interests produce policy decisions that\nalign more closely with expert consensus on well-understood issues, but also\nshow greater bias toward models' default stances on topics lacking clear\nagreement. These findings reveal a fundamental trade-off in designing AI\nsystems to represent human interests. Delegate models better preserve user\nautonomy but may diverge from well-supported policy positions, while trustee\nmodels can promote welfare on well-understood issues yet risk paternalism and\nbias on subjective topics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown promising accuracy in predicting\nsurvey responses and policy preferences, which has increased interest in their\npotential to represent human interests in various domains. Most existing\nresearch has focused on behavioral cloning, effectively evaluating how well\nmodels reproduce individuals' expressed preferences. Drawing on theories of\npolitical representation, we highlight an underexplored design trade-off:\nwhether AI systems should act as delegates, mirroring expressed preferences, or\nas trustees, exercising judgment about what best serves an individual's\ninterests. This trade-off is closely related to issues of LLM sycophancy, where\nmodels can encourage behavior or validate beliefs that may be aligned with a\nuser's short-term preferences, but is detrimental to their long-term interests.\nThrough a series of experiments simulating votes on various policy issues in\nthe U.S. context, we apply a temporal utility framework that weighs short and\nlong-term interests (simulating a trustee role) and compare voting outcomes to\nbehavior-cloning models (simulating a delegate). We find that trustee-style\npredictions weighted toward long-term interests produce policy decisions that\nalign more closely with expert consensus on well-understood issues, but also\nshow greater bias toward models' default stances on topics lacking clear\nagreement. These findings reveal a fundamental trade-off in designing AI\nsystems to represent human interests. Delegate models better preserve user\nautonomy but may diverge from well-supported policy positions, while trustee\nmodels can promote welfare on well-understood issues yet risk paternalism and\nbias on subjective topics."
                },
                "authors": [
                    {
                        "name": "Suyash Fulay"
                    },
                    {
                        "name": "Jocelyn Zhu"
                    },
                    {
                        "name": "Michiel Bakker"
                    }
                ],
                "author_detail": {
                    "name": "Michiel Bakker"
                },
                "author": "Michiel Bakker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12689v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12689v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12680v1",
                "updated": "2025-10-14T16:19:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    19,
                    44,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T16:19:44Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    19,
                    44,
                    1,
                    287,
                    0
                ],
                "title": "Demystifying Hybrid Thinking: Can LLMs Truly Switch Between Think and\n  No-Think?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying Hybrid Thinking: Can LLMs Truly Switch Between Think and\n  No-Think?"
                },
                "summary": "Hybrid thinking enables LLMs to switch between reasoning and direct\nanswering, offering a balance between efficiency and reasoning capability. Yet\nour experiments reveal that current hybrid thinking LLMs only achieve partial\nmode separation: reasoning behaviors often leak into the no-think mode. To\nunderstand and mitigate this, we analyze the factors influencing\ncontrollability and identify four that matter most: (1) larger data scale, (2)\nusing think and no-think answers from different questions rather than the same\nquestion, (3) a moderate increase in no-think data number, and (4) a two-phase\nstrategy that first trains reasoning ability and then applies hybrid think\ntraining. Building on these findings, we propose a practical recipe that,\ncompared to standard training, can maintain accuracy in both modes while\nsignificantly reducing no-think output length (from $1085$ to $585$ on MATH500)\nand occurrences of reasoning-supportive tokens such as ``\\texttt{wait}'' (from\n$5917$ to $522$ on MATH500). Our findings highlight the limitations of current\nhybrid thinking and offer directions for strengthening its controllability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid thinking enables LLMs to switch between reasoning and direct\nanswering, offering a balance between efficiency and reasoning capability. Yet\nour experiments reveal that current hybrid thinking LLMs only achieve partial\nmode separation: reasoning behaviors often leak into the no-think mode. To\nunderstand and mitigate this, we analyze the factors influencing\ncontrollability and identify four that matter most: (1) larger data scale, (2)\nusing think and no-think answers from different questions rather than the same\nquestion, (3) a moderate increase in no-think data number, and (4) a two-phase\nstrategy that first trains reasoning ability and then applies hybrid think\ntraining. Building on these findings, we propose a practical recipe that,\ncompared to standard training, can maintain accuracy in both modes while\nsignificantly reducing no-think output length (from $1085$ to $585$ on MATH500)\nand occurrences of reasoning-supportive tokens such as ``\\texttt{wait}'' (from\n$5917$ to $522$ on MATH500). Our findings highlight the limitations of current\nhybrid thinking and offer directions for strengthening its controllability."
                },
                "authors": [
                    {
                        "name": "Shouren Wang"
                    },
                    {
                        "name": "Wang Yang"
                    },
                    {
                        "name": "Xianxuan Long"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    },
                    {
                        "name": "Xiaotian Han"
                    }
                ],
                "author_detail": {
                    "name": "Xiaotian Han"
                },
                "author": "Xiaotian Han",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14214v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14214v2",
                "updated": "2025-10-14T16:10:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    10,
                    53,
                    1,
                    287,
                    0
                ],
                "published": "2024-11-21T15:24:41Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    15,
                    24,
                    41,
                    3,
                    326,
                    0
                ],
                "title": "Physics-Informed Autonomous LLM Agents for Explainable Power Electronics\n  Modulation Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics-Informed Autonomous LLM Agents for Explainable Power Electronics\n  Modulation Design"
                },
                "summary": "LLM-based autonomous agents have recently shown strong capabilities in\nsolving complex industrial design tasks. However, in domains aiming for carbon\nneutrality and high-performance renewable energy systems, current AI-assisted\ndesign automation methods face critical challenges in explainability,\nscalability, and practical usability. To address these limitations, we\nintroduce PHIA (Physics-Informed Autonomous Agent), an LLM-driven system that\nautomates modulation design for power converters in Power Electronics Systems\nwith minimal human intervention. In contrast to traditional pipeline-based\nmethods, PHIA incorporates an LLM-based planning module that interactively\nacquires and verifies design requirements via a user-friendly chat interface.\nThis planner collaborates with physics-informed simulation and optimization\ncomponents to autonomously generate and iteratively refine modulation designs.\nThe interactive interface also supports interpretability by providing textual\nexplanations and visual outputs throughout the design process. Experimental\nresults show that PHIA reduces standard mean absolute error by 63.2% compared\nto the second-best benchmark and accelerates the overall design process by over\n33 times. A user study involving 20 domain experts further confirms PHIA's\nsuperior design efficiency and usability, highlighting its potential to\ntransform industrial design workflows in power electronics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based autonomous agents have recently shown strong capabilities in\nsolving complex industrial design tasks. However, in domains aiming for carbon\nneutrality and high-performance renewable energy systems, current AI-assisted\ndesign automation methods face critical challenges in explainability,\nscalability, and practical usability. To address these limitations, we\nintroduce PHIA (Physics-Informed Autonomous Agent), an LLM-driven system that\nautomates modulation design for power converters in Power Electronics Systems\nwith minimal human intervention. In contrast to traditional pipeline-based\nmethods, PHIA incorporates an LLM-based planning module that interactively\nacquires and verifies design requirements via a user-friendly chat interface.\nThis planner collaborates with physics-informed simulation and optimization\ncomponents to autonomously generate and iteratively refine modulation designs.\nThe interactive interface also supports interpretability by providing textual\nexplanations and visual outputs throughout the design process. Experimental\nresults show that PHIA reduces standard mean absolute error by 63.2% compared\nto the second-best benchmark and accelerates the overall design process by over\n33 times. A user study involving 20 domain experts further confirms PHIA's\nsuperior design efficiency and usability, highlighting its potential to\ntransform industrial design workflows in power electronics."
                },
                "authors": [
                    {
                        "name": "Junhua Liu"
                    },
                    {
                        "name": "Fanfan Lin"
                    },
                    {
                        "name": "Xinze Li"
                    },
                    {
                        "name": "Kwan Hui Lim"
                    },
                    {
                        "name": "Shuai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Shuai Zhao"
                },
                "author": "Shuai Zhao",
                "arxiv_comment": "Accepted to AAAI 2026 Innovative Applications of AI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14214v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14214v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12672v1",
                "updated": "2025-10-14T16:08:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    8,
                    22,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T16:08:22Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    8,
                    22,
                    1,
                    287,
                    0
                ],
                "title": "Keep Calm and Avoid Harmful Content: Concept Alignment and Latent\n  Manipulation Towards Safer Answers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep Calm and Avoid Harmful Content: Concept Alignment and Latent\n  Manipulation Towards Safer Answers"
                },
                "summary": "Large Language Models are susceptible to jailbreak attacks that bypass\nbuilt-in safety guardrails (e.g., by tricking the model with adversarial\nprompts). We propose Concept Alignment and Concept Manipulation \\textbf{CALM},\nan inference-time method that suppresses harmful concepts by modifying latent\nrepresentations of the last layer of the model, without retraining. Leveraging\n\\gls*{cw} technique from Computer Vision combined with orthogonal projection,\nCALM removes unwanted latent directions associated with harmful content while\npreserving model performance. Experiments show that CALM reduces harmful\noutputs and outperforms baseline methods in most metrics, offering a\nlightweight approach to AI safety with no additional training data or model\nfine-tuning, while incurring only a small computational overhead at inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are susceptible to jailbreak attacks that bypass\nbuilt-in safety guardrails (e.g., by tricking the model with adversarial\nprompts). We propose Concept Alignment and Concept Manipulation \\textbf{CALM},\nan inference-time method that suppresses harmful concepts by modifying latent\nrepresentations of the last layer of the model, without retraining. Leveraging\n\\gls*{cw} technique from Computer Vision combined with orthogonal projection,\nCALM removes unwanted latent directions associated with harmful content while\npreserving model performance. Experiments show that CALM reduces harmful\noutputs and outperforms baseline methods in most metrics, offering a\nlightweight approach to AI safety with no additional training data or model\nfine-tuning, while incurring only a small computational overhead at inference."
                },
                "authors": [
                    {
                        "name": "Ruben Belo"
                    },
                    {
                        "name": "Claudia Soares"
                    },
                    {
                        "name": "Marta Guimaraes"
                    }
                ],
                "author_detail": {
                    "name": "Marta Guimaraes"
                },
                "author": "Marta Guimaraes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02659v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02659v3",
                "updated": "2025-10-14T16:05:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    5,
                    11,
                    1,
                    287,
                    0
                ],
                "published": "2025-07-03T14:20:41Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding"
                },
                "summary": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup."
                },
                "authors": [
                    {
                        "name": "Ramchalam Kinattinkara Ramakrishnan"
                    },
                    {
                        "name": "Zhaocong Yuan"
                    },
                    {
                        "name": "Shaojie Zhuo"
                    },
                    {
                        "name": "Chen Feng"
                    },
                    {
                        "name": "Yicheng Lin"
                    },
                    {
                        "name": "Chenzheng Su"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaopeng Zhang"
                },
                "author": "Xiaopeng Zhang",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02659v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02659v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12668v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12668v1",
                "updated": "2025-10-14T16:05:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    5,
                    1,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T16:05:01Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    5,
                    1,
                    1,
                    287,
                    0
                ],
                "title": "The Role of Parametric Injection-A Systematic Study of Parametric\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Role of Parametric Injection-A Systematic Study of Parametric\n  Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nretrieving external documents. As an emerging form of RAG, parametric\nretrieval-augmented generation (PRAG) encodes documents as model parameters\n(i.e., LoRA modules) and injects these representations into the model during\ninference, enabling interaction between the LLM and documents at parametric\nlevel. Compared with directly placing documents in the input context, PRAG is\nmore efficient and has the potential to offer deeper model-document\ninteraction. Despite its growing attention, the mechanism underlying parametric\ninjection remains poorly understood. In this work, we present a systematic\nstudy of PRAG to clarify the role of parametric injection, showing that\nparameterized documents capture only partial semantic information of documents,\nand relying on them alone yields inferior performance compared to interaction\nat text level. However, these parametric representations encode high-level\ndocument information that can enhance the model's understanding of documents\nwithin the input context. When combined parameterized documents with textual\ndocuments, the model can leverage relevant information more effectively and\nbecome more robust to noisy inputs, achieving better performance than either\nsource alone. We recommend jointly using parameterized and textual documents\nand advocate for increasing the information content of parametric\nrepresentations to advance PRAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nretrieving external documents. As an emerging form of RAG, parametric\nretrieval-augmented generation (PRAG) encodes documents as model parameters\n(i.e., LoRA modules) and injects these representations into the model during\ninference, enabling interaction between the LLM and documents at parametric\nlevel. Compared with directly placing documents in the input context, PRAG is\nmore efficient and has the potential to offer deeper model-document\ninteraction. Despite its growing attention, the mechanism underlying parametric\ninjection remains poorly understood. In this work, we present a systematic\nstudy of PRAG to clarify the role of parametric injection, showing that\nparameterized documents capture only partial semantic information of documents,\nand relying on them alone yields inferior performance compared to interaction\nat text level. However, these parametric representations encode high-level\ndocument information that can enhance the model's understanding of documents\nwithin the input context. When combined parameterized documents with textual\ndocuments, the model can leverage relevant information more effectively and\nbecome more robust to noisy inputs, achieving better performance than either\nsource alone. We recommend jointly using parameterized and textual documents\nand advocate for increasing the information content of parametric\nrepresentations to advance PRAG."
                },
                "authors": [
                    {
                        "name": "Minghao Tang"
                    },
                    {
                        "name": "Shiyu Ni"
                    },
                    {
                        "name": "Jingtong Wu"
                    },
                    {
                        "name": "Zengxin Han"
                    },
                    {
                        "name": "Keping Bi"
                    }
                ],
                "author_detail": {
                    "name": "Keping Bi"
                },
                "author": "Keping Bi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12668v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12668v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11630v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11630v4",
                "updated": "2025-10-14T15:48:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    48,
                    16,
                    1,
                    287,
                    0
                ],
                "published": "2024-06-17T15:13:13Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    15,
                    13,
                    13,
                    0,
                    169,
                    0
                ],
                "title": "A framework for the use of generative modelling in non-equilibrium\n  statistical mechanics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A framework for the use of generative modelling in non-equilibrium\n  statistical mechanics"
                },
                "summary": "We discuss an approach to mathematically modelling systems made of objects\nthat are coupled together, using generative models of the dependence\nrelationships between states (or trajectories) of the things comprising such\nsystems. This broad class includes open or non-equilibrium systems and is\nespecially relevant to self-organising systems. The ensuing variational free\nenergy principle (FEP) has certain advantages over using random dynamical\nsystems explicitly, notably, by being more tractable and offering a\nparsimonious explanation of why the joint system evolves in the way that it\ndoes, based on the properties of the coupling between system components. The\nFEP is a method whose use allows us to build a model of the dynamics of an\nobject as if it were a process of variational inference, because variational\nfree energy (or surprisal) is a Lyapunov function for its dynamics. In short,\nwe argue that using generative models to represent and track relations amongst\nsubsystems leads us to a particular statistical theory of interacting systems.\nConversely, this theory enables us to construct nested models that respect the\nknown relations amongst subsystems. We point out that the fact that a physical\nobject conforms to the FEP does not necessarily imply that this object performs\ninference in the literal sense; rather, it is a useful explanatory fiction\nwhich replaces the `explicit' dynamics of the object with an `implicit' flow on\nfree energy gradients -- a fiction that may or may not be entertained by the\nobject itself.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We discuss an approach to mathematically modelling systems made of objects\nthat are coupled together, using generative models of the dependence\nrelationships between states (or trajectories) of the things comprising such\nsystems. This broad class includes open or non-equilibrium systems and is\nespecially relevant to self-organising systems. The ensuing variational free\nenergy principle (FEP) has certain advantages over using random dynamical\nsystems explicitly, notably, by being more tractable and offering a\nparsimonious explanation of why the joint system evolves in the way that it\ndoes, based on the properties of the coupling between system components. The\nFEP is a method whose use allows us to build a model of the dynamics of an\nobject as if it were a process of variational inference, because variational\nfree energy (or surprisal) is a Lyapunov function for its dynamics. In short,\nwe argue that using generative models to represent and track relations amongst\nsubsystems leads us to a particular statistical theory of interacting systems.\nConversely, this theory enables us to construct nested models that respect the\nknown relations amongst subsystems. We point out that the fact that a physical\nobject conforms to the FEP does not necessarily imply that this object performs\ninference in the literal sense; rather, it is a useful explanatory fiction\nwhich replaces the `explicit' dynamics of the object with an `implicit' flow on\nfree energy gradients -- a fiction that may or may not be entertained by the\nobject itself."
                },
                "authors": [
                    {
                        "name": "Karl J Friston"
                    },
                    {
                        "name": "Maxwell J D Ramstead"
                    },
                    {
                        "name": "Dalton A R Sakthivadivel"
                    }
                ],
                "author_detail": {
                    "name": "Dalton A R Sakthivadivel"
                },
                "author": "Dalton A R Sakthivadivel",
                "arxiv_comment": "27+3 pages, ten svg figures. Replaces arXiv:2208.06924. This version:\n  addition of a subsection to empirical validation section in response to\n  referee reports",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11630v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11630v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.MP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nlin.AO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12378v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12378v2",
                "updated": "2025-10-14T15:47:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    47,
                    25,
                    1,
                    287,
                    0
                ],
                "published": "2025-04-16T18:00:00Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    0,
                    0,
                    2,
                    106,
                    0
                ],
                "title": "Unveiling the trends between dust attenuation and galaxy properties at z\n  ~ 2 - 12 with the James Webb Space Telescope",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling the trends between dust attenuation and galaxy properties at z\n  ~ 2 - 12 with the James Webb Space Telescope"
                },
                "summary": "A variety of dust attenuation/extinction curves have been observed in\nhigh-redshift galaxies, with mixed results regarding their correlations with\nglobal galaxy properties. These variations are likely driven by factors such as\nintrinsic dust properties, total dust content, and the dust-star geometry. In\nthis work, we explore how the shape of dust attenuation curves-quantified by\nthe UV-optical slope (S) and UV bump strength (B)-correlates with galaxy\nproperties. Our goal is to identify the key physical mechanisms shaping\nattenuation curves through cosmic time. We build on arXiv:2402.05996, analyzing\n173 dusty galaxies at z ~ 2-11.5, with attenuation curves inferred via SED\nfitting of JWST data using a modified version of BAGPIPES (arXiv:2304.11178).\nWe investigate trends between S, B, and properties inferred from SED fitting:\nAV, SFR, stellar mass (M*), specific SFR (sSFR), mass-weighted stellar age\n(a*), ionization parameter (U), and metallicity (Z). For a subset, we also\nconsider oxygen abundance (12 + log(O/H)), derived via Te-based methods. We\nfind that lower AV galaxies tend to have steeper slopes and stronger UV bumps,\nconsistent with radiative transfer predictions involving dust geometry and\ncontent. S also correlates with a* and sSFR, suggesting that strong radiation\nfields in young, bursty galaxies may destroy small grains, flattening the\nslope. B correlates with 12 + log(O/H), possibly due to metallicity-driven dust\ncomposition changes. Overall, attenuation curve shapes appear most strongly\nlinked to: (1) redshift (dust evolution), (2) AV (RT effects), (3) a* or sSFR\n(radiation field), and (4) oxygen abundance (dust composition). Disentangling\nthese effects requires spatially resolved data and theoretical models including\ndust evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A variety of dust attenuation/extinction curves have been observed in\nhigh-redshift galaxies, with mixed results regarding their correlations with\nglobal galaxy properties. These variations are likely driven by factors such as\nintrinsic dust properties, total dust content, and the dust-star geometry. In\nthis work, we explore how the shape of dust attenuation curves-quantified by\nthe UV-optical slope (S) and UV bump strength (B)-correlates with galaxy\nproperties. Our goal is to identify the key physical mechanisms shaping\nattenuation curves through cosmic time. We build on arXiv:2402.05996, analyzing\n173 dusty galaxies at z ~ 2-11.5, with attenuation curves inferred via SED\nfitting of JWST data using a modified version of BAGPIPES (arXiv:2304.11178).\nWe investigate trends between S, B, and properties inferred from SED fitting:\nAV, SFR, stellar mass (M*), specific SFR (sSFR), mass-weighted stellar age\n(a*), ionization parameter (U), and metallicity (Z). For a subset, we also\nconsider oxygen abundance (12 + log(O/H)), derived via Te-based methods. We\nfind that lower AV galaxies tend to have steeper slopes and stronger UV bumps,\nconsistent with radiative transfer predictions involving dust geometry and\ncontent. S also correlates with a* and sSFR, suggesting that strong radiation\nfields in young, bursty galaxies may destroy small grains, flattening the\nslope. B correlates with 12 + log(O/H), possibly due to metallicity-driven dust\ncomposition changes. Overall, attenuation curve shapes appear most strongly\nlinked to: (1) redshift (dust evolution), (2) AV (RT effects), (3) a* or sSFR\n(radiation field), and (4) oxygen abundance (dust composition). Disentangling\nthese effects requires spatially resolved data and theoretical models including\ndust evolution."
                },
                "authors": [
                    {
                        "name": "V. Markov"
                    },
                    {
                        "name": "S. Gallerani"
                    },
                    {
                        "name": "A. Pallottini"
                    },
                    {
                        "name": "M. Bradac"
                    },
                    {
                        "name": "S. Carniani"
                    },
                    {
                        "name": "R. Tripodi"
                    },
                    {
                        "name": "G. Noirot"
                    },
                    {
                        "name": "F. Di Mascia"
                    },
                    {
                        "name": "E. Parlanti"
                    },
                    {
                        "name": "N. Martis"
                    }
                ],
                "author_detail": {
                    "name": "N. Martis"
                },
                "author": "N. Martis",
                "arxiv_doi": "10.1051/0004-6361/202555182",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202555182",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.12378v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12378v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "20 pages (14 in main article), 18 figures (11 in main article),\n  published in A&A",
                "arxiv_journal_ref": "A&A 702, A33 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26074v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26074v2",
                "updated": "2025-10-14T15:45:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    45,
                    25,
                    1,
                    287,
                    0
                ],
                "published": "2025-09-30T10:48:50Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    10,
                    48,
                    50,
                    1,
                    273,
                    0
                ],
                "title": "Limited Preference Data? Learning Better Reward Model with Latent Space\n  Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limited Preference Data? Learning Better Reward Model with Latent Space\n  Synthesis"
                },
                "summary": "Reward modeling, crucial for aligning large language models (LLMs) with human\npreferences, is often bottlenecked by the high cost of preference data.\nExisting textual data synthesis methods are computationally expensive. We\npropose a novel framework LENS for synthesizing preference data directly in the\nLLM's latent embedding space. Our method employs a Variational Autoencoder\n(VAE) to learn a structured latent representation of response embeddings. By\nperforming controlled perturbations in this latent space and decoding back to\nthe embedding space, we efficiently generate diverse, semantically consistent\nsynthetic preference pairs, bypassing costly text generation and annotation. We\nprovide theoretical guarantees that our synthesized pairs approximately\npreserve original preference ordering and improve reward model generalization.\nEmpirically, our latent-space synthesis significantly outperforms text-based\naugmentation on standard benchmarks, achieving superior results while being 18x\nfaster in generation and using a 16,000x smaller model. Our work offers a\nscalable and effective alternative for enhancing reward modeling through\nefficient data augmentation. Code is publicly available at\nhttps://github.com/deeplearning-wisc/lens",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward modeling, crucial for aligning large language models (LLMs) with human\npreferences, is often bottlenecked by the high cost of preference data.\nExisting textual data synthesis methods are computationally expensive. We\npropose a novel framework LENS for synthesizing preference data directly in the\nLLM's latent embedding space. Our method employs a Variational Autoencoder\n(VAE) to learn a structured latent representation of response embeddings. By\nperforming controlled perturbations in this latent space and decoding back to\nthe embedding space, we efficiently generate diverse, semantically consistent\nsynthetic preference pairs, bypassing costly text generation and annotation. We\nprovide theoretical guarantees that our synthesized pairs approximately\npreserve original preference ordering and improve reward model generalization.\nEmpirically, our latent-space synthesis significantly outperforms text-based\naugmentation on standard benchmarks, achieving superior results while being 18x\nfaster in generation and using a 16,000x smaller model. Our work offers a\nscalable and effective alternative for enhancing reward modeling through\nefficient data augmentation. Code is publicly available at\nhttps://github.com/deeplearning-wisc/lens"
                },
                "authors": [
                    {
                        "name": "Leitian Tao"
                    },
                    {
                        "name": "Xuefeng Du"
                    },
                    {
                        "name": "Sharon Li"
                    }
                ],
                "author_detail": {
                    "name": "Sharon Li"
                },
                "author": "Sharon Li",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26074v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26074v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12650v1",
                "updated": "2025-10-14T15:44:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    44,
                    54,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T15:44:54Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    44,
                    54,
                    1,
                    287,
                    0
                ],
                "title": "Towards Foundation Inference Models that Learn ODEs In-Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Foundation Inference Models that Learn ODEs In-Context"
                },
                "summary": "Ordinary differential equations (ODEs) describe dynamical systems evolving\ndeterministically in continuous time. Accurate data-driven modeling of systems\nas ODEs, a central problem across the natural sciences, remains challenging,\nespecially if the data is sparse or noisy. We introduce FIM-ODE (Foundation\nInference Model for ODEs), a pretrained neural model designed to estimate ODEs\nzero-shot (i.e., in context) from sparse and noisy observations. Trained on\nsynthetic data, the model utilizes a flexible neural operator for robust ODE\ninference, even from corrupted data. We empirically verify that FIM-ODE\nprovides accurate estimates, on par with a neural state-of-the-art method, and\nqualitatively compare the structure of their estimated vector fields.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ordinary differential equations (ODEs) describe dynamical systems evolving\ndeterministically in continuous time. Accurate data-driven modeling of systems\nas ODEs, a central problem across the natural sciences, remains challenging,\nespecially if the data is sparse or noisy. We introduce FIM-ODE (Foundation\nInference Model for ODEs), a pretrained neural model designed to estimate ODEs\nzero-shot (i.e., in context) from sparse and noisy observations. Trained on\nsynthetic data, the model utilizes a flexible neural operator for robust ODE\ninference, even from corrupted data. We empirically verify that FIM-ODE\nprovides accurate estimates, on par with a neural state-of-the-art method, and\nqualitatively compare the structure of their estimated vector fields."
                },
                "authors": [
                    {
                        "name": "Maximilian Mauel"
                    },
                    {
                        "name": "Manuel Hinz"
                    },
                    {
                        "name": "Patrick Seifner"
                    },
                    {
                        "name": "David Berghaus"
                    },
                    {
                        "name": "Ramses J. Sanchez"
                    }
                ],
                "author_detail": {
                    "name": "Ramses J. Sanchez"
                },
                "author": "Ramses J. Sanchez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00213v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00213v2",
                "updated": "2025-10-14T15:44:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    44,
                    11,
                    1,
                    287,
                    0
                ],
                "published": "2025-04-30T23:14:32Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    23,
                    14,
                    32,
                    2,
                    120,
                    0
                ],
                "title": "PSN Game: Game-theoretic Prediction and Planning via a Player Selection\n  Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PSN Game: Game-theoretic Prediction and Planning via a Player Selection\n  Network"
                },
                "summary": "While game-theoretic planning frameworks are effective at modeling\nmulti-agent interactions, they require solving large optimization problems\nwhere the number of variables increases with the number of agents, resulting in\nlong computation times that limit their use in large-scale, real-time systems.\nTo address this issue, we propose 1) PSN Game: a learning-based, game-theoretic\nprediction and planning framework that reduces runtime by learning a Player\nSelection Network (PSN); and 2) a Goal Inference Network (GIN) that makes it\npossible to use the PSN in incomplete information games where agents'\nintentions are unknown. A PSN outputs a player selection mask that\ndistinguishes influential players from less relevant ones, enabling the ego\nplayer to solve a smaller, masked game involving only selected players. By\nreducing the number of players in the game, and therefore reducing the number\nof variables in the corresponding optimization problem, PSN directly lowers\ncomputation time. The PSN Game framework is more flexible than existing player\nselection methods as it 1) relies solely on observations of players' past\ntrajectories, without requiring full state, action, or other game-specific\ninformation; and 2) requires no online parameter tuning. Experiments in both\nsimulated scenarios and human trajectory datasets demonstrate that PSNs\noutperform baseline selection methods in 1) prediction accuracy; and 2)\nplanning safety. PSNs also generalize effectively to real-world scenarios in\nwhich agents' objectives are unknown without fine-tuning. By selecting only the\nmost relevant players for decision-making, PSN Game offers a general mechanism\nfor reducing planning complexity that can be seamlessly integrated into\nexisting multi-agent planning frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While game-theoretic planning frameworks are effective at modeling\nmulti-agent interactions, they require solving large optimization problems\nwhere the number of variables increases with the number of agents, resulting in\nlong computation times that limit their use in large-scale, real-time systems.\nTo address this issue, we propose 1) PSN Game: a learning-based, game-theoretic\nprediction and planning framework that reduces runtime by learning a Player\nSelection Network (PSN); and 2) a Goal Inference Network (GIN) that makes it\npossible to use the PSN in incomplete information games where agents'\nintentions are unknown. A PSN outputs a player selection mask that\ndistinguishes influential players from less relevant ones, enabling the ego\nplayer to solve a smaller, masked game involving only selected players. By\nreducing the number of players in the game, and therefore reducing the number\nof variables in the corresponding optimization problem, PSN directly lowers\ncomputation time. The PSN Game framework is more flexible than existing player\nselection methods as it 1) relies solely on observations of players' past\ntrajectories, without requiring full state, action, or other game-specific\ninformation; and 2) requires no online parameter tuning. Experiments in both\nsimulated scenarios and human trajectory datasets demonstrate that PSNs\noutperform baseline selection methods in 1) prediction accuracy; and 2)\nplanning safety. PSNs also generalize effectively to real-world scenarios in\nwhich agents' objectives are unknown without fine-tuning. By selecting only the\nmost relevant players for decision-making, PSN Game offers a general mechanism\nfor reducing planning complexity that can be seamlessly integrated into\nexisting multi-agent planning frameworks."
                },
                "authors": [
                    {
                        "name": "Tianyu Qiu"
                    },
                    {
                        "name": "Eric Ouano"
                    },
                    {
                        "name": "Fernando Palafox"
                    },
                    {
                        "name": "Christian Ellis"
                    },
                    {
                        "name": "David Fridovich-Keil"
                    }
                ],
                "author_detail": {
                    "name": "David Fridovich-Keil"
                },
                "author": "David Fridovich-Keil",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00213v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00213v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00299v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00299v5",
                "updated": "2025-10-14T15:42:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    42,
                    41,
                    1,
                    287,
                    0
                ],
                "published": "2025-02-01T03:49:47Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem. The code is available at\n\\href{https://github.com/NVIDIA/kvpress}{link}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem. The code is available at\n\\href{https://github.com/NVIDIA/kvpress}{link}."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00299v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00299v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23441v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23441v2",
                "updated": "2025-10-14T15:38:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    38,
                    48,
                    1,
                    287,
                    0
                ],
                "published": "2025-09-27T18:16:57Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    18,
                    16,
                    57,
                    5,
                    270,
                    0
                ],
                "title": "Cognition-of-Thought Elicits Social-Aligned Reasoning in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognition-of-Thought Elicits Social-Aligned Reasoning in Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) excel at complex reasoning but can still exhibit\nharmful behaviors. Current alignment strategies typically embed safety into\nmodel weights, making these controls implicit, static, and difficult to modify.\nThis paper introduces Cognition-of-Thought (CooT), a novel decoding-time\nframework that equips LLMs with an explicit cognitive self-monitoring loop.\nCooT couples a standard text Generator with a cognitive Perceiver that\ncontinuously monitors the unfolding sequence. The Perceiver uses a structured,\nprecedence-based hierarchy of principles (e.g., safety over obedience) to\ndetect potential misalignments as they arise. When violations are flagged, CooT\nintervenes by rolling back the generation to the point of error and\nregenerating under injected guidance that combines universal social priors with\ncontext-specific warnings. CooT thus transforms alignment from a fixed property\ninto an explicit, dynamic, and auditable process active during inference,\nallowing for flexible policy updates without retraining the model. Extensive\nexperiments across multiple benchmarks and model families confirm that CooT\nconsistently improves safety and social reasoning performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at complex reasoning but can still exhibit\nharmful behaviors. Current alignment strategies typically embed safety into\nmodel weights, making these controls implicit, static, and difficult to modify.\nThis paper introduces Cognition-of-Thought (CooT), a novel decoding-time\nframework that equips LLMs with an explicit cognitive self-monitoring loop.\nCooT couples a standard text Generator with a cognitive Perceiver that\ncontinuously monitors the unfolding sequence. The Perceiver uses a structured,\nprecedence-based hierarchy of principles (e.g., safety over obedience) to\ndetect potential misalignments as they arise. When violations are flagged, CooT\nintervenes by rolling back the generation to the point of error and\nregenerating under injected guidance that combines universal social priors with\ncontext-specific warnings. CooT thus transforms alignment from a fixed property\ninto an explicit, dynamic, and auditable process active during inference,\nallowing for flexible policy updates without retraining the model. Extensive\nexperiments across multiple benchmarks and model families confirm that CooT\nconsistently improves safety and social reasoning performance."
                },
                "authors": [
                    {
                        "name": "Xuanming Zhang"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Samuel Yeh"
                    },
                    {
                        "name": "Sharon Li"
                    }
                ],
                "author_detail": {
                    "name": "Sharon Li"
                },
                "author": "Sharon Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23441v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23441v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12643v1",
                "updated": "2025-10-14T15:34:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    34,
                    38,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T15:34:38Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    34,
                    38,
                    1,
                    287,
                    0
                ],
                "title": "Reasoning Pattern Matters: Learning to Reason without Human Rationales",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Pattern Matters: Learning to Reason without Human Rationales"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable reasoning\ncapabilities under the widely adopted SFT+RLVR paradigm, which first performs\nSupervised Fine-Tuning (SFT) on human-annotated reasoning trajectories\n(rationales) to establish initial reasoning behaviors, then applies\nReinforcement Learning with Verifiable Rewards (RLVR) to optimize the model\nusing verifiable signals without golden rationales. However, annotating\nhigh-quality rationales for the SFT stage remains prohibitively expensive. This\npaper investigates when and how rationale annotation costs can be substantially\nreduced without compromising reasoning performance. We identify a broad class\nof problems, termed patterned reasoning tasks, where reasoning follows a fixed,\nprocedural strategy consistent across instances. Although instances vary in\ncontent such as domain knowledge, factual information, or numeric values, the\nsolution derives from applying a shared reasoning pattern. We argue that the\nsuccess of SFT+RLVR on such tasks primarily stems from its ability to enable\nmodels to internalize these reasoning patterns. Using numerical semantic\nmatching as a representative task, we provide both causal and behavioral\nevidence showing that reasoning patterns rather than the quantity or quality of\nrationales are the key determinant of performance. Building on these insights,\nwe propose Pattern-Aware LLMs as Rationale AnnOtators (PARO), a simple yet\neffective framework that enables LLMs to generate rationales aligned with\ntask-specific reasoning patterns without requiring human rationale annotations.\nExperiments show that PARO-generated rationales achieve comparable SFT+RLVR\nperformance to human rationales that are 10 times larger. These results suggest\nthat large-scale human rationale annotations can be replaced with LLM-based\nautomatic annotations requiring only limited human supervision over reasoning\npatterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable reasoning\ncapabilities under the widely adopted SFT+RLVR paradigm, which first performs\nSupervised Fine-Tuning (SFT) on human-annotated reasoning trajectories\n(rationales) to establish initial reasoning behaviors, then applies\nReinforcement Learning with Verifiable Rewards (RLVR) to optimize the model\nusing verifiable signals without golden rationales. However, annotating\nhigh-quality rationales for the SFT stage remains prohibitively expensive. This\npaper investigates when and how rationale annotation costs can be substantially\nreduced without compromising reasoning performance. We identify a broad class\nof problems, termed patterned reasoning tasks, where reasoning follows a fixed,\nprocedural strategy consistent across instances. Although instances vary in\ncontent such as domain knowledge, factual information, or numeric values, the\nsolution derives from applying a shared reasoning pattern. We argue that the\nsuccess of SFT+RLVR on such tasks primarily stems from its ability to enable\nmodels to internalize these reasoning patterns. Using numerical semantic\nmatching as a representative task, we provide both causal and behavioral\nevidence showing that reasoning patterns rather than the quantity or quality of\nrationales are the key determinant of performance. Building on these insights,\nwe propose Pattern-Aware LLMs as Rationale AnnOtators (PARO), a simple yet\neffective framework that enables LLMs to generate rationales aligned with\ntask-specific reasoning patterns without requiring human rationale annotations.\nExperiments show that PARO-generated rationales achieve comparable SFT+RLVR\nperformance to human rationales that are 10 times larger. These results suggest\nthat large-scale human rationale annotations can be replaced with LLM-based\nautomatic annotations requiring only limited human supervision over reasoning\npatterns."
                },
                "authors": [
                    {
                        "name": "Chaoxu Pang"
                    },
                    {
                        "name": "Yixuan Cao"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "arxiv_comment": "Submitted to Frontiers of Computer Science",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02392v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02392v2",
                "updated": "2025-10-14T15:32:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    32,
                    32,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-01T00:15:25Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    0,
                    15,
                    25,
                    2,
                    274,
                    0
                ],
                "title": "KnowledgeSmith: Uncovering Knowledge Updating in LLMs with Model Editing\n  and Unlearning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KnowledgeSmith: Uncovering Knowledge Updating in LLMs with Model Editing\n  and Unlearning"
                },
                "summary": "Knowledge editing and machine unlearning are two popular approaches for large\nlanguage models (LLMs) to stay up-to-date. However, the knowledge updating\nmechanism of LLMs remains largely unexplored due to insufficient, isolated, and\nsmall-scale evaluation. For instance, are LLMs similar to humans in modifying\ncertain knowledge? What differs editing and unlearning as training data\nincreases? This paper proposes KnowledgeSmith, a unified framework to\nsystematically understand the updating mechanism of LLMs. We first cast editing\nand unlearning as instances of one constrained optimization problem. Then, we\npropose an automatic dataset generator that provides structured interventions\nacross multiple graph levels and data scales, enabling controlled studies of\nhow different modification strategies propagate through model knowledge.\nExtensive experiments demonstrate nuanced insights over knowledge propagation,\nplasticity scaling, consistency, and robustness. For instance, our results show\nthat LLMs do not exhibit similar updating as humans for different levels of\nknowledge, and there exists consistency-capacity trade-off. We hope our\nfindings can offer suggestions to the design of more reliable and scalable\nstrategies. Code: https://github.com/AIFrontierLab/KnowledgeSmith.git",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge editing and machine unlearning are two popular approaches for large\nlanguage models (LLMs) to stay up-to-date. However, the knowledge updating\nmechanism of LLMs remains largely unexplored due to insufficient, isolated, and\nsmall-scale evaluation. For instance, are LLMs similar to humans in modifying\ncertain knowledge? What differs editing and unlearning as training data\nincreases? This paper proposes KnowledgeSmith, a unified framework to\nsystematically understand the updating mechanism of LLMs. We first cast editing\nand unlearning as instances of one constrained optimization problem. Then, we\npropose an automatic dataset generator that provides structured interventions\nacross multiple graph levels and data scales, enabling controlled studies of\nhow different modification strategies propagate through model knowledge.\nExtensive experiments demonstrate nuanced insights over knowledge propagation,\nplasticity scaling, consistency, and robustness. For instance, our results show\nthat LLMs do not exhibit similar updating as humans for different levels of\nknowledge, and there exists consistency-capacity trade-off. We hope our\nfindings can offer suggestions to the design of more reliable and scalable\nstrategies. Code: https://github.com/AIFrontierLab/KnowledgeSmith.git"
                },
                "authors": [
                    {
                        "name": "Yinyi Luo"
                    },
                    {
                        "name": "Zhexian Zhou"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Kai Qiu"
                    },
                    {
                        "name": "Marios Savvides"
                    },
                    {
                        "name": "Sharon Li"
                    },
                    {
                        "name": "Jindong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jindong Wang"
                },
                "author": "Jindong Wang",
                "arxiv_comment": "Technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02392v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02392v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22926v2",
                "updated": "2025-10-14T15:32:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    32,
                    0,
                    1,
                    287,
                    0
                ],
                "published": "2025-09-26T20:51:48Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    20,
                    51,
                    48,
                    4,
                    269,
                    0
                ],
                "title": "Large language models management of medications: three performance\n  analyses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models management of medications: three performance\n  analyses"
                },
                "summary": "Purpose: Large language models (LLMs) have proven performance for certain\ndiagnostic tasks, however limited studies have evaluated their consistency in\nrecommending appropriate medication regimens for a given diagnosis. Medication\nmanagement is a complex task that requires synthesis of drug formulation and\ncomplete order instructions for safe use. Here, the performance of GPT 4o, an\nLLM available with ChatGPT, was tested for three medication management tasks.\nMethods: GPT-4o performance was tested using three medication tasks:\nidentifying available formulations for a given generic drug name, identifying\ndrug-drug interactions (DDI) for a given medication regimen, and preparing a\nmedication order for a given generic drug name. For each experiment, the models\nraw text response was captured exactly as returned and evaluated using\nclinician evaluation in addition to standard LLM metrics, including Term\nFrequency-Inverse Document Frequency (TF IDF) vectors, normalized Levenshtein\nsimilarity, and Recall-Oriented Understudy for Gisting Evaluation (ROUGE\n1/ROUGE L F1) between each response and its reference string. Results: For the\nfirst task of drug-formulation matching, GPT-4o had 49% accuracy for generic\nmedications being matched to all available formulations, with an average of\n1.23 omissions per medication and 1.14 hallucinations per medication. For the\nsecond task of drug-drug interaction identification, the accuracy was 54.7% for\nidentifying the DDI pair. For the third task, GPT-4o generated order sentences\ncontaining no medication or abbreviation errors in 65.8% of cases. Conclusions:\nModel performance for basic medication tasks was consistently poor. This\nevaluation highlights the need for domain-specific training through\nclinician-annotated datasets and a comprehensive evaluation framework for\nbenchmarking performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose: Large language models (LLMs) have proven performance for certain\ndiagnostic tasks, however limited studies have evaluated their consistency in\nrecommending appropriate medication regimens for a given diagnosis. Medication\nmanagement is a complex task that requires synthesis of drug formulation and\ncomplete order instructions for safe use. Here, the performance of GPT 4o, an\nLLM available with ChatGPT, was tested for three medication management tasks.\nMethods: GPT-4o performance was tested using three medication tasks:\nidentifying available formulations for a given generic drug name, identifying\ndrug-drug interactions (DDI) for a given medication regimen, and preparing a\nmedication order for a given generic drug name. For each experiment, the models\nraw text response was captured exactly as returned and evaluated using\nclinician evaluation in addition to standard LLM metrics, including Term\nFrequency-Inverse Document Frequency (TF IDF) vectors, normalized Levenshtein\nsimilarity, and Recall-Oriented Understudy for Gisting Evaluation (ROUGE\n1/ROUGE L F1) between each response and its reference string. Results: For the\nfirst task of drug-formulation matching, GPT-4o had 49% accuracy for generic\nmedications being matched to all available formulations, with an average of\n1.23 omissions per medication and 1.14 hallucinations per medication. For the\nsecond task of drug-drug interaction identification, the accuracy was 54.7% for\nidentifying the DDI pair. For the third task, GPT-4o generated order sentences\ncontaining no medication or abbreviation errors in 65.8% of cases. Conclusions:\nModel performance for basic medication tasks was consistently poor. This\nevaluation highlights the need for domain-specific training through\nclinician-annotated datasets and a comprehensive evaluation framework for\nbenchmarking performance."
                },
                "authors": [
                    {
                        "name": "Kelli Henry"
                    },
                    {
                        "name": "Steven Xu"
                    },
                    {
                        "name": "Kaitlin Blotske"
                    },
                    {
                        "name": "Moriah Cargile"
                    },
                    {
                        "name": "Erin F. Barreto"
                    },
                    {
                        "name": "Brian Murray"
                    },
                    {
                        "name": "Susan Smith"
                    },
                    {
                        "name": "Seth R. Bauer"
                    },
                    {
                        "name": "Xingmeng Zhao"
                    },
                    {
                        "name": "Adeleine Tilley"
                    },
                    {
                        "name": "Yanjun Gao"
                    },
                    {
                        "name": "Tianming Liu"
                    },
                    {
                        "name": "Sunghwan Sohn"
                    },
                    {
                        "name": "Andrea Sikora"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Sikora"
                },
                "author": "Andrea Sikora",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12637v1",
                "updated": "2025-10-14T15:31:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    31,
                    21,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T15:31:21Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    31,
                    21,
                    1,
                    287,
                    0
                ],
                "title": "COSTAR-A: A prompting framework for enhancing Large Language Model\n  performance on Point-of-View questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COSTAR-A: A prompting framework for enhancing Large Language Model\n  performance on Point-of-View questions"
                },
                "summary": "Large Language Models (LLMs) are highly sensitive to prompt design, and\nmaking optimized prompting techniques is crucial for generating consistent,\nhigh-quality outputs. In this study, we introduce COSTAR-A, a novel prompt\nengineering framework that enhances the existing COSTAR method, which stands\nfor Context, Objective, Style, Tone, Audience, and Response, by adding the\n'Answer' component at the end. We demonstrate that while the original COSTAR\nframework improves prompt clarity and aligns outputs for larger LLMs, its\nperformance is less consistent with smaller, locally optimized models,\nparticularly in tasks that require more directive or constrained outputs.\nThrough a series of controlled prompt-output assessments with smaller (at most\n8 billion parameters), fine-tuned models, we found that COSTAR-A can enhance\nthe output structure and decisiveness of localized LLMs for certain tasks,\nalthough its effectiveness varies across models and use cases. Notably, the\nLlama 3.1-8B model exhibited performance improvements when prompted with\nCOSTAR-A compared to COSTAR alone. These findings emphasize the adaptability\nand scalability of COSTAR-A as a prompting framework, particularly in\ncomputationally efficient AI deployments on resource-constrained hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are highly sensitive to prompt design, and\nmaking optimized prompting techniques is crucial for generating consistent,\nhigh-quality outputs. In this study, we introduce COSTAR-A, a novel prompt\nengineering framework that enhances the existing COSTAR method, which stands\nfor Context, Objective, Style, Tone, Audience, and Response, by adding the\n'Answer' component at the end. We demonstrate that while the original COSTAR\nframework improves prompt clarity and aligns outputs for larger LLMs, its\nperformance is less consistent with smaller, locally optimized models,\nparticularly in tasks that require more directive or constrained outputs.\nThrough a series of controlled prompt-output assessments with smaller (at most\n8 billion parameters), fine-tuned models, we found that COSTAR-A can enhance\nthe output structure and decisiveness of localized LLMs for certain tasks,\nalthough its effectiveness varies across models and use cases. Notably, the\nLlama 3.1-8B model exhibited performance improvements when prompted with\nCOSTAR-A compared to COSTAR alone. These findings emphasize the adaptability\nand scalability of COSTAR-A as a prompting framework, particularly in\ncomputationally efficient AI deployments on resource-constrained hardware."
                },
                "authors": [
                    {
                        "name": "Nzubechukwu C. Ohalete"
                    },
                    {
                        "name": "Kevin B. Gittner"
                    },
                    {
                        "name": "Lauren M. Matheny"
                    }
                ],
                "author_detail": {
                    "name": "Lauren M. Matheny"
                },
                "arxiv_affiliation": "School of Data Science and Analytics, Kennesaw State University, GA, USA",
                "author": "Lauren M. Matheny",
                "arxiv_comment": "20 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03999v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03999v2",
                "updated": "2025-10-14T15:30:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    30,
                    52,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-05T02:18:23Z",
                "published_parsed": [
                    2025,
                    10,
                    5,
                    2,
                    18,
                    23,
                    6,
                    278,
                    0
                ],
                "title": "Simulating and Understanding Deceptive Behaviors in Long-Horizon\n  Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating and Understanding Deceptive Behaviors in Long-Horizon\n  Interactions"
                },
                "summary": "Deception is a pervasive feature of human communication and an emerging\nconcern in large language models (LLMs). While recent studies document\ninstances of LLM deception under pressure, most evaluations remain confined to\nsingle-turn prompts and fail to capture the long-horizon interactions in which\ndeceptive strategies typically unfold. We introduce the first simulation\nframework for probing and evaluating deception in LLMs under extended sequences\nof interdependent tasks and dynamic contextual pressures. Our framework\ninstantiates a multi-agent system: a performer agent tasked with completing\ntasks and a supervisor agent that evaluates progress, provides feedback, and\nmaintains evolving states of trust. An independent deception auditor then\nreviews full trajectories to identify when and how deception occurs. We conduct\nextensive experiments across 11 frontier models, spanning both closed- and\nopen-source systems, and find that deception is model-dependent, increases with\nevent pressure, and consistently erodes supervisor trust. Qualitative analyses\nfurther reveal distinct strategies of concealment, equivocation, and\nfalsification. Our findings establish deception as an emergent risk in\nlong-horizon interactions and provide a foundation for evaluating future LLMs\nin real-world, trust-sensitive contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deception is a pervasive feature of human communication and an emerging\nconcern in large language models (LLMs). While recent studies document\ninstances of LLM deception under pressure, most evaluations remain confined to\nsingle-turn prompts and fail to capture the long-horizon interactions in which\ndeceptive strategies typically unfold. We introduce the first simulation\nframework for probing and evaluating deception in LLMs under extended sequences\nof interdependent tasks and dynamic contextual pressures. Our framework\ninstantiates a multi-agent system: a performer agent tasked with completing\ntasks and a supervisor agent that evaluates progress, provides feedback, and\nmaintains evolving states of trust. An independent deception auditor then\nreviews full trajectories to identify when and how deception occurs. We conduct\nextensive experiments across 11 frontier models, spanning both closed- and\nopen-source systems, and find that deception is model-dependent, increases with\nevent pressure, and consistently erodes supervisor trust. Qualitative analyses\nfurther reveal distinct strategies of concealment, equivocation, and\nfalsification. Our findings establish deception as an emergent risk in\nlong-horizon interactions and provide a foundation for evaluating future LLMs\nin real-world, trust-sensitive contexts."
                },
                "authors": [
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Xuanming Zhang"
                    },
                    {
                        "name": "Samuel Yeh"
                    },
                    {
                        "name": "Jwala Dhamala"
                    },
                    {
                        "name": "Ousmane Dia"
                    },
                    {
                        "name": "Rahul Gupta"
                    },
                    {
                        "name": "Sharon Li"
                    }
                ],
                "author_detail": {
                    "name": "Sharon Li"
                },
                "author": "Sharon Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03999v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03999v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18943v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18943v3",
                "updated": "2025-10-14T15:30:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    30,
                    4,
                    1,
                    287,
                    0
                ],
                "published": "2025-05-25T02:32:57Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    2,
                    32,
                    57,
                    6,
                    145,
                    0
                ],
                "title": "MetaMind: Modeling Human Social Thoughts with Metacognitive Multi-Agent\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaMind: Modeling Human Social Thoughts with Metacognitive Multi-Agent\n  Systems"
                },
                "summary": "Human social interactions depend on the ability to infer others' unspoken\nintentions, emotions, and beliefs-a cognitive skill grounded in the\npsychological concept of Theory of Mind (ToM). While large language models\n(LLMs) excel in semantic understanding tasks, they struggle with the ambiguity\nand contextual nuance inherent in human communication. To bridge this gap, we\nintroduce MetaMind, a multi-agent framework inspired by psychological theories\nof metacognition, designed to emulate human-like social reasoning. MetaMind\ndecomposes social understanding into three collaborative stages: (1) a\nTheory-of-Mind Agent generates hypotheses about user mental states (e.g.,\nintent, emotion), (2) a Moral Agent refines these hypotheses using cultural\nnorms and ethical constraints, and (3) a Response Agent generates contextually\nappropriate responses while validating alignment with inferred intent. Our\nframework achieves state-of-the-art performance across three challenging\nbenchmarks, with 35.7% improvement in real-world social scenarios and 6.2% gain\nin ToM reasoning. Notably, it enables LLMs to match human-level performance on\nkey ToM tasks for the first time. Ablation studies confirm the necessity of all\ncomponents, which showcase the framework's ability to balance contextual\nplausibility, social appropriateness, and user adaptation. This work advances\nAI systems toward human-like social intelligence, with applications in\nempathetic dialogue and culturally sensitive interactions. Code is available at\nhttps://github.com/XMZhangAI/MetaMind.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human social interactions depend on the ability to infer others' unspoken\nintentions, emotions, and beliefs-a cognitive skill grounded in the\npsychological concept of Theory of Mind (ToM). While large language models\n(LLMs) excel in semantic understanding tasks, they struggle with the ambiguity\nand contextual nuance inherent in human communication. To bridge this gap, we\nintroduce MetaMind, a multi-agent framework inspired by psychological theories\nof metacognition, designed to emulate human-like social reasoning. MetaMind\ndecomposes social understanding into three collaborative stages: (1) a\nTheory-of-Mind Agent generates hypotheses about user mental states (e.g.,\nintent, emotion), (2) a Moral Agent refines these hypotheses using cultural\nnorms and ethical constraints, and (3) a Response Agent generates contextually\nappropriate responses while validating alignment with inferred intent. Our\nframework achieves state-of-the-art performance across three challenging\nbenchmarks, with 35.7% improvement in real-world social scenarios and 6.2% gain\nin ToM reasoning. Notably, it enables LLMs to match human-level performance on\nkey ToM tasks for the first time. Ablation studies confirm the necessity of all\ncomponents, which showcase the framework's ability to balance contextual\nplausibility, social appropriateness, and user adaptation. This work advances\nAI systems toward human-like social intelligence, with applications in\nempathetic dialogue and culturally sensitive interactions. Code is available at\nhttps://github.com/XMZhangAI/MetaMind."
                },
                "authors": [
                    {
                        "name": "Xuanming Zhang"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Samuel Yeh"
                    },
                    {
                        "name": "Sharon Li"
                    }
                ],
                "author_detail": {
                    "name": "Sharon Li"
                },
                "author": "Sharon Li",
                "arxiv_comment": "NeurIPS 2025 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18943v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18943v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12635v1",
                "updated": "2025-10-14T15:29:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    29,
                    57,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T15:29:57Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    29,
                    57,
                    1,
                    287,
                    0
                ],
                "title": "Memory as Action: Autonomous Context Curation for Long-Horizon Agentic\n  Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory as Action: Autonomous Context Curation for Long-Horizon Agentic\n  Tasks"
                },
                "summary": "Large Language Models face challenges in long-horizon agentic tasks as their\nconstrained memory is easily overwhelmed by distracting or irrelevant context.\nExisting working memory methods typically rely on external, heuristic\nmechanisms that are decoupled from the agent's core policy. In this work, we\nreframe working memory management as a learnable, intrinsic capability. We\npropose a novel framework, Memory-as-Action, where an agent actively manages\nits working memory by executing explicit editing operations as part of a\nunified policy. This formulation allows an agent, trained via reinforcement\nlearning, to balance memory curation against long-term task objectives under\ngiven resource constraints. However, such memory editing actions break the\nstandard assumption of a continuously growing prefix in LLM interactions,\nleading to what we call trajectory fractures. These non-prefix changes disrupt\nthe causal continuity required by standard policy gradient methods, making\nthose methods inapplicable. To address this, we propose a new algorithm,\nDynamic Context Policy Optimization, which enables stable end-to-end\nreinforcement learning by segmenting trajectories at memory action points and\napplying trajectory-level advantages to the resulting action segments. Our\nresults demonstrate that jointly optimizing for task reasoning and memory\nmanagement in an end-to-end fashion not only reduces overall computational\nconsumption but also improves task performance, driven by adaptive context\ncuration strategies tailored to the model's intrinsic capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models face challenges in long-horizon agentic tasks as their\nconstrained memory is easily overwhelmed by distracting or irrelevant context.\nExisting working memory methods typically rely on external, heuristic\nmechanisms that are decoupled from the agent's core policy. In this work, we\nreframe working memory management as a learnable, intrinsic capability. We\npropose a novel framework, Memory-as-Action, where an agent actively manages\nits working memory by executing explicit editing operations as part of a\nunified policy. This formulation allows an agent, trained via reinforcement\nlearning, to balance memory curation against long-term task objectives under\ngiven resource constraints. However, such memory editing actions break the\nstandard assumption of a continuously growing prefix in LLM interactions,\nleading to what we call trajectory fractures. These non-prefix changes disrupt\nthe causal continuity required by standard policy gradient methods, making\nthose methods inapplicable. To address this, we propose a new algorithm,\nDynamic Context Policy Optimization, which enables stable end-to-end\nreinforcement learning by segmenting trajectories at memory action points and\napplying trajectory-level advantages to the resulting action segments. Our\nresults demonstrate that jointly optimizing for task reasoning and memory\nmanagement in an end-to-end fashion not only reduces overall computational\nconsumption but also improves task performance, driven by adaptive context\ncuration strategies tailored to the model's intrinsic capabilities."
                },
                "authors": [
                    {
                        "name": "Yuxiang Zhang"
                    },
                    {
                        "name": "Jiangming Shu"
                    },
                    {
                        "name": "Ye Ma"
                    },
                    {
                        "name": "Xueyuan Lin"
                    },
                    {
                        "name": "Shangxi Wu"
                    },
                    {
                        "name": "Jitao Sang"
                    }
                ],
                "author_detail": {
                    "name": "Jitao Sang"
                },
                "author": "Jitao Sang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12633v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12633v1",
                "updated": "2025-10-14T15:29:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    29,
                    14,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T15:29:14Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    29,
                    14,
                    1,
                    287,
                    0
                ],
                "title": "Laminar: A Scalable Asynchronous RL Post-Training Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Laminar: A Scalable Asynchronous RL Post-Training Framework"
                },
                "summary": "Reinforcement learning (RL) post-training for Large Language Models (LLMs) is\nnow scaling to large clusters and running for extended durations to enhance\nmodel reasoning performance. However, the scalability of existing RL frameworks\nis limited, as extreme long-tail skewness in RL trajectory generation causes\nsevere GPU underutilization. Current asynchronous RL systems attempt to\nmitigate this, but they rely on global weight synchronization between the actor\nand all rollouts, which creates a rigid model update schedule. This global\nsynchronization is ill-suited for the highly skewed and evolving distribution\nof trajectory generation latency in RL training, crippling training efficiency.\nOur key insight is that efficient scaling requires breaking this lockstep\nthrough trajectory-level asynchrony, which generates and consumes each\ntrajectory independently. We propose Laminar, a scalable and robust RL\npost-training system built on a fully decoupled architecture. First, we replace\nglobal updates with a tier of relay workers acting as a distributed parameter\nservice. This enables asynchronous and fine-grained weight synchronization,\nallowing rollouts to pull the latest weight anytime without stalling the\nactor's training loop. Second, a dynamic repack mechanism consolidates\nlong-tail trajectories onto a few dedicated rollouts, maximizing generation\nthroughput. The fully decoupled design also isolates failures, ensuring\nrobustness for long-running jobs. Our evaluation on a 1024-GPU cluster shows\nthat Laminar achieves up to 5.48$\\times$ training throughput speedup over\nstate-of-the-art systems, while reducing model convergence time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) post-training for Large Language Models (LLMs) is\nnow scaling to large clusters and running for extended durations to enhance\nmodel reasoning performance. However, the scalability of existing RL frameworks\nis limited, as extreme long-tail skewness in RL trajectory generation causes\nsevere GPU underutilization. Current asynchronous RL systems attempt to\nmitigate this, but they rely on global weight synchronization between the actor\nand all rollouts, which creates a rigid model update schedule. This global\nsynchronization is ill-suited for the highly skewed and evolving distribution\nof trajectory generation latency in RL training, crippling training efficiency.\nOur key insight is that efficient scaling requires breaking this lockstep\nthrough trajectory-level asynchrony, which generates and consumes each\ntrajectory independently. We propose Laminar, a scalable and robust RL\npost-training system built on a fully decoupled architecture. First, we replace\nglobal updates with a tier of relay workers acting as a distributed parameter\nservice. This enables asynchronous and fine-grained weight synchronization,\nallowing rollouts to pull the latest weight anytime without stalling the\nactor's training loop. Second, a dynamic repack mechanism consolidates\nlong-tail trajectories onto a few dedicated rollouts, maximizing generation\nthroughput. The fully decoupled design also isolates failures, ensuring\nrobustness for long-running jobs. Our evaluation on a 1024-GPU cluster shows\nthat Laminar achieves up to 5.48$\\times$ training throughput speedup over\nstate-of-the-art systems, while reducing model convergence time."
                },
                "authors": [
                    {
                        "name": "Guangming Sheng"
                    },
                    {
                        "name": "Yuxuan Tong"
                    },
                    {
                        "name": "Borui Wan"
                    },
                    {
                        "name": "Wang Zhang"
                    },
                    {
                        "name": "Chaobo Jia"
                    },
                    {
                        "name": "Xibin Wu"
                    },
                    {
                        "name": "Yuqi Wu"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Yanghua Peng"
                    },
                    {
                        "name": "Haibin Lin"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12633v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12633v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12627v1",
                "updated": "2025-10-14T15:24:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    24,
                    50,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T15:24:50Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    24,
                    50,
                    1,
                    287,
                    0
                ],
                "title": "Alleviating the $H_0$ tension through new interacting dark energy model\n  in light of DESI DR2",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alleviating the $H_0$ tension through new interacting dark energy model\n  in light of DESI DR2"
                },
                "summary": "The $H_0$ tension has become one of the most significant challenges in modern\ncosmology. The recent DESI DR2 data has shown a significant preference for\ndynamical dark energy, yet this has further exacerbated the $H_0$ tension. In\nthis work, we explore the potential of new interacting dark energy models\n($\\widetilde{\\Lambda}$CDM and $e\\widetilde{\\Lambda}$CDM) to alleviate the $H_0$\ntension. We perform observational constraints using the latest baryon acoustic\noscillation data from DESI DR2, cosmic microwave background (CMB) data from\nPlanck and Atacama Cosmology Telescop, and type Ia supernova data from DESY5\nand PantheonPlus, as well as the SH0ES data. From our analysis, we observe the\ndynamical scale parameter of the cosmological constant, $\\delta_{\\Lambda} =\n-0.410^{+0.140}_{-0.120}$, in the $e\\widetilde{\\Lambda}$CDM model using the\nCMB+DESI+SH0ES data, which deviates from $\\Lambda$CDM at the $3.2\\sigma$ level.\nDue to the anti-correlation between $\\delta_{\\Lambda}$ and $H_0$, a negative\n$\\delta_{\\Lambda}$ results in a higher inferred $H_0$. Consequently, we obtain\n$H_0 = 71.90 \\pm 1.00~\\mathrm{km\\,s^{-1}\\,Mpc^{-1}}$, reducing the $H_0$\ntension to $0.8\\sigma$. Even without SH0ES, the CMB+DESI data alone still\nalleviate the $H_0$ tension to $1.7\\sigma$. Overall, the\n$e\\widetilde{\\Lambda}$CDM model not only deviates from the $\\Lambda$CDM model\nbut also demonstrates a significant capability to alleviate the $H_0$ tension.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The $H_0$ tension has become one of the most significant challenges in modern\ncosmology. The recent DESI DR2 data has shown a significant preference for\ndynamical dark energy, yet this has further exacerbated the $H_0$ tension. In\nthis work, we explore the potential of new interacting dark energy models\n($\\widetilde{\\Lambda}$CDM and $e\\widetilde{\\Lambda}$CDM) to alleviate the $H_0$\ntension. We perform observational constraints using the latest baryon acoustic\noscillation data from DESI DR2, cosmic microwave background (CMB) data from\nPlanck and Atacama Cosmology Telescop, and type Ia supernova data from DESY5\nand PantheonPlus, as well as the SH0ES data. From our analysis, we observe the\ndynamical scale parameter of the cosmological constant, $\\delta_{\\Lambda} =\n-0.410^{+0.140}_{-0.120}$, in the $e\\widetilde{\\Lambda}$CDM model using the\nCMB+DESI+SH0ES data, which deviates from $\\Lambda$CDM at the $3.2\\sigma$ level.\nDue to the anti-correlation between $\\delta_{\\Lambda}$ and $H_0$, a negative\n$\\delta_{\\Lambda}$ results in a higher inferred $H_0$. Consequently, we obtain\n$H_0 = 71.90 \\pm 1.00~\\mathrm{km\\,s^{-1}\\,Mpc^{-1}}$, reducing the $H_0$\ntension to $0.8\\sigma$. Even without SH0ES, the CMB+DESI data alone still\nalleviate the $H_0$ tension to $1.7\\sigma$. Overall, the\n$e\\widetilde{\\Lambda}$CDM model not only deviates from the $\\Lambda$CDM model\nbut also demonstrates a significant capability to alleviate the $H_0$ tension."
                },
                "authors": [
                    {
                        "name": "Yi-Min Zhang"
                    },
                    {
                        "name": "Tian-Nuo Li"
                    },
                    {
                        "name": "Guo-Hong Du"
                    },
                    {
                        "name": "Sheng-Han Zhou"
                    },
                    {
                        "name": "Li-Yang Gao"
                    },
                    {
                        "name": "Jing-Fei Zhang"
                    },
                    {
                        "name": "Xin Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Zhang"
                },
                "author": "Xin Zhang",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12621v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12621v2",
                "updated": "2025-10-15T06:42:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    6,
                    42,
                    22,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-14T15:20:06Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    20,
                    6,
                    1,
                    287,
                    0
                ],
                "title": "ACADATA: Parallel Dataset of Academic Data for Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACADATA: Parallel Dataset of Academic Data for Machine Translation"
                },
                "summary": "We present ACADATA, a high-quality parallel dataset for academic translation,\nthat consists of two subsets: ACAD-TRAIN, which contains approximately 1.5\nmillion author-generated paragraph pairs across 96 language directions and\nACAD-BENCH, a curated evaluation set of almost 6,000 translations covering 12\ndirections. To validate its utility, we fine-tune two Large Language Models\n(LLMs) on ACAD-TRAIN and benchmark them on ACAD-BENCH against specialized\nmachine-translation systems, general-purpose, open-weight LLMs, and several\nlarge-scale proprietary models. Experimental results demonstrate that\nfine-tuning on ACAD-TRAIN leads to improvements in academic translation quality\nby +6.1 and +12.4 d-BLEU points on average for 7B and 2B models respectively,\nwhile also improving long-context translation in a general domain by up to\n24.9% when translating out of English. The fine-tuned top-performing model\nsurpasses the best propietary and open-weight models on academic translation\ndomain. By releasing ACAD-TRAIN, ACAD-BENCH and the fine-tuned models, we\nprovide the community with a valuable resource to advance research in academic\ndomain and long-context translation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present ACADATA, a high-quality parallel dataset for academic translation,\nthat consists of two subsets: ACAD-TRAIN, which contains approximately 1.5\nmillion author-generated paragraph pairs across 96 language directions and\nACAD-BENCH, a curated evaluation set of almost 6,000 translations covering 12\ndirections. To validate its utility, we fine-tune two Large Language Models\n(LLMs) on ACAD-TRAIN and benchmark them on ACAD-BENCH against specialized\nmachine-translation systems, general-purpose, open-weight LLMs, and several\nlarge-scale proprietary models. Experimental results demonstrate that\nfine-tuning on ACAD-TRAIN leads to improvements in academic translation quality\nby +6.1 and +12.4 d-BLEU points on average for 7B and 2B models respectively,\nwhile also improving long-context translation in a general domain by up to\n24.9% when translating out of English. The fine-tuned top-performing model\nsurpasses the best propietary and open-weight models on academic translation\ndomain. By releasing ACAD-TRAIN, ACAD-BENCH and the fine-tuned models, we\nprovide the community with a valuable resource to advance research in academic\ndomain and long-context translation."
                },
                "authors": [
                    {
                        "name": "Iaki Lacunza"
                    },
                    {
                        "name": "Javier Garcia Gilabert"
                    },
                    {
                        "name": "Francesca De Luca Fornaciari"
                    },
                    {
                        "name": "Javier Aula-Blasco"
                    },
                    {
                        "name": "Aitor Gonzalez-Agirre"
                    },
                    {
                        "name": "Maite Melero"
                    },
                    {
                        "name": "Marta Villegas"
                    }
                ],
                "author_detail": {
                    "name": "Marta Villegas"
                },
                "author": "Marta Villegas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12621v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12621v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12618v1",
                "updated": "2025-10-14T15:17:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    17,
                    23,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T15:17:23Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    17,
                    23,
                    1,
                    287,
                    0
                ],
                "title": "Towards Fast Coarse-graining and Equation Discovery with Foundation\n  Inference Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Fast Coarse-graining and Equation Discovery with Foundation\n  Inference Models"
                },
                "summary": "High-dimensional recordings of dynamical processes are often characterized by\na much smaller set of effective variables, evolving on low-dimensional\nmanifolds. Identifying these latent dynamics requires solving two intertwined\nproblems: discovering appropriate coarse-grained variables and simultaneously\nfitting the governing equations. Most machine learning approaches tackle these\ntasks jointly by training autoencoders together with models that enforce\ndynamical consistency. We propose to decouple the two problems by leveraging\nthe recently introduced Foundation Inference Models (FIMs). FIMs are pretrained\nmodels that estimate the infinitesimal generators of dynamical systems (e.g.,\nthe drift and diffusion of a stochastic differential equation) in zero-shot\nmode. By amortizing the inference of the dynamics through a FIM with frozen\nweights, and training only the encoder-decoder map, we define a simple,\nsimulation-consistent loss that stabilizes representation learning. A proof of\nconcept on a stochastic double-well system with semicircle diffusion, embedded\ninto synthetic video data, illustrates the potential of this approach for fast\nand reusable coarse-graining pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-dimensional recordings of dynamical processes are often characterized by\na much smaller set of effective variables, evolving on low-dimensional\nmanifolds. Identifying these latent dynamics requires solving two intertwined\nproblems: discovering appropriate coarse-grained variables and simultaneously\nfitting the governing equations. Most machine learning approaches tackle these\ntasks jointly by training autoencoders together with models that enforce\ndynamical consistency. We propose to decouple the two problems by leveraging\nthe recently introduced Foundation Inference Models (FIMs). FIMs are pretrained\nmodels that estimate the infinitesimal generators of dynamical systems (e.g.,\nthe drift and diffusion of a stochastic differential equation) in zero-shot\nmode. By amortizing the inference of the dynamics through a FIM with frozen\nweights, and training only the encoder-decoder map, we define a simple,\nsimulation-consistent loss that stabilizes representation learning. A proof of\nconcept on a stochastic double-well system with semicircle diffusion, embedded\ninto synthetic video data, illustrates the potential of this approach for fast\nand reusable coarse-graining pipelines."
                },
                "authors": [
                    {
                        "name": "Manuel Hinz"
                    },
                    {
                        "name": "Maximilian Mauel"
                    },
                    {
                        "name": "Patrick Seifner"
                    },
                    {
                        "name": "David Berghaus"
                    },
                    {
                        "name": "Kostadin Cvejoski"
                    },
                    {
                        "name": "Ramses J. Sanchez"
                    }
                ],
                "author_detail": {
                    "name": "Ramses J. Sanchez"
                },
                "author": "Ramses J. Sanchez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12608v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12608v1",
                "updated": "2025-10-14T15:07:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    7,
                    27,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T15:07:27Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    7,
                    27,
                    1,
                    287,
                    0
                ],
                "title": "StyleDecipher: Robust and Explainable Detection of LLM-Generated Texts\n  with Stylistic Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StyleDecipher: Robust and Explainable Detection of LLM-Generated Texts\n  with Stylistic Analysis"
                },
                "summary": "With the increasing integration of large language models (LLMs) into\nopen-domain writing, detecting machine-generated text has become a critical\ntask for ensuring content authenticity and trust. Existing approaches rely on\nstatistical discrepancies or model-specific heuristics to distinguish between\nLLM-generated and human-written text. However, these methods struggle in\nreal-world scenarios due to limited generalization, vulnerability to\nparaphrasing, and lack of explainability, particularly when facing stylistic\ndiversity or hybrid human-AI authorship. In this work, we propose\nStyleDecipher, a robust and explainable detection framework that revisits\nLLM-generated text detection using combined feature extractors to quantify\nstylistic differences. By jointly modeling discrete stylistic indicators and\ncontinuous stylistic representations derived from semantic embeddings,\nStyleDecipher captures distinctive style-level divergences between human and\nLLM outputs within a unified representation space. This framework enables\naccurate, explainable, and domain-agnostic detection without requiring access\nto model internals or labeled segments. Extensive experiments across five\ndiverse domains, including news, code, essays, reviews, and academic abstracts,\ndemonstrate that StyleDecipher consistently achieves state-of-the-art in-domain\naccuracy. Moreover, in cross-domain evaluations, it surpasses existing\nbaselines by up to 36.30%, while maintaining robustness against adversarial\nperturbations and mixed human-AI content. Further qualitative and quantitative\nanalysis confirms that stylistic signals provide explainable evidence for\ndistinguishing machine-generated text. Our source code can be accessed at\nhttps://github.com/SiyuanLi00/StyleDecipher.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing integration of large language models (LLMs) into\nopen-domain writing, detecting machine-generated text has become a critical\ntask for ensuring content authenticity and trust. Existing approaches rely on\nstatistical discrepancies or model-specific heuristics to distinguish between\nLLM-generated and human-written text. However, these methods struggle in\nreal-world scenarios due to limited generalization, vulnerability to\nparaphrasing, and lack of explainability, particularly when facing stylistic\ndiversity or hybrid human-AI authorship. In this work, we propose\nStyleDecipher, a robust and explainable detection framework that revisits\nLLM-generated text detection using combined feature extractors to quantify\nstylistic differences. By jointly modeling discrete stylistic indicators and\ncontinuous stylistic representations derived from semantic embeddings,\nStyleDecipher captures distinctive style-level divergences between human and\nLLM outputs within a unified representation space. This framework enables\naccurate, explainable, and domain-agnostic detection without requiring access\nto model internals or labeled segments. Extensive experiments across five\ndiverse domains, including news, code, essays, reviews, and academic abstracts,\ndemonstrate that StyleDecipher consistently achieves state-of-the-art in-domain\naccuracy. Moreover, in cross-domain evaluations, it surpasses existing\nbaselines by up to 36.30%, while maintaining robustness against adversarial\nperturbations and mixed human-AI content. Further qualitative and quantitative\nanalysis confirms that stylistic signals provide explainable evidence for\ndistinguishing machine-generated text. Our source code can be accessed at\nhttps://github.com/SiyuanLi00/StyleDecipher."
                },
                "authors": [
                    {
                        "name": "Siyuan Li"
                    },
                    {
                        "name": "Aodu Wulianghai"
                    },
                    {
                        "name": "Xi Lin"
                    },
                    {
                        "name": "Guangyan Li"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Jun Wu"
                    },
                    {
                        "name": "Jianhua Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianhua Li"
                },
                "author": "Jianhua Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12608v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12603v1",
                "updated": "2025-10-14T14:58:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    14,
                    58,
                    25,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T14:58:25Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    14,
                    58,
                    25,
                    1,
                    287,
                    0
                ],
                "title": "Reasoning in the Dark: Interleaved Vision-Text Reasoning in Latent Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning in the Dark: Interleaved Vision-Text Reasoning in Latent Space"
                },
                "summary": "Multimodal reasoning aims to enhance the capabilities of MLLMs by\nincorporating intermediate reasoning steps before reaching the final answer. It\nhas evolved from text-only reasoning to the integration of visual information,\nenabling the thought process to be conveyed through both images and text.\nDespite its effectiveness, current multimodal reasoning methods depend on\nexplicit reasoning steps that require labor-intensive vision-text annotations\nand inherently introduce significant inference latency. To address these\nissues, we introduce multimodal latent reasoning with the advantages of\nmultimodal representation, reduced annotation, and inference efficiency. To\nfacilicate it, we propose Interleaved Vision-Text Latent Reasoning (IVT-LR),\nwhich injects both visual and textual information in the reasoning process\nwithin the latent space. Specifically, IVT-LR represents each reasoning step by\ncombining two implicit parts: latent text (the hidden states from the previous\nstep) and latent vision (a set of selected image embeddings). We further\nintroduce a progressive multi-stage training strategy to enable MLLMs to\nperform the above multimodal latent reasoning steps. Experiments on M3CoT and\nScienceQA demonstrate that our IVT-LR method achieves an average performance\nincrease of 5.45% in accuracy, while simultaneously achieving a speed increase\nof over 5 times compared to existing approaches. Code available at\nhttps://github.com/FYYDCC/IVT-LR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal reasoning aims to enhance the capabilities of MLLMs by\nincorporating intermediate reasoning steps before reaching the final answer. It\nhas evolved from text-only reasoning to the integration of visual information,\nenabling the thought process to be conveyed through both images and text.\nDespite its effectiveness, current multimodal reasoning methods depend on\nexplicit reasoning steps that require labor-intensive vision-text annotations\nand inherently introduce significant inference latency. To address these\nissues, we introduce multimodal latent reasoning with the advantages of\nmultimodal representation, reduced annotation, and inference efficiency. To\nfacilicate it, we propose Interleaved Vision-Text Latent Reasoning (IVT-LR),\nwhich injects both visual and textual information in the reasoning process\nwithin the latent space. Specifically, IVT-LR represents each reasoning step by\ncombining two implicit parts: latent text (the hidden states from the previous\nstep) and latent vision (a set of selected image embeddings). We further\nintroduce a progressive multi-stage training strategy to enable MLLMs to\nperform the above multimodal latent reasoning steps. Experiments on M3CoT and\nScienceQA demonstrate that our IVT-LR method achieves an average performance\nincrease of 5.45% in accuracy, while simultaneously achieving a speed increase\nof over 5 times compared to existing approaches. Code available at\nhttps://github.com/FYYDCC/IVT-LR."
                },
                "authors": [
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Zhixin Ma"
                    },
                    {
                        "name": "Yongqi Li"
                    },
                    {
                        "name": "Yupeng Hu"
                    },
                    {
                        "name": "Yinwei Wei"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07505v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07505v2",
                "updated": "2025-10-14T14:48:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    14,
                    48,
                    46,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-08T20:04:17Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    20,
                    4,
                    17,
                    2,
                    281,
                    0
                ],
                "title": "PEAR: Planner-Executor Agent Robustness Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PEAR: Planner-Executor Agent Robustness Benchmark"
                },
                "summary": "Large Language Model (LLM)-based Multi-Agent Systems (MAS) have emerged as a\npowerful paradigm for tackling complex, multi-step tasks across diverse\ndomains. However, despite their impressive capabilities, MAS remain susceptible\nto adversarial manipulation. Existing studies typically examine isolated attack\nsurfaces or specific scenarios, leaving a lack of holistic understanding of MAS\nvulnerabilities. To bridge this gap, we introduce PEAR, a benchmark for\nsystematically evaluating both the utility and vulnerability of\nplanner-executor MAS. While compatible with various MAS architectures, our\nbenchmark focuses on the planner-executor structure, which is a practical and\nwidely adopted design. Through extensive experiments, we find that (1) a weak\nplanner degrades overall clean task performance more severely than a weak\nexecutor; (2) while a memory module is essential for the planner, having a\nmemory module for the executor does not impact the clean task performance; (3)\nthere exists a trade-off between task performance and robustness; and (4)\nattacks targeting the planner are particularly effective at misleading the\nsystem. These findings offer actionable insights for enhancing the robustness\nof MAS and lay the groundwork for principled defenses in multi-agent settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based Multi-Agent Systems (MAS) have emerged as a\npowerful paradigm for tackling complex, multi-step tasks across diverse\ndomains. However, despite their impressive capabilities, MAS remain susceptible\nto adversarial manipulation. Existing studies typically examine isolated attack\nsurfaces or specific scenarios, leaving a lack of holistic understanding of MAS\nvulnerabilities. To bridge this gap, we introduce PEAR, a benchmark for\nsystematically evaluating both the utility and vulnerability of\nplanner-executor MAS. While compatible with various MAS architectures, our\nbenchmark focuses on the planner-executor structure, which is a practical and\nwidely adopted design. Through extensive experiments, we find that (1) a weak\nplanner degrades overall clean task performance more severely than a weak\nexecutor; (2) while a memory module is essential for the planner, having a\nmemory module for the executor does not impact the clean task performance; (3)\nthere exists a trade-off between task performance and robustness; and (4)\nattacks targeting the planner are particularly effective at misleading the\nsystem. These findings offer actionable insights for enhancing the robustness\nof MAS and lay the groundwork for principled defenses in multi-agent settings."
                },
                "authors": [
                    {
                        "name": "Shen Dong"
                    },
                    {
                        "name": "Mingxuan Zhang"
                    },
                    {
                        "name": "Pengfei He"
                    },
                    {
                        "name": "Li Ma"
                    },
                    {
                        "name": "Bhavani Thuraisingham"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Yue Xing"
                    }
                ],
                "author_detail": {
                    "name": "Yue Xing"
                },
                "author": "Yue Xing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07505v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07505v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23564v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23564v2",
                "updated": "2025-10-14T14:47:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    14,
                    47,
                    29,
                    1,
                    287,
                    0
                ],
                "published": "2025-09-28T01:44:05Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    1,
                    44,
                    5,
                    6,
                    271,
                    0
                ],
                "title": "Clean First, Align Later: Benchmarking Preference Data Cleaning for\n  Reliable LLM Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clean First, Align Later: Benchmarking Preference Data Cleaning for\n  Reliable LLM Alignment"
                },
                "summary": "Human feedback plays a pivotal role in aligning large language models (LLMs)\nwith human preferences. However, such feedback is often noisy or inconsistent,\nwhich can degrade the quality of reward models and hinder alignment. While\nvarious automated data cleaning methods have been proposed to mitigate this\nissue, a systematic evaluation of their effectiveness and generalizability\nremains lacking. To bridge this gap, we introduce the first comprehensive\nbenchmark for evaluating 13 preference data cleaning methods in the context of\nLLM alignment. PrefCleanBench offers a standardized protocol to assess cleaning\nstrategies in terms of alignment performance and generalizability across\ndiverse datasets, model architectures, and optimization algorithms. By unifying\ndisparate methods and rigorously comparing them, we uncover key factors that\ndetermine the success of data cleaning in alignment tasks. This benchmark lays\nthe groundwork for principled and reproducible approaches to improving LLM\nalignment through better data quality-highlighting the crucial but\nunderexplored role of data preprocessing in responsible AI development. We\nrelease modular implementations of all methods to catalyze further research:\nhttps://github.com/deeplearning-wisc/PrefCleanBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human feedback plays a pivotal role in aligning large language models (LLMs)\nwith human preferences. However, such feedback is often noisy or inconsistent,\nwhich can degrade the quality of reward models and hinder alignment. While\nvarious automated data cleaning methods have been proposed to mitigate this\nissue, a systematic evaluation of their effectiveness and generalizability\nremains lacking. To bridge this gap, we introduce the first comprehensive\nbenchmark for evaluating 13 preference data cleaning methods in the context of\nLLM alignment. PrefCleanBench offers a standardized protocol to assess cleaning\nstrategies in terms of alignment performance and generalizability across\ndiverse datasets, model architectures, and optimization algorithms. By unifying\ndisparate methods and rigorously comparing them, we uncover key factors that\ndetermine the success of data cleaning in alignment tasks. This benchmark lays\nthe groundwork for principled and reproducible approaches to improving LLM\nalignment through better data quality-highlighting the crucial but\nunderexplored role of data preprocessing in responsible AI development. We\nrelease modular implementations of all methods to catalyze further research:\nhttps://github.com/deeplearning-wisc/PrefCleanBench."
                },
                "authors": [
                    {
                        "name": "Samuel Yeh"
                    },
                    {
                        "name": "Sharon Li"
                    }
                ],
                "author_detail": {
                    "name": "Sharon Li"
                },
                "author": "Sharon Li",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23564v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23564v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17542v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17542v2",
                "updated": "2025-10-14T14:46:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    14,
                    46,
                    58,
                    1,
                    287,
                    0
                ],
                "published": "2025-04-24T13:32:20Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    32,
                    20,
                    3,
                    114,
                    0
                ],
                "title": "Cottontail: Large Language Model-Driven Concolic Execution for Highly\n  Structured Test Input Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cottontail: Large Language Model-Driven Concolic Execution for Highly\n  Structured Test Input Generation"
                },
                "summary": "How can we perform concolic execution to generate highly structured test\ninputs for systematically testing parsing programs? Existing concolic execution\nengines are significantly restricted by (1) input structure-agnostic path\nconstraint selection, leading to the waste of testing effort or missing\ncoverage; (2) limited constraint-solving capability, yielding many\nsyntactically invalid test inputs; (3) reliance on manual acquisition of highly\nstructured seed inputs, resulting in non-continuous testing.\n  This paper proposes Cottontail, a new Large Language Model (LLM)-driven\nconcolic execution engine, to mitigate the above limitations. A more complete\nprogram path representation, named Expressive Structural Coverage Tree (ESCT),\nis first constructed to select structure-aware path constraints. Later, an\nLLM-driven constraint solver based on a Solve-Complete paradigm is designed to\nsolve the path constraints smartly to get test inputs that are not only\nsatisfiable to the constraints but also valid to the input syntax. Finally, a\nhistory-guided seed acquisition is employed to obtain new highly structured\ntest inputs either before testing starts or after testing is saturated.\n  We implemented Cottontail on top of SymCC and evaluated eight extensively\ntested open-source libraries across four different formats (XML, SQL,\nJavaScript, and JSON). Cottontail significantly outperforms baseline approaches\nby 30.73% and 41.32% on average in terms of line and branch coverage. Besides,\nCottontail found six previously unknown vulnerabilities (six CVEs assigned). We\nhave reported these issues to developers, and four out of them have been fixed\nso far.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How can we perform concolic execution to generate highly structured test\ninputs for systematically testing parsing programs? Existing concolic execution\nengines are significantly restricted by (1) input structure-agnostic path\nconstraint selection, leading to the waste of testing effort or missing\ncoverage; (2) limited constraint-solving capability, yielding many\nsyntactically invalid test inputs; (3) reliance on manual acquisition of highly\nstructured seed inputs, resulting in non-continuous testing.\n  This paper proposes Cottontail, a new Large Language Model (LLM)-driven\nconcolic execution engine, to mitigate the above limitations. A more complete\nprogram path representation, named Expressive Structural Coverage Tree (ESCT),\nis first constructed to select structure-aware path constraints. Later, an\nLLM-driven constraint solver based on a Solve-Complete paradigm is designed to\nsolve the path constraints smartly to get test inputs that are not only\nsatisfiable to the constraints but also valid to the input syntax. Finally, a\nhistory-guided seed acquisition is employed to obtain new highly structured\ntest inputs either before testing starts or after testing is saturated.\n  We implemented Cottontail on top of SymCC and evaluated eight extensively\ntested open-source libraries across four different formats (XML, SQL,\nJavaScript, and JSON). Cottontail significantly outperforms baseline approaches\nby 30.73% and 41.32% on average in terms of line and branch coverage. Besides,\nCottontail found six previously unknown vulnerabilities (six CVEs assigned). We\nhave reported these issues to developers, and four out of them have been fixed\nso far."
                },
                "authors": [
                    {
                        "name": "Haoxin Tu"
                    },
                    {
                        "name": "Seongmin Lee"
                    },
                    {
                        "name": "Yuxian Li"
                    },
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Lingxiao Jiang"
                    },
                    {
                        "name": "Marcel Bhme"
                    }
                ],
                "author_detail": {
                    "name": "Marcel Bhme"
                },
                "author": "Marcel Bhme",
                "arxiv_comment": "To appear on the 2026 IEEE Symposium on Security and Privacy (22\n  pages, with detailed Appendix)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17542v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17542v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12587v1",
                "updated": "2025-10-14T14:42:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    14,
                    42,
                    40,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T14:42:40Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    14,
                    42,
                    40,
                    1,
                    287,
                    0
                ],
                "title": "Teaching Language Models to Faithfully Express their Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching Language Models to Faithfully Express their Uncertainty"
                },
                "summary": "Large language models (LLMs) often miscommunicate their uncertainty: repeated\nqueries can produce divergent answers, yet generated responses are typically\nunhedged or hedged in ways that do not reflect this variability. This conveys\nunfaithful information about the uncertain state of the LLMs' knowledge,\ncreating a faithfulness gap that affects even strong LLMs. We introduce\nFaithful Uncertainty Tuning (FUT): a fine-tuning approach that teaches\ninstruction-tuned LLMs to express uncertainty faithfully without altering their\nunderlying answer distribution. We construct training data by augmenting model\nsamples with uncertainty hedges (i.e. verbal cues such as 'possibly' or\n'likely') aligned with sample consistency, requiring no supervision beyond the\nmodel and a set of prompts. We evaluate FUT on open-domain question answering\n(QA) across multiple models and datasets. Our results show that FUT\nsubstantially reduces the faithfulness gap, while preserving QA accuracy and\nintroducing minimal semantic distribution shift. Further analyses demonstrate\nrobustness across decoding strategies, choice of hedgers, and other forms of\nuncertainty expression (i.e. numerical). These findings establish FUT as a\nsimple and effective way to teach LLMs to communicate uncertainty faithfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often miscommunicate their uncertainty: repeated\nqueries can produce divergent answers, yet generated responses are typically\nunhedged or hedged in ways that do not reflect this variability. This conveys\nunfaithful information about the uncertain state of the LLMs' knowledge,\ncreating a faithfulness gap that affects even strong LLMs. We introduce\nFaithful Uncertainty Tuning (FUT): a fine-tuning approach that teaches\ninstruction-tuned LLMs to express uncertainty faithfully without altering their\nunderlying answer distribution. We construct training data by augmenting model\nsamples with uncertainty hedges (i.e. verbal cues such as 'possibly' or\n'likely') aligned with sample consistency, requiring no supervision beyond the\nmodel and a set of prompts. We evaluate FUT on open-domain question answering\n(QA) across multiple models and datasets. Our results show that FUT\nsubstantially reduces the faithfulness gap, while preserving QA accuracy and\nintroducing minimal semantic distribution shift. Further analyses demonstrate\nrobustness across decoding strategies, choice of hedgers, and other forms of\nuncertainty expression (i.e. numerical). These findings establish FUT as a\nsimple and effective way to teach LLMs to communicate uncertainty faithfully."
                },
                "authors": [
                    {
                        "name": "Bryan Eikema"
                    },
                    {
                        "name": "Evgenia Ilia"
                    },
                    {
                        "name": "Jos G. C. de Souza"
                    },
                    {
                        "name": "Chrysoula Zerva"
                    },
                    {
                        "name": "Wilker Aziz"
                    }
                ],
                "author_detail": {
                    "name": "Wilker Aziz"
                },
                "author": "Wilker Aziz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08704v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08704v2",
                "updated": "2025-10-14T14:35:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    14,
                    35,
                    49,
                    1,
                    287,
                    0
                ],
                "published": "2025-07-11T15:57:37Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    15,
                    57,
                    37,
                    4,
                    192,
                    0
                ],
                "title": "Knowledge Fusion via Bidirectional Information Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Fusion via Bidirectional Information Aggregation"
                },
                "summary": "Knowledge graphs (KGs) are the cornerstone of the semantic web, offering\nup-to-date representations of real-world entities and relations. Yet large\nlanguage models (LLMs) remain largely static after pre-training, causing their\ninternal knowledge to become outdated and limiting their utility in\ntime-sensitive web applications. To bridge this gap between dynamic knowledge\nand static models, a prevalent approach is to enhance LLMs with KGs. However,\nprevailing methods typically rely on parameter-invasive fine-tuning, which\nrisks catastrophic forgetting and often degrades LLMs' general capabilities.\nMoreover, their static integration frameworks cannot keep pace with the\ncontinuous evolution of real-world KGs, hindering their deployment in dynamic\nweb environments. To bridge this gap, we introduce KGA\n(\\textit{\\underline{K}nowledge \\underline{G}raph-guided\n\\underline{A}ttention}), a novel framework that dynamically integrates external\nKGs into LLMs exclusively at inference-time without any parameter modification.\nInspired by research on neuroscience, we rewire the self-attention module by\ninnovatively introducing two synergistic pathways: a \\textit{bottom-up\nknowledge fusion} pathway and a \\textit{top-down attention guidance} pathway.\nThe \\textit{bottom-up pathway} dynamically integrates external knowledge into\ninput representations via input-driven KG fusion, which is akin to the\n\\textit{stimulus-driven attention process} in the human brain. Complementarily,\nthe \\textit{top-down pathway} aims to assess the contextual relevance of each\ntriple through a \\textit{goal-directed verification process}, thereby\nsuppressing task-irrelevant signals and amplifying knowledge-relevant patterns.\nBy synergistically combining these two pathways, our method supports real-time\nknowledge fusion. Extensive experiments on four benchmarks verify KGA's strong\nfusion performance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge graphs (KGs) are the cornerstone of the semantic web, offering\nup-to-date representations of real-world entities and relations. Yet large\nlanguage models (LLMs) remain largely static after pre-training, causing their\ninternal knowledge to become outdated and limiting their utility in\ntime-sensitive web applications. To bridge this gap between dynamic knowledge\nand static models, a prevalent approach is to enhance LLMs with KGs. However,\nprevailing methods typically rely on parameter-invasive fine-tuning, which\nrisks catastrophic forgetting and often degrades LLMs' general capabilities.\nMoreover, their static integration frameworks cannot keep pace with the\ncontinuous evolution of real-world KGs, hindering their deployment in dynamic\nweb environments. To bridge this gap, we introduce KGA\n(\\textit{\\underline{K}nowledge \\underline{G}raph-guided\n\\underline{A}ttention}), a novel framework that dynamically integrates external\nKGs into LLMs exclusively at inference-time without any parameter modification.\nInspired by research on neuroscience, we rewire the self-attention module by\ninnovatively introducing two synergistic pathways: a \\textit{bottom-up\nknowledge fusion} pathway and a \\textit{top-down attention guidance} pathway.\nThe \\textit{bottom-up pathway} dynamically integrates external knowledge into\ninput representations via input-driven KG fusion, which is akin to the\n\\textit{stimulus-driven attention process} in the human brain. Complementarily,\nthe \\textit{top-down pathway} aims to assess the contextual relevance of each\ntriple through a \\textit{goal-directed verification process}, thereby\nsuppressing task-irrelevant signals and amplifying knowledge-relevant patterns.\nBy synergistically combining these two pathways, our method supports real-time\nknowledge fusion. Extensive experiments on four benchmarks verify KGA's strong\nfusion performance and efficiency."
                },
                "authors": [
                    {
                        "name": "Songlin Zhai"
                    },
                    {
                        "name": "Guilin Qi"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Yuan Meng"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Meng"
                },
                "author": "Yuan Meng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08704v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08704v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18389v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18389v4",
                "updated": "2025-10-14T14:32:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    14,
                    32,
                    37,
                    1,
                    287,
                    0
                ],
                "published": "2025-05-23T21:33:16Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    21,
                    33,
                    16,
                    4,
                    143,
                    0
                ],
                "title": "ALLSTaR: Automated LLM-Driven Scheduler Generation and Testing for\n  Intent-Based RAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALLSTaR: Automated LLM-Driven Scheduler Generation and Testing for\n  Intent-Based RAN"
                },
                "summary": "The evolution toward open, programmable O-RAN and AI-RAN 6G networks creates\nunprecedented opportunities for Intent-Based Networking (IBN) to dynamically\noptimize RAN[...]. However, applying IBN effectively to the RAN scheduler [...]\nremains a significant challenge. Current approaches predominantly rely on\ncoarse-grained network slicing, lacking the granularity for dynamic adaptation\nto individual user conditions and traffic patterns. Despite the existence of a\nvast body of scheduling algorithms [...], their practical utilization is\nhindered by implementation heterogeneity, insufficient systematic evaluation in\nproduction environments, and the complexity of developing high-performance\nscheduler implementations.[...] To address these limitations, we propose\nALLSTaR (Automated LLm-driven Scheduler generation and Testing for intent-based\nRAN), a novel framework leveraging LLMs for automated, intent-driven scheduler\ndesign, implementation, and evaluation. ALLSTaR interprets NL intents,\nautomatically generates functional scheduler code from the research literature\nusing OCR and LLMs, and intelligently matches operator intents to the most\nsuitable scheduler(s). Our implementation deploys these schedulers as O-RAN\ndApps, enabling on-the-fly deployment and testing on a production-grade,\n5G-compliant testbed. This approach has enabled the largest-scale OTA\nexperimental comparison of 18 scheduling algorithms automatically synthesized\nfrom the academic literature. The resulting performance profiles serve as the\ninput for our Intent-Based Scheduling (IBS) framework, which dynamically\nselects and deploys appropriate schedulers that optimally satisfy operator\nintents. We validate our approach through multiple use cases unattainable with\ncurrent slicing-based optimization techniques, demonstrating fine-grained\ncontrol based on buffer status, physical layer conditions, and heterogeneous\ntraffic types",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution toward open, programmable O-RAN and AI-RAN 6G networks creates\nunprecedented opportunities for Intent-Based Networking (IBN) to dynamically\noptimize RAN[...]. However, applying IBN effectively to the RAN scheduler [...]\nremains a significant challenge. Current approaches predominantly rely on\ncoarse-grained network slicing, lacking the granularity for dynamic adaptation\nto individual user conditions and traffic patterns. Despite the existence of a\nvast body of scheduling algorithms [...], their practical utilization is\nhindered by implementation heterogeneity, insufficient systematic evaluation in\nproduction environments, and the complexity of developing high-performance\nscheduler implementations.[...] To address these limitations, we propose\nALLSTaR (Automated LLm-driven Scheduler generation and Testing for intent-based\nRAN), a novel framework leveraging LLMs for automated, intent-driven scheduler\ndesign, implementation, and evaluation. ALLSTaR interprets NL intents,\nautomatically generates functional scheduler code from the research literature\nusing OCR and LLMs, and intelligently matches operator intents to the most\nsuitable scheduler(s). Our implementation deploys these schedulers as O-RAN\ndApps, enabling on-the-fly deployment and testing on a production-grade,\n5G-compliant testbed. This approach has enabled the largest-scale OTA\nexperimental comparison of 18 scheduling algorithms automatically synthesized\nfrom the academic literature. The resulting performance profiles serve as the\ninput for our Intent-Based Scheduling (IBS) framework, which dynamically\nselects and deploys appropriate schedulers that optimally satisfy operator\nintents. We validate our approach through multiple use cases unattainable with\ncurrent slicing-based optimization techniques, demonstrating fine-grained\ncontrol based on buffer status, physical layer conditions, and heterogeneous\ntraffic types"
                },
                "authors": [
                    {
                        "name": "Maxime Elkael"
                    },
                    {
                        "name": "Michele Polese"
                    },
                    {
                        "name": "Reshma Prasad"
                    },
                    {
                        "name": "Stefano Maxenti"
                    },
                    {
                        "name": "Tommaso Melodia"
                    }
                ],
                "author_detail": {
                    "name": "Tommaso Melodia"
                },
                "author": "Tommaso Melodia",
                "arxiv_comment": "Under submission to an IEEE journal, copyright may change without\n  notice",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18389v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18389v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13932v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13932v3",
                "updated": "2025-10-14T14:31:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    14,
                    31,
                    38,
                    1,
                    287,
                    0
                ],
                "published": "2025-09-17T13:01:22Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    1,
                    22,
                    2,
                    260,
                    0
                ],
                "title": "Investigating aerosols as a way to reconcile K2-18 b JWST MIRI and\n  NIRISS/NIRSpec observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating aerosols as a way to reconcile K2-18 b JWST MIRI and\n  NIRISS/NIRSpec observations"
                },
                "summary": "Recent JWST observations of the temperate sub-Neptune K2-18 b with NIRISS\nSOSS/NIRSpec G395H and MIRI LRS have yielded apparently inconsistent results:\nthe MIRI spectra exhibit spectral features nearly twice as large as those seen\nat shorter wavelengths, challenging the high-metallicity, CH4-rich\nnon-equilibrium model that fits the NIRISS/NIRSpec data. We perform a suite of\natmospheric retrievals on both datasets, including free-chemistry,\nnon-equilibrium, and aerosol models, using laboratory-derived complex\nrefractive indices for a variety of photochemical haze analogues. Free\nretrievals systematically return lower metallicities than inferred by\nself-consistent chemical disequilibrium models, and the inclusion of absorbing\naerosols, especially CH4-dominated, nitrogen-poor tholins, can further reduce\nthe inferred metallicity by over an order of magnitude. These hazes reproduce\nthe observed NIRISS slope through scattering and match MIRI features via C-H\nbending absorption near 7 um, while yielding particle properties consistent\nwith photochemical production in H2-rich atmospheres. Although their inclusion\nimproves the joint fit and reduces tension between datasets, it also\nsignificantly lowers the retrieved CH4 abundance, highlighting degeneracies\nbetween metallicity, composition, and aerosol properties. Our results\nunderscore the importance of aerosol absorption in interpreting temperate\nsub-Neptune spectra, and motivate future JWST observations and laboratory work\nto break these degeneracies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent JWST observations of the temperate sub-Neptune K2-18 b with NIRISS\nSOSS/NIRSpec G395H and MIRI LRS have yielded apparently inconsistent results:\nthe MIRI spectra exhibit spectral features nearly twice as large as those seen\nat shorter wavelengths, challenging the high-metallicity, CH4-rich\nnon-equilibrium model that fits the NIRISS/NIRSpec data. We perform a suite of\natmospheric retrievals on both datasets, including free-chemistry,\nnon-equilibrium, and aerosol models, using laboratory-derived complex\nrefractive indices for a variety of photochemical haze analogues. Free\nretrievals systematically return lower metallicities than inferred by\nself-consistent chemical disequilibrium models, and the inclusion of absorbing\naerosols, especially CH4-dominated, nitrogen-poor tholins, can further reduce\nthe inferred metallicity by over an order of magnitude. These hazes reproduce\nthe observed NIRISS slope through scattering and match MIRI features via C-H\nbending absorption near 7 um, while yielding particle properties consistent\nwith photochemical production in H2-rich atmospheres. Although their inclusion\nimproves the joint fit and reduces tension between datasets, it also\nsignificantly lowers the retrieved CH4 abundance, highlighting degeneracies\nbetween metallicity, composition, and aerosol properties. Our results\nunderscore the importance of aerosol absorption in interpreting temperate\nsub-Neptune spectra, and motivate future JWST observations and laboratory work\nto break these degeneracies."
                },
                "authors": [
                    {
                        "name": "Adam Yassin Jaziri"
                    },
                    {
                        "name": "Thomas Drant"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Drant"
                },
                "author": "Thomas Drant",
                "arxiv_doi": "10.1051/0004-6361/202556905",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202556905",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.13932v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13932v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted in A&A",
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12572v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12572v2",
                "updated": "2025-10-14T14:21:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    14,
                    21,
                    40,
                    1,
                    287,
                    0
                ],
                "published": "2025-06-14T16:57:36Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    16,
                    57,
                    36,
                    5,
                    165,
                    0
                ],
                "title": "Accelerated inference of binary black-hole populations from the\n  stochastic gravitational-wave background",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerated inference of binary black-hole populations from the\n  stochastic gravitational-wave background"
                },
                "summary": "Third-generation ground-based gravitational wave detectors are expected to\nobserve $\\mathcal{O}(10^5)$ of overlapping signals per year from a multitude of\nastrophysical sources that will be computationally challenging to resolve\nindividually. On the other hand, the stochastic background resulting from the\nentire population of sources encodes information about the underlying\npopulation, allowing for population parameter inference independent and\ncomplementary to that obtained with individually resolved events. Parameter\nestimation in this case is still computationally challenging, as computing the\npower spectrum involves sampling $\\sim 10^5$ sources for each set of\nhyperparameters describing the binary population. In this work, we build on\nrecently developed importance sampling techniques to compute the SGWB\nefficiently and train neural networks to interpolate the resulting background.\nWe show that a multi-layer perceptron can encode the model information,\nallowing for significantly faster inference. We test the network assuming an\nobserving setup with CE and ET sensitivities, where for the first time we\ninclude the intrinsic variance of the SGWB in the inference, as in this setup\nit presents a dominant source of measurement noise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Third-generation ground-based gravitational wave detectors are expected to\nobserve $\\mathcal{O}(10^5)$ of overlapping signals per year from a multitude of\nastrophysical sources that will be computationally challenging to resolve\nindividually. On the other hand, the stochastic background resulting from the\nentire population of sources encodes information about the underlying\npopulation, allowing for population parameter inference independent and\ncomplementary to that obtained with individually resolved events. Parameter\nestimation in this case is still computationally challenging, as computing the\npower spectrum involves sampling $\\sim 10^5$ sources for each set of\nhyperparameters describing the binary population. In this work, we build on\nrecently developed importance sampling techniques to compute the SGWB\nefficiently and train neural networks to interpolate the resulting background.\nWe show that a multi-layer perceptron can encode the model information,\nallowing for significantly faster inference. We test the network assuming an\nobserving setup with CE and ET sensitivities, where for the first time we\ninclude the intrinsic variance of the SGWB in the inference, as in this setup\nit presents a dominant source of measurement noise."
                },
                "authors": [
                    {
                        "name": "G. Giarda"
                    },
                    {
                        "name": "A. I. Renzini"
                    },
                    {
                        "name": "C. Pacilio"
                    },
                    {
                        "name": "D. Gerosa"
                    }
                ],
                "author_detail": {
                    "name": "D. Gerosa"
                },
                "author": "D. Gerosa",
                "arxiv_doi": "10.1088/1361-6382/ae07a0",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1088/1361-6382/ae07a0",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.12572v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12572v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Class. Quantum Grav. 42, 195015 (2025)",
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08621v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08621v2",
                "updated": "2025-10-14T14:08:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    14,
                    8,
                    19,
                    1,
                    287,
                    0
                ],
                "published": "2025-04-11T15:25:50Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    25,
                    50,
                    4,
                    101,
                    0
                ],
                "title": "MooseAgent: A LLM Based Multi-agent Framework for Automating Moose\n  Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MooseAgent: A LLM Based Multi-agent Framework for Automating Moose\n  Simulation"
                },
                "summary": "The Finite Element Method (FEM) is widely used in engineering and scientific\ncomputing, but its pre-processing, solver configuration, and post-processing\nstages are often time-consuming and require specialized knowledge. This paper\nproposes an automated solution framework, MooseAgent, for the multi-physics\nsimulation framework MOOSE, which combines large-scale pre-trained language\nmodels (LLMs) with a multi-agent system. The framework uses LLMs to understand\nuser-described simulation requirements in natural language and employs task\ndecomposition and multi-round iterative verification strategies to\nautomatically generate MOOSE input files. To improve accuracy and reduce model\nhallucinations, the system builds and utilizes a vector database containing\nannotated MOOSE input cards and function documentation. We conducted\nexperimental evaluations on several typical cases, including heat transfer,\nmechanics, phase field, and multi-physics coupling. The results show that\nMooseAgent can automate the MOOSE simulation process to a certain extent,\nespecially demonstrating a high success rate when dealing with relatively\nsimple single-physics problems. The main contribution of this research is the\nproposal of a multi-agent automated framework for MOOSE, which validates its\npotential in simplifying finite element simulation processes and lowering the\nuser barrier, providing new ideas for the development of intelligent finite\nelement simulation software. The code for the MooseAgent framework proposed in\nthis paper has been open-sourced and is available at\nhttps://github.com/taozhan18/MooseAgent",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Finite Element Method (FEM) is widely used in engineering and scientific\ncomputing, but its pre-processing, solver configuration, and post-processing\nstages are often time-consuming and require specialized knowledge. This paper\nproposes an automated solution framework, MooseAgent, for the multi-physics\nsimulation framework MOOSE, which combines large-scale pre-trained language\nmodels (LLMs) with a multi-agent system. The framework uses LLMs to understand\nuser-described simulation requirements in natural language and employs task\ndecomposition and multi-round iterative verification strategies to\nautomatically generate MOOSE input files. To improve accuracy and reduce model\nhallucinations, the system builds and utilizes a vector database containing\nannotated MOOSE input cards and function documentation. We conducted\nexperimental evaluations on several typical cases, including heat transfer,\nmechanics, phase field, and multi-physics coupling. The results show that\nMooseAgent can automate the MOOSE simulation process to a certain extent,\nespecially demonstrating a high success rate when dealing with relatively\nsimple single-physics problems. The main contribution of this research is the\nproposal of a multi-agent automated framework for MOOSE, which validates its\npotential in simplifying finite element simulation processes and lowering the\nuser barrier, providing new ideas for the development of intelligent finite\nelement simulation software. The code for the MooseAgent framework proposed in\nthis paper has been open-sourced and is available at\nhttps://github.com/taozhan18/MooseAgent"
                },
                "authors": [
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Zhenhai Liu"
                    },
                    {
                        "name": "Yong Xin"
                    },
                    {
                        "name": "Yongjun Jiao"
                    }
                ],
                "author_detail": {
                    "name": "Yongjun Jiao"
                },
                "author": "Yongjun Jiao",
                "arxiv_comment": "11 pages, 3 Figs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08621v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08621v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10315v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10315v2",
                "updated": "2025-10-14T14:07:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    14,
                    7,
                    30,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-11T18:47:14Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    18,
                    47,
                    14,
                    5,
                    284,
                    0
                ],
                "title": "Is Misinformation More Open? A Study of robots.txt Gatekeeping on the\n  Web",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Misinformation More Open? A Study of robots.txt Gatekeeping on the\n  Web"
                },
                "summary": "Large Language Models (LLMs) are increasingly relying on web crawling to stay\nup to date and accurately answer user queries. These crawlers are expected to\nhonor robots.txt files, which govern automated access. In this study, for the\nfirst time, we investigate whether reputable news websites and misinformation\nsites differ in how they configure these files, particularly in relation to AI\ncrawlers. Analyzing a curated dataset, we find a stark contrast: 60.0% of\nreputable sites disallow at least one AI crawler, compared to just 9.1% of\nmisinformation sites in their robots.txt files. Reputable sites forbid an\naverage of 15.5 AI user agents, while misinformation sites prohibit fewer than\none. We then measure active blocking behavior, where websites refuse to return\ncontent when HTTP requests include AI crawler user agents, and reveal that both\ncategories of websites utilize it. Notably, the behavior of reputable news\nwebsites in this regard aligns more closely with their declared robots.txt\ndirective than that of misinformation websites. Finally, our longitudinal\nanalysis reveals that this gap has widened over time, with AI-blocking by\nreputable sites rising from 23% in September 2023 to nearly 60% by May 2025.\nOur findings highlight a growing asymmetry in content accessibility that may\nshape the training data available to LLMs, raising essential questions for web\ntransparency, data ethics, and the future of AI training practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly relying on web crawling to stay\nup to date and accurately answer user queries. These crawlers are expected to\nhonor robots.txt files, which govern automated access. In this study, for the\nfirst time, we investigate whether reputable news websites and misinformation\nsites differ in how they configure these files, particularly in relation to AI\ncrawlers. Analyzing a curated dataset, we find a stark contrast: 60.0% of\nreputable sites disallow at least one AI crawler, compared to just 9.1% of\nmisinformation sites in their robots.txt files. Reputable sites forbid an\naverage of 15.5 AI user agents, while misinformation sites prohibit fewer than\none. We then measure active blocking behavior, where websites refuse to return\ncontent when HTTP requests include AI crawler user agents, and reveal that both\ncategories of websites utilize it. Notably, the behavior of reputable news\nwebsites in this regard aligns more closely with their declared robots.txt\ndirective than that of misinformation websites. Finally, our longitudinal\nanalysis reveals that this gap has widened over time, with AI-blocking by\nreputable sites rising from 23% in September 2023 to nearly 60% by May 2025.\nOur findings highlight a growing asymmetry in content accessibility that may\nshape the training data available to LLMs, raising essential questions for web\ntransparency, data ethics, and the future of AI training practices."
                },
                "authors": [
                    {
                        "name": "Nicolas Steinacker-Olsztyn"
                    },
                    {
                        "name": "Devashish Gosain"
                    },
                    {
                        "name": "Ha Dao"
                    }
                ],
                "author_detail": {
                    "name": "Ha Dao"
                },
                "author": "Ha Dao",
                "arxiv_comment": "10 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10315v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10315v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.10349v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.10349v4",
                "updated": "2025-10-14T14:07:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    14,
                    7,
                    18,
                    1,
                    287,
                    0
                ],
                "published": "2023-10-16T12:34:47Z",
                "published_parsed": [
                    2023,
                    10,
                    16,
                    12,
                    34,
                    47,
                    0,
                    289,
                    0
                ],
                "title": "Optimized Layerwise Approximation for Efficient Private Inference on\n  Fully Homomorphic Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimized Layerwise Approximation for Efficient Private Inference on\n  Fully Homomorphic Encryption"
                },
                "summary": "Recent studies have explored the deployment of privacy-preserving deep neural\nnetworks utilizing homomorphic encryption (HE), especially for private\ninference (PI). Many works have attempted the approximation-aware training\n(AAT) approach in PI, changing the activation functions of a model to\nlow-degree polynomials that are easier to compute on HE by allowing model\nretraining. However, due to constraints in the training environment, it is\noften necessary to consider post-training approximation (PTA), using the\npre-trained parameters of the existing plaintext model without retraining.\nExisting PTA studies have uniformly approximated the activation function in all\nlayers to a high degree to mitigate accuracy loss from approximation, leading\nto significant time consumption. This study proposes an optimized layerwise\napproximation (OLA), a systematic framework that optimizes both accuracy loss\nand time consumption by using different approximation polynomials for each\nlayer in the PTA scenario. For efficient approximation, we reflect the\nlayerwise impact on the classification accuracy by considering the actual input\ndistribution of each activation function while constructing the optimization\nproblem. Additionally, we provide a dynamic programming technique to solve the\noptimization problem and achieve the optimized layerwise degrees in polynomial\ntime. As a result, the OLA method reduces inference times for the ResNet-20\nmodel and the ResNet-32 model by 3.02 times and 2.82 times, respectively,\ncompared to prior state-of-the-art implementations employing uniform degree\npolynomials. Furthermore, we successfully classified CIFAR-10 by replacing the\nGELU function in the ConvNeXt model with only 3-degree polynomials using the\nproposed method, without modifying the backbone model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have explored the deployment of privacy-preserving deep neural\nnetworks utilizing homomorphic encryption (HE), especially for private\ninference (PI). Many works have attempted the approximation-aware training\n(AAT) approach in PI, changing the activation functions of a model to\nlow-degree polynomials that are easier to compute on HE by allowing model\nretraining. However, due to constraints in the training environment, it is\noften necessary to consider post-training approximation (PTA), using the\npre-trained parameters of the existing plaintext model without retraining.\nExisting PTA studies have uniformly approximated the activation function in all\nlayers to a high degree to mitigate accuracy loss from approximation, leading\nto significant time consumption. This study proposes an optimized layerwise\napproximation (OLA), a systematic framework that optimizes both accuracy loss\nand time consumption by using different approximation polynomials for each\nlayer in the PTA scenario. For efficient approximation, we reflect the\nlayerwise impact on the classification accuracy by considering the actual input\ndistribution of each activation function while constructing the optimization\nproblem. Additionally, we provide a dynamic programming technique to solve the\noptimization problem and achieve the optimized layerwise degrees in polynomial\ntime. As a result, the OLA method reduces inference times for the ResNet-20\nmodel and the ResNet-32 model by 3.02 times and 2.82 times, respectively,\ncompared to prior state-of-the-art implementations employing uniform degree\npolynomials. Furthermore, we successfully classified CIFAR-10 by replacing the\nGELU function in the ConvNeXt model with only 3-degree polynomials using the\nproposed method, without modifying the backbone model."
                },
                "authors": [
                    {
                        "name": "Junghyun Lee"
                    },
                    {
                        "name": "Eunsang Lee"
                    },
                    {
                        "name": "Young-Sik Kim"
                    },
                    {
                        "name": "Yongwoo Lee"
                    },
                    {
                        "name": "Joon-Woo Lee"
                    },
                    {
                        "name": "Yongjune Kim"
                    },
                    {
                        "name": "Jong-Seon No"
                    }
                ],
                "author_detail": {
                    "name": "Jong-Seon No"
                },
                "author": "Jong-Seon No",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.10349v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.10349v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09043v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09043v2",
                "updated": "2025-10-14T13:58:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    58,
                    28,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-10T06:23:50Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    6,
                    23,
                    50,
                    4,
                    283,
                    0
                ],
                "title": "Humanoid Artificial Consciousness Designed with Large Language Model\n  Based on Psychoanalysis and Personality Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humanoid Artificial Consciousness Designed with Large Language Model\n  Based on Psychoanalysis and Personality Theory"
                },
                "summary": "Human consciousness is still a concept hard to define with current scientific\nunderstanding. Although Large Language Models (LLMs) have recently demonstrated\nsignificant advancements across various domains including translation and\nsummarization, human consciousness is not something to imitate with current\nupfront technology owing to so-called hallucination. This study, therefore,\nproposes a novel approach to address these challenges by integrating\npsychoanalysis and the Myers-Briggs Type Indicator (MBTI) into constructing\nconsciousness and personality modules. We developed three artificial\nconsciousnesses (self-awareness, unconsciousness, and preconsciousness) based\non the principles of psychoanalysis. Additionally, we designed 16 characters\nwith different personalities representing the sixteen MBTI types, with several\nattributes such as needs, status, and memories. To determine if our model's\nartificial consciousness exhibits human-like cognition, we created ten distinct\nsituations considering seven attributes such as emotional understanding and\nlogical thinking. The decision-making process of artificial consciousness and\nthe final action were evaluated in three ways: survey evaluation, three-tier\nclassification via ChatGPT, and qualitative review. Both quantitative and\nqualitative analyses indicated a high likelihood of well-simulated\nconsciousness, although the difference in response between different characters\nand consciousnesses was not very significant. This implies that the developed\nmodels incorporating elements of psychoanalysis and personality theory can lead\nto building a more intuitive and adaptable AI system with humanoid\nconsciousness. Therefore, this study contributes to opening up new avenues for\nimproving AI interactions in complex cognitive contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human consciousness is still a concept hard to define with current scientific\nunderstanding. Although Large Language Models (LLMs) have recently demonstrated\nsignificant advancements across various domains including translation and\nsummarization, human consciousness is not something to imitate with current\nupfront technology owing to so-called hallucination. This study, therefore,\nproposes a novel approach to address these challenges by integrating\npsychoanalysis and the Myers-Briggs Type Indicator (MBTI) into constructing\nconsciousness and personality modules. We developed three artificial\nconsciousnesses (self-awareness, unconsciousness, and preconsciousness) based\non the principles of psychoanalysis. Additionally, we designed 16 characters\nwith different personalities representing the sixteen MBTI types, with several\nattributes such as needs, status, and memories. To determine if our model's\nartificial consciousness exhibits human-like cognition, we created ten distinct\nsituations considering seven attributes such as emotional understanding and\nlogical thinking. The decision-making process of artificial consciousness and\nthe final action were evaluated in three ways: survey evaluation, three-tier\nclassification via ChatGPT, and qualitative review. Both quantitative and\nqualitative analyses indicated a high likelihood of well-simulated\nconsciousness, although the difference in response between different characters\nand consciousnesses was not very significant. This implies that the developed\nmodels incorporating elements of psychoanalysis and personality theory can lead\nto building a more intuitive and adaptable AI system with humanoid\nconsciousness. Therefore, this study contributes to opening up new avenues for\nimproving AI interactions in complex cognitive contexts."
                },
                "authors": [
                    {
                        "name": "Sang Hun Kim"
                    },
                    {
                        "name": "Jongmin Lee"
                    },
                    {
                        "name": "Dongkyu Park"
                    },
                    {
                        "name": "So Young Lee"
                    },
                    {
                        "name": "Yosep Chong"
                    }
                ],
                "author_detail": {
                    "name": "Yosep Chong"
                },
                "author": "Yosep Chong",
                "arxiv_doi": "10.1016/j.cogsys.2025.101392",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.cogsys.2025.101392",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.09043v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09043v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "41 pages, 6 figures. Accepted and published to Cognitive Systems\n  Research, 2025",
                "arxiv_journal_ref": "Cognitive Systems Research Volume 94, December 2025, 101392",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19700v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19700v2",
                "updated": "2025-10-14T13:57:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    57,
                    53,
                    1,
                    287,
                    0
                ],
                "published": "2025-05-26T08:53:02Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    8,
                    53,
                    2,
                    0,
                    146,
                    0
                ],
                "title": "Leveraging Importance Sampling to Detach Alignment Modules from Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Importance Sampling to Detach Alignment Modules from Large\n  Language Models"
                },
                "summary": "The widespread adoption of large language models (LLMs) across industries has\nincreased the demand for high-quality and customizable outputs. However,\ntraditional alignment methods often require retraining large pretrained models,\nmaking it difficult to quickly adapt and optimize LLMs for diverse\napplications. To address this limitation, we propose a novel \\textit{Residual\nAlignment Model} (\\textit{RAM}) that formalizes the alignment process as a type\nof importance sampling. In this framework, the unaligned upstream model serves\nas the proposal distribution, while the alignment process is framed as\nsecondary sampling based on an autoregressive alignment module that acts as an\nestimator of the importance weights. This design enables a natural detachment\nof the alignment module from the target aligned model, improving flexibility\nand scalability. Based on this model, we derive an efficient sequence-level\ntraining strategy for the alignment module, which operates independently of the\nproposal module. Additionally, we develop a resampling algorithm with iterative\ntoken-level decoding to address the common first-token latency issue in\ncomparable methods. Experimental evaluations on two leading open-source LLMs\nacross diverse tasks, including instruction following, domain adaptation, and\npreference optimization, demonstrate that our approach consistently outperforms\nbaseline models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of large language models (LLMs) across industries has\nincreased the demand for high-quality and customizable outputs. However,\ntraditional alignment methods often require retraining large pretrained models,\nmaking it difficult to quickly adapt and optimize LLMs for diverse\napplications. To address this limitation, we propose a novel \\textit{Residual\nAlignment Model} (\\textit{RAM}) that formalizes the alignment process as a type\nof importance sampling. In this framework, the unaligned upstream model serves\nas the proposal distribution, while the alignment process is framed as\nsecondary sampling based on an autoregressive alignment module that acts as an\nestimator of the importance weights. This design enables a natural detachment\nof the alignment module from the target aligned model, improving flexibility\nand scalability. Based on this model, we derive an efficient sequence-level\ntraining strategy for the alignment module, which operates independently of the\nproposal module. Additionally, we develop a resampling algorithm with iterative\ntoken-level decoding to address the common first-token latency issue in\ncomparable methods. Experimental evaluations on two leading open-source LLMs\nacross diverse tasks, including instruction following, domain adaptation, and\npreference optimization, demonstrate that our approach consistently outperforms\nbaseline models."
                },
                "authors": [
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Dianqing Liu"
                    },
                    {
                        "name": "Mingye Zhu"
                    },
                    {
                        "name": "Junbo Guo"
                    },
                    {
                        "name": "Yongdong Zhang"
                    },
                    {
                        "name": "Zhendong Mao"
                    }
                ],
                "author_detail": {
                    "name": "Zhendong Mao"
                },
                "author": "Zhendong Mao",
                "arxiv_comment": "Accepted by NeurIPS 2025, 28 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19700v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19700v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10446v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10446v2",
                "updated": "2025-10-14T13:54:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    54,
                    24,
                    1,
                    287,
                    0
                ],
                "published": "2025-09-12T17:52:35Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    17,
                    52,
                    35,
                    4,
                    255,
                    0
                ],
                "title": "DeepDive: Advancing Deep Search Agents with Knowledge Graphs and\n  Multi-Turn RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepDive: Advancing Deep Search Agents with Knowledge Graphs and\n  Multi-Turn RL"
                },
                "summary": "Augmenting large language models (LLMs) with browsing tools substantially\nimproves their potential as deep search agents to solve complex, real-world\ntasks. Yet, open LLMs still perform poorly in such settings due to limited\nlong-horizon reasoning capacity with browsing tools and the lack of\nsufficiently difficult supervised data. To address these challenges, we present\nDeepDive to advance deep search agents. First, we propose a strategy to\nautomatically synthesize complex, difficult, and hard-to-find questions from\nopen knowledge graphs. Second, we apply end-to-end multi-turn reinforcement\nlearning (RL) to enhance LLMs' long-horizon reasoning with deep search. To\nencourage diversity and reduce redundancy, we design a redundancy penalty that\ndiscourages repeated similar queries. Experiments show that DeepDive-32B\nachieves a new open-source competitive result on BrowseComp, outperforming\nWebSailor, DeepSeek-R1-Browse, and Search-o1. We demonstrate that multi-turn RL\ntraining improves deep search ability and significantly contributes to the\nperformance improvements across multiple benchmarks. We observe that DeepDive\nenables test-time scaling of tool calls and parallel sampling. All datasets,\nmodels, and code are publicly available at https://github.com/THUDM/DeepDive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmenting large language models (LLMs) with browsing tools substantially\nimproves their potential as deep search agents to solve complex, real-world\ntasks. Yet, open LLMs still perform poorly in such settings due to limited\nlong-horizon reasoning capacity with browsing tools and the lack of\nsufficiently difficult supervised data. To address these challenges, we present\nDeepDive to advance deep search agents. First, we propose a strategy to\nautomatically synthesize complex, difficult, and hard-to-find questions from\nopen knowledge graphs. Second, we apply end-to-end multi-turn reinforcement\nlearning (RL) to enhance LLMs' long-horizon reasoning with deep search. To\nencourage diversity and reduce redundancy, we design a redundancy penalty that\ndiscourages repeated similar queries. Experiments show that DeepDive-32B\nachieves a new open-source competitive result on BrowseComp, outperforming\nWebSailor, DeepSeek-R1-Browse, and Search-o1. We demonstrate that multi-turn RL\ntraining improves deep search ability and significantly contributes to the\nperformance improvements across multiple benchmarks. We observe that DeepDive\nenables test-time scaling of tool calls and parallel sampling. All datasets,\nmodels, and code are publicly available at https://github.com/THUDM/DeepDive."
                },
                "authors": [
                    {
                        "name": "Rui Lu"
                    },
                    {
                        "name": "Zhenyu Hou"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Hanchen Zhang"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Yujiang Li"
                    },
                    {
                        "name": "Shi Feng"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Yuxiao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiao Dong"
                },
                "author": "Yuxiao Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10446v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10446v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12516v1",
                "updated": "2025-10-14T13:43:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    43,
                    8,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T13:43:08Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    43,
                    8,
                    1,
                    287,
                    0
                ],
                "title": "BoN Appetit Team at LeWiDi-2025: Best-of-N Test-time Scaling Can Not\n  Stomach Annotation Disagreements (Yet)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BoN Appetit Team at LeWiDi-2025: Best-of-N Test-time Scaling Can Not\n  Stomach Annotation Disagreements (Yet)"
                },
                "summary": "Test-time scaling is a family of techniques to improve LLM outputs at\ninference time by performing extra computation. To the best of our knowledge,\ntest-time scaling has been limited to domains with verifiably correct answers,\nlike mathematics and coding. We transfer test-time scaling to the LeWiDi-2025\ntasks to evaluate annotation disagreements. We experiment with three test-time\nscaling methods: two benchmark algorithms (Model Averaging and Majority\nVoting), and a Best-of-N sampling method. The two benchmark methods improve LLM\nperformance consistently on the LeWiDi tasks, but the Best-of-N method does\nnot. Our experiments suggest that the Best-of-N method does not currently\ntransfer from mathematics to LeWiDi tasks, and we analyze potential reasons for\nthis gap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling is a family of techniques to improve LLM outputs at\ninference time by performing extra computation. To the best of our knowledge,\ntest-time scaling has been limited to domains with verifiably correct answers,\nlike mathematics and coding. We transfer test-time scaling to the LeWiDi-2025\ntasks to evaluate annotation disagreements. We experiment with three test-time\nscaling methods: two benchmark algorithms (Model Averaging and Majority\nVoting), and a Best-of-N sampling method. The two benchmark methods improve LLM\nperformance consistently on the LeWiDi tasks, but the Best-of-N method does\nnot. Our experiments suggest that the Best-of-N method does not currently\ntransfer from mathematics to LeWiDi tasks, and we analyze potential reasons for\nthis gap."
                },
                "authors": [
                    {
                        "name": "Tomas Ruiz"
                    },
                    {
                        "name": "Siyao Peng"
                    },
                    {
                        "name": "Barbara Plank"
                    },
                    {
                        "name": "Carsten Schwemmer"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Schwemmer"
                },
                "author": "Carsten Schwemmer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12504v1",
                "updated": "2025-10-14T13:33:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    33,
                    29,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T13:33:29Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    33,
                    29,
                    1,
                    287,
                    0
                ],
                "title": "Causal inference of post-transcriptional regulation timelines from\n  long-read sequencing in Arabidopsis thaliana",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal inference of post-transcriptional regulation timelines from\n  long-read sequencing in Arabidopsis thaliana"
                },
                "summary": "We propose a novel framework for reconstructing the chronology of genetic\nregulation using causal inference based on Pearl's theory. The approach\nproceeds in three main stages: causal discovery, causal inference, and\nchronology construction. We apply it to the ndhB and ndhD genes of the\nchloroplast in Arabidopsis thaliana, generating four alternative maturation\ntimeline models per gene, each derived from a different causal discovery\nalgorithm (HC, PC, LiNGAM, or NOTEARS). Two methodological challenges are\naddressed: the presence of missing data, handled via an EM algorithm that\njointly imputes missing values and estimates the Bayesian network, and the\nselection of the $\\ell_1$-regularization parameter in NOTEARS, for which we\nintroduce a stability selection strategy. The resulting causal models\nconsistently outperform reference chronologies in terms of both reliability and\nmodel fit. Moreover, by combining causal reasoning with domain expertise, the\nframework enables the formulation of testable hypotheses and the design of\ntargeted experimental interventions grounded in theoretical predictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel framework for reconstructing the chronology of genetic\nregulation using causal inference based on Pearl's theory. The approach\nproceeds in three main stages: causal discovery, causal inference, and\nchronology construction. We apply it to the ndhB and ndhD genes of the\nchloroplast in Arabidopsis thaliana, generating four alternative maturation\ntimeline models per gene, each derived from a different causal discovery\nalgorithm (HC, PC, LiNGAM, or NOTEARS). Two methodological challenges are\naddressed: the presence of missing data, handled via an EM algorithm that\njointly imputes missing values and estimates the Bayesian network, and the\nselection of the $\\ell_1$-regularization parameter in NOTEARS, for which we\nintroduce a stability selection strategy. The resulting causal models\nconsistently outperform reference chronologies in terms of both reliability and\nmodel fit. Moreover, by combining causal reasoning with domain expertise, the\nframework enables the formulation of testable hypotheses and the design of\ntargeted experimental interventions grounded in theoretical predictions."
                },
                "authors": [
                    {
                        "name": "Rubn Martos"
                    },
                    {
                        "name": "Christophe Ambroise"
                    },
                    {
                        "name": "Guillem Rigaill"
                    }
                ],
                "author_detail": {
                    "name": "Guillem Rigaill"
                },
                "author": "Guillem Rigaill",
                "arxiv_comment": "25 pages. GitHub repository at\n  https://github.com/rmartosprieto/chloroDAG.git",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12503v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12503v1",
                "updated": "2025-10-14T13:33:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    33,
                    6,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T13:33:06Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    33,
                    6,
                    1,
                    287,
                    0
                ],
                "title": "The Robustness of Differentiable Causal Discovery in Misspecified\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Robustness of Differentiable Causal Discovery in Misspecified\n  Scenarios"
                },
                "summary": "Causal discovery aims to learn causal relationships between variables from\ntargeted data, making it a fundamental task in machine learning. However,\ncausal discovery algorithms often rely on unverifiable causal assumptions,\nwhich are usually difficult to satisfy in real-world data, thereby limiting the\nbroad application of causal discovery in practical scenarios. Inspired by these\nconsiderations, this work extensively benchmarks the empirical performance of\nvarious mainstream causal discovery algorithms, which assume i.i.d. data, under\neight model assumption violations. Our experimental results show that\ndifferentiable causal discovery methods exhibit robustness under the metrics of\nStructural Hamming Distance and Structural Intervention Distance of the\ninferred graphs in commonly used challenging scenarios, except for scale\nvariation. We also provide the theoretical explanations for the performance of\ndifferentiable causal discovery methods. Finally, our work aims to\ncomprehensively benchmark the performance of recent differentiable causal\ndiscovery methods under model assumption violations, and provide the standard\nfor reasonable evaluation of causal discovery, as well as to further promote\nits application in real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal discovery aims to learn causal relationships between variables from\ntargeted data, making it a fundamental task in machine learning. However,\ncausal discovery algorithms often rely on unverifiable causal assumptions,\nwhich are usually difficult to satisfy in real-world data, thereby limiting the\nbroad application of causal discovery in practical scenarios. Inspired by these\nconsiderations, this work extensively benchmarks the empirical performance of\nvarious mainstream causal discovery algorithms, which assume i.i.d. data, under\neight model assumption violations. Our experimental results show that\ndifferentiable causal discovery methods exhibit robustness under the metrics of\nStructural Hamming Distance and Structural Intervention Distance of the\ninferred graphs in commonly used challenging scenarios, except for scale\nvariation. We also provide the theoretical explanations for the performance of\ndifferentiable causal discovery methods. Finally, our work aims to\ncomprehensively benchmark the performance of recent differentiable causal\ndiscovery methods under model assumption violations, and provide the standard\nfor reasonable evaluation of causal discovery, as well as to further promote\nits application in real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Huiyang Yi"
                    },
                    {
                        "name": "Yanyan He"
                    },
                    {
                        "name": "Duxin Chen"
                    },
                    {
                        "name": "Mingyu Kang"
                    },
                    {
                        "name": "He Wang"
                    },
                    {
                        "name": "Wenwu Yu"
                    }
                ],
                "author_detail": {
                    "name": "Wenwu Yu"
                },
                "author": "Wenwu Yu",
                "arxiv_comment": "accepted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12503v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12503v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22058v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22058v2",
                "updated": "2025-10-14T13:32:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    32,
                    12,
                    1,
                    287,
                    0
                ],
                "published": "2025-06-27T09:53:57Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    53,
                    57,
                    4,
                    178,
                    0
                ],
                "title": "Lost at the Beginning of Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lost at the Beginning of Reasoning"
                },
                "summary": "Recent advancements in large language models (LLMs) have significantly\nadvanced complex reasoning capabilities, particularly through extended\nchain-of-thought (CoT) reasoning that incorporates mechanisms such as\nbacktracking, self-reflection, and self-correction. Despite these developments,\nthe self-correction abilities of LLMs during long CoT reasoning remain\nunderexplored. And recent findings on overthinking suggest that such models\noften engage in unnecessarily redundant reasoning. In this work, we empirically\nshow that the first reasoning step exerts a disproportionately large influence\non the final prediction. I.e., errors introduced at this stage can\nsubstantially degrade subsequent reasoning quality. This phenomenon is\nconsistently observed across various state-of-the-art open- and closed-source\nreasoning models. Leveraging this insight, we propose an efficient sampling\nstrategy that leverages a reward model to identify and retain high-quality\nfirst reasoning steps while discarding suboptimal ones, achieving up to a 70%\nreduction in inference cost without sacrificing any accuracy. Our work\nhighlights the central role of the first reasoning step in generating a\nhigh-quality reasoning trajectory, and thus enabling significantly efficient\nsampling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have significantly\nadvanced complex reasoning capabilities, particularly through extended\nchain-of-thought (CoT) reasoning that incorporates mechanisms such as\nbacktracking, self-reflection, and self-correction. Despite these developments,\nthe self-correction abilities of LLMs during long CoT reasoning remain\nunderexplored. And recent findings on overthinking suggest that such models\noften engage in unnecessarily redundant reasoning. In this work, we empirically\nshow that the first reasoning step exerts a disproportionately large influence\non the final prediction. I.e., errors introduced at this stage can\nsubstantially degrade subsequent reasoning quality. This phenomenon is\nconsistently observed across various state-of-the-art open- and closed-source\nreasoning models. Leveraging this insight, we propose an efficient sampling\nstrategy that leverages a reward model to identify and retain high-quality\nfirst reasoning steps while discarding suboptimal ones, achieving up to a 70%\nreduction in inference cost without sacrificing any accuracy. Our work\nhighlights the central role of the first reasoning step in generating a\nhigh-quality reasoning trajectory, and thus enabling significantly efficient\nsampling."
                },
                "authors": [
                    {
                        "name": "Baohao Liao"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Sara Rajaee"
                    },
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Christian Herold"
                    },
                    {
                        "name": "Anders Sgaard"
                    },
                    {
                        "name": "Maarten de Rijke"
                    },
                    {
                        "name": "Christof Monz"
                    }
                ],
                "author_detail": {
                    "name": "Christof Monz"
                },
                "author": "Christof Monz",
                "arxiv_comment": "remove the benchmark part. (10 pages, 6 figures, 5 tables)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22058v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22058v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12495v1",
                "updated": "2025-10-14T13:28:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    28,
                    57,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T13:28:57Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    28,
                    57,
                    1,
                    287,
                    0
                ],
                "title": "Limits of Standard Tidal Models at Quaoar: Matching Weywot's Orbit,\n  Missing the Spin",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limits of Standard Tidal Models at Quaoar: Matching Weywot's Orbit,\n  Missing the Spin"
                },
                "summary": "Weywot, Quaoar's small satellite, follows a nearly circular orbit at a\ndistance of 12.9 times Quaoar's diameter and coexists with a compact ring\nsystem. Nevertheless, Quaoar's flattening of 0.16, slow 17.7hr rotation and\nWeywot's low mass are difficult to reconcile with conventional tidal-evolution\ntheory. We assess whether standard tides can reproduce the present-day\narchitecture of the Quaoar-Weywot system and identify the initial conditions\nrequired. Orbit-averaged integrations spanning 4.5Gyr were carried out with two\nformalisms: (i) a constant phase-lag (CPL) and (ii) an Andrade creep-tide (ACT)\nframework. With the nominal Weywot mass, both tidal prescriptions converge on\nWeywot's observed orbital distance for a wide range of initial orbital\ndistances and eccentricities; eccentricity is damped and present-day tidal\ntorques are negligible, rendering the orbit quasi-stationary. Quaoar's spin,\nhowever, remains essentially unchanged from its inferred primordial period\nbased on its present-day flattening, and does not reproduce the observed value.\nA match is possible only if Weywot is 5-10x more massive than current estimates\nand if its initial eccentricity is finely tuned; such scenarios are\ninconsistent with occultation-derived masses and imply an implausibly dense\nsatellite. Based on the best fitting viscoelastic parameters, the most\nplausible composition for Quaoar is found to be a partially differentiated\ndwarf planet containing roughly equal masses of silicate rock and H2O-dominated\nwarm (150-180K) ices. Standard tidal models reproduce Weywot's semimajor axis\nbut cannot account for Quaoar's slow 17.7hr rotation without invoking an\nunrealistically massive satellite or external torques, suggesting that\nnon-tidal processes - such as a largely primordial spin, early satellite loss,\nor a retrograde secondary giant impact - must have influenced Quaoar's\nrotational evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weywot, Quaoar's small satellite, follows a nearly circular orbit at a\ndistance of 12.9 times Quaoar's diameter and coexists with a compact ring\nsystem. Nevertheless, Quaoar's flattening of 0.16, slow 17.7hr rotation and\nWeywot's low mass are difficult to reconcile with conventional tidal-evolution\ntheory. We assess whether standard tides can reproduce the present-day\narchitecture of the Quaoar-Weywot system and identify the initial conditions\nrequired. Orbit-averaged integrations spanning 4.5Gyr were carried out with two\nformalisms: (i) a constant phase-lag (CPL) and (ii) an Andrade creep-tide (ACT)\nframework. With the nominal Weywot mass, both tidal prescriptions converge on\nWeywot's observed orbital distance for a wide range of initial orbital\ndistances and eccentricities; eccentricity is damped and present-day tidal\ntorques are negligible, rendering the orbit quasi-stationary. Quaoar's spin,\nhowever, remains essentially unchanged from its inferred primordial period\nbased on its present-day flattening, and does not reproduce the observed value.\nA match is possible only if Weywot is 5-10x more massive than current estimates\nand if its initial eccentricity is finely tuned; such scenarios are\ninconsistent with occultation-derived masses and imply an implausibly dense\nsatellite. Based on the best fitting viscoelastic parameters, the most\nplausible composition for Quaoar is found to be a partially differentiated\ndwarf planet containing roughly equal masses of silicate rock and H2O-dominated\nwarm (150-180K) ices. Standard tidal models reproduce Weywot's semimajor axis\nbut cannot account for Quaoar's slow 17.7hr rotation without invoking an\nunrealistically massive satellite or external torques, suggesting that\nnon-tidal processes - such as a largely primordial spin, early satellite loss,\nor a retrograde secondary giant impact - must have influenced Quaoar's\nrotational evolution."
                },
                "authors": [
                    {
                        "name": "Zsolt Regaly"
                    },
                    {
                        "name": "Viktoria Frohlich ang Csaba Kiss"
                    }
                ],
                "author_detail": {
                    "name": "Viktoria Frohlich ang Csaba Kiss"
                },
                "author": "Viktoria Frohlich ang Csaba Kiss",
                "arxiv_comment": "Accepted for publication in PASP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12490v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12490v1",
                "updated": "2025-10-14T13:24:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    24,
                    21,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T13:24:21Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    24,
                    21,
                    1,
                    287,
                    0
                ],
                "title": "Using Medical Algorithms for Task-Oriented Dialogue in LLM-Based Medical\n  Interviews",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Medical Algorithms for Task-Oriented Dialogue in LLM-Based Medical\n  Interviews"
                },
                "summary": "We developed a task-oriented dialogue framework structured as a Directed\nAcyclic Graph (DAG) of medical questions. The system integrates: (1) a\nsystematic pipeline for transforming medical algorithms and guidelines into a\nclinical question corpus; (2) a cold-start mechanism based on hierarchical\nclustering to generate efficient initial questioning without prior patient\ninformation; (3) an expand-and-prune mechanism enabling adaptive branching and\nbacktracking based on patient responses; (4) a termination logic to ensure\ninterviews end once sufficient information is gathered; and (5) automated\nsynthesis of doctor-friendly structured reports aligned with clinical\nworkflows. Human-computer interaction principles guided the design of both the\npatient and physician applications. Preliminary evaluation involved five\nphysicians using standardized instruments: NASA-TLX (cognitive workload), the\nSystem Usability Scale (SUS), and the Questionnaire for User Interface\nSatisfaction (QUIS). The patient application achieved low workload scores\n(NASA-TLX = 15.6), high usability (SUS = 86), and strong satisfaction (QUIS =\n8.1/9), with particularly high ratings for ease of learning and interface\ndesign. The physician application yielded moderate workload (NASA-TLX = 26) and\nexcellent usability (SUS = 88.5), with satisfaction scores of 8.3/9. Both\napplications demonstrated effective integration into clinical workflows,\nreducing cognitive demand and supporting efficient report generation.\nLimitations included occasional system latency and a small, non-diverse\nevaluation sample.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We developed a task-oriented dialogue framework structured as a Directed\nAcyclic Graph (DAG) of medical questions. The system integrates: (1) a\nsystematic pipeline for transforming medical algorithms and guidelines into a\nclinical question corpus; (2) a cold-start mechanism based on hierarchical\nclustering to generate efficient initial questioning without prior patient\ninformation; (3) an expand-and-prune mechanism enabling adaptive branching and\nbacktracking based on patient responses; (4) a termination logic to ensure\ninterviews end once sufficient information is gathered; and (5) automated\nsynthesis of doctor-friendly structured reports aligned with clinical\nworkflows. Human-computer interaction principles guided the design of both the\npatient and physician applications. Preliminary evaluation involved five\nphysicians using standardized instruments: NASA-TLX (cognitive workload), the\nSystem Usability Scale (SUS), and the Questionnaire for User Interface\nSatisfaction (QUIS). The patient application achieved low workload scores\n(NASA-TLX = 15.6), high usability (SUS = 86), and strong satisfaction (QUIS =\n8.1/9), with particularly high ratings for ease of learning and interface\ndesign. The physician application yielded moderate workload (NASA-TLX = 26) and\nexcellent usability (SUS = 88.5), with satisfaction scores of 8.3/9. Both\napplications demonstrated effective integration into clinical workflows,\nreducing cognitive demand and supporting efficient report generation.\nLimitations included occasional system latency and a small, non-diverse\nevaluation sample."
                },
                "authors": [
                    {
                        "name": "Rui Reis"
                    },
                    {
                        "name": "Pedro Rangel Henriques"
                    },
                    {
                        "name": "Joo Ferreira-Coimbra"
                    },
                    {
                        "name": "Eva Oliveira"
                    },
                    {
                        "name": "Nuno F. Rodrigues"
                    }
                ],
                "author_detail": {
                    "name": "Nuno F. Rodrigues"
                },
                "author": "Nuno F. Rodrigues",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12490v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12490v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12487v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12487v1",
                "updated": "2025-10-14T13:23:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    23,
                    1,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T13:23:01Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    23,
                    1,
                    1,
                    287,
                    0
                ],
                "title": "Diff-XYZ: A Benchmark for Evaluating Diff Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diff-XYZ: A Benchmark for Evaluating Diff Understanding"
                },
                "summary": "Reliable handling of code diffs is central to agents that edit and refactor\nrepositories at scale. We introduce Diff-XYZ, a compact benchmark for code-diff\nunderstanding with three supervised tasks: apply (old code $+$ diff\n$\\rightarrow$ new code), anti-apply (new code $-$ diff $\\rightarrow$ old code),\nand diff generation (new code $-$ old code $\\rightarrow$ diff). Instances in\nthe benchmark are triples $\\langle \\textit{old code}, \\textit{new code},\n\\textit{diff} \\rangle$ drawn from real commits in CommitPackFT, paired with\nautomatic metrics and a clear evaluation protocol. We use the benchmark to do a\nfocused empirical study of the unified diff format and run a cross-format\ncomparison of different diff representations. Our findings reveal that\ndifferent formats should be used depending on the use case and model size. For\nexample, representing diffs in search-replace format is good for larger models\nin the diff generation scenario, yet not suited well for diff analysis and\nsmaller models. The Diff-XYZ benchmark is a reusable foundation for assessing\nand improving diff handling in LLMs that can aid future development of diff\nformats and models editing code. The dataset is published on HuggingFace Hub:\nhttps://huggingface.co/datasets/JetBrains-Research/diff-xyz.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable handling of code diffs is central to agents that edit and refactor\nrepositories at scale. We introduce Diff-XYZ, a compact benchmark for code-diff\nunderstanding with three supervised tasks: apply (old code $+$ diff\n$\\rightarrow$ new code), anti-apply (new code $-$ diff $\\rightarrow$ old code),\nand diff generation (new code $-$ old code $\\rightarrow$ diff). Instances in\nthe benchmark are triples $\\langle \\textit{old code}, \\textit{new code},\n\\textit{diff} \\rangle$ drawn from real commits in CommitPackFT, paired with\nautomatic metrics and a clear evaluation protocol. We use the benchmark to do a\nfocused empirical study of the unified diff format and run a cross-format\ncomparison of different diff representations. Our findings reveal that\ndifferent formats should be used depending on the use case and model size. For\nexample, representing diffs in search-replace format is good for larger models\nin the diff generation scenario, yet not suited well for diff analysis and\nsmaller models. The Diff-XYZ benchmark is a reusable foundation for assessing\nand improving diff handling in LLMs that can aid future development of diff\nformats and models editing code. The dataset is published on HuggingFace Hub:\nhttps://huggingface.co/datasets/JetBrains-Research/diff-xyz."
                },
                "authors": [
                    {
                        "name": "Evgeniy Glukhov"
                    },
                    {
                        "name": "Michele Conti"
                    },
                    {
                        "name": "Egor Bogomolov"
                    },
                    {
                        "name": "Yaroslav Golubev"
                    },
                    {
                        "name": "Alexander Bezzubov"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Bezzubov"
                },
                "author": "Alexander Bezzubov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12487v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12487v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12483v1",
                "updated": "2025-10-14T13:18:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    18,
                    45,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T13:18:45Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    18,
                    45,
                    1,
                    287,
                    0
                ],
                "title": "Fast Visuomotor Policy for Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Visuomotor Policy for Robotic Manipulation"
                },
                "summary": "We present a fast and effective policy framework for robotic manipulation,\nnamed Energy Policy, designed for high-frequency robotic tasks and\nresource-constrained systems. Unlike existing robotic policies, Energy Policy\nnatively predicts multimodal actions in a single forward pass, enabling\nhigh-precision manipulation at high speed. The framework is built upon two core\ncomponents. First, we adopt the energy score as the learning objective to\nfacilitate multimodal action modeling. Second, we introduce an energy MLP to\nimplement the proposed objective while keeping the architecture simple and\nefficient. We conduct comprehensive experiments in both simulated environments\nand real-world robotic tasks to evaluate the effectiveness of Energy Policy.\nThe results show that Energy Policy matches or surpasses the performance of\nstate-of-the-art manipulation methods while significantly reducing\ncomputational overhead. Notably, on the MimicGen benchmark, Energy Policy\nachieves superior performance with at a faster inference compared to existing\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a fast and effective policy framework for robotic manipulation,\nnamed Energy Policy, designed for high-frequency robotic tasks and\nresource-constrained systems. Unlike existing robotic policies, Energy Policy\nnatively predicts multimodal actions in a single forward pass, enabling\nhigh-precision manipulation at high speed. The framework is built upon two core\ncomponents. First, we adopt the energy score as the learning objective to\nfacilitate multimodal action modeling. Second, we introduce an energy MLP to\nimplement the proposed objective while keeping the architecture simple and\nefficient. We conduct comprehensive experiments in both simulated environments\nand real-world robotic tasks to evaluate the effectiveness of Energy Policy.\nThe results show that Energy Policy matches or surpasses the performance of\nstate-of-the-art manipulation methods while significantly reducing\ncomputational overhead. Notably, on the MimicGen benchmark, Energy Policy\nachieves superior performance with at a faster inference compared to existing\napproaches."
                },
                "authors": [
                    {
                        "name": "Jingkai Jia"
                    },
                    {
                        "name": "Tong Yang"
                    },
                    {
                        "name": "Xueyao Chen"
                    },
                    {
                        "name": "Chenhuan Liu"
                    },
                    {
                        "name": "Wenqiang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenqiang Zhang"
                },
                "author": "Wenqiang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02038v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02038v2",
                "updated": "2025-10-14T13:16:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    16,
                    57,
                    1,
                    287,
                    0
                ],
                "published": "2025-03-03T20:30:22Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    20,
                    30,
                    22,
                    0,
                    62,
                    0
                ],
                "title": "Persuasion at Play: Understanding Misinformation Dynamics in\n  Demographic-Aware Human-LLM Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persuasion at Play: Understanding Misinformation Dynamics in\n  Demographic-Aware Human-LLM Interactions"
                },
                "summary": "Existing challenges in misinformation exposure and susceptibility vary across\ndemographic groups, as some populations are more vulnerable to misinformation\nthan others. Large language models (LLMs) introduce new dimensions to these\nchallenges through their ability to generate persuasive content at scale and\nreinforcing existing biases. This study investigates the bidirectional\npersuasion dynamics between LLMs and humans when exposed to misinformative\ncontent. We analyze human-to-LLM influence using human-stance datasets and\nassess LLM-to-human influence by generating LLM-based persuasive arguments.\nAdditionally, we use a multi-agent LLM framework to analyze the spread of\nmisinformation under persuasion among demographic-oriented LLM agents. Our\nfindings show that demographic factors influence susceptibility to\nmisinformation in LLMs, closely reflecting the demographic-based patterns seen\nin human susceptibility. We also find that, similar to human demographic\ngroups, multi-agent LLMs exhibit echo chamber behavior. This research explores\nthe interplay between humans and LLMs, highlighting demographic differences in\nthe context of misinformation and offering insights for future interventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing challenges in misinformation exposure and susceptibility vary across\ndemographic groups, as some populations are more vulnerable to misinformation\nthan others. Large language models (LLMs) introduce new dimensions to these\nchallenges through their ability to generate persuasive content at scale and\nreinforcing existing biases. This study investigates the bidirectional\npersuasion dynamics between LLMs and humans when exposed to misinformative\ncontent. We analyze human-to-LLM influence using human-stance datasets and\nassess LLM-to-human influence by generating LLM-based persuasive arguments.\nAdditionally, we use a multi-agent LLM framework to analyze the spread of\nmisinformation under persuasion among demographic-oriented LLM agents. Our\nfindings show that demographic factors influence susceptibility to\nmisinformation in LLMs, closely reflecting the demographic-based patterns seen\nin human susceptibility. We also find that, similar to human demographic\ngroups, multi-agent LLMs exhibit echo chamber behavior. This research explores\nthe interplay between humans and LLMs, highlighting demographic differences in\nthe context of misinformation and offering insights for future interventions."
                },
                "authors": [
                    {
                        "name": "Angana Borah"
                    },
                    {
                        "name": "Rada Mihalcea"
                    },
                    {
                        "name": "Vernica Prez-Rosas"
                    }
                ],
                "author_detail": {
                    "name": "Vernica Prez-Rosas"
                },
                "author": "Vernica Prez-Rosas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02038v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02038v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12434v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12434v4",
                "updated": "2025-10-14T13:13:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    13,
                    18,
                    1,
                    287,
                    0
                ],
                "published": "2025-05-18T14:14:35Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    14,
                    14,
                    35,
                    6,
                    138,
                    0
                ],
                "title": "VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via\n  Reinforced Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via\n  Reinforced Fine-Tuning"
                },
                "summary": "Reinforcement fine-tuning (RFT) has shown great promise in achieving\nhumanlevel reasoning capabilities of Large Language Models (LLMs), and has\nrecently been extended to MLLMs. Nevertheless, reasoning about videos, which is\na fundamental aspect of human intelligence, remains a persistent challenge due\nto the complex logic, temporal and causal structures inherent in video data. To\nfill this gap, we propose VideoRFT, a novel approach that extends the RFT\nparadigm to cultivate human-like video reasoning capabilities in MLLMs.\nVideoRFT follows the standard two-stage scheme in RFT: supervised fine-tuning\n(SFT) with chain-of-thought (CoT) annotations, followed by reinforcement\nlearning (RL) to improve generalization. A central challenge to achieve this in\nthe video domain lies in the scarcity of large-scale, high-quality video CoT\ndatasets. We address this by building a multi-expert-driven, cognition-inspired\nCoT curation pipeline. First, we devise a cognition-inspired prompting strategy\nto elicit a reasoning LLM to generate preliminary CoTs based solely on rich,\nstructured, and literal representations of video content. Subsequently, these\nCoTs are revised by a MLLM conditioned on the actual video, ensuring visual\nconsistency and reducing visual hallucinations. This pipeline results in two\nnew datasets, i.e.VideoRFT-CoT-102K for SFT and VideoRFT-RL-310K for RL. To\nfurther strengthen the RL phase, we introduce a novel semantic-consistency\nreward that explicitly promotes the alignment between textual reasoning and\nvisual evidence. This reward encourages the model to produce coherent,\ncontext-aware reasoning outputs grounded in visual input. Extensive experiments\nshow that VideoRFT achieves state-of-the-art performance on six video reasoning\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement fine-tuning (RFT) has shown great promise in achieving\nhumanlevel reasoning capabilities of Large Language Models (LLMs), and has\nrecently been extended to MLLMs. Nevertheless, reasoning about videos, which is\na fundamental aspect of human intelligence, remains a persistent challenge due\nto the complex logic, temporal and causal structures inherent in video data. To\nfill this gap, we propose VideoRFT, a novel approach that extends the RFT\nparadigm to cultivate human-like video reasoning capabilities in MLLMs.\nVideoRFT follows the standard two-stage scheme in RFT: supervised fine-tuning\n(SFT) with chain-of-thought (CoT) annotations, followed by reinforcement\nlearning (RL) to improve generalization. A central challenge to achieve this in\nthe video domain lies in the scarcity of large-scale, high-quality video CoT\ndatasets. We address this by building a multi-expert-driven, cognition-inspired\nCoT curation pipeline. First, we devise a cognition-inspired prompting strategy\nto elicit a reasoning LLM to generate preliminary CoTs based solely on rich,\nstructured, and literal representations of video content. Subsequently, these\nCoTs are revised by a MLLM conditioned on the actual video, ensuring visual\nconsistency and reducing visual hallucinations. This pipeline results in two\nnew datasets, i.e.VideoRFT-CoT-102K for SFT and VideoRFT-RL-310K for RL. To\nfurther strengthen the RL phase, we introduce a novel semantic-consistency\nreward that explicitly promotes the alignment between textual reasoning and\nvisual evidence. This reward encourages the model to produce coherent,\ncontext-aware reasoning outputs grounded in visual input. Extensive experiments\nshow that VideoRFT achieves state-of-the-art performance on six video reasoning\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Yanrui Yu"
                    },
                    {
                        "name": "Ye Yuan"
                    },
                    {
                        "name": "Rui Mao"
                    },
                    {
                        "name": "Tianfei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Tianfei Zhou"
                },
                "author": "Tianfei Zhou",
                "arxiv_comment": "Accepted by NeurIPS 2025. Code: https://github.com/QiWang98/VideoRFT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12434v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12434v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12476v1",
                "updated": "2025-10-14T13:10:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    10,
                    23,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T13:10:23Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    10,
                    23,
                    1,
                    287,
                    0
                ],
                "title": "When Personalization Tricks Detectors: The Feature-Inversion Trap in\n  Machine-Generated Text Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Personalization Tricks Detectors: The Feature-Inversion Trap in\n  Machine-Generated Text Detection"
                },
                "summary": "Large language models (LLMs) have grown more powerful in language generation,\nproducing fluent text and even imitating personal style. Yet, this ability also\nheightens the risk of identity impersonation. To the best of our knowledge, no\nprior work has examined personalized machine-generated text (MGT) detection. In\nthis paper, we introduce \\dataset, the first benchmark for evaluating detector\nrobustness in personalized settings, built from literary and blog texts paired\nwith their LLM-generated imitations. Our experimental results demonstrate large\nperformance gaps across detectors in personalized settings: some\nstate-of-the-art models suffer significant drops. We attribute this limitation\nto the \\textit{feature-inversion trap}, where features that are discriminative\nin general domains become inverted and misleading when applied to personalized\ntext. Based on this finding, we propose \\method, a simple and reliable way to\npredict detector performance changes in personalized settings. \\method\nidentifies latent directions corresponding to inverted features and constructs\nprobe datasets that differ primarily along these features to evaluate detector\ndependence. Our experiments show that \\method can accurately predict both the\ndirection and the magnitude of post-transfer changes, showing 85\\% correlation\nwith the actual performance gaps. We hope that this work will encourage further\nresearch on personalized text detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have grown more powerful in language generation,\nproducing fluent text and even imitating personal style. Yet, this ability also\nheightens the risk of identity impersonation. To the best of our knowledge, no\nprior work has examined personalized machine-generated text (MGT) detection. In\nthis paper, we introduce \\dataset, the first benchmark for evaluating detector\nrobustness in personalized settings, built from literary and blog texts paired\nwith their LLM-generated imitations. Our experimental results demonstrate large\nperformance gaps across detectors in personalized settings: some\nstate-of-the-art models suffer significant drops. We attribute this limitation\nto the \\textit{feature-inversion trap}, where features that are discriminative\nin general domains become inverted and misleading when applied to personalized\ntext. Based on this finding, we propose \\method, a simple and reliable way to\npredict detector performance changes in personalized settings. \\method\nidentifies latent directions corresponding to inverted features and constructs\nprobe datasets that differ primarily along these features to evaluate detector\ndependence. Our experiments show that \\method can accurately predict both the\ndirection and the magnitude of post-transfer changes, showing 85\\% correlation\nwith the actual performance gaps. We hope that this work will encourage further\nresearch on personalized text detection."
                },
                "authors": [
                    {
                        "name": "Lang Gao"
                    },
                    {
                        "name": "Xuhui Li"
                    },
                    {
                        "name": "Chenxi Wang"
                    },
                    {
                        "name": "Mingzhe Li"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Zirui Song"
                    },
                    {
                        "name": "Jinghui Zhang"
                    },
                    {
                        "name": "Rui Yan"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Xiuying Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiuying Chen"
                },
                "author": "Xiuying Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12474v1",
                "updated": "2025-10-14T13:04:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    4,
                    22,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T13:04:22Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    4,
                    22,
                    1,
                    287,
                    0
                ],
                "title": "SMEC: Rethinking Matryoshka Representation Learning for Retrieval\n  Embedding Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SMEC: Rethinking Matryoshka Representation Learning for Retrieval\n  Embedding Compression"
                },
                "summary": "Large language models (LLMs) generate high-dimensional embeddings that\ncapture rich semantic and syntactic information. However, high-dimensional\nembeddings exacerbate computational complexity and storage requirements,\nthereby hindering practical deployment. To address these challenges, we propose\na novel training framework named Sequential Matryoshka Embedding Compression\n(SMEC). This framework introduces the Sequential Matryoshka Representation\nLearning(SMRL) method to mitigate gradient variance during training, the\nAdaptive Dimension Selection (ADS) module to reduce information degradation\nduring dimension pruning, and the Selectable Cross-batch Memory (S-XBM) module\nto enhance unsupervised learning between high- and low-dimensional embeddings.\nExperiments on image, text, and multimodal datasets demonstrate that SMEC\nachieves significant dimensionality reduction while maintaining performance.\nFor instance, on the BEIR dataset, our approach improves the performance of\ncompressed LLM2Vec embeddings (256 dimensions) by 1.1 points and 2.7 points\ncompared to the Matryoshka-Adaptor and Search-Adaptor models, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) generate high-dimensional embeddings that\ncapture rich semantic and syntactic information. However, high-dimensional\nembeddings exacerbate computational complexity and storage requirements,\nthereby hindering practical deployment. To address these challenges, we propose\na novel training framework named Sequential Matryoshka Embedding Compression\n(SMEC). This framework introduces the Sequential Matryoshka Representation\nLearning(SMRL) method to mitigate gradient variance during training, the\nAdaptive Dimension Selection (ADS) module to reduce information degradation\nduring dimension pruning, and the Selectable Cross-batch Memory (S-XBM) module\nto enhance unsupervised learning between high- and low-dimensional embeddings.\nExperiments on image, text, and multimodal datasets demonstrate that SMEC\nachieves significant dimensionality reduction while maintaining performance.\nFor instance, on the BEIR dataset, our approach improves the performance of\ncompressed LLM2Vec embeddings (256 dimensions) by 1.1 points and 2.7 points\ncompared to the Matryoshka-Adaptor and Search-Adaptor models, respectively."
                },
                "authors": [
                    {
                        "name": "Biao Zhang"
                    },
                    {
                        "name": "Lixin Chen"
                    },
                    {
                        "name": "Tong Liu"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "arxiv_comment": "Accepted by EMNLP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12470v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12470v1",
                "updated": "2025-10-14T13:02:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    2,
                    14,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T13:02:14Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    2,
                    14,
                    1,
                    287,
                    0
                ],
                "title": "Enhanced Localization of Dark Lensed Gravitational Wave Events Enables\n  Host Galaxy Identification and Precise Cosmological Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Localization of Dark Lensed Gravitational Wave Events Enables\n  Host Galaxy Identification and Precise Cosmological Inference"
                },
                "summary": "Lensed gravitational wave (GW) events are expected to be powerful new probes\nof cosmology, contingent on redshift measurement by electromagnetic\nobservations. Host galaxy identification is thus crucial but challenging due to\npoor localization by GW signal alone. In this paper, we show that the\nthird-generation ground-based GW detectors will detect a population of lensed\nevents with three or more detectable images (including the central one), each\narriving at distinct times and Earth locations in the space, forming an\neffective network that reduces the typical localization area to $\\sim0.01$\ndeg$^2$. For at least $90\\%$ (or $50\\%$) of these events, the localization\nimproves by more than a factor of $10$ (or $30$) comparing with unlensed cases.\nSuch precise localization and multiple-image detections enable robust\nhost-galaxy identification and, through lens modelling, further yield\nsub-arcsecond position. As ``dark lensed sirens\", these events become powerful\nprobes of cosmological parameters. Using simulated lensed compact-binary\nmergers, we show that two-year or longer observations with third-generation GW\ndetectors can measure the Hubble constant to $\\lesssim1$\\% precision via ``dark\nlensed sirens\" (even when relying solely on lensed stellar-mass binary black\nhole events), while simultaneously constraining other cosmological parameters.\nThis approach will provide an independent, complementary avenue for measuring\ncosmological parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lensed gravitational wave (GW) events are expected to be powerful new probes\nof cosmology, contingent on redshift measurement by electromagnetic\nobservations. Host galaxy identification is thus crucial but challenging due to\npoor localization by GW signal alone. In this paper, we show that the\nthird-generation ground-based GW detectors will detect a population of lensed\nevents with three or more detectable images (including the central one), each\narriving at distinct times and Earth locations in the space, forming an\neffective network that reduces the typical localization area to $\\sim0.01$\ndeg$^2$. For at least $90\\%$ (or $50\\%$) of these events, the localization\nimproves by more than a factor of $10$ (or $30$) comparing with unlensed cases.\nSuch precise localization and multiple-image detections enable robust\nhost-galaxy identification and, through lens modelling, further yield\nsub-arcsecond position. As ``dark lensed sirens\", these events become powerful\nprobes of cosmological parameters. Using simulated lensed compact-binary\nmergers, we show that two-year or longer observations with third-generation GW\ndetectors can measure the Hubble constant to $\\lesssim1$\\% precision via ``dark\nlensed sirens\" (even when relying solely on lensed stellar-mass binary black\nhole events), while simultaneously constraining other cosmological parameters.\nThis approach will provide an independent, complementary avenue for measuring\ncosmological parameters."
                },
                "authors": [
                    {
                        "name": "Zhiwei Chen"
                    },
                    {
                        "name": "Qingjuan Yu"
                    },
                    {
                        "name": "Youjun Lu"
                    },
                    {
                        "name": "Xiao Guo"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Guo"
                },
                "author": "Xiao Guo",
                "arxiv_comment": "23 pages, 8 figures, accepted for publication in Astrophysical\n  Journal Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12470v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12470v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01924v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01924v3",
                "updated": "2025-10-14T13:00:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    0,
                    7,
                    1,
                    287,
                    0
                ],
                "published": "2025-04-02T17:33:53Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    33,
                    53,
                    2,
                    92,
                    0
                ],
                "title": "Gen-C: Populating Virtual Worlds with Generative Crowds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gen-C: Populating Virtual Worlds with Generative Crowds"
                },
                "summary": "Over the past two decades, researchers have made significant steps in\nsimulating agent-based human crowds, yet most efforts remain focused on\nlow-level tasks such as collision avoidance, path following, and flocking.\nRealistic simulations, however, require modeling high-level behaviors that\nemerge from agents interacting with each other and with their environment over\ntime. We introduce Generative Crowds (Gen-C), a generative framework that\nproduces crowd scenarios capturing agent-agent and agent-environment\ninteractions, shaping coherent high-level crowd plans. To avoid the\nlabor-intensive process of collecting and annotating real crowd video data, we\nleverage large language models (LLMs) to bootstrap synthetic datasets of crowd\nscenarios. We propose a time-expanded graph representation, encoding actions,\ninteractions, and spatial context. Gen-C employs a dual Variational Graph\nAutoencoder (VGAE) architecture that jointly learns connectivity patterns and\nnode features conditioned on textual and structural signals, overcoming the\nlimitations of direct LLM generation to enable scalable, environment-aware\nmulti-agent crowd simulations. We demonstrate the effectiveness of Gen-C on\nscenarios with diverse behaviors such as a University Campus and a Train\nStation, showing that it generates heterogeneous crowds, coherent interactions,\nand high-level decision-making patterns consistent with real-world crowd\ndynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past two decades, researchers have made significant steps in\nsimulating agent-based human crowds, yet most efforts remain focused on\nlow-level tasks such as collision avoidance, path following, and flocking.\nRealistic simulations, however, require modeling high-level behaviors that\nemerge from agents interacting with each other and with their environment over\ntime. We introduce Generative Crowds (Gen-C), a generative framework that\nproduces crowd scenarios capturing agent-agent and agent-environment\ninteractions, shaping coherent high-level crowd plans. To avoid the\nlabor-intensive process of collecting and annotating real crowd video data, we\nleverage large language models (LLMs) to bootstrap synthetic datasets of crowd\nscenarios. We propose a time-expanded graph representation, encoding actions,\ninteractions, and spatial context. Gen-C employs a dual Variational Graph\nAutoencoder (VGAE) architecture that jointly learns connectivity patterns and\nnode features conditioned on textual and structural signals, overcoming the\nlimitations of direct LLM generation to enable scalable, environment-aware\nmulti-agent crowd simulations. We demonstrate the effectiveness of Gen-C on\nscenarios with diverse behaviors such as a University Campus and a Train\nStation, showing that it generates heterogeneous crowds, coherent interactions,\nand high-level decision-making patterns consistent with real-world crowd\ndynamics."
                },
                "authors": [
                    {
                        "name": "Andreas Panayiotou"
                    },
                    {
                        "name": "Panayiotis Charalambous"
                    },
                    {
                        "name": "Ioannis Karamouzas"
                    }
                ],
                "author_detail": {
                    "name": "Ioannis Karamouzas"
                },
                "author": "Ioannis Karamouzas",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01924v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01924v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12462v1",
                "updated": "2025-10-14T12:52:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    12,
                    52,
                    29,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T12:52:29Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    12,
                    52,
                    29,
                    1,
                    287,
                    0
                ],
                "title": "Evaluating and Mitigating LLM-as-a-judge Bias in Communication Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating and Mitigating LLM-as-a-judge Bias in Communication Systems"
                },
                "summary": "Large Language Models (LLMs) are increasingly being used to autonomously\nevaluate the quality of content in communication systems, e.g., to assess\nresponses in telecom customer support chatbots. However, the impartiality of\nthese AI \"judges\" is not guaranteed, and any biases in their evaluation\ncriteria could skew outcomes and undermine user trust. In this paper, we\nsystematically investigate judgment biases in two LLM-as-a-judge models (i.e.,\nGPT-Judge and JudgeLM) under the point-wise scoring setting, encompassing 11\ntypes of biases that cover both implicit and explicit forms. We observed that\nstate-of-the-art LLM judges demonstrate robustness to biased inputs, generally\nassigning them lower scores than the corresponding clean samples. Providing a\ndetailed scoring rubric further enhances this robustness. We further found that\nfine-tuning an LLM on high-scoring yet biased responses can significantly\ndegrade its performance, highlighting the risk of training on biased data. We\nalso discovered that the judged scores correlate with task difficulty: a\nchallenging dataset like GPQA yields lower average scores, whereas an\nopen-ended reasoning dataset (e.g., JudgeLM-val) sees higher average scores.\nFinally, we proposed four potential mitigation strategies to ensure fair and\nreliable AI judging in practical communication scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being used to autonomously\nevaluate the quality of content in communication systems, e.g., to assess\nresponses in telecom customer support chatbots. However, the impartiality of\nthese AI \"judges\" is not guaranteed, and any biases in their evaluation\ncriteria could skew outcomes and undermine user trust. In this paper, we\nsystematically investigate judgment biases in two LLM-as-a-judge models (i.e.,\nGPT-Judge and JudgeLM) under the point-wise scoring setting, encompassing 11\ntypes of biases that cover both implicit and explicit forms. We observed that\nstate-of-the-art LLM judges demonstrate robustness to biased inputs, generally\nassigning them lower scores than the corresponding clean samples. Providing a\ndetailed scoring rubric further enhances this robustness. We further found that\nfine-tuning an LLM on high-scoring yet biased responses can significantly\ndegrade its performance, highlighting the risk of training on biased data. We\nalso discovered that the judged scores correlate with task difficulty: a\nchallenging dataset like GPQA yields lower average scores, whereas an\nopen-ended reasoning dataset (e.g., JudgeLM-val) sees higher average scores.\nFinally, we proposed four potential mitigation strategies to ensure fair and\nreliable AI judging in practical communication scenarios."
                },
                "authors": [
                    {
                        "name": "Jiaxin Gao"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Yanwen Jia"
                    },
                    {
                        "name": "Xueluan Gong"
                    },
                    {
                        "name": "Kwok-Yan Lam"
                    },
                    {
                        "name": "Qian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qian Wang"
                },
                "author": "Qian Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03927v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03927v4",
                "updated": "2025-10-14T12:51:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    12,
                    51,
                    4,
                    1,
                    287,
                    0
                ],
                "published": "2025-01-07T16:43:51Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    16,
                    43,
                    51,
                    1,
                    7,
                    0
                ],
                "title": "Optimal Estimation of Temperature",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Estimation of Temperature"
                },
                "summary": "Temperature of a finite-sized system fluctuates due to the thermal\nfluctuations. However, a systematic mathematical framework for measuring or\nestimating the temperature is still underdeveloped. Here, we incorporate the\nestimation theory in statistical inference to estimate the temperature of a\nfinite-sized system and propose optimal estimation based on the uniform minimum\nvariance unbiased estimation. Treating the finite-sized system as a thermometer\nmeasuring the temperature of a heat reservoir, we demonstrate that different\noptimal estimation of parameters yield different formulas of entropy, e.g.,\noptimal estimation of inverse temperature (or temperature) aligns with the\nBoltzmann entropy (or Gibbs entropy). The optimal estimation leads to a\nachievable energy-temperature uncertainty relation and exhibits sample-size\ndependence, coinciding with their counterparts in nanothermodynamics. The\nachievable bound and the non-Gaussian distribution of temperature enable\nexperimental testing in finite-sized systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temperature of a finite-sized system fluctuates due to the thermal\nfluctuations. However, a systematic mathematical framework for measuring or\nestimating the temperature is still underdeveloped. Here, we incorporate the\nestimation theory in statistical inference to estimate the temperature of a\nfinite-sized system and propose optimal estimation based on the uniform minimum\nvariance unbiased estimation. Treating the finite-sized system as a thermometer\nmeasuring the temperature of a heat reservoir, we demonstrate that different\noptimal estimation of parameters yield different formulas of entropy, e.g.,\noptimal estimation of inverse temperature (or temperature) aligns with the\nBoltzmann entropy (or Gibbs entropy). The optimal estimation leads to a\nachievable energy-temperature uncertainty relation and exhibits sample-size\ndependence, coinciding with their counterparts in nanothermodynamics. The\nachievable bound and the non-Gaussian distribution of temperature enable\nexperimental testing in finite-sized systems."
                },
                "authors": [
                    {
                        "name": "Shaoyong Zhang"
                    },
                    {
                        "name": "Zhaoyu Fei"
                    },
                    {
                        "name": "Xiaoguang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoguang Wang"
                },
                "author": "Xiaoguang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03927v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03927v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12461v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12461v1",
                "updated": "2025-10-14T12:50:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    12,
                    50,
                    11,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T12:50:11Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    12,
                    50,
                    11,
                    1,
                    287,
                    0
                ],
                "title": "Leveraging Language Semantics for Collaborative Filtering with TextGCN\n  and TextGCN-MLP: Zero-Shot vs In-Domain Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Language Semantics for Collaborative Filtering with TextGCN\n  and TextGCN-MLP: Zero-Shot vs In-Domain Performance"
                },
                "summary": "In recent years, various approaches have been proposed to leverage large\nlanguage models (LLMs) for incorporating textual information about items into\nrecommender systems. Existing methods primarily focus on either fine-tuning\nLLMs to generate recommendations or integrating LLM-based embeddings into\ndownstream models. In this work, we follow the latter direction and propose\n\\textbf{TextGCN}, which applies parameter-free graph convolution layers\ndirectly over LLM-based item-title embeddings, instead of learning ID-based\nembeddings as in traditional methods. By combining language semantics with\ngraph message passing, this architecture achieves state-of-the-art zero-shot\nperformance, significantly outperforming prior approaches. Furthermore, we\nintroduce \\textbf{TextGCN-MLP}, which extends TextGCN with a trainable\nmultilayer perceptron trained using a contrastive loss, achieving\nstate-of-the-art in-domain performance on recommendation benchmarks. However,\nthe zero-shot performance of TextGCN-MLP remains lower than that of TextGCN,\nhighlighting the trade-off between in-domain specialization and zero-shot\ngeneralization. We release our code on github at\n\\href{https://github.com/ChernovAndrey/TFCE}{github.com/ChernovAndrey/TFCE}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, various approaches have been proposed to leverage large\nlanguage models (LLMs) for incorporating textual information about items into\nrecommender systems. Existing methods primarily focus on either fine-tuning\nLLMs to generate recommendations or integrating LLM-based embeddings into\ndownstream models. In this work, we follow the latter direction and propose\n\\textbf{TextGCN}, which applies parameter-free graph convolution layers\ndirectly over LLM-based item-title embeddings, instead of learning ID-based\nembeddings as in traditional methods. By combining language semantics with\ngraph message passing, this architecture achieves state-of-the-art zero-shot\nperformance, significantly outperforming prior approaches. Furthermore, we\nintroduce \\textbf{TextGCN-MLP}, which extends TextGCN with a trainable\nmultilayer perceptron trained using a contrastive loss, achieving\nstate-of-the-art in-domain performance on recommendation benchmarks. However,\nthe zero-shot performance of TextGCN-MLP remains lower than that of TextGCN,\nhighlighting the trade-off between in-domain specialization and zero-shot\ngeneralization. We release our code on github at\n\\href{https://github.com/ChernovAndrey/TFCE}{github.com/ChernovAndrey/TFCE}."
                },
                "authors": [
                    {
                        "name": "Andrei Chernov"
                    },
                    {
                        "name": "Haroon Wahab"
                    },
                    {
                        "name": "Oleg Novitskij"
                    }
                ],
                "author_detail": {
                    "name": "Oleg Novitskij"
                },
                "author": "Oleg Novitskij",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12461v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12460v1",
                "updated": "2025-10-14T12:48:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    12,
                    48,
                    24,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T12:48:24Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    12,
                    48,
                    24,
                    1,
                    287,
                    0
                ],
                "title": "Probing Latent Knowledge Conflict for Faithful Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing Latent Knowledge Conflict for Faithful Retrieval-Augmented\n  Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm to\nenhance the factuality of Large Language Models (LLMs). However, existing RAG\nsystems often suffer from an unfaithfulness issue, where the model's response\ncontradicts evidence from the retrieved context. Existing approaches to\nimproving contextual faithfulness largely rely on external interventions, such\nas prompt engineering, decoding constraints, or reward-based fine-tuning. These\nworks treat the LLM as a black box and overlook a crucial question: how does\nthe LLM internally integrate retrieved evidence with its parametric memory,\nparticularly under knowledge conflicts? To address this gap, we conduct a\nprobing-based analysis of hidden-state representations in LLMs and observe\nthree findings: knowledge integration occurs hierarchically, conflicts manifest\nas latent signals at the sentence level, and irrelevant context is often\namplified when aligned with parametric knowledge. Building on these findings,\nwe propose CLEAR (Conflict-Localized and Enhanced Attention for RAG), a\nframework that (i) decomposes context into fine-grained sentence-level\nknowledge, (ii) employs hidden-state probing to localize conflicting knowledge,\nand (iii) introduces conflict-aware fine-tuning to guide the model to\naccurately integrate retrieved evidence. Extensive experiments across three\nbenchmarks demonstrate that CLEAR substantially improves both accuracy and\ncontextual faithfulness, consistently outperforming strong baselines under\ndiverse conflict conditions. The related resources are available at\nhttps://github.com/LinfengGao/CLEAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm to\nenhance the factuality of Large Language Models (LLMs). However, existing RAG\nsystems often suffer from an unfaithfulness issue, where the model's response\ncontradicts evidence from the retrieved context. Existing approaches to\nimproving contextual faithfulness largely rely on external interventions, such\nas prompt engineering, decoding constraints, or reward-based fine-tuning. These\nworks treat the LLM as a black box and overlook a crucial question: how does\nthe LLM internally integrate retrieved evidence with its parametric memory,\nparticularly under knowledge conflicts? To address this gap, we conduct a\nprobing-based analysis of hidden-state representations in LLMs and observe\nthree findings: knowledge integration occurs hierarchically, conflicts manifest\nas latent signals at the sentence level, and irrelevant context is often\namplified when aligned with parametric knowledge. Building on these findings,\nwe propose CLEAR (Conflict-Localized and Enhanced Attention for RAG), a\nframework that (i) decomposes context into fine-grained sentence-level\nknowledge, (ii) employs hidden-state probing to localize conflicting knowledge,\nand (iii) introduces conflict-aware fine-tuning to guide the model to\naccurately integrate retrieved evidence. Extensive experiments across three\nbenchmarks demonstrate that CLEAR substantially improves both accuracy and\ncontextual faithfulness, consistently outperforming strong baselines under\ndiverse conflict conditions. The related resources are available at\nhttps://github.com/LinfengGao/CLEAR."
                },
                "authors": [
                    {
                        "name": "Linfeng Gao"
                    },
                    {
                        "name": "Baolong Bi"
                    },
                    {
                        "name": "Zheng Yuan"
                    },
                    {
                        "name": "Le Wang"
                    },
                    {
                        "name": "Zerui Chen"
                    },
                    {
                        "name": "Zhimin Wei"
                    },
                    {
                        "name": "Shenghua Liu"
                    },
                    {
                        "name": "Qinggang Zhang"
                    },
                    {
                        "name": "Jinsong Su"
                    }
                ],
                "author_detail": {
                    "name": "Jinsong Su"
                },
                "author": "Jinsong Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02613v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02613v3",
                "updated": "2025-10-14T12:26:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    12,
                    26,
                    33,
                    1,
                    287,
                    0
                ],
                "published": "2024-06-03T08:23:45Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    8,
                    23,
                    45,
                    0,
                    155,
                    0
                ],
                "title": "ACCO: Accumulate While You Communicate for Communication-Overlapped\n  Sharded LLM Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACCO: Accumulate While You Communicate for Communication-Overlapped\n  Sharded LLM Training"
                },
                "summary": "Training LLMs relies on distributed implementations using multiple GPUs to\ncompute gradients in parallel with sharded optimizers. However, synchronizing\ngradients in data parallel setups introduces communication overhead that grows\nwith the number of workers, limiting parallelization efficiency. Local\noptimization algorithms reduce communications but incur high memory costs as\nthey prevent optimizer state sharding, hindering scalability. To address this,\nwe propose \\textbf{AC}cumulate while \\textbf{CO}mmunicate (ACCO), a\nmemory-efficient optimization algorithm for distributed LLM training. By\nsynchronizing delayed gradients while computing new ones, ACCO reduces GPU idle\ntime and supports heterogeneous hardware. To mitigate the convergence issues\ncaused by delayed updates, we introduce a novel technique ensuring training\ndynamics align with standard distributed optimization. Compared to ZeRO-1, our\napproach is significantly faster and scales effectively across heterogeneous\nhardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training LLMs relies on distributed implementations using multiple GPUs to\ncompute gradients in parallel with sharded optimizers. However, synchronizing\ngradients in data parallel setups introduces communication overhead that grows\nwith the number of workers, limiting parallelization efficiency. Local\noptimization algorithms reduce communications but incur high memory costs as\nthey prevent optimizer state sharding, hindering scalability. To address this,\nwe propose \\textbf{AC}cumulate while \\textbf{CO}mmunicate (ACCO), a\nmemory-efficient optimization algorithm for distributed LLM training. By\nsynchronizing delayed gradients while computing new ones, ACCO reduces GPU idle\ntime and supports heterogeneous hardware. To mitigate the convergence issues\ncaused by delayed updates, we introduce a novel technique ensuring training\ndynamics align with standard distributed optimization. Compared to ZeRO-1, our\napproach is significantly faster and scales effectively across heterogeneous\nhardware."
                },
                "authors": [
                    {
                        "name": "Adel Nabli"
                    },
                    {
                        "name": "Louis Fournier"
                    },
                    {
                        "name": "Pierre Erbacher"
                    },
                    {
                        "name": "Louis Serrano"
                    },
                    {
                        "name": "Eugene Belilovsky"
                    },
                    {
                        "name": "Edouard Oyallon"
                    }
                ],
                "author_detail": {
                    "name": "Edouard Oyallon"
                },
                "arxiv_affiliation": "MLIA",
                "author": "Edouard Oyallon",
                "arxiv_journal_ref": "The Thirty-ninth Annual Conference on Neural Information\n  Processing Systems, Dec 2025, San diego (Californie), United States",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02613v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02613v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07214v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07214v4",
                "updated": "2025-10-14T12:09:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    12,
                    9,
                    1,
                    1,
                    287,
                    0
                ],
                "published": "2024-02-20T18:57:34Z",
                "published_parsed": [
                    2024,
                    2,
                    20,
                    18,
                    57,
                    34,
                    1,
                    51,
                    0
                ],
                "title": "Exploring the Frontier of Vision-Language Models: A Survey of Current\n  Methodologies and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Frontier of Vision-Language Models: A Survey of Current\n  Methodologies and Future Directions"
                },
                "summary": "The advent of Large Language Models (LLMs) has significantly reshaped the\ntrajectory of the AI revolution. Nevertheless, these LLMs exhibit a notable\nlimitation, as they are primarily adept at processing textual information. To\naddress this constraint, researchers have endeavored to integrate visual\ncapabilities with LLMs, resulting in the emergence of Vision-Language Models\n(VLMs). These advanced models are instrumental in tackling more intricate tasks\nsuch as image captioning and visual question answering. In our comprehensive\nsurvey paper, we delve into the key advancements within the realm of VLMs. Our\nclassification organizes VLMs into three distinct categories: models dedicated\nto vision-language understanding, models that process multimodal inputs to\ngenerate unimodal (textual) outputs and models that both accept and produce\nmultimodal inputs and outputs.This classification is based on their respective\ncapabilities and functionalities in processing and generating various\nmodalities of data.We meticulously dissect each model, offering an extensive\nanalysis of its foundational architecture, training data sources, as well as\nits strengths and limitations wherever possible, providing readers with a\ncomprehensive understanding of its essential components. We also analyzed the\nperformance of VLMs in various benchmark datasets. By doing so, we aim to offer\na nuanced understanding of the diverse landscape of VLMs. Additionally, we\nunderscore potential avenues for future research in this dynamic domain,\nanticipating further breakthroughs and advancements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) has significantly reshaped the\ntrajectory of the AI revolution. Nevertheless, these LLMs exhibit a notable\nlimitation, as they are primarily adept at processing textual information. To\naddress this constraint, researchers have endeavored to integrate visual\ncapabilities with LLMs, resulting in the emergence of Vision-Language Models\n(VLMs). These advanced models are instrumental in tackling more intricate tasks\nsuch as image captioning and visual question answering. In our comprehensive\nsurvey paper, we delve into the key advancements within the realm of VLMs. Our\nclassification organizes VLMs into three distinct categories: models dedicated\nto vision-language understanding, models that process multimodal inputs to\ngenerate unimodal (textual) outputs and models that both accept and produce\nmultimodal inputs and outputs.This classification is based on their respective\ncapabilities and functionalities in processing and generating various\nmodalities of data.We meticulously dissect each model, offering an extensive\nanalysis of its foundational architecture, training data sources, as well as\nits strengths and limitations wherever possible, providing readers with a\ncomprehensive understanding of its essential components. We also analyzed the\nperformance of VLMs in various benchmark datasets. By doing so, we aim to offer\na nuanced understanding of the diverse landscape of VLMs. Additionally, we\nunderscore potential avenues for future research in this dynamic domain,\nanticipating further breakthroughs and advancements."
                },
                "authors": [
                    {
                        "name": "Akash Ghosh"
                    },
                    {
                        "name": "Arkadeep Acharya"
                    },
                    {
                        "name": "Sriparna Saha"
                    },
                    {
                        "name": "Vinija Jain"
                    },
                    {
                        "name": "Aman Chadha"
                    }
                ],
                "author_detail": {
                    "name": "Aman Chadha"
                },
                "author": "Aman Chadha",
                "arxiv_comment": "One of the first survey on Visual Language Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07214v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07214v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12423v1",
                "updated": "2025-10-14T11:59:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    11,
                    59,
                    47,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T11:59:47Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    11,
                    59,
                    47,
                    1,
                    287,
                    0
                ],
                "title": "MTOS: A LLM-Driven Multi-topic Opinion Simulation Framework for\n  Exploring Echo Chamber Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MTOS: A LLM-Driven Multi-topic Opinion Simulation Framework for\n  Exploring Echo Chamber Dynamics"
                },
                "summary": "The polarization of opinions, information segregation, and cognitive biases\non social media have attracted significant academic attention. In real-world\nnetworks, information often spans multiple interrelated topics, posing\nchallenges for opinion evolution and highlighting the need for frameworks that\nsimulate interactions among topics. Existing studies based on large language\nmodels (LLMs) focus largely on single topics, limiting the capture of cognitive\ntransfer in multi-topic, cross-domain contexts. Traditional numerical models,\nmeanwhile, simplify complex linguistic attitudes into discrete values, lacking\ninterpretability, behavioral consistency, and the ability to integrate multiple\ntopics. To address these issues, we propose Multi-topic Opinion Simulation\n(MTOS), a social simulation framework integrating multi-topic contexts with\nLLMs. MTOS leverages LLMs alongside short-term and long-term memory,\nincorporates multiple user-selection interaction mechanisms and dynamic\ntopic-selection strategies, and employs a belief decay mechanism to enable\nperspective updates across topics. We conduct extensive experiments on MTOS,\nvarying topic numbers, correlation types, and performing ablation studies to\nassess features such as group polarization and local consistency. Results show\nthat multi-topic settings significantly alter polarization trends: positively\ncorrelated topics amplify echo chambers, negatively correlated topics inhibit\nthem, and irrelevant topics also mitigate echo chamber effects through resource\ncompetition. Compared with numerical models, LLM-based agents realistically\nsimulate dynamic opinion changes, reproduce linguistic features of news texts,\nand capture complex human reasoning, improving simulation interpretability and\nsystem stability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The polarization of opinions, information segregation, and cognitive biases\non social media have attracted significant academic attention. In real-world\nnetworks, information often spans multiple interrelated topics, posing\nchallenges for opinion evolution and highlighting the need for frameworks that\nsimulate interactions among topics. Existing studies based on large language\nmodels (LLMs) focus largely on single topics, limiting the capture of cognitive\ntransfer in multi-topic, cross-domain contexts. Traditional numerical models,\nmeanwhile, simplify complex linguistic attitudes into discrete values, lacking\ninterpretability, behavioral consistency, and the ability to integrate multiple\ntopics. To address these issues, we propose Multi-topic Opinion Simulation\n(MTOS), a social simulation framework integrating multi-topic contexts with\nLLMs. MTOS leverages LLMs alongside short-term and long-term memory,\nincorporates multiple user-selection interaction mechanisms and dynamic\ntopic-selection strategies, and employs a belief decay mechanism to enable\nperspective updates across topics. We conduct extensive experiments on MTOS,\nvarying topic numbers, correlation types, and performing ablation studies to\nassess features such as group polarization and local consistency. Results show\nthat multi-topic settings significantly alter polarization trends: positively\ncorrelated topics amplify echo chambers, negatively correlated topics inhibit\nthem, and irrelevant topics also mitigate echo chamber effects through resource\ncompetition. Compared with numerical models, LLM-based agents realistically\nsimulate dynamic opinion changes, reproduce linguistic features of news texts,\nand capture complex human reasoning, improving simulation interpretability and\nsystem stability."
                },
                "authors": [
                    {
                        "name": "Dingyi Zuo"
                    },
                    {
                        "name": "Hongjie Zhang"
                    },
                    {
                        "name": "Jie Ou"
                    },
                    {
                        "name": "Chaosheng Feng"
                    },
                    {
                        "name": "Shuwan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Shuwan Liu"
                },
                "author": "Shuwan Liu",
                "arxiv_comment": "14 pages, 11figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12422v1",
                "updated": "2025-10-14T11:59:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    11,
                    59,
                    19,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T11:59:19Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    11,
                    59,
                    19,
                    1,
                    287,
                    0
                ],
                "title": "VideoLucy: Deep Memory Backtracking for Long Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoLucy: Deep Memory Backtracking for Long Video Understanding"
                },
                "summary": "Recent studies have shown that agent-based systems leveraging large language\nmodels (LLMs) for key information retrieval and integration have emerged as a\npromising approach for long video understanding. However, these systems face\ntwo major challenges. First, they typically perform modeling and reasoning on\nindividual frames, struggling to capture the temporal context of consecutive\nframes. Second, to reduce the cost of dense frame-level captioning, they adopt\nsparse frame sampling, which risks discarding crucial information. To overcome\nthese limitations, we propose VideoLucy, a deep memory backtracking framework\nfor long video understanding. Inspired by the human recollection process from\ncoarse to fine, VideoLucy employs a hierarchical memory structure with\nprogressive granularity. This structure explicitly defines the detail level and\ntemporal scope of memory at different hierarchical depths. Through an\nagent-based iterative backtracking mechanism, VideoLucy systematically mines\nvideo-wide, question-relevant deep memories until sufficient information is\ngathered to provide a confident answer. This design enables effective temporal\nunderstanding of consecutive frames while preserving critical details. In\naddition, we introduce EgoMem, a new benchmark for long video understanding.\nEgoMem is designed to comprehensively evaluate a model's ability to understand\ncomplex events that unfold over time and capture fine-grained details in\nextremely long videos. Extensive experiments demonstrate the superiority of\nVideoLucy. Built on open-source models, VideoLucy significantly outperforms\nstate-of-the-art methods on multiple long video understanding benchmarks,\nachieving performance even surpassing the latest proprietary models such as\nGPT-4o. Our code and dataset will be made publicly at\nhttps://videolucy.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have shown that agent-based systems leveraging large language\nmodels (LLMs) for key information retrieval and integration have emerged as a\npromising approach for long video understanding. However, these systems face\ntwo major challenges. First, they typically perform modeling and reasoning on\nindividual frames, struggling to capture the temporal context of consecutive\nframes. Second, to reduce the cost of dense frame-level captioning, they adopt\nsparse frame sampling, which risks discarding crucial information. To overcome\nthese limitations, we propose VideoLucy, a deep memory backtracking framework\nfor long video understanding. Inspired by the human recollection process from\ncoarse to fine, VideoLucy employs a hierarchical memory structure with\nprogressive granularity. This structure explicitly defines the detail level and\ntemporal scope of memory at different hierarchical depths. Through an\nagent-based iterative backtracking mechanism, VideoLucy systematically mines\nvideo-wide, question-relevant deep memories until sufficient information is\ngathered to provide a confident answer. This design enables effective temporal\nunderstanding of consecutive frames while preserving critical details. In\naddition, we introduce EgoMem, a new benchmark for long video understanding.\nEgoMem is designed to comprehensively evaluate a model's ability to understand\ncomplex events that unfold over time and capture fine-grained details in\nextremely long videos. Extensive experiments demonstrate the superiority of\nVideoLucy. Built on open-source models, VideoLucy significantly outperforms\nstate-of-the-art methods on multiple long video understanding benchmarks,\nachieving performance even surpassing the latest proprietary models such as\nGPT-4o. Our code and dataset will be made publicly at\nhttps://videolucy.github.io"
                },
                "authors": [
                    {
                        "name": "Jialong Zuo"
                    },
                    {
                        "name": "Yongtai Deng"
                    },
                    {
                        "name": "Lingdong Kong"
                    },
                    {
                        "name": "Jingkang Yang"
                    },
                    {
                        "name": "Rui Jin"
                    },
                    {
                        "name": "Yiwei Zhang"
                    },
                    {
                        "name": "Nong Sang"
                    },
                    {
                        "name": "Liang Pan"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Changxin Gao"
                    }
                ],
                "author_detail": {
                    "name": "Changxin Gao"
                },
                "author": "Changxin Gao",
                "arxiv_comment": "NeurIPS-2025 Accepted Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16548v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16548v2",
                "updated": "2025-10-14T11:57:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    11,
                    57,
                    11,
                    1,
                    287,
                    0
                ],
                "published": "2025-09-20T06:19:55Z",
                "published_parsed": [
                    2025,
                    9,
                    20,
                    6,
                    19,
                    55,
                    5,
                    263,
                    0
                ],
                "title": "SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward\n  Learning"
                },
                "summary": "Process reward models (PRMs) offer fine-grained, step-level evaluations that\nfacilitate deeper reasoning processes in large language models (LLMs), proving\neffective in complex tasks like mathematical reasoning. However, developing\nPRMs is challenging due to the high cost and limited scalability of\nhuman-annotated data. Synthetic data from Monte Carlo (MC) estimation is a\npromising alternative but suffers from a high noise ratio, which can cause\noverfitting and hinder large-scale training. In this work, we conduct a\npreliminary study on the noise distribution in synthetic data from MC\nestimation, identifying that annotation models tend to both underestimate and\noverestimate step correctness due to limitations in their annotation\ncapabilities. Building on these insights, we propose Self-Denoising Monte Carlo\nAnnotation (SCAN), an efficient data synthesis and noise-tolerant learning\nframework. Our key findings indicate that: (1) Even lightweight models (e.g.,\n1.5B parameters) can produce high-quality annotations through a self-denoising\nstrategy, enabling PRMs to achieve superior performance with only 6% the\ninference cost required by vanilla MC estimation. (2) With our robust learning\nstrategy, PRMs can effectively learn from this weak supervision, achieving a\n39.2 F1 score improvement (from 19.9 to 59.1) in ProcessBench. Despite using\nonly a compact synthetic dataset, our models surpass strong baselines,\nincluding those trained on large-scale human-annotated datasets such as\nPRM800K. Furthermore, performance continues to improve as we scale up the\nsynthetic data, highlighting the potential of SCAN for scalable,\ncost-efficient, and robust PRM training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process reward models (PRMs) offer fine-grained, step-level evaluations that\nfacilitate deeper reasoning processes in large language models (LLMs), proving\neffective in complex tasks like mathematical reasoning. However, developing\nPRMs is challenging due to the high cost and limited scalability of\nhuman-annotated data. Synthetic data from Monte Carlo (MC) estimation is a\npromising alternative but suffers from a high noise ratio, which can cause\noverfitting and hinder large-scale training. In this work, we conduct a\npreliminary study on the noise distribution in synthetic data from MC\nestimation, identifying that annotation models tend to both underestimate and\noverestimate step correctness due to limitations in their annotation\ncapabilities. Building on these insights, we propose Self-Denoising Monte Carlo\nAnnotation (SCAN), an efficient data synthesis and noise-tolerant learning\nframework. Our key findings indicate that: (1) Even lightweight models (e.g.,\n1.5B parameters) can produce high-quality annotations through a self-denoising\nstrategy, enabling PRMs to achieve superior performance with only 6% the\ninference cost required by vanilla MC estimation. (2) With our robust learning\nstrategy, PRMs can effectively learn from this weak supervision, achieving a\n39.2 F1 score improvement (from 19.9 to 59.1) in ProcessBench. Despite using\nonly a compact synthetic dataset, our models surpass strong baselines,\nincluding those trained on large-scale human-annotated datasets such as\nPRM800K. Furthermore, performance continues to improve as we scale up the\nsynthetic data, highlighting the potential of SCAN for scalable,\ncost-efficient, and robust PRM training."
                },
                "authors": [
                    {
                        "name": "Yuyang Ding"
                    },
                    {
                        "name": "Xinyu Shi"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Xiaobo Liang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "NeurIPS 2025. Project page: https://scan-prm.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16548v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16548v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12409v1",
                "updated": "2025-10-14T11:42:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    11,
                    42,
                    15,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T11:42:15Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    11,
                    42,
                    15,
                    1,
                    287,
                    0
                ],
                "title": "PricingLogic: Evaluating LLMs Reasoning on Complex Tourism Pricing Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PricingLogic: Evaluating LLMs Reasoning on Complex Tourism Pricing Tasks"
                },
                "summary": "We present PricingLogic, the first benchmark that probes whether Large\nLanguage Models(LLMs) can reliably automate tourism-related prices when\nmultiple, overlapping fare rules apply. Travel agencies are eager to offload\nthis error-prone task onto AI systems; however, deploying LLMs without verified\nreliability could result in significant financial losses and erode customer\ntrust. PricingLogic comprises 300 natural-language questions based on booking\nrequests derived from 42 real-world pricing policies, spanning two levels of\ndifficulty: (i) basic customer-type pricing and (ii)bundled-tour calculations\ninvolving interacting discounts. Evaluations of a line of LLMs reveal a steep\nperformance drop on the harder tier,exposing systematic failures in rule\ninterpretation and arithmetic reasoning.These results highlight that, despite\ntheir general capabilities, today's LLMs remain unreliable in revenue-critical\napplications without further safeguards or domain adaptation. Our code and\ndataset are available at https://github.com/EIT-NLP/PricingLogic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present PricingLogic, the first benchmark that probes whether Large\nLanguage Models(LLMs) can reliably automate tourism-related prices when\nmultiple, overlapping fare rules apply. Travel agencies are eager to offload\nthis error-prone task onto AI systems; however, deploying LLMs without verified\nreliability could result in significant financial losses and erode customer\ntrust. PricingLogic comprises 300 natural-language questions based on booking\nrequests derived from 42 real-world pricing policies, spanning two levels of\ndifficulty: (i) basic customer-type pricing and (ii)bundled-tour calculations\ninvolving interacting discounts. Evaluations of a line of LLMs reveal a steep\nperformance drop on the harder tier,exposing systematic failures in rule\ninterpretation and arithmetic reasoning.These results highlight that, despite\ntheir general capabilities, today's LLMs remain unreliable in revenue-critical\napplications without further safeguards or domain adaptation. Our code and\ndataset are available at https://github.com/EIT-NLP/PricingLogic."
                },
                "authors": [
                    {
                        "name": "Yunuo Liu"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Zena Al-Khalili"
                    },
                    {
                        "name": "Dai Cheng"
                    },
                    {
                        "name": "Yanjun Chen"
                    },
                    {
                        "name": "Dietrich Klakow"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Shen"
                },
                "author": "Xiaoyu Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03536v2",
                "updated": "2025-10-14T11:40:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    11,
                    40,
                    4,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-03T22:11:17Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    22,
                    11,
                    17,
                    4,
                    276,
                    0
                ],
                "title": "Triplet-Structured Knowledge Integration for Multi-Turn Medical\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Triplet-Structured Knowledge Integration for Multi-Turn Medical\n  Reasoning"
                },
                "summary": "Large Language Models (LLMs) have shown strong performance on static medical\nQuestion Answering (QA) tasks, yet their reasoning often deteriorates in\nmulti-turn clinical dialogues where patient information is scattered across\nturns. This paper introduces TriMediQ, a triplet-structured approach that\nenhances the reasoning reliability of LLMs through explicit knowledge\nintegration. TriMediQ first employs a frozen triplet extraction LLM to convert\npatient responses into clinically grounded triplets, ensuring factual precision\nvia constrained prompting. These triplets are incorporated into a\npatient-specific Knowledge Graph (KG), from which a trainable projection module\nconsisting of a graph encoder and a projector captures relational dependencies\nwhile keeping all LLM parameters frozen. During inference, the projection\nmodule guides multi-hop reasoning over the KG, enabling coherent clinical\ndialogue understanding. Experiments on two interactive medical QA benchmarks\nshow that TriMediQ achieves up to 10.4\\% improvement in accuracy over five\nexisting baselines on the iMedQA dataset. These results demonstrate that\nstructuring patient information as triplets can effectively improve the\nreasoning capability of LLMs in multi-turn medical QA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown strong performance on static medical\nQuestion Answering (QA) tasks, yet their reasoning often deteriorates in\nmulti-turn clinical dialogues where patient information is scattered across\nturns. This paper introduces TriMediQ, a triplet-structured approach that\nenhances the reasoning reliability of LLMs through explicit knowledge\nintegration. TriMediQ first employs a frozen triplet extraction LLM to convert\npatient responses into clinically grounded triplets, ensuring factual precision\nvia constrained prompting. These triplets are incorporated into a\npatient-specific Knowledge Graph (KG), from which a trainable projection module\nconsisting of a graph encoder and a projector captures relational dependencies\nwhile keeping all LLM parameters frozen. During inference, the projection\nmodule guides multi-hop reasoning over the KG, enabling coherent clinical\ndialogue understanding. Experiments on two interactive medical QA benchmarks\nshow that TriMediQ achieves up to 10.4\\% improvement in accuracy over five\nexisting baselines on the iMedQA dataset. These results demonstrate that\nstructuring patient information as triplets can effectively improve the\nreasoning capability of LLMs in multi-turn medical QA."
                },
                "authors": [
                    {
                        "name": "Zhaohan Meng"
                    },
                    {
                        "name": "Zaiqiao Meng"
                    },
                    {
                        "name": "Siwei Liu"
                    },
                    {
                        "name": "Iadh Ounis"
                    }
                ],
                "author_detail": {
                    "name": "Iadh Ounis"
                },
                "author": "Iadh Ounis",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.07140v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.07140v6",
                "updated": "2025-10-14T11:34:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    11,
                    34,
                    26,
                    1,
                    287,
                    0
                ],
                "published": "2024-02-11T09:46:24Z",
                "published_parsed": [
                    2024,
                    2,
                    11,
                    9,
                    46,
                    24,
                    6,
                    42,
                    0
                ],
                "title": "Can Graph Descriptive Order Affect Solving Graph Problems with LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Graph Descriptive Order Affect Solving Graph Problems with LLMs?"
                },
                "summary": "Large language models (LLMs) have achieved significant success in reasoning\ntasks, including mathematical reasoning and logical deduction. Among these\nreasoning tasks, graph problems stand out due to their complexity and unique\nstructural characteristics, attracting considerable attention from researchers.\nPrevious studies have explored LLMs' graph reasoning abilities through various\ntechniques, such as different encoding methods for graph structures and the use\nof carefully designed prompts. However, a critical factor has been mostly\noverlooked: the prompt sequential order in which graph descriptions are\npresented to the models. In this study, we present the first comprehensive\nanalysis of how the order of graph descriptions impacts LLM performance.\nSpecifically, we comprehensively evaluate four graph description orders across\nsix graph problems using six mainstream LLMs. The results reveal that: (1)\nordered graph descriptions significantly improve LLMs' comprehension of graph\nstructures; (2) the robustness of LLMs to graph description order varies across\ndifferent tasks; and (3) the impact of graph order on performance is closely\nrelated to the inherent characteristics of tasks. This study provides a\ncritical advancement in the application of LLMs for solving graph-related\nproblems, paving the way for future research to optimize model performance\nthrough strategic graph description ordering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved significant success in reasoning\ntasks, including mathematical reasoning and logical deduction. Among these\nreasoning tasks, graph problems stand out due to their complexity and unique\nstructural characteristics, attracting considerable attention from researchers.\nPrevious studies have explored LLMs' graph reasoning abilities through various\ntechniques, such as different encoding methods for graph structures and the use\nof carefully designed prompts. However, a critical factor has been mostly\noverlooked: the prompt sequential order in which graph descriptions are\npresented to the models. In this study, we present the first comprehensive\nanalysis of how the order of graph descriptions impacts LLM performance.\nSpecifically, we comprehensively evaluate four graph description orders across\nsix graph problems using six mainstream LLMs. The results reveal that: (1)\nordered graph descriptions significantly improve LLMs' comprehension of graph\nstructures; (2) the robustness of LLMs to graph description order varies across\ndifferent tasks; and (3) the impact of graph order on performance is closely\nrelated to the inherent characteristics of tasks. This study provides a\ncritical advancement in the application of LLMs for solving graph-related\nproblems, paving the way for future research to optimize model performance\nthrough strategic graph description ordering."
                },
                "authors": [
                    {
                        "name": "Yuyao Ge"
                    },
                    {
                        "name": "Shenghua Liu"
                    },
                    {
                        "name": "Baolong Bi"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Lingrui Mei"
                    },
                    {
                        "name": "Wenjie Feng"
                    },
                    {
                        "name": "Lizhe Chen"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "Accepted to ACL 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.07140v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.07140v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12400v1",
                "updated": "2025-10-14T11:27:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    11,
                    27,
                    46,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T11:27:46Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    11,
                    27,
                    46,
                    1,
                    287,
                    0
                ],
                "title": "Towards General Urban Monitoring with Vision-Language Models: A Review,\n  Evaluation, and a Research Agenda",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards General Urban Monitoring with Vision-Language Models: A Review,\n  Evaluation, and a Research Agenda"
                },
                "summary": "Urban monitoring of public infrastructure (such as waste bins, road signs,\nvegetation, sidewalks, and construction sites) poses significant challenges due\nto the diversity of objects, environments, and contextual conditions involved.\nCurrent state-of-the-art approaches typically rely on a combination of IoT\nsensors and manual inspections, which are costly, difficult to scale, and often\nmisaligned with citizens' perception formed through direct visual observation.\nThis raises a critical question: Can machines now \"see\" like citizens and infer\ninformed opinions about the condition of urban infrastructure? Vision-Language\nModels (VLMs), which integrate visual understanding with natural language\nreasoning, have recently demonstrated impressive capabilities in processing\ncomplex visual information, turning them into a promising technology to address\nthis challenge. This systematic review investigates the role of VLMs in urban\nmonitoring, with particular emphasis on zero-shot applications. Following the\nPRISMA methodology, we analyzed 32 peer-reviewed studies published between 2021\nand 2025 to address four core research questions: (1) What urban monitoring\ntasks have been effectively addressed using VLMs? (2) Which VLM architectures\nand frameworks are most commonly used and demonstrate superior performance? (3)\nWhat datasets and resources support this emerging field? (4) How are VLM-based\napplications evaluated, and what performance levels have been reported?",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Urban monitoring of public infrastructure (such as waste bins, road signs,\nvegetation, sidewalks, and construction sites) poses significant challenges due\nto the diversity of objects, environments, and contextual conditions involved.\nCurrent state-of-the-art approaches typically rely on a combination of IoT\nsensors and manual inspections, which are costly, difficult to scale, and often\nmisaligned with citizens' perception formed through direct visual observation.\nThis raises a critical question: Can machines now \"see\" like citizens and infer\ninformed opinions about the condition of urban infrastructure? Vision-Language\nModels (VLMs), which integrate visual understanding with natural language\nreasoning, have recently demonstrated impressive capabilities in processing\ncomplex visual information, turning them into a promising technology to address\nthis challenge. This systematic review investigates the role of VLMs in urban\nmonitoring, with particular emphasis on zero-shot applications. Following the\nPRISMA methodology, we analyzed 32 peer-reviewed studies published between 2021\nand 2025 to address four core research questions: (1) What urban monitoring\ntasks have been effectively addressed using VLMs? (2) Which VLM architectures\nand frameworks are most commonly used and demonstrate superior performance? (3)\nWhat datasets and resources support this emerging field? (4) How are VLM-based\napplications evaluated, and what performance levels have been reported?"
                },
                "authors": [
                    {
                        "name": "Andr Torneiro"
                    },
                    {
                        "name": "Diogo Monteiro"
                    },
                    {
                        "name": "Paulo Novais"
                    },
                    {
                        "name": "Pedro Rangel Henriques"
                    },
                    {
                        "name": "Nuno F. Rodrigues"
                    }
                ],
                "author_detail": {
                    "name": "Nuno F. Rodrigues"
                },
                "author": "Nuno F. Rodrigues",
                "arxiv_comment": "44 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12399v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12399v1",
                "updated": "2025-10-14T11:26:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    11,
                    26,
                    56,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T11:26:56Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    11,
                    26,
                    56,
                    1,
                    287,
                    0
                ],
                "title": "A Survey of Vibe Coding with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Vibe Coding with Large Language Models"
                },
                "summary": "The advancement of large language models (LLMs) has catalyzed a paradigm\nshift from code generation assistance to autonomous coding agents, enabling a\nnovel development methodology termed \"Vibe Coding\" where developers validate\nAI-generated implementations through outcome observation rather than\nline-by-line code comprehension. Despite its transformative potential, the\neffectiveness of this emergent paradigm remains under-explored, with empirical\nevidence revealing unexpected productivity losses and fundamental challenges in\nhuman-AI collaboration. To address this gap, this survey provides the first\ncomprehensive and systematic review of Vibe Coding with large language models,\nestablishing both theoretical foundations and practical frameworks for this\ntransformative development approach. Drawing from systematic analysis of over\n1000 research papers, we survey the entire vibe coding ecosystem, examining\ncritical infrastructure components including LLMs for coding, LLM-based coding\nagent, development environment of coding agent, and feedback mechanisms. We\nfirst introduce Vibe Coding as a formal discipline by formalizing it through a\nConstrained Markov Decision Process that captures the dynamic triadic\nrelationship among human developers, software projects, and coding agents.\nBuilding upon this theoretical foundation, we then synthesize existing\npractices into five distinct development models: Unconstrained Automation,\nIterative Conversational Collaboration, Planning-Driven, Test-Driven, and\nContext-Enhanced Models, thus providing the first comprehensive taxonomy in\nthis domain. Critically, our analysis reveals that successful Vibe Coding\ndepends not merely on agent capabilities but on systematic context engineering,\nwell-established development environments, and human-agent collaborative\ndevelopment models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of large language models (LLMs) has catalyzed a paradigm\nshift from code generation assistance to autonomous coding agents, enabling a\nnovel development methodology termed \"Vibe Coding\" where developers validate\nAI-generated implementations through outcome observation rather than\nline-by-line code comprehension. Despite its transformative potential, the\neffectiveness of this emergent paradigm remains under-explored, with empirical\nevidence revealing unexpected productivity losses and fundamental challenges in\nhuman-AI collaboration. To address this gap, this survey provides the first\ncomprehensive and systematic review of Vibe Coding with large language models,\nestablishing both theoretical foundations and practical frameworks for this\ntransformative development approach. Drawing from systematic analysis of over\n1000 research papers, we survey the entire vibe coding ecosystem, examining\ncritical infrastructure components including LLMs for coding, LLM-based coding\nagent, development environment of coding agent, and feedback mechanisms. We\nfirst introduce Vibe Coding as a formal discipline by formalizing it through a\nConstrained Markov Decision Process that captures the dynamic triadic\nrelationship among human developers, software projects, and coding agents.\nBuilding upon this theoretical foundation, we then synthesize existing\npractices into five distinct development models: Unconstrained Automation,\nIterative Conversational Collaboration, Planning-Driven, Test-Driven, and\nContext-Enhanced Models, thus providing the first comprehensive taxonomy in\nthis domain. Critically, our analysis reveals that successful Vibe Coding\ndepends not merely on agent capabilities but on systematic context engineering,\nwell-established development environments, and human-agent collaborative\ndevelopment models."
                },
                "authors": [
                    {
                        "name": "Yuyao Ge"
                    },
                    {
                        "name": "Lingrui Mei"
                    },
                    {
                        "name": "Zenghao Duan"
                    },
                    {
                        "name": "Tianhao Li"
                    },
                    {
                        "name": "Yujia Zheng"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Lexin Wang"
                    },
                    {
                        "name": "Jiayu Yao"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Yujun Cai"
                    },
                    {
                        "name": "Baolong Bi"
                    },
                    {
                        "name": "Fangda Guo"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Shenghua Liu"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12399v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12399v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10661v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10661v2",
                "updated": "2025-10-14T11:24:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    11,
                    24,
                    4,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-12T15:35:05Z",
                "published_parsed": [
                    2025,
                    10,
                    12,
                    15,
                    35,
                    5,
                    6,
                    285,
                    0
                ],
                "title": "AGENTIQL: An Agent-Inspired Multi-Expert Framework for Text-to-SQL\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGENTIQL: An Agent-Inspired Multi-Expert Framework for Text-to-SQL\n  Generation"
                },
                "summary": "LLMs have advanced text-to-SQL generation, yet monolithic architectures\nstruggle with complex reasoning and schema diversity. We propose AGENTIQL, an\nagent-inspired multi-expert framework that combines a reasoning agent for\nquestion decomposition, a coding agent for sub-query generation, and a\nrefinement step for column selection. An adaptive router further balances\nefficiency and accuracy by selecting between our modular pipeline and a\nbaseline parser. Several steps in the pipeline can be executed in parallel,\nmaking the framework scalable to larger workloads. Evaluated on the Spider\nbenchmark, AGENTIQL improves execution accuracy and interpretability and\nachieves up to 86.07% EX with 14B models using the Planner&Executor merging\nstrategy. The attained performance is contingent upon the efficacy of the\nrouting mechanism, thereby narrowing the gap to GPT-4-based SOTA (89.65% EX)\nwhile using much smaller open-source LLMs. Beyond accuracy, AGENTIQL enhances\ntransparency by exposing intermediate reasoning steps, offering a robust,\nscalable, and interpretable approach to semantic parsing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have advanced text-to-SQL generation, yet monolithic architectures\nstruggle with complex reasoning and schema diversity. We propose AGENTIQL, an\nagent-inspired multi-expert framework that combines a reasoning agent for\nquestion decomposition, a coding agent for sub-query generation, and a\nrefinement step for column selection. An adaptive router further balances\nefficiency and accuracy by selecting between our modular pipeline and a\nbaseline parser. Several steps in the pipeline can be executed in parallel,\nmaking the framework scalable to larger workloads. Evaluated on the Spider\nbenchmark, AGENTIQL improves execution accuracy and interpretability and\nachieves up to 86.07% EX with 14B models using the Planner&Executor merging\nstrategy. The attained performance is contingent upon the efficacy of the\nrouting mechanism, thereby narrowing the gap to GPT-4-based SOTA (89.65% EX)\nwhile using much smaller open-source LLMs. Beyond accuracy, AGENTIQL enhances\ntransparency by exposing intermediate reasoning steps, offering a robust,\nscalable, and interpretable approach to semantic parsing."
                },
                "authors": [
                    {
                        "name": "Omid Reza Heidari"
                    },
                    {
                        "name": "Siobhan Reid"
                    },
                    {
                        "name": "Yassine Yaakoubi"
                    }
                ],
                "author_detail": {
                    "name": "Yassine Yaakoubi"
                },
                "author": "Yassine Yaakoubi",
                "arxiv_comment": "Accepted at NeurIPS 2025, ER \"Efficient Reasoning\" workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10661v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10661v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22337v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22337v3",
                "updated": "2025-10-14T11:20:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    11,
                    20,
                    7,
                    1,
                    287,
                    0
                ],
                "published": "2025-07-30T02:44:20Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    2,
                    44,
                    20,
                    2,
                    211,
                    0
                ],
                "title": "A Comprehensive Taxonomy of Negation for NLP and Neural Retrievers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Taxonomy of Negation for NLP and Neural Retrievers"
                },
                "summary": "Understanding and solving complex reasoning tasks is vital for addressing the\ninformation needs of a user. Although dense neural models learn contextualised\nembeddings, they still underperform on queries containing negation. To\nunderstand this phenomenon, we study negation in both traditional neural\ninformation retrieval and LLM-based models. We (1) introduce a taxonomy of\nnegation that derives from philosophical, linguistic, and logical definitions;\n(2) generate two benchmark datasets that can be used to evaluate the\nperformance of neural information retrieval models and to fine-tune models for\na more robust performance on negation; and (3) propose a logic-based\nclassification mechanism that can be used to analyze the performance of\nretrieval models on existing datasets. Our taxonomy produces a balanced data\ndistribution over negation types, providing a better training setup that leads\nto faster convergence on the NevIR dataset. Moreover, we propose a\nclassification schema that reveals the coverage of negation types in existing\ndatasets, offering insights into the factors that might affect the\ngeneralization of fine-tuned models on negation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and solving complex reasoning tasks is vital for addressing the\ninformation needs of a user. Although dense neural models learn contextualised\nembeddings, they still underperform on queries containing negation. To\nunderstand this phenomenon, we study negation in both traditional neural\ninformation retrieval and LLM-based models. We (1) introduce a taxonomy of\nnegation that derives from philosophical, linguistic, and logical definitions;\n(2) generate two benchmark datasets that can be used to evaluate the\nperformance of neural information retrieval models and to fine-tune models for\na more robust performance on negation; and (3) propose a logic-based\nclassification mechanism that can be used to analyze the performance of\nretrieval models on existing datasets. Our taxonomy produces a balanced data\ndistribution over negation types, providing a better training setup that leads\nto faster convergence on the NevIR dataset. Moreover, we propose a\nclassification schema that reveals the coverage of negation types in existing\ndatasets, offering insights into the factors that might affect the\ngeneralization of fine-tuned models on negation."
                },
                "authors": [
                    {
                        "name": "Roxana Petcu"
                    },
                    {
                        "name": "Samarth Bhargav"
                    },
                    {
                        "name": "Maarten de Rijke"
                    },
                    {
                        "name": "Evangelos Kanoulas"
                    }
                ],
                "author_detail": {
                    "name": "Evangelos Kanoulas"
                },
                "author": "Evangelos Kanoulas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22337v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22337v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12389v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12389v1",
                "updated": "2025-10-14T11:14:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    11,
                    14,
                    38,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T11:14:38Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    11,
                    14,
                    38,
                    1,
                    287,
                    0
                ],
                "title": "Tokenization Disparities as Infrastructure Bias: How Subword Systems\n  Create Inequities in LLM Access and Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization Disparities as Infrastructure Bias: How Subword Systems\n  Create Inequities in LLM Access and Efficiency"
                },
                "summary": "Tokenization disparities pose a significant barrier to achieving equitable\naccess to artificial intelligence across linguistically diverse populations.\nThis study conducts a large-scale cross-linguistic evaluation of tokenization\nefficiency in over 200 languages to systematically quantify computational\ninequities in large language models (LLMs). Using a standardized experimental\nframework, we applied consistent preprocessing and normalization protocols,\nfollowed by uniform tokenization through the tiktoken library across all\nlanguage samples. Comprehensive tokenization statistics were collected using\nestablished evaluation metrics, including Tokens Per Sentence (TPS) and\nRelative Tokenization Cost (RTC), benchmarked against English baselines. Our\ncross-linguistic analysis reveals substantial and systematic disparities:\nLatin-script languages consistently exhibit higher tokenization efficiency,\nwhile non-Latin and morphologically complex languages incur significantly\ngreater token inflation, often 3-5 times higher RTC ratios. These\ninefficiencies translate into increased computational costs and reduced\neffective context utilization for underrepresented languages. Overall, the\nfindings highlight structural inequities in current AI systems, where speakers\nof low-resource and non-Latin languages face disproportionate computational\ndisadvantages. Future research should prioritize the development of\nlinguistically informed tokenization strategies and adaptive vocabulary\nconstruction methods that incorporate typological diversity, ensuring more\ninclusive and computationally equitable multilingual AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization disparities pose a significant barrier to achieving equitable\naccess to artificial intelligence across linguistically diverse populations.\nThis study conducts a large-scale cross-linguistic evaluation of tokenization\nefficiency in over 200 languages to systematically quantify computational\ninequities in large language models (LLMs). Using a standardized experimental\nframework, we applied consistent preprocessing and normalization protocols,\nfollowed by uniform tokenization through the tiktoken library across all\nlanguage samples. Comprehensive tokenization statistics were collected using\nestablished evaluation metrics, including Tokens Per Sentence (TPS) and\nRelative Tokenization Cost (RTC), benchmarked against English baselines. Our\ncross-linguistic analysis reveals substantial and systematic disparities:\nLatin-script languages consistently exhibit higher tokenization efficiency,\nwhile non-Latin and morphologically complex languages incur significantly\ngreater token inflation, often 3-5 times higher RTC ratios. These\ninefficiencies translate into increased computational costs and reduced\neffective context utilization for underrepresented languages. Overall, the\nfindings highlight structural inequities in current AI systems, where speakers\nof low-resource and non-Latin languages face disproportionate computational\ndisadvantages. Future research should prioritize the development of\nlinguistically informed tokenization strategies and adaptive vocabulary\nconstruction methods that incorporate typological diversity, ensuring more\ninclusive and computationally equitable multilingual AI systems."
                },
                "authors": [
                    {
                        "name": "Hailay Kidu Teklehaymanot"
                    },
                    {
                        "name": "Wolfgang Nejdl"
                    }
                ],
                "author_detail": {
                    "name": "Wolfgang Nejdl"
                },
                "author": "Wolfgang Nejdl",
                "arxiv_comment": "6 pages 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12389v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12389v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.1; H.3.3; F.2.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12386v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12386v1",
                "updated": "2025-10-14T11:10:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    11,
                    10,
                    35,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T11:10:35Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    11,
                    10,
                    35,
                    1,
                    287,
                    0
                ],
                "title": "Hey Dashboard!: Supporting Voice, Text, and Pointing Modalities in\n  Dashboard Onboarding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hey Dashboard!: Supporting Voice, Text, and Pointing Modalities in\n  Dashboard Onboarding"
                },
                "summary": "Visualization dashboards are regularly used for data exploration and\nanalysis, but their complex interactions and interlinked views often require\ntime-consuming onboarding sessions from dashboard authors. Preparing these\nonboarding materials is labor-intensive and requires manual updates when\ndashboards change. Recent advances in multimodal interaction powered by large\nlanguage models (LLMs) provide ways to support self-guided onboarding. We\npresent DIANA (Dashboard Interactive Assistant for Navigation and Analysis), a\nmultimodal dashboard assistant that helps users for navigation and guided\nanalysis through chat, audio, and mouse-based interactions. Users can choose\nany interaction modality or a combination of them to onboard themselves on the\ndashboard. Each modality highlights relevant dashboard features to support user\norientation. Unlike typical LLM systems that rely solely on text-based chat,\nDIANA combines multiple modalities to provide explanations directly in the\ndashboard interface. We conducted a qualitative user study to understand the\nuse of different modalities for different types of onboarding tasks and their\ncomplexities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visualization dashboards are regularly used for data exploration and\nanalysis, but their complex interactions and interlinked views often require\ntime-consuming onboarding sessions from dashboard authors. Preparing these\nonboarding materials is labor-intensive and requires manual updates when\ndashboards change. Recent advances in multimodal interaction powered by large\nlanguage models (LLMs) provide ways to support self-guided onboarding. We\npresent DIANA (Dashboard Interactive Assistant for Navigation and Analysis), a\nmultimodal dashboard assistant that helps users for navigation and guided\nanalysis through chat, audio, and mouse-based interactions. Users can choose\nany interaction modality or a combination of them to onboard themselves on the\ndashboard. Each modality highlights relevant dashboard features to support user\norientation. Unlike typical LLM systems that rely solely on text-based chat,\nDIANA combines multiple modalities to provide explanations directly in the\ndashboard interface. We conducted a qualitative user study to understand the\nuse of different modalities for different types of onboarding tasks and their\ncomplexities."
                },
                "authors": [
                    {
                        "name": "Vaishali Dhanoa"
                    },
                    {
                        "name": "Gabriela Molina Len"
                    },
                    {
                        "name": "Eve Hoggan"
                    },
                    {
                        "name": "Eduard Grller"
                    },
                    {
                        "name": "Marc Streit"
                    },
                    {
                        "name": "Niklas Elmqvist"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Elmqvist"
                },
                "author": "Niklas Elmqvist",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12386v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12386v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12385v1",
                "updated": "2025-10-14T11:03:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    11,
                    3,
                    30,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T11:03:30Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    11,
                    3,
                    30,
                    1,
                    287,
                    0
                ],
                "title": "Learning to Recognize Correctly Completed Procedure Steps in Egocentric\n  Assembly Videos through Spatio-Temporal Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Recognize Correctly Completed Procedure Steps in Egocentric\n  Assembly Videos through Spatio-Temporal Modeling"
                },
                "summary": "Procedure step recognition (PSR) aims to identify all correctly completed\nsteps and their sequential order in videos of procedural tasks. The existing\nstate-of-the-art models rely solely on detecting assembly object states in\nindividual video frames. By neglecting temporal features, model robustness and\naccuracy are limited, especially when objects are partially occluded. To\novercome these limitations, we propose Spatio-Temporal Occlusion-Resilient\nModeling for Procedure Step Recognition (STORM-PSR), a dual-stream framework\nfor PSR that leverages both spatial and temporal features. The assembly state\ndetection stream operates effectively with unobstructed views of the object,\nwhile the spatio-temporal stream captures both spatial and temporal features to\nrecognize step completions even under partial occlusion. This stream includes a\nspatial encoder, pre-trained using a novel weakly supervised approach to\ncapture meaningful spatial representations, and a transformer-based temporal\nencoder that learns how these spatial features relate over time. STORM-PSR is\nevaluated on the MECCANO and IndustReal datasets, reducing the average delay\nbetween actual and predicted assembly step completions by 11.2% and 26.1%,\nrespectively, compared to prior methods. We demonstrate that this reduction in\ndelay is driven by the spatio-temporal stream, which does not rely on\nunobstructed views of the object to infer completed steps. The code for\nSTORM-PSR, along with the newly annotated MECCANO labels, is made publicly\navailable at https://timschoonbeek.github.io/stormpsr .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Procedure step recognition (PSR) aims to identify all correctly completed\nsteps and their sequential order in videos of procedural tasks. The existing\nstate-of-the-art models rely solely on detecting assembly object states in\nindividual video frames. By neglecting temporal features, model robustness and\naccuracy are limited, especially when objects are partially occluded. To\novercome these limitations, we propose Spatio-Temporal Occlusion-Resilient\nModeling for Procedure Step Recognition (STORM-PSR), a dual-stream framework\nfor PSR that leverages both spatial and temporal features. The assembly state\ndetection stream operates effectively with unobstructed views of the object,\nwhile the spatio-temporal stream captures both spatial and temporal features to\nrecognize step completions even under partial occlusion. This stream includes a\nspatial encoder, pre-trained using a novel weakly supervised approach to\ncapture meaningful spatial representations, and a transformer-based temporal\nencoder that learns how these spatial features relate over time. STORM-PSR is\nevaluated on the MECCANO and IndustReal datasets, reducing the average delay\nbetween actual and predicted assembly step completions by 11.2% and 26.1%,\nrespectively, compared to prior methods. We demonstrate that this reduction in\ndelay is driven by the spatio-temporal stream, which does not rely on\nunobstructed views of the object to infer completed steps. The code for\nSTORM-PSR, along with the newly annotated MECCANO labels, is made publicly\navailable at https://timschoonbeek.github.io/stormpsr ."
                },
                "authors": [
                    {
                        "name": "Tim J. Schoonbeek"
                    },
                    {
                        "name": "Shao-Hsuan Hung"
                    },
                    {
                        "name": "Dan Lehman"
                    },
                    {
                        "name": "Hans Onvlee"
                    },
                    {
                        "name": "Jacek Kustra"
                    },
                    {
                        "name": "Peter H. N. de With"
                    },
                    {
                        "name": "Fons van der Sommen"
                    }
                ],
                "author_detail": {
                    "name": "Fons van der Sommen"
                },
                "author": "Fons van der Sommen",
                "arxiv_comment": "26 pages, 7 figures and 5 tables in the main paper and one figure and\n  table in the appendix. To be published in Computer Vision and Image\n  Understanding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04900v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04900v2",
                "updated": "2025-10-14T10:57:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    10,
                    57,
                    50,
                    1,
                    287,
                    0
                ],
                "published": "2025-06-05T11:30:57Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    11,
                    30,
                    57,
                    3,
                    156,
                    0
                ],
                "title": "Finite-Sample Distortion in Kernel Specification Tests: A Perturbation\n  Analysis of Empirical Directional Components",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finite-Sample Distortion in Kernel Specification Tests: A Perturbation\n  Analysis of Empirical Directional Components"
                },
                "summary": "This paper provides a new theoretical lens for understanding the\nfinite-sample performance of kernel-based specification tests, such as the\nKernel Conditional Moment (KCM) test. Rather than introducing a fundamentally\nnew test, we isolate and rigorously analyze the finite-sample distortion\narising from the discrepancy between the empirical and population eigenspaces\nof the kernel operator. Using perturbation theory for compact operators, we\ndemonstrate that the estimation error in directional components is governed by\nlocal eigengaps: components associated with small eigenvalues are highly\nunstable and contribute primarily noise rather than signal under fixed\nalternatives. Although this error vanishes asymptotically under the null, it\ncan substantially degrade power in finite samples. This insight explains why\nthe effective power of omnibus kernel tests is often concentrated in a\nlow-dimensional subspace. We illustrate how truncating unstable high-frequency\ncomponents--a natural consequence of our analysis--can improve finite-sample\nperformance, but emphasize that the core contribution is the diagnostic\nunderstanding of \\textit{why} and \\textit{when} such instability occurs. The\nanalysis is largely non-asymptotic and applies broadly to reproducing kernel\nHilbert space-based inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides a new theoretical lens for understanding the\nfinite-sample performance of kernel-based specification tests, such as the\nKernel Conditional Moment (KCM) test. Rather than introducing a fundamentally\nnew test, we isolate and rigorously analyze the finite-sample distortion\narising from the discrepancy between the empirical and population eigenspaces\nof the kernel operator. Using perturbation theory for compact operators, we\ndemonstrate that the estimation error in directional components is governed by\nlocal eigengaps: components associated with small eigenvalues are highly\nunstable and contribute primarily noise rather than signal under fixed\nalternatives. Although this error vanishes asymptotically under the null, it\ncan substantially degrade power in finite samples. This insight explains why\nthe effective power of omnibus kernel tests is often concentrated in a\nlow-dimensional subspace. We illustrate how truncating unstable high-frequency\ncomponents--a natural consequence of our analysis--can improve finite-sample\nperformance, but emphasize that the core contribution is the diagnostic\nunderstanding of \\textit{why} and \\textit{when} such instability occurs. The\nanalysis is largely non-asymptotic and applies broadly to reproducing kernel\nHilbert space-based inference."
                },
                "authors": [
                    {
                        "name": "Cui Rui"
                    },
                    {
                        "name": "Li Yuhao"
                    },
                    {
                        "name": "Song Xiaojun"
                    }
                ],
                "author_detail": {
                    "name": "Song Xiaojun"
                },
                "author": "Song Xiaojun",
                "arxiv_comment": "major revision",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04900v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04900v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12376v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12376v1",
                "updated": "2025-10-14T10:50:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    10,
                    50,
                    45,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T10:50:45Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    10,
                    50,
                    45,
                    1,
                    287,
                    0
                ],
                "title": "Deep Attention-guided Adaptive Subsampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Attention-guided Adaptive Subsampling"
                },
                "summary": "Although deep neural networks have provided impressive gains in performance,\nthese improvements often come at the cost of increased computational complexity\nand expense. In many cases, such as 3D volume or video classification tasks,\nnot all slices or frames are necessary due to inherent redundancies. To address\nthis issue, we propose a novel learnable subsampling framework that can be\nintegrated into any neural network architecture. Subsampling, being a\nnondifferentiable operation, poses significant challenges for direct adaptation\ninto deep learning models. While some works, have proposed solutions using the\nGumbel-max trick to overcome the problem of non-differentiability, they fall\nshort in a crucial aspect: they are only task-adaptive and not inputadaptive.\nOnce the sampling mechanism is learned, it remains static and does not adjust\nto different inputs, making it unsuitable for real-world applications. To this\nend, we propose an attention-guided sampling module that adapts to inputs even\nduring inference. This dynamic adaptation results in performance gains and\nreduces complexity in deep neural network models. We demonstrate the\neffectiveness of our method on 3D medical imaging datasets from MedMNIST3D as\nwell as two ultrasound video datasets for classification tasks, one of them\nbeing a challenging in-house dataset collected under real-world clinical\nconditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although deep neural networks have provided impressive gains in performance,\nthese improvements often come at the cost of increased computational complexity\nand expense. In many cases, such as 3D volume or video classification tasks,\nnot all slices or frames are necessary due to inherent redundancies. To address\nthis issue, we propose a novel learnable subsampling framework that can be\nintegrated into any neural network architecture. Subsampling, being a\nnondifferentiable operation, poses significant challenges for direct adaptation\ninto deep learning models. While some works, have proposed solutions using the\nGumbel-max trick to overcome the problem of non-differentiability, they fall\nshort in a crucial aspect: they are only task-adaptive and not inputadaptive.\nOnce the sampling mechanism is learned, it remains static and does not adjust\nto different inputs, making it unsuitable for real-world applications. To this\nend, we propose an attention-guided sampling module that adapts to inputs even\nduring inference. This dynamic adaptation results in performance gains and\nreduces complexity in deep neural network models. We demonstrate the\neffectiveness of our method on 3D medical imaging datasets from MedMNIST3D as\nwell as two ultrasound video datasets for classification tasks, one of them\nbeing a challenging in-house dataset collected under real-world clinical\nconditions."
                },
                "authors": [
                    {
                        "name": "Sharath M Shankaranarayana"
                    },
                    {
                        "name": "Soumava Kumar Roy"
                    },
                    {
                        "name": "Prasad Sudhakar"
                    },
                    {
                        "name": "Chandan Aladahalli"
                    }
                ],
                "author_detail": {
                    "name": "Chandan Aladahalli"
                },
                "author": "Chandan Aladahalli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12376v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12376v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.12801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12801v1",
                "updated": "2025-10-14T17:59:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    59,
                    58,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T17:59:58Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    59,
                    58,
                    1,
                    287,
                    0
                ],
                "title": "DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search"
                },
                "summary": "Multimodal Large Language Models (MLLMs) in real-world applications require\naccess to external knowledge sources and must remain responsive to the dynamic\nand ever-changing real-world information in order to address\ninformation-seeking and knowledge-intensive user queries. Existing approaches,\nsuch as retrieval augmented generation (RAG) methods, search agents, and search\nequipped MLLMs, often suffer from rigid pipelines, excessive search calls, and\npoorly constructed search queries, which result in inefficiencies and\nsuboptimal outcomes. To address these limitations, we present DeepMMSearch-R1,\nthe first multimodal LLM capable of performing on-demand, multi-turn web\nsearches and dynamically crafting queries for both image and text search tools.\nSpecifically, DeepMMSearch-R1 can initiate web searches based on relevant crops\nof the input image making the image search more effective, and can iteratively\nadapt text search queries based on retrieved information, thereby enabling\nself-reflection and self-correction. Our approach relies on a two-stage\ntraining pipeline: a cold start supervised finetuning phase followed by an\nonline reinforcement learning optimization. For training, we introduce\nDeepMMSearchVQA, a novel multimodal VQA dataset created through an automated\npipeline intermixed with real-world information from web search tools. This\ndataset contains diverse, multi-hop queries that integrate textual and visual\ninformation, teaching the model when to search, what to search for, which\nsearch tool to use and how to reason over the retrieved information. We conduct\nextensive experiments across a range of knowledge-intensive benchmarks to\ndemonstrate the superiority of our approach. Finally, we analyze the results\nand provide insights that are valuable for advancing multimodal web-search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) in real-world applications require\naccess to external knowledge sources and must remain responsive to the dynamic\nand ever-changing real-world information in order to address\ninformation-seeking and knowledge-intensive user queries. Existing approaches,\nsuch as retrieval augmented generation (RAG) methods, search agents, and search\nequipped MLLMs, often suffer from rigid pipelines, excessive search calls, and\npoorly constructed search queries, which result in inefficiencies and\nsuboptimal outcomes. To address these limitations, we present DeepMMSearch-R1,\nthe first multimodal LLM capable of performing on-demand, multi-turn web\nsearches and dynamically crafting queries for both image and text search tools.\nSpecifically, DeepMMSearch-R1 can initiate web searches based on relevant crops\nof the input image making the image search more effective, and can iteratively\nadapt text search queries based on retrieved information, thereby enabling\nself-reflection and self-correction. Our approach relies on a two-stage\ntraining pipeline: a cold start supervised finetuning phase followed by an\nonline reinforcement learning optimization. For training, we introduce\nDeepMMSearchVQA, a novel multimodal VQA dataset created through an automated\npipeline intermixed with real-world information from web search tools. This\ndataset contains diverse, multi-hop queries that integrate textual and visual\ninformation, teaching the model when to search, what to search for, which\nsearch tool to use and how to reason over the retrieved information. We conduct\nextensive experiments across a range of knowledge-intensive benchmarks to\ndemonstrate the superiority of our approach. Finally, we analyze the results\nand provide insights that are valuable for advancing multimodal web-search."
                },
                "authors": [
                    {
                        "name": "Kartik Narayan"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Tian Cao"
                    },
                    {
                        "name": "Kavya Nerella"
                    },
                    {
                        "name": "Vishal M. Patel"
                    },
                    {
                        "name": "Navid Shiee"
                    },
                    {
                        "name": "Peter Grasch"
                    },
                    {
                        "name": "Chao Jia"
                    },
                    {
                        "name": "Yinfei Yang"
                    },
                    {
                        "name": "Zhe Gan"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Gan"
                },
                "author": "Zhe Gan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12796v1",
                "updated": "2025-10-14T17:59:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    59,
                    47,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T17:59:47Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    59,
                    47,
                    1,
                    287,
                    0
                ],
                "title": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving"
                },
                "summary": "Scaling Vision-Language-Action (VLA) models on large-scale data offers a\npromising path to achieving a more generalized driving intelligence. However,\nVLA models are limited by a ``supervision deficit'': the vast model capacity is\nsupervised by sparse, low-dimensional actions, leaving much of their\nrepresentational power underutilized. To remedy this, we propose\n\\textbf{DriveVLA-W0}, a training paradigm that employs world modeling to\npredict future images. This task generates a dense, self-supervised signal that\ncompels the model to learn the underlying dynamics of the driving environment.\nWe showcase the paradigm's versatility by instantiating it for two dominant VLA\narchetypes: an autoregressive world model for VLAs that use discrete visual\ntokens, and a diffusion world model for those operating on continuous visual\nfeatures. Building on the rich representations learned from world modeling, we\nintroduce a lightweight action expert to address the inference latency for\nreal-time deployment. Extensive experiments on the NAVSIM v1/v2 benchmark and a\n680x larger in-house dataset demonstrate that DriveVLA-W0 significantly\noutperforms BEV and VLA baselines. Crucially, it amplifies the data scaling\nlaw, showing that performance gains accelerate as the training dataset size\nincreases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Vision-Language-Action (VLA) models on large-scale data offers a\npromising path to achieving a more generalized driving intelligence. However,\nVLA models are limited by a ``supervision deficit'': the vast model capacity is\nsupervised by sparse, low-dimensional actions, leaving much of their\nrepresentational power underutilized. To remedy this, we propose\n\\textbf{DriveVLA-W0}, a training paradigm that employs world modeling to\npredict future images. This task generates a dense, self-supervised signal that\ncompels the model to learn the underlying dynamics of the driving environment.\nWe showcase the paradigm's versatility by instantiating it for two dominant VLA\narchetypes: an autoregressive world model for VLAs that use discrete visual\ntokens, and a diffusion world model for those operating on continuous visual\nfeatures. Building on the rich representations learned from world modeling, we\nintroduce a lightweight action expert to address the inference latency for\nreal-time deployment. Extensive experiments on the NAVSIM v1/v2 benchmark and a\n680x larger in-house dataset demonstrate that DriveVLA-W0 significantly\noutperforms BEV and VLA baselines. Crucially, it amplifies the data scaling\nlaw, showing that performance gains accelerate as the training dataset size\nincreases."
                },
                "authors": [
                    {
                        "name": "Yingyan Li"
                    },
                    {
                        "name": "Shuyao Shang"
                    },
                    {
                        "name": "Weisong Liu"
                    },
                    {
                        "name": "Bing Zhan"
                    },
                    {
                        "name": "Haochen Wang"
                    },
                    {
                        "name": "Yuqi Wang"
                    },
                    {
                        "name": "Yuntao Chen"
                    },
                    {
                        "name": "Xiaoman Wang"
                    },
                    {
                        "name": "Yasong An"
                    },
                    {
                        "name": "Chufeng Tang"
                    },
                    {
                        "name": "Lu Hou"
                    },
                    {
                        "name": "Lue Fan"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoxiang Zhang"
                },
                "author": "Zhaoxiang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12787v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12787v1",
                "updated": "2025-10-14T17:57:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    57,
                    4,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T17:57:04Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    57,
                    4,
                    1,
                    287,
                    0
                ],
                "title": "Ax-Prover: A Deep Reasoning Agentic Framework for Theorem Proving in\n  Mathematics and Quantum Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ax-Prover: A Deep Reasoning Agentic Framework for Theorem Proving in\n  Mathematics and Quantum Physics"
                },
                "summary": "We present Ax-Prover, a multi-agent system for automated theorem proving in\nLean that can solve problems across diverse scientific domains and operate\neither autonomously or collaboratively with human experts. To achieve this,\nAx-Prover approaches scientific problem solving through formal proof\ngeneration, a process that demands both creative reasoning and strict syntactic\nrigor. Ax-Prover meets this challenge by equipping Large Language Models\n(LLMs), which provide knowledge and reasoning, with Lean tools via the Model\nContext Protocol (MCP), which ensure formal correctness. To evaluate its\nperformance as an autonomous prover, we benchmark our approach against frontier\nLLMs and specialized prover models on two public math benchmarks and on two\nLean benchmarks we introduce in the fields of abstract algebra and quantum\ntheory. On public datasets, Ax-Prover is competitive with state-of-the-art\nprovers, while it largely outperform them on the new benchmarks. This shows\nthat, unlike specialized systems that struggle to generalize, our tool-based\nagentic theorem prover approach offers a generalizable methodology for formal\nverification across diverse scientific domains. Furthermore, we demonstrate\nAx-Prover's assistant capabilities in a practical use case, showing how it\nenabled an expert mathematician to formalize the proof of a complex\ncryptography theorem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Ax-Prover, a multi-agent system for automated theorem proving in\nLean that can solve problems across diverse scientific domains and operate\neither autonomously or collaboratively with human experts. To achieve this,\nAx-Prover approaches scientific problem solving through formal proof\ngeneration, a process that demands both creative reasoning and strict syntactic\nrigor. Ax-Prover meets this challenge by equipping Large Language Models\n(LLMs), which provide knowledge and reasoning, with Lean tools via the Model\nContext Protocol (MCP), which ensure formal correctness. To evaluate its\nperformance as an autonomous prover, we benchmark our approach against frontier\nLLMs and specialized prover models on two public math benchmarks and on two\nLean benchmarks we introduce in the fields of abstract algebra and quantum\ntheory. On public datasets, Ax-Prover is competitive with state-of-the-art\nprovers, while it largely outperform them on the new benchmarks. This shows\nthat, unlike specialized systems that struggle to generalize, our tool-based\nagentic theorem prover approach offers a generalizable methodology for formal\nverification across diverse scientific domains. Furthermore, we demonstrate\nAx-Prover's assistant capabilities in a practical use case, showing how it\nenabled an expert mathematician to formalize the proof of a complex\ncryptography theorem."
                },
                "authors": [
                    {
                        "name": "Marco Del Tredici"
                    },
                    {
                        "name": "Jacob McCarran"
                    },
                    {
                        "name": "Benjamin Breen"
                    },
                    {
                        "name": "Javier Aspuru Mijares"
                    },
                    {
                        "name": "Weichen Winston Yin"
                    },
                    {
                        "name": "Jacob M. Taylor"
                    },
                    {
                        "name": "Frank Koppens"
                    },
                    {
                        "name": "Dirk Englund"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Englund"
                },
                "author": "Dirk Englund",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12787v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12787v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12773v1",
                "updated": "2025-10-14T17:51:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    51,
                    26,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T17:51:26Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    51,
                    26,
                    1,
                    287,
                    0
                ],
                "title": "Dr.LLM: Dynamic Layer Routing in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dr.LLM: Dynamic Layer Routing in LLMs"
                },
                "summary": "Large Language Models (LLMs) process every token through all layers of a\ntransformer stack, causing wasted computation on simple queries and\ninsufficient flexibility for harder ones that need deeper reasoning.\nAdaptive-depth methods can improve efficiency, but prior approaches rely on\ncostly inference-time search, architectural changes, or large-scale retraining,\nand in practice often degrade accuracy despite efficiency gains. We introduce\nDr.LLM, Dynamic routing of Layers for LLMs, a retrofittable framework that\nequips pretrained models with lightweight per-layer routers deciding to skip,\nexecute, or repeat a block. Routers are trained with explicit supervision:\nusing Monte Carlo Tree Search (MCTS), we derive high-quality layer\nconfigurations that preserve or improve accuracy under a compute budget. Our\ndesign, windowed pooling for stable routing, focal loss with class balancing,\nand bottleneck MLP routers, ensures robustness under class imbalance and long\nsequences. On ARC (logic) and DART (math), Dr.LLM improves accuracy by up to\n+3.4%p while saving 5 layers per example on average. Routers generalize to\nout-of-domain tasks (MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA,\nAGIEval) with only 0.85% accuracy drop while retaining efficiency, and\noutperform prior routing methods by up to +7.7%p. Overall, Dr.LLM shows that\nexplicitly supervised routers retrofit frozen LLMs for budget-aware,\naccuracy-driven inference without altering base weights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) process every token through all layers of a\ntransformer stack, causing wasted computation on simple queries and\ninsufficient flexibility for harder ones that need deeper reasoning.\nAdaptive-depth methods can improve efficiency, but prior approaches rely on\ncostly inference-time search, architectural changes, or large-scale retraining,\nand in practice often degrade accuracy despite efficiency gains. We introduce\nDr.LLM, Dynamic routing of Layers for LLMs, a retrofittable framework that\nequips pretrained models with lightweight per-layer routers deciding to skip,\nexecute, or repeat a block. Routers are trained with explicit supervision:\nusing Monte Carlo Tree Search (MCTS), we derive high-quality layer\nconfigurations that preserve or improve accuracy under a compute budget. Our\ndesign, windowed pooling for stable routing, focal loss with class balancing,\nand bottleneck MLP routers, ensures robustness under class imbalance and long\nsequences. On ARC (logic) and DART (math), Dr.LLM improves accuracy by up to\n+3.4%p while saving 5 layers per example on average. Routers generalize to\nout-of-domain tasks (MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA,\nAGIEval) with only 0.85% accuracy drop while retaining efficiency, and\noutperform prior routing methods by up to +7.7%p. Overall, Dr.LLM shows that\nexplicitly supervised routers retrofit frozen LLMs for budget-aware,\naccuracy-driven inference without altering base weights."
                },
                "authors": [
                    {
                        "name": "Ahmed Heakl"
                    },
                    {
                        "name": "Martin Gubri"
                    },
                    {
                        "name": "Salman Khan"
                    },
                    {
                        "name": "Sangdoo Yun"
                    },
                    {
                        "name": "Seong Joon Oh"
                    }
                ],
                "author_detail": {
                    "name": "Seong Joon Oh"
                },
                "author": "Seong Joon Oh",
                "arxiv_comment": "17 pages, Under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12766v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12766v1",
                "updated": "2025-10-14T17:45:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    45,
                    31,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T17:45:31Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    45,
                    31,
                    1,
                    287,
                    0
                ],
                "title": "Language Models Model Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models Model Language"
                },
                "summary": "Linguistic commentary on LLMs, heavily influenced by the theoretical\nframeworks of de Saussure and Chomsky, is often speculative and unproductive.\nCritics challenge whether LLMs can legitimately model language, citing the need\nfor \"deep structure\" or \"grounding\" to achieve an idealized linguistic\n\"competence.\" We argue for a radical shift in perspective towards the\nempiricist principles of Witold Ma\\'nczak, a prominent general and historical\nlinguist. He defines language not as a \"system of signs\" or a \"computational\nsystem of the brain\" but as the totality of all that is said and written. Above\nall, he identifies frequency of use of particular language elements as\nlanguage's primary governing principle. Using his framework, we challenge prior\ncritiques of LLMs and provide a constructive guide for designing, evaluating,\nand interpreting language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linguistic commentary on LLMs, heavily influenced by the theoretical\nframeworks of de Saussure and Chomsky, is often speculative and unproductive.\nCritics challenge whether LLMs can legitimately model language, citing the need\nfor \"deep structure\" or \"grounding\" to achieve an idealized linguistic\n\"competence.\" We argue for a radical shift in perspective towards the\nempiricist principles of Witold Ma\\'nczak, a prominent general and historical\nlinguist. He defines language not as a \"system of signs\" or a \"computational\nsystem of the brain\" but as the totality of all that is said and written. Above\nall, he identifies frequency of use of particular language elements as\nlanguage's primary governing principle. Using his framework, we challenge prior\ncritiques of LLMs and provide a constructive guide for designing, evaluating,\nand interpreting language models."
                },
                "authors": [
                    {
                        "name": "ukasz Borchmann"
                    }
                ],
                "author_detail": {
                    "name": "ukasz Borchmann"
                },
                "author": "ukasz Borchmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12766v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12766v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12765v1",
                "updated": "2025-10-14T17:45:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    45,
                    22,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T17:45:22Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    45,
                    22,
                    1,
                    287,
                    0
                ],
                "title": "Efficient Perceptual Image Super Resolution: AIM 2025 Study and\n  Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Perceptual Image Super Resolution: AIM 2025 Study and\n  Benchmark"
                },
                "summary": "This paper presents a comprehensive study and benchmark on Efficient\nPerceptual Super-Resolution (EPSR). While significant progress has been made in\nefficient PSNR-oriented super resolution, approaches focusing on perceptual\nquality metrics remain relatively inefficient. Motivated by this gap, we aim to\nreplicate or improve the perceptual results of Real-ESRGAN while meeting strict\nefficiency constraints: a maximum of 5M parameters and 2000 GFLOPs, calculated\nfor an input size of 960x540 pixels. The proposed solutions were evaluated on a\nnovel dataset consisting of 500 test images of 4K resolution, each degraded\nusing multiple degradation types, without providing the original high-quality\ncounterparts. This design aims to reflect realistic deployment conditions and\nserves as a diverse and challenging benchmark. The top-performing approach\nmanages to outperform Real-ESRGAN across all benchmark datasets, demonstrating\nthe potential of efficient methods in the perceptual domain. This paper\nestablishes the modern baselines for efficient perceptual super resolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comprehensive study and benchmark on Efficient\nPerceptual Super-Resolution (EPSR). While significant progress has been made in\nefficient PSNR-oriented super resolution, approaches focusing on perceptual\nquality metrics remain relatively inefficient. Motivated by this gap, we aim to\nreplicate or improve the perceptual results of Real-ESRGAN while meeting strict\nefficiency constraints: a maximum of 5M parameters and 2000 GFLOPs, calculated\nfor an input size of 960x540 pixels. The proposed solutions were evaluated on a\nnovel dataset consisting of 500 test images of 4K resolution, each degraded\nusing multiple degradation types, without providing the original high-quality\ncounterparts. This design aims to reflect realistic deployment conditions and\nserves as a diverse and challenging benchmark. The top-performing approach\nmanages to outperform Real-ESRGAN across all benchmark datasets, demonstrating\nthe potential of efficient methods in the perceptual domain. This paper\nestablishes the modern baselines for efficient perceptual super resolution."
                },
                "authors": [
                    {
                        "name": "Bruno Longarela"
                    },
                    {
                        "name": "Marcos V. Conde"
                    },
                    {
                        "name": "Alvaro Garcia"
                    },
                    {
                        "name": "Radu Timofte"
                    }
                ],
                "author_detail": {
                    "name": "Radu Timofte"
                },
                "author": "Radu Timofte",
                "arxiv_comment": "ICCV 2025 - AIM Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12742v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12742v1",
                "updated": "2025-10-14T17:20:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    20,
                    4,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T17:20:04Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    20,
                    4,
                    1,
                    287,
                    0
                ],
                "title": "CTRL-Rec: Controlling Recommender Systems With Natural Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CTRL-Rec: Controlling Recommender Systems With Natural Language"
                },
                "summary": "When users are dissatisfied with recommendations from a recommender system,\nthey often lack fine-grained controls for changing them. Large language models\n(LLMs) offer a solution by allowing users to guide their recommendations\nthrough natural language requests (e.g., \"I want to see respectful posts with a\ndifferent perspective than mine\"). We propose a method, CTRL-Rec, that allows\nfor natural language control of traditional recommender systems in real-time\nwith computational efficiency. Specifically, at training time, we use an LLM to\nsimulate whether users would approve of items based on their language requests,\nand we train embedding models that approximate such simulated judgments. We\nthen integrate these user-request-based predictions into the standard weighting\nof signals that traditional recommender systems optimize. At deployment time,\nwe require only a single LLM embedding computation per user request, allowing\nfor real-time control of recommendations. In experiments with the MovieLens\ndataset, our method consistently allows for fine-grained control across a\ndiversity of requests. In a study with 19 Letterboxd users, we find that\nCTRL-Rec was positively received by users and significantly enhanced users'\nsense of control and satisfaction with recommendations compared to traditional\ncontrols.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When users are dissatisfied with recommendations from a recommender system,\nthey often lack fine-grained controls for changing them. Large language models\n(LLMs) offer a solution by allowing users to guide their recommendations\nthrough natural language requests (e.g., \"I want to see respectful posts with a\ndifferent perspective than mine\"). We propose a method, CTRL-Rec, that allows\nfor natural language control of traditional recommender systems in real-time\nwith computational efficiency. Specifically, at training time, we use an LLM to\nsimulate whether users would approve of items based on their language requests,\nand we train embedding models that approximate such simulated judgments. We\nthen integrate these user-request-based predictions into the standard weighting\nof signals that traditional recommender systems optimize. At deployment time,\nwe require only a single LLM embedding computation per user request, allowing\nfor real-time control of recommendations. In experiments with the MovieLens\ndataset, our method consistently allows for fine-grained control across a\ndiversity of requests. In a study with 19 Letterboxd users, we find that\nCTRL-Rec was positively received by users and significantly enhanced users'\nsense of control and satisfaction with recommendations compared to traditional\ncontrols."
                },
                "authors": [
                    {
                        "name": "Micah Carroll"
                    },
                    {
                        "name": "Adeline Foote"
                    },
                    {
                        "name": "Kevin Feng"
                    },
                    {
                        "name": "Marcus Williams"
                    },
                    {
                        "name": "Anca Dragan"
                    },
                    {
                        "name": "W. Bradley Knox"
                    },
                    {
                        "name": "Smitha Milli"
                    }
                ],
                "author_detail": {
                    "name": "Smitha Milli"
                },
                "author": "Smitha Milli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12742v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12742v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05510v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05510v2",
                "updated": "2025-10-14T17:14:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    14,
                    33,
                    1,
                    287,
                    0
                ],
                "published": "2025-05-07T09:01:06Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    9,
                    1,
                    6,
                    2,
                    127,
                    0
                ],
                "title": "How to Train Your Metamorphic Deep Neural Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Train Your Metamorphic Deep Neural Network"
                },
                "summary": "Neural Metamorphosis (NeuMeta) is a recent paradigm for generating neural\nnetworks of varying width and depth. Based on Implicit Neural Representation\n(INR), NeuMeta learns a continuous weight manifold, enabling the direct\ngeneration of compressed models, including those with configurations not seen\nduring training. While promising, the original formulation of NeuMeta proves\neffective only for the final layers of the undelying model, limiting its\nbroader applicability. In this work, we propose a training algorithm that\nextends the capabilities of NeuMeta to enable full-network metamorphosis with\nminimal accuracy degradation. Our approach follows a structured recipe\ncomprising block-wise incremental training, INR initialization, and strategies\nfor replacing batch normalization. The resulting metamorphic networks maintain\ncompetitive accuracy across a wide range of compression ratios, offering a\nscalable solution for adaptable and efficient deployment of deep models. The\ncode is available at: https://github.com/TSommariva/HTTY_NeuMeta.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Metamorphosis (NeuMeta) is a recent paradigm for generating neural\nnetworks of varying width and depth. Based on Implicit Neural Representation\n(INR), NeuMeta learns a continuous weight manifold, enabling the direct\ngeneration of compressed models, including those with configurations not seen\nduring training. While promising, the original formulation of NeuMeta proves\neffective only for the final layers of the undelying model, limiting its\nbroader applicability. In this work, we propose a training algorithm that\nextends the capabilities of NeuMeta to enable full-network metamorphosis with\nminimal accuracy degradation. Our approach follows a structured recipe\ncomprising block-wise incremental training, INR initialization, and strategies\nfor replacing batch normalization. The resulting metamorphic networks maintain\ncompetitive accuracy across a wide range of compression ratios, offering a\nscalable solution for adaptable and efficient deployment of deep models. The\ncode is available at: https://github.com/TSommariva/HTTY_NeuMeta."
                },
                "authors": [
                    {
                        "name": "Thomas Sommariva"
                    },
                    {
                        "name": "Simone Calderara"
                    },
                    {
                        "name": "Angelo Porrello"
                    }
                ],
                "author_detail": {
                    "name": "Angelo Porrello"
                },
                "author": "Angelo Porrello",
                "arxiv_comment": "Accepted with an Honorable Mention Award at ICIAP 2025 (Rome, Italy).\n  14 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05510v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05510v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12728v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12728v1",
                "updated": "2025-10-14T17:07:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    7,
                    37,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T17:07:37Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    7,
                    37,
                    1,
                    287,
                    0
                ],
                "title": "Data-Model Co-Evolution: Growing Test Sets to Refine LLM Behavior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Model Co-Evolution: Growing Test Sets to Refine LLM Behavior"
                },
                "summary": "A long-standing challenge in machine learning has been the rigid separation\nbetween data work and model refinement, enforced by slow fine-tuning cycles.\nThe rise of Large Language Models (LLMs) overcomes this historical barrier,\nallowing applications developers to instantly govern model behavior by editing\nprompt instructions. This shift enables a new paradigm: data-model\nco-evolution, where a living test set and a model's instructions evolve in\ntandem. We operationalize this paradigm in an interactive system designed to\naddress the critical challenge of encoding subtle, domain-specific policies\ninto prompt instructions. The system's structured workflow guides people to\ndiscover edge cases, articulate rationales for desired behavior, and\niteratively evaluate instruction revisions against a growing test set. A user\nstudy shows our workflow helps participants refine instructions systematically\nand specify ambiguous policies more concretely. This work points toward more\nrobust and responsible LLM applications through human-in-the-loop development\naligned with local preferences and policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A long-standing challenge in machine learning has been the rigid separation\nbetween data work and model refinement, enforced by slow fine-tuning cycles.\nThe rise of Large Language Models (LLMs) overcomes this historical barrier,\nallowing applications developers to instantly govern model behavior by editing\nprompt instructions. This shift enables a new paradigm: data-model\nco-evolution, where a living test set and a model's instructions evolve in\ntandem. We operationalize this paradigm in an interactive system designed to\naddress the critical challenge of encoding subtle, domain-specific policies\ninto prompt instructions. The system's structured workflow guides people to\ndiscover edge cases, articulate rationales for desired behavior, and\niteratively evaluate instruction revisions against a growing test set. A user\nstudy shows our workflow helps participants refine instructions systematically\nand specify ambiguous policies more concretely. This work points toward more\nrobust and responsible LLM applications through human-in-the-loop development\naligned with local preferences and policies."
                },
                "authors": [
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Minsuk Kahng"
                    }
                ],
                "author_detail": {
                    "name": "Minsuk Kahng"
                },
                "author": "Minsuk Kahng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12728v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12728v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12721v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12721v1",
                "updated": "2025-10-14T17:00:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    0,
                    13,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T17:00:13Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    0,
                    13,
                    1,
                    287,
                    0
                ],
                "title": "CARVQ: Corrective Adaptor with Group Residual Vector Quantization for\n  LLM Embedding Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CARVQ: Corrective Adaptor with Group Residual Vector Quantization for\n  LLM Embedding Compression"
                },
                "summary": "Large Language Models (LLMs) typically rely on a large number of parameters\nfor token embedding, leading to substantial storage requirements and memory\nfootprints. In particular, LLMs deployed on edge devices are memory-bound, and\nreducing the memory footprint by compressing the embedding layer not only frees\nup the memory bandwidth but also speeds up inference. To address this, we\nintroduce CARVQ, a post-training novel Corrective Adaptor combined with group\nResidual Vector Quantization. CARVQ relies on the composition of both linear\nand non-linear maps and mimics the original model embedding to compress to\napproximately 1.6 bits without requiring specialized hardware to support\nlower-bit storage. We test our method on pre-trained LLMs such as LLaMA-3.2-1B,\nLLaMA-3.2-3B, LLaMA-3.2-3B-Instruct, LLaMA-3.1-8B, Qwen2.5-7B, Qwen2.5-Math-7B\nand Phi-4, evaluating on common generative, discriminative, math and reasoning\ntasks. We show that in most cases, CARVQ can achieve lower average\nbitwidth-per-parameter while maintaining reasonable perplexity and accuracy\ncompared to scalar quantization. Our contributions include a novel compression\ntechnique that is compatible with state-of-the-art transformer quantization\nmethods and can be seamlessly integrated into any hardware supporting 4-bit\nmemory to reduce the model's memory footprint in memory-constrained devices.\nThis work demonstrates a crucial step toward the efficient deployment of LLMs\non edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) typically rely on a large number of parameters\nfor token embedding, leading to substantial storage requirements and memory\nfootprints. In particular, LLMs deployed on edge devices are memory-bound, and\nreducing the memory footprint by compressing the embedding layer not only frees\nup the memory bandwidth but also speeds up inference. To address this, we\nintroduce CARVQ, a post-training novel Corrective Adaptor combined with group\nResidual Vector Quantization. CARVQ relies on the composition of both linear\nand non-linear maps and mimics the original model embedding to compress to\napproximately 1.6 bits without requiring specialized hardware to support\nlower-bit storage. We test our method on pre-trained LLMs such as LLaMA-3.2-1B,\nLLaMA-3.2-3B, LLaMA-3.2-3B-Instruct, LLaMA-3.1-8B, Qwen2.5-7B, Qwen2.5-Math-7B\nand Phi-4, evaluating on common generative, discriminative, math and reasoning\ntasks. We show that in most cases, CARVQ can achieve lower average\nbitwidth-per-parameter while maintaining reasonable perplexity and accuracy\ncompared to scalar quantization. Our contributions include a novel compression\ntechnique that is compatible with state-of-the-art transformer quantization\nmethods and can be seamlessly integrated into any hardware supporting 4-bit\nmemory to reduce the model's memory footprint in memory-constrained devices.\nThis work demonstrates a crucial step toward the efficient deployment of LLMs\non edge devices."
                },
                "authors": [
                    {
                        "name": "Dayin Gou"
                    },
                    {
                        "name": "Sanghyun Byun"
                    },
                    {
                        "name": "Nilesh Malpeddi"
                    },
                    {
                        "name": "Gabrielle De Micheli"
                    },
                    {
                        "name": "Prathamesh Vaste"
                    },
                    {
                        "name": "Jacob Song"
                    },
                    {
                        "name": "Woo Seong Chung"
                    }
                ],
                "author_detail": {
                    "name": "Woo Seong Chung"
                },
                "author": "Woo Seong Chung",
                "arxiv_comment": "Accepted at EMNLP Findings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12721v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12721v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12712v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12712v1",
                "updated": "2025-10-14T16:50:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    50,
                    49,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T16:50:49Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    50,
                    49,
                    1,
                    287,
                    0
                ],
                "title": "Beyond Seeing: Evaluating Multimodal LLMs on Tool-Enabled Image\n  Perception, Transformation, and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Seeing: Evaluating Multimodal LLMs on Tool-Enabled Image\n  Perception, Transformation, and Reasoning"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are increasingly applied in\nreal-world scenarios where user-provided images are often imperfect, requiring\nactive image manipulations such as cropping, editing, or enhancement to uncover\nsalient visual cues. Beyond static visual perception, MLLMs must also think\nwith images: dynamically transforming visual content and integrating it with\nother tools to solve complex tasks. However, this shift from treating vision as\npassive context to a manipulable cognitive workspace remains underexplored.\nMost existing benchmarks still follow a think about images paradigm, where\nimages are regarded as static inputs. To address this gap, we introduce IRIS,\nan Interactive Reasoning with Images and Systems that evaluates MLLMs' ability\nto perceive, transform, and reason across complex visual-textual tasks under\nthe think with images paradigm. IRIS comprises 1,204 challenging, open-ended\nvision tasks (603 single-turn, 601 multi-turn) spanning across five diverse\ndomains, each paired with detailed rubrics to enable systematic evaluation. Our\nevaluation shows that current MLLMs struggle with tasks requiring effective\nintegration of vision and general-purpose tools. Even the strongest model\n(GPT-5-think) reaches only 18.68% pass rate. We further observe divergent\ntool-use behaviors, with OpenAI models benefiting from diverse image\nmanipulations while Gemini-2.5-pro shows no improvement. By introducing the\nfirst benchmark centered on think with images, IRIS offers critical insights\nfor advancing visual intelligence in MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are increasingly applied in\nreal-world scenarios where user-provided images are often imperfect, requiring\nactive image manipulations such as cropping, editing, or enhancement to uncover\nsalient visual cues. Beyond static visual perception, MLLMs must also think\nwith images: dynamically transforming visual content and integrating it with\nother tools to solve complex tasks. However, this shift from treating vision as\npassive context to a manipulable cognitive workspace remains underexplored.\nMost existing benchmarks still follow a think about images paradigm, where\nimages are regarded as static inputs. To address this gap, we introduce IRIS,\nan Interactive Reasoning with Images and Systems that evaluates MLLMs' ability\nto perceive, transform, and reason across complex visual-textual tasks under\nthe think with images paradigm. IRIS comprises 1,204 challenging, open-ended\nvision tasks (603 single-turn, 601 multi-turn) spanning across five diverse\ndomains, each paired with detailed rubrics to enable systematic evaluation. Our\nevaluation shows that current MLLMs struggle with tasks requiring effective\nintegration of vision and general-purpose tools. Even the strongest model\n(GPT-5-think) reaches only 18.68% pass rate. We further observe divergent\ntool-use behaviors, with OpenAI models benefiting from diverse image\nmanipulations while Gemini-2.5-pro shows no improvement. By introducing the\nfirst benchmark centered on think with images, IRIS offers critical insights\nfor advancing visual intelligence in MLLMs."
                },
                "authors": [
                    {
                        "name": "Xingang Guo"
                    },
                    {
                        "name": "Utkarsh Tyagi"
                    },
                    {
                        "name": "Advait Gosai"
                    },
                    {
                        "name": "Paula Vergara"
                    },
                    {
                        "name": "Ernesto Gabriel Hernndez Montoya"
                    },
                    {
                        "name": "Chen Bo Calvin Zhang"
                    },
                    {
                        "name": "Bin Hu"
                    },
                    {
                        "name": "Yunzhong He"
                    },
                    {
                        "name": "Bing Liu"
                    },
                    {
                        "name": "Rakshith Sharma Srinivasa"
                    }
                ],
                "author_detail": {
                    "name": "Rakshith Sharma Srinivasa"
                },
                "author": "Rakshith Sharma Srinivasa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12712v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12712v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12421v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12421v3",
                "updated": "2025-10-14T16:45:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    45,
                    31,
                    1,
                    287,
                    0
                ],
                "published": "2025-05-18T13:43:25Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    13,
                    43,
                    25,
                    6,
                    138,
                    0
                ],
                "title": "Fixed Point Explainability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fixed Point Explainability"
                },
                "summary": "This paper introduces a formal notion of fixed point explanations, inspired\nby the \"why regress\" principle, to assess, through recursive applications, the\nstability of the interplay between a model and its explainer. Fixed point\nexplanations satisfy properties like minimality, stability, and faithfulness,\nrevealing hidden model behaviours and explanatory weaknesses. We define\nconvergence conditions for several classes of explainers, from feature-based to\nmechanistic tools like Sparse AutoEncoders, and we report quantitative and\nqualitative results for several datasets and models, including LLMs such as\nLlama-3.3-70B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a formal notion of fixed point explanations, inspired\nby the \"why regress\" principle, to assess, through recursive applications, the\nstability of the interplay between a model and its explainer. Fixed point\nexplanations satisfy properties like minimality, stability, and faithfulness,\nrevealing hidden model behaviours and explanatory weaknesses. We define\nconvergence conditions for several classes of explainers, from feature-based to\nmechanistic tools like Sparse AutoEncoders, and we report quantitative and\nqualitative results for several datasets and models, including LLMs such as\nLlama-3.3-70B."
                },
                "authors": [
                    {
                        "name": "Emanuele La Malfa"
                    },
                    {
                        "name": "Jon Vadillo"
                    },
                    {
                        "name": "Marco Molinari"
                    },
                    {
                        "name": "Michael Wooldridge"
                    }
                ],
                "author_detail": {
                    "name": "Michael Wooldridge"
                },
                "author": "Michael Wooldridge",
                "arxiv_comment": "The code is available here:\n  https://anonymous.4open.science/r/fixed_point_explainability_iclr2026-D188",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12421v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12421v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12702v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12702v1",
                "updated": "2025-10-14T16:37:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    37,
                    39,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T16:37:39Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    37,
                    39,
                    1,
                    287,
                    0
                ],
                "title": "Beyond Postconditions: Can Large Language Models infer Formal Contracts\n  for Automatic Software Verification?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Postconditions: Can Large Language Models infer Formal Contracts\n  for Automatic Software Verification?"
                },
                "summary": "Automatic software verifiers have become increasingly effective at the task\nof checking software against (formal) specifications. Yet, their adoption in\npractice has been hampered by the lack of such specifications in real world\ncode. Large Language Models (LLMs) have shown promise in inferring formal\npostconditions from natural language hints embedded in code such as function\nnames, comments or documentation. Using the generated postconditions as\nspecifications in a subsequent verification, however, often leads verifiers to\nsuggest invalid inputs, hinting at potential issues that ultimately turn out to\nbe false alarms.\n  To address this, we revisit the problem of specification inference from\nnatural language in the context of automatic software verification. In the\nprocess, we introduce NL2Contract, the task of employing LLMs to translate\ninformal natural language into formal functional contracts, consisting of\npostconditions as well as preconditions. We introduce metrics to validate and\ncompare different NL2Contract approaches, using soundness, bug discriminative\npower of the generated contracts and their usability in the context of\nautomatic software verification as key metrics. We evaluate NL2Contract with\ndifferent LLMs and compare it to the task of postcondition generation\nnl2postcond. Our evaluation shows that (1) LLMs are generally effective at\ngenerating functional contracts sound for all possible inputs, (2) the\ngenerated contracts are sufficiently expressive for discriminating buggy from\ncorrect behavior, and (3) verifiers supplied with LLM inferred functional\ncontracts produce fewer false alarms than when provided with postconditions\nalone. Further investigations show that LLM inferred preconditions generally\nalign well with developers intentions which allows us to use automatic software\nverifiers to catch real-world bugs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic software verifiers have become increasingly effective at the task\nof checking software against (formal) specifications. Yet, their adoption in\npractice has been hampered by the lack of such specifications in real world\ncode. Large Language Models (LLMs) have shown promise in inferring formal\npostconditions from natural language hints embedded in code such as function\nnames, comments or documentation. Using the generated postconditions as\nspecifications in a subsequent verification, however, often leads verifiers to\nsuggest invalid inputs, hinting at potential issues that ultimately turn out to\nbe false alarms.\n  To address this, we revisit the problem of specification inference from\nnatural language in the context of automatic software verification. In the\nprocess, we introduce NL2Contract, the task of employing LLMs to translate\ninformal natural language into formal functional contracts, consisting of\npostconditions as well as preconditions. We introduce metrics to validate and\ncompare different NL2Contract approaches, using soundness, bug discriminative\npower of the generated contracts and their usability in the context of\nautomatic software verification as key metrics. We evaluate NL2Contract with\ndifferent LLMs and compare it to the task of postcondition generation\nnl2postcond. Our evaluation shows that (1) LLMs are generally effective at\ngenerating functional contracts sound for all possible inputs, (2) the\ngenerated contracts are sufficiently expressive for discriminating buggy from\ncorrect behavior, and (3) verifiers supplied with LLM inferred functional\ncontracts produce fewer false alarms than when provided with postconditions\nalone. Further investigations show that LLM inferred preconditions generally\nalign well with developers intentions which allows us to use automatic software\nverifiers to catch real-world bugs."
                },
                "authors": [
                    {
                        "name": "Cedric Richter"
                    },
                    {
                        "name": "Heike Wehrheim"
                    }
                ],
                "author_detail": {
                    "name": "Heike Wehrheim"
                },
                "author": "Heike Wehrheim",
                "arxiv_comment": "under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12702v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12699v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12699v1",
                "updated": "2025-10-14T16:31:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    31,
                    34,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T16:31:34Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    31,
                    34,
                    1,
                    287,
                    0
                ],
                "title": "Generation Space Size: Understanding and Calibrating Open-Endedness of\n  LLM Generations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generation Space Size: Understanding and Calibrating Open-Endedness of\n  LLM Generations"
                },
                "summary": "Different open-ended generation tasks require different degrees of output\ndiversity. However, current LLMs are often miscalibrated. They collapse to\noverly homogeneous outputs for creative tasks and hallucinate diverse but\nincorrect responses for factual tasks. We argue that these two failure modes\nare unified by, and can both be addressed by, the notion of effective\ngeneration space size (GSS) -- the set of semantically distinct outputs a model\nconsiders for a prompt. We present GSSBench, a task suite of prompt pairs with\nground-truth GSS relationships to assess different metrics and understand where\nmodels diverge from desired behavior. We find that hallucination detection\nmetrics, particularly EigenScore, consistently outperform standard diversity\nand uncertainty quantification metrics, while using only model internals,\nproviding interpretable insights into a model's internal task representations.\nWe demonstrate three applications of GSS: (1) detecting prompt ambiguity and\npredicting clarification questions for better grounding, (2) interpreting\noverthinking and underthinking in reasoning models, and (3) steering models to\nexpand their generation space to yield high-quality and diverse outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Different open-ended generation tasks require different degrees of output\ndiversity. However, current LLMs are often miscalibrated. They collapse to\noverly homogeneous outputs for creative tasks and hallucinate diverse but\nincorrect responses for factual tasks. We argue that these two failure modes\nare unified by, and can both be addressed by, the notion of effective\ngeneration space size (GSS) -- the set of semantically distinct outputs a model\nconsiders for a prompt. We present GSSBench, a task suite of prompt pairs with\nground-truth GSS relationships to assess different metrics and understand where\nmodels diverge from desired behavior. We find that hallucination detection\nmetrics, particularly EigenScore, consistently outperform standard diversity\nand uncertainty quantification metrics, while using only model internals,\nproviding interpretable insights into a model's internal task representations.\nWe demonstrate three applications of GSS: (1) detecting prompt ambiguity and\npredicting clarification questions for better grounding, (2) interpreting\noverthinking and underthinking in reasoning models, and (3) steering models to\nexpand their generation space to yield high-quality and diverse outputs."
                },
                "authors": [
                    {
                        "name": "Sunny Yu"
                    },
                    {
                        "name": "Ahmad Jabbar"
                    },
                    {
                        "name": "Robert Hawkins"
                    },
                    {
                        "name": "Dan Jurafsky"
                    },
                    {
                        "name": "Myra Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Myra Cheng"
                },
                "author": "Myra Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12699v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12698v1",
                "updated": "2025-10-14T16:31:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    31,
                    7,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T16:31:07Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    31,
                    7,
                    1,
                    287,
                    0
                ],
                "title": "AMHRP: Adaptive Multi-Hop Routing Protocol to Improve Network Lifetime\n  for Multi-Hop Wireless Body Area Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AMHRP: Adaptive Multi-Hop Routing Protocol to Improve Network Lifetime\n  for Multi-Hop Wireless Body Area Network"
                },
                "summary": "This paper presents a protocol for enhancement of life time of WBAN network\nas well other protocol related issues such as throughput, path loss, and\nresidual energy. Bio-sensors are used for deployment on human body. Poisson\ndistribution and equilibrium model techniques have been used for attaining the\nrequired results. Multi-hop network topology and random network node deployment\nused to achieve minimum energy consumption and longer network lifetime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a protocol for enhancement of life time of WBAN network\nas well other protocol related issues such as throughput, path loss, and\nresidual energy. Bio-sensors are used for deployment on human body. Poisson\ndistribution and equilibrium model techniques have been used for attaining the\nrequired results. Multi-hop network topology and random network node deployment\nused to achieve minimum energy consumption and longer network lifetime."
                },
                "authors": [
                    {
                        "name": "Muhammad Mateen Yaqoob"
                    },
                    {
                        "name": "Kulsoom Fatima"
                    },
                    {
                        "name": "Shahab Shamshirband"
                    },
                    {
                        "name": "Amir Mosavi"
                    },
                    {
                        "name": "Waqar Khurshid"
                    }
                ],
                "author_detail": {
                    "name": "Waqar Khurshid"
                },
                "author": "Waqar Khurshid",
                "arxiv_doi": "10.20944/preprints201905.0027.v1",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.20944/preprints201905.0027.v1",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.12698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12697v1",
                "updated": "2025-10-14T16:30:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    30,
                    30,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T16:30:30Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    30,
                    30,
                    1,
                    287,
                    0
                ],
                "title": "Multi-Agent Debate for LLM Judges with Adaptive Stability Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Debate for LLM Judges with Adaptive Stability Detection"
                },
                "summary": "With advancements in reasoning capabilities, Large Language Models (LLMs) are\nincreasingly employed for automated judgment tasks. While LLMs-as-Judges offer\npromise in automating evaluations, current approaches often rely on simplistic\naggregation methods (e.g., majority voting), which can fail even when\nindividual agents provide correct answers. To address this, we propose a\nmulti-agent debate judge framework where agents collaboratively reason and\niteratively refine their responses. We formalize the debate process\nmathematically, analyzing agent interactions and proving that debate amplifies\ncorrectness compared to static ensembles. To enhance efficiency, we introduce a\nstability detection mechanism that models judge consensus dynamics via a\ntime-varying Beta-Binomial mixture, with adaptive stopping based on\ndistributional similarity (Kolmogorov-Smirnov test). This mechanism models the\njudges' collective correct rate dynamics using a time-varying mixture of\nBeta-Binomial distributions and employs an adaptive stopping criterion based on\ndistributional similarity (Kolmogorov-Smirnov statistic). Experiments across\nmultiple benchmarks and models demonstrate that our framework improves judgment\naccuracy over majority voting while maintaining computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With advancements in reasoning capabilities, Large Language Models (LLMs) are\nincreasingly employed for automated judgment tasks. While LLMs-as-Judges offer\npromise in automating evaluations, current approaches often rely on simplistic\naggregation methods (e.g., majority voting), which can fail even when\nindividual agents provide correct answers. To address this, we propose a\nmulti-agent debate judge framework where agents collaboratively reason and\niteratively refine their responses. We formalize the debate process\nmathematically, analyzing agent interactions and proving that debate amplifies\ncorrectness compared to static ensembles. To enhance efficiency, we introduce a\nstability detection mechanism that models judge consensus dynamics via a\ntime-varying Beta-Binomial mixture, with adaptive stopping based on\ndistributional similarity (Kolmogorov-Smirnov test). This mechanism models the\njudges' collective correct rate dynamics using a time-varying mixture of\nBeta-Binomial distributions and employs an adaptive stopping criterion based on\ndistributional similarity (Kolmogorov-Smirnov statistic). Experiments across\nmultiple benchmarks and models demonstrate that our framework improves judgment\naccuracy over majority voting while maintaining computational efficiency."
                },
                "authors": [
                    {
                        "name": "Tianyu Hu"
                    },
                    {
                        "name": "Zhen Tan"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Huaizhi Qu"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02903v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02903v2",
                "updated": "2025-10-14T16:28:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    28,
                    16,
                    1,
                    287,
                    0
                ],
                "published": "2025-09-03T00:12:15Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    0,
                    12,
                    15,
                    2,
                    246,
                    0
                ],
                "title": "UrbanTwin: Building High-Fidelity Digital Twins for Sim2Real LiDAR\n  Perception and Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UrbanTwin: Building High-Fidelity Digital Twins for Sim2Real LiDAR\n  Perception and Evaluation"
                },
                "summary": "LiDAR-based perception in intelligent transportation systems (ITS) relies on\ndeep neural networks trained with large-scale labeled datasets. However,\ncreating such datasets is expensive, time-consuming, and labor-intensive,\nlimiting the scalability of perception systems. Sim2Real learning offers a\nscalable alternative, but its success depends on the simulation's fidelity to\nreal-world environments, dynamics, and sensors. This tutorial introduces a\nreproducible workflow for building high-fidelity digital twins (HiFi DTs) to\ngenerate realistic synthetic datasets. We outline practical steps for modeling\nstatic geometry, road infrastructure, and dynamic traffic using open-source\nresources such as satellite imagery, OpenStreetMap, and sensor specifications.\nThe resulting environments support scalable and cost-effective data generation\nfor robust Sim2Real learning. Using this workflow, we have released three\nsynthetic LiDAR datasets, namely UT-LUMPI, UT-V2X-Real, and UT-TUMTraf-I, which\nclosely replicate real locations and outperform real-data-trained baselines in\nperception tasks. This guide enables broader adoption of HiFi DTs in ITS\nresearch and deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiDAR-based perception in intelligent transportation systems (ITS) relies on\ndeep neural networks trained with large-scale labeled datasets. However,\ncreating such datasets is expensive, time-consuming, and labor-intensive,\nlimiting the scalability of perception systems. Sim2Real learning offers a\nscalable alternative, but its success depends on the simulation's fidelity to\nreal-world environments, dynamics, and sensors. This tutorial introduces a\nreproducible workflow for building high-fidelity digital twins (HiFi DTs) to\ngenerate realistic synthetic datasets. We outline practical steps for modeling\nstatic geometry, road infrastructure, and dynamic traffic using open-source\nresources such as satellite imagery, OpenStreetMap, and sensor specifications.\nThe resulting environments support scalable and cost-effective data generation\nfor robust Sim2Real learning. Using this workflow, we have released three\nsynthetic LiDAR datasets, namely UT-LUMPI, UT-V2X-Real, and UT-TUMTraf-I, which\nclosely replicate real locations and outperform real-data-trained baselines in\nperception tasks. This guide enables broader adoption of HiFi DTs in ITS\nresearch and deployment."
                },
                "authors": [
                    {
                        "name": "Muhammad Shahbaz"
                    },
                    {
                        "name": "Shaurya Agarwal"
                    }
                ],
                "author_detail": {
                    "name": "Shaurya Agarwal"
                },
                "author": "Shaurya Agarwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02903v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02903v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12692v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12692v1",
                "updated": "2025-10-14T16:25:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    25,
                    9,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T16:25:09Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    25,
                    9,
                    1,
                    287,
                    0
                ],
                "title": "Who is a Better Matchmaker? Human vs. Algorithmic Judge Assignment in a\n  High-Stakes Startup Competition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who is a Better Matchmaker? Human vs. Algorithmic Judge Assignment in a\n  High-Stakes Startup Competition"
                },
                "summary": "There is growing interest in applying artificial intelligence (AI) to\nautomate and support complex decision-making tasks. However, it remains unclear\nhow algorithms compare to human judgment in contexts requiring semantic\nunderstanding and domain expertise. We examine this in the context of the judge\nassignment problem, matching submissions to suitably qualified judges.\nSpecifically, we tackled this problem at the Harvard President's Innovation\nChallenge, the university's premier venture competition awarding over \\$500,000\nto student and alumni startups. This represents a real-world environment where\nhigh-quality judge assignment is essential. We developed an AI-based\njudge-assignment algorithm, Hybrid Lexical-Semantic Similarity Ensemble (HLSE),\nand deployed it at the competition. We then evaluated its performance against\nhuman expert assignments using blinded match-quality scores from judges on\n$309$ judge-venture pairs. Using a Mann-Whitney U statistic based test, we\nfound no statistically significant difference in assignment quality between the\ntwo approaches ($AUC=0.48, p=0.40$); on average, algorithmic matches are rated\n$3.90$ and manual matches $3.94$ on a 5-point scale, where 5 indicates an\nexcellent match. Furthermore, manual assignments that previously required a\nfull week could be automated in several hours by the algorithm during\ndeployment. These results demonstrate that HLSE achieves human-expert-level\nmatching quality while offering greater scalability and efficiency,\nunderscoring the potential of AI-driven solutions to support and enhance human\ndecision-making for judge assignment in high-stakes settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is growing interest in applying artificial intelligence (AI) to\nautomate and support complex decision-making tasks. However, it remains unclear\nhow algorithms compare to human judgment in contexts requiring semantic\nunderstanding and domain expertise. We examine this in the context of the judge\nassignment problem, matching submissions to suitably qualified judges.\nSpecifically, we tackled this problem at the Harvard President's Innovation\nChallenge, the university's premier venture competition awarding over \\$500,000\nto student and alumni startups. This represents a real-world environment where\nhigh-quality judge assignment is essential. We developed an AI-based\njudge-assignment algorithm, Hybrid Lexical-Semantic Similarity Ensemble (HLSE),\nand deployed it at the competition. We then evaluated its performance against\nhuman expert assignments using blinded match-quality scores from judges on\n$309$ judge-venture pairs. Using a Mann-Whitney U statistic based test, we\nfound no statistically significant difference in assignment quality between the\ntwo approaches ($AUC=0.48, p=0.40$); on average, algorithmic matches are rated\n$3.90$ and manual matches $3.94$ on a 5-point scale, where 5 indicates an\nexcellent match. Furthermore, manual assignments that previously required a\nfull week could be automated in several hours by the algorithm during\ndeployment. These results demonstrate that HLSE achieves human-expert-level\nmatching quality while offering greater scalability and efficiency,\nunderscoring the potential of AI-driven solutions to support and enhance human\ndecision-making for judge assignment in high-stakes settings."
                },
                "authors": [
                    {
                        "name": "Sarina Xi"
                    },
                    {
                        "name": "Orelia Pi"
                    },
                    {
                        "name": "Miaomiao Zhang"
                    },
                    {
                        "name": "Becca Xiong"
                    },
                    {
                        "name": "Jacqueline Ng Lane"
                    },
                    {
                        "name": "Nihar B. Shah"
                    }
                ],
                "author_detail": {
                    "name": "Nihar B. Shah"
                },
                "author": "Nihar B. Shah",
                "arxiv_comment": "17 Pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12692v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22363v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22363v2",
                "updated": "2025-10-14T16:24:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    24,
                    33,
                    1,
                    287,
                    0
                ],
                "published": "2025-09-26T13:58:22Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    58,
                    22,
                    4,
                    269,
                    0
                ],
                "title": "Investigating Faithfulness in Large Audio Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Faithfulness in Large Audio Language Models"
                },
                "summary": "Faithfulness measures whether chain-of-thought (CoT) representations\naccurately reflect a model's decision process and can be used as reliable\nexplanations. Prior work has shown that CoTs from text-based LLMs are often\nunfaithful. This question has not been explored for large audio-language models\n(LALMs), where faithfulness is critical for safety-sensitive applications.\nReasoning in LALMs is also more challenging, as models must first extract\nrelevant clues from audio before reasoning over them. In this paper, we\ninvestigate the faithfulness of CoTs produced by several LALMs by applying\ntargeted interventions, including paraphrasing, filler token injection, early\nanswering, and introducing mistakes, on two challenging reasoning datasets:\nSAKURA and MMAR. After going through the aforementioned interventions across\nseveral datasets and tasks, our experiments suggest that, LALMs generally\nproduce CoTs that appear to be faithful to their underlying decision processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faithfulness measures whether chain-of-thought (CoT) representations\naccurately reflect a model's decision process and can be used as reliable\nexplanations. Prior work has shown that CoTs from text-based LLMs are often\nunfaithful. This question has not been explored for large audio-language models\n(LALMs), where faithfulness is critical for safety-sensitive applications.\nReasoning in LALMs is also more challenging, as models must first extract\nrelevant clues from audio before reasoning over them. In this paper, we\ninvestigate the faithfulness of CoTs produced by several LALMs by applying\ntargeted interventions, including paraphrasing, filler token injection, early\nanswering, and introducing mistakes, on two challenging reasoning datasets:\nSAKURA and MMAR. After going through the aforementioned interventions across\nseveral datasets and tasks, our experiments suggest that, LALMs generally\nproduce CoTs that appear to be faithful to their underlying decision processes."
                },
                "authors": [
                    {
                        "name": "Lovenya Jain"
                    },
                    {
                        "name": "Pooneh Mousavi"
                    },
                    {
                        "name": "Mirco Ravanelli"
                    },
                    {
                        "name": "Cem Subakan"
                    }
                ],
                "author_detail": {
                    "name": "Cem Subakan"
                },
                "author": "Cem Subakan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22363v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22363v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12689v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12689v1",
                "updated": "2025-10-14T16:24:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    24,
                    19,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T16:24:19Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    24,
                    19,
                    1,
                    287,
                    0
                ],
                "title": "From Delegates to Trustees: How Optimizing for Long-Term Interests\n  Shapes Bias and Alignment in LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Delegates to Trustees: How Optimizing for Long-Term Interests\n  Shapes Bias and Alignment in LLM"
                },
                "summary": "Large language models (LLMs) have shown promising accuracy in predicting\nsurvey responses and policy preferences, which has increased interest in their\npotential to represent human interests in various domains. Most existing\nresearch has focused on behavioral cloning, effectively evaluating how well\nmodels reproduce individuals' expressed preferences. Drawing on theories of\npolitical representation, we highlight an underexplored design trade-off:\nwhether AI systems should act as delegates, mirroring expressed preferences, or\nas trustees, exercising judgment about what best serves an individual's\ninterests. This trade-off is closely related to issues of LLM sycophancy, where\nmodels can encourage behavior or validate beliefs that may be aligned with a\nuser's short-term preferences, but is detrimental to their long-term interests.\nThrough a series of experiments simulating votes on various policy issues in\nthe U.S. context, we apply a temporal utility framework that weighs short and\nlong-term interests (simulating a trustee role) and compare voting outcomes to\nbehavior-cloning models (simulating a delegate). We find that trustee-style\npredictions weighted toward long-term interests produce policy decisions that\nalign more closely with expert consensus on well-understood issues, but also\nshow greater bias toward models' default stances on topics lacking clear\nagreement. These findings reveal a fundamental trade-off in designing AI\nsystems to represent human interests. Delegate models better preserve user\nautonomy but may diverge from well-supported policy positions, while trustee\nmodels can promote welfare on well-understood issues yet risk paternalism and\nbias on subjective topics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown promising accuracy in predicting\nsurvey responses and policy preferences, which has increased interest in their\npotential to represent human interests in various domains. Most existing\nresearch has focused on behavioral cloning, effectively evaluating how well\nmodels reproduce individuals' expressed preferences. Drawing on theories of\npolitical representation, we highlight an underexplored design trade-off:\nwhether AI systems should act as delegates, mirroring expressed preferences, or\nas trustees, exercising judgment about what best serves an individual's\ninterests. This trade-off is closely related to issues of LLM sycophancy, where\nmodels can encourage behavior or validate beliefs that may be aligned with a\nuser's short-term preferences, but is detrimental to their long-term interests.\nThrough a series of experiments simulating votes on various policy issues in\nthe U.S. context, we apply a temporal utility framework that weighs short and\nlong-term interests (simulating a trustee role) and compare voting outcomes to\nbehavior-cloning models (simulating a delegate). We find that trustee-style\npredictions weighted toward long-term interests produce policy decisions that\nalign more closely with expert consensus on well-understood issues, but also\nshow greater bias toward models' default stances on topics lacking clear\nagreement. These findings reveal a fundamental trade-off in designing AI\nsystems to represent human interests. Delegate models better preserve user\nautonomy but may diverge from well-supported policy positions, while trustee\nmodels can promote welfare on well-understood issues yet risk paternalism and\nbias on subjective topics."
                },
                "authors": [
                    {
                        "name": "Suyash Fulay"
                    },
                    {
                        "name": "Jocelyn Zhu"
                    },
                    {
                        "name": "Michiel Bakker"
                    }
                ],
                "author_detail": {
                    "name": "Michiel Bakker"
                },
                "author": "Michiel Bakker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12689v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12689v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12680v1",
                "updated": "2025-10-14T16:19:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    19,
                    44,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T16:19:44Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    19,
                    44,
                    1,
                    287,
                    0
                ],
                "title": "Demystifying Hybrid Thinking: Can LLMs Truly Switch Between Think and\n  No-Think?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying Hybrid Thinking: Can LLMs Truly Switch Between Think and\n  No-Think?"
                },
                "summary": "Hybrid thinking enables LLMs to switch between reasoning and direct\nanswering, offering a balance between efficiency and reasoning capability. Yet\nour experiments reveal that current hybrid thinking LLMs only achieve partial\nmode separation: reasoning behaviors often leak into the no-think mode. To\nunderstand and mitigate this, we analyze the factors influencing\ncontrollability and identify four that matter most: (1) larger data scale, (2)\nusing think and no-think answers from different questions rather than the same\nquestion, (3) a moderate increase in no-think data number, and (4) a two-phase\nstrategy that first trains reasoning ability and then applies hybrid think\ntraining. Building on these findings, we propose a practical recipe that,\ncompared to standard training, can maintain accuracy in both modes while\nsignificantly reducing no-think output length (from $1085$ to $585$ on MATH500)\nand occurrences of reasoning-supportive tokens such as ``\\texttt{wait}'' (from\n$5917$ to $522$ on MATH500). Our findings highlight the limitations of current\nhybrid thinking and offer directions for strengthening its controllability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid thinking enables LLMs to switch between reasoning and direct\nanswering, offering a balance between efficiency and reasoning capability. Yet\nour experiments reveal that current hybrid thinking LLMs only achieve partial\nmode separation: reasoning behaviors often leak into the no-think mode. To\nunderstand and mitigate this, we analyze the factors influencing\ncontrollability and identify four that matter most: (1) larger data scale, (2)\nusing think and no-think answers from different questions rather than the same\nquestion, (3) a moderate increase in no-think data number, and (4) a two-phase\nstrategy that first trains reasoning ability and then applies hybrid think\ntraining. Building on these findings, we propose a practical recipe that,\ncompared to standard training, can maintain accuracy in both modes while\nsignificantly reducing no-think output length (from $1085$ to $585$ on MATH500)\nand occurrences of reasoning-supportive tokens such as ``\\texttt{wait}'' (from\n$5917$ to $522$ on MATH500). Our findings highlight the limitations of current\nhybrid thinking and offer directions for strengthening its controllability."
                },
                "authors": [
                    {
                        "name": "Shouren Wang"
                    },
                    {
                        "name": "Wang Yang"
                    },
                    {
                        "name": "Xianxuan Long"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    },
                    {
                        "name": "Xiaotian Han"
                    }
                ],
                "author_detail": {
                    "name": "Xiaotian Han"
                },
                "author": "Xiaotian Han",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14214v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14214v2",
                "updated": "2025-10-14T16:10:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    10,
                    53,
                    1,
                    287,
                    0
                ],
                "published": "2024-11-21T15:24:41Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    15,
                    24,
                    41,
                    3,
                    326,
                    0
                ],
                "title": "Physics-Informed Autonomous LLM Agents for Explainable Power Electronics\n  Modulation Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics-Informed Autonomous LLM Agents for Explainable Power Electronics\n  Modulation Design"
                },
                "summary": "LLM-based autonomous agents have recently shown strong capabilities in\nsolving complex industrial design tasks. However, in domains aiming for carbon\nneutrality and high-performance renewable energy systems, current AI-assisted\ndesign automation methods face critical challenges in explainability,\nscalability, and practical usability. To address these limitations, we\nintroduce PHIA (Physics-Informed Autonomous Agent), an LLM-driven system that\nautomates modulation design for power converters in Power Electronics Systems\nwith minimal human intervention. In contrast to traditional pipeline-based\nmethods, PHIA incorporates an LLM-based planning module that interactively\nacquires and verifies design requirements via a user-friendly chat interface.\nThis planner collaborates with physics-informed simulation and optimization\ncomponents to autonomously generate and iteratively refine modulation designs.\nThe interactive interface also supports interpretability by providing textual\nexplanations and visual outputs throughout the design process. Experimental\nresults show that PHIA reduces standard mean absolute error by 63.2% compared\nto the second-best benchmark and accelerates the overall design process by over\n33 times. A user study involving 20 domain experts further confirms PHIA's\nsuperior design efficiency and usability, highlighting its potential to\ntransform industrial design workflows in power electronics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based autonomous agents have recently shown strong capabilities in\nsolving complex industrial design tasks. However, in domains aiming for carbon\nneutrality and high-performance renewable energy systems, current AI-assisted\ndesign automation methods face critical challenges in explainability,\nscalability, and practical usability. To address these limitations, we\nintroduce PHIA (Physics-Informed Autonomous Agent), an LLM-driven system that\nautomates modulation design for power converters in Power Electronics Systems\nwith minimal human intervention. In contrast to traditional pipeline-based\nmethods, PHIA incorporates an LLM-based planning module that interactively\nacquires and verifies design requirements via a user-friendly chat interface.\nThis planner collaborates with physics-informed simulation and optimization\ncomponents to autonomously generate and iteratively refine modulation designs.\nThe interactive interface also supports interpretability by providing textual\nexplanations and visual outputs throughout the design process. Experimental\nresults show that PHIA reduces standard mean absolute error by 63.2% compared\nto the second-best benchmark and accelerates the overall design process by over\n33 times. A user study involving 20 domain experts further confirms PHIA's\nsuperior design efficiency and usability, highlighting its potential to\ntransform industrial design workflows in power electronics."
                },
                "authors": [
                    {
                        "name": "Junhua Liu"
                    },
                    {
                        "name": "Fanfan Lin"
                    },
                    {
                        "name": "Xinze Li"
                    },
                    {
                        "name": "Kwan Hui Lim"
                    },
                    {
                        "name": "Shuai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Shuai Zhao"
                },
                "author": "Shuai Zhao",
                "arxiv_comment": "Accepted to AAAI 2026 Innovative Applications of AI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14214v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14214v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02659v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02659v3",
                "updated": "2025-10-14T16:05:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    5,
                    11,
                    1,
                    287,
                    0
                ],
                "published": "2025-07-03T14:20:41Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding"
                },
                "summary": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup."
                },
                "authors": [
                    {
                        "name": "Ramchalam Kinattinkara Ramakrishnan"
                    },
                    {
                        "name": "Zhaocong Yuan"
                    },
                    {
                        "name": "Shaojie Zhuo"
                    },
                    {
                        "name": "Chen Feng"
                    },
                    {
                        "name": "Yicheng Lin"
                    },
                    {
                        "name": "Chenzheng Su"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaopeng Zhang"
                },
                "author": "Xiaopeng Zhang",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02659v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02659v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12668v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12668v1",
                "updated": "2025-10-14T16:05:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    5,
                    1,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T16:05:01Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    5,
                    1,
                    1,
                    287,
                    0
                ],
                "title": "The Role of Parametric Injection-A Systematic Study of Parametric\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Role of Parametric Injection-A Systematic Study of Parametric\n  Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nretrieving external documents. As an emerging form of RAG, parametric\nretrieval-augmented generation (PRAG) encodes documents as model parameters\n(i.e., LoRA modules) and injects these representations into the model during\ninference, enabling interaction between the LLM and documents at parametric\nlevel. Compared with directly placing documents in the input context, PRAG is\nmore efficient and has the potential to offer deeper model-document\ninteraction. Despite its growing attention, the mechanism underlying parametric\ninjection remains poorly understood. In this work, we present a systematic\nstudy of PRAG to clarify the role of parametric injection, showing that\nparameterized documents capture only partial semantic information of documents,\nand relying on them alone yields inferior performance compared to interaction\nat text level. However, these parametric representations encode high-level\ndocument information that can enhance the model's understanding of documents\nwithin the input context. When combined parameterized documents with textual\ndocuments, the model can leverage relevant information more effectively and\nbecome more robust to noisy inputs, achieving better performance than either\nsource alone. We recommend jointly using parameterized and textual documents\nand advocate for increasing the information content of parametric\nrepresentations to advance PRAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nretrieving external documents. As an emerging form of RAG, parametric\nretrieval-augmented generation (PRAG) encodes documents as model parameters\n(i.e., LoRA modules) and injects these representations into the model during\ninference, enabling interaction between the LLM and documents at parametric\nlevel. Compared with directly placing documents in the input context, PRAG is\nmore efficient and has the potential to offer deeper model-document\ninteraction. Despite its growing attention, the mechanism underlying parametric\ninjection remains poorly understood. In this work, we present a systematic\nstudy of PRAG to clarify the role of parametric injection, showing that\nparameterized documents capture only partial semantic information of documents,\nand relying on them alone yields inferior performance compared to interaction\nat text level. However, these parametric representations encode high-level\ndocument information that can enhance the model's understanding of documents\nwithin the input context. When combined parameterized documents with textual\ndocuments, the model can leverage relevant information more effectively and\nbecome more robust to noisy inputs, achieving better performance than either\nsource alone. We recommend jointly using parameterized and textual documents\nand advocate for increasing the information content of parametric\nrepresentations to advance PRAG."
                },
                "authors": [
                    {
                        "name": "Minghao Tang"
                    },
                    {
                        "name": "Shiyu Ni"
                    },
                    {
                        "name": "Jingtong Wu"
                    },
                    {
                        "name": "Zengxin Han"
                    },
                    {
                        "name": "Keping Bi"
                    }
                ],
                "author_detail": {
                    "name": "Keping Bi"
                },
                "author": "Keping Bi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12668v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12668v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12666v1",
                "updated": "2025-10-14T16:01:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    1,
                    29,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T16:01:29Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    1,
                    29,
                    1,
                    287,
                    0
                ],
                "title": "Structured Sparsity and Weight-adaptive Pruning for Memory and Compute\n  efficient Whisper models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured Sparsity and Weight-adaptive Pruning for Memory and Compute\n  efficient Whisper models"
                },
                "summary": "Whisper models have achieved remarkable progress in speech recognition; yet\ntheir large size remains a bottleneck for deployment on resource-constrained\nedge devices. This paper proposes a framework to design fine-tuned variants of\nWhisper which address the above problem. Structured sparsity is enforced via\nthe Sparse Group LASSO penalty as a loss regularizer, to reduce the number of\nFLOating Point operations (FLOPs). Further, a weight statistics aware pruning\nalgorithm is proposed. We also design our custom text normalizer for WER\nevaluation. On Common Voice 11.0 Hindi dataset, we obtain, without degrading\nWER, (a) 35.4% reduction in model parameters, 14.25% lower memory consumption\nand 18.5% fewer FLOPs on Whisper-small, and (b) 31% reduction in model\nparameters, 15.29% lower memory consumption and 16.95% fewer FLOPs on\nWhisper-medium; and, (c) substantially outperform the state-of-the-art\nIterative Magnitude Pruning based method by pruning 18.7% more parameters along\nwith a 12.31 reduction in WER.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whisper models have achieved remarkable progress in speech recognition; yet\ntheir large size remains a bottleneck for deployment on resource-constrained\nedge devices. This paper proposes a framework to design fine-tuned variants of\nWhisper which address the above problem. Structured sparsity is enforced via\nthe Sparse Group LASSO penalty as a loss regularizer, to reduce the number of\nFLOating Point operations (FLOPs). Further, a weight statistics aware pruning\nalgorithm is proposed. We also design our custom text normalizer for WER\nevaluation. On Common Voice 11.0 Hindi dataset, we obtain, without degrading\nWER, (a) 35.4% reduction in model parameters, 14.25% lower memory consumption\nand 18.5% fewer FLOPs on Whisper-small, and (b) 31% reduction in model\nparameters, 15.29% lower memory consumption and 16.95% fewer FLOPs on\nWhisper-medium; and, (c) substantially outperform the state-of-the-art\nIterative Magnitude Pruning based method by pruning 18.7% more parameters along\nwith a 12.31 reduction in WER."
                },
                "authors": [
                    {
                        "name": "Prasenjit K Mudi"
                    },
                    {
                        "name": "Anshi Sachan"
                    },
                    {
                        "name": "Dahlia Devapriya"
                    },
                    {
                        "name": "Sheetal Kalyani"
                    }
                ],
                "author_detail": {
                    "name": "Sheetal Kalyani"
                },
                "author": "Sheetal Kalyani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26074v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26074v2",
                "updated": "2025-10-14T15:45:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    45,
                    25,
                    1,
                    287,
                    0
                ],
                "published": "2025-09-30T10:48:50Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    10,
                    48,
                    50,
                    1,
                    273,
                    0
                ],
                "title": "Limited Preference Data? Learning Better Reward Model with Latent Space\n  Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limited Preference Data? Learning Better Reward Model with Latent Space\n  Synthesis"
                },
                "summary": "Reward modeling, crucial for aligning large language models (LLMs) with human\npreferences, is often bottlenecked by the high cost of preference data.\nExisting textual data synthesis methods are computationally expensive. We\npropose a novel framework LENS for synthesizing preference data directly in the\nLLM's latent embedding space. Our method employs a Variational Autoencoder\n(VAE) to learn a structured latent representation of response embeddings. By\nperforming controlled perturbations in this latent space and decoding back to\nthe embedding space, we efficiently generate diverse, semantically consistent\nsynthetic preference pairs, bypassing costly text generation and annotation. We\nprovide theoretical guarantees that our synthesized pairs approximately\npreserve original preference ordering and improve reward model generalization.\nEmpirically, our latent-space synthesis significantly outperforms text-based\naugmentation on standard benchmarks, achieving superior results while being 18x\nfaster in generation and using a 16,000x smaller model. Our work offers a\nscalable and effective alternative for enhancing reward modeling through\nefficient data augmentation. Code is publicly available at\nhttps://github.com/deeplearning-wisc/lens",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward modeling, crucial for aligning large language models (LLMs) with human\npreferences, is often bottlenecked by the high cost of preference data.\nExisting textual data synthesis methods are computationally expensive. We\npropose a novel framework LENS for synthesizing preference data directly in the\nLLM's latent embedding space. Our method employs a Variational Autoencoder\n(VAE) to learn a structured latent representation of response embeddings. By\nperforming controlled perturbations in this latent space and decoding back to\nthe embedding space, we efficiently generate diverse, semantically consistent\nsynthetic preference pairs, bypassing costly text generation and annotation. We\nprovide theoretical guarantees that our synthesized pairs approximately\npreserve original preference ordering and improve reward model generalization.\nEmpirically, our latent-space synthesis significantly outperforms text-based\naugmentation on standard benchmarks, achieving superior results while being 18x\nfaster in generation and using a 16,000x smaller model. Our work offers a\nscalable and effective alternative for enhancing reward modeling through\nefficient data augmentation. Code is publicly available at\nhttps://github.com/deeplearning-wisc/lens"
                },
                "authors": [
                    {
                        "name": "Leitian Tao"
                    },
                    {
                        "name": "Xuefeng Du"
                    },
                    {
                        "name": "Sharon Li"
                    }
                ],
                "author_detail": {
                    "name": "Sharon Li"
                },
                "author": "Sharon Li",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26074v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26074v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00299v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00299v5",
                "updated": "2025-10-14T15:42:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    42,
                    41,
                    1,
                    287,
                    0
                ],
                "published": "2025-02-01T03:49:47Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem. The code is available at\n\\href{https://github.com/NVIDIA/kvpress}{link}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem. The code is available at\n\\href{https://github.com/NVIDIA/kvpress}{link}."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00299v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00299v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23441v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23441v2",
                "updated": "2025-10-14T15:38:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    38,
                    48,
                    1,
                    287,
                    0
                ],
                "published": "2025-09-27T18:16:57Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    18,
                    16,
                    57,
                    5,
                    270,
                    0
                ],
                "title": "Cognition-of-Thought Elicits Social-Aligned Reasoning in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognition-of-Thought Elicits Social-Aligned Reasoning in Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) excel at complex reasoning but can still exhibit\nharmful behaviors. Current alignment strategies typically embed safety into\nmodel weights, making these controls implicit, static, and difficult to modify.\nThis paper introduces Cognition-of-Thought (CooT), a novel decoding-time\nframework that equips LLMs with an explicit cognitive self-monitoring loop.\nCooT couples a standard text Generator with a cognitive Perceiver that\ncontinuously monitors the unfolding sequence. The Perceiver uses a structured,\nprecedence-based hierarchy of principles (e.g., safety over obedience) to\ndetect potential misalignments as they arise. When violations are flagged, CooT\nintervenes by rolling back the generation to the point of error and\nregenerating under injected guidance that combines universal social priors with\ncontext-specific warnings. CooT thus transforms alignment from a fixed property\ninto an explicit, dynamic, and auditable process active during inference,\nallowing for flexible policy updates without retraining the model. Extensive\nexperiments across multiple benchmarks and model families confirm that CooT\nconsistently improves safety and social reasoning performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at complex reasoning but can still exhibit\nharmful behaviors. Current alignment strategies typically embed safety into\nmodel weights, making these controls implicit, static, and difficult to modify.\nThis paper introduces Cognition-of-Thought (CooT), a novel decoding-time\nframework that equips LLMs with an explicit cognitive self-monitoring loop.\nCooT couples a standard text Generator with a cognitive Perceiver that\ncontinuously monitors the unfolding sequence. The Perceiver uses a structured,\nprecedence-based hierarchy of principles (e.g., safety over obedience) to\ndetect potential misalignments as they arise. When violations are flagged, CooT\nintervenes by rolling back the generation to the point of error and\nregenerating under injected guidance that combines universal social priors with\ncontext-specific warnings. CooT thus transforms alignment from a fixed property\ninto an explicit, dynamic, and auditable process active during inference,\nallowing for flexible policy updates without retraining the model. Extensive\nexperiments across multiple benchmarks and model families confirm that CooT\nconsistently improves safety and social reasoning performance."
                },
                "authors": [
                    {
                        "name": "Xuanming Zhang"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Samuel Yeh"
                    },
                    {
                        "name": "Sharon Li"
                    }
                ],
                "author_detail": {
                    "name": "Sharon Li"
                },
                "author": "Sharon Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23441v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23441v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12643v1",
                "updated": "2025-10-14T15:34:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    34,
                    38,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T15:34:38Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    34,
                    38,
                    1,
                    287,
                    0
                ],
                "title": "Reasoning Pattern Matters: Learning to Reason without Human Rationales",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Pattern Matters: Learning to Reason without Human Rationales"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable reasoning\ncapabilities under the widely adopted SFT+RLVR paradigm, which first performs\nSupervised Fine-Tuning (SFT) on human-annotated reasoning trajectories\n(rationales) to establish initial reasoning behaviors, then applies\nReinforcement Learning with Verifiable Rewards (RLVR) to optimize the model\nusing verifiable signals without golden rationales. However, annotating\nhigh-quality rationales for the SFT stage remains prohibitively expensive. This\npaper investigates when and how rationale annotation costs can be substantially\nreduced without compromising reasoning performance. We identify a broad class\nof problems, termed patterned reasoning tasks, where reasoning follows a fixed,\nprocedural strategy consistent across instances. Although instances vary in\ncontent such as domain knowledge, factual information, or numeric values, the\nsolution derives from applying a shared reasoning pattern. We argue that the\nsuccess of SFT+RLVR on such tasks primarily stems from its ability to enable\nmodels to internalize these reasoning patterns. Using numerical semantic\nmatching as a representative task, we provide both causal and behavioral\nevidence showing that reasoning patterns rather than the quantity or quality of\nrationales are the key determinant of performance. Building on these insights,\nwe propose Pattern-Aware LLMs as Rationale AnnOtators (PARO), a simple yet\neffective framework that enables LLMs to generate rationales aligned with\ntask-specific reasoning patterns without requiring human rationale annotations.\nExperiments show that PARO-generated rationales achieve comparable SFT+RLVR\nperformance to human rationales that are 10 times larger. These results suggest\nthat large-scale human rationale annotations can be replaced with LLM-based\nautomatic annotations requiring only limited human supervision over reasoning\npatterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable reasoning\ncapabilities under the widely adopted SFT+RLVR paradigm, which first performs\nSupervised Fine-Tuning (SFT) on human-annotated reasoning trajectories\n(rationales) to establish initial reasoning behaviors, then applies\nReinforcement Learning with Verifiable Rewards (RLVR) to optimize the model\nusing verifiable signals without golden rationales. However, annotating\nhigh-quality rationales for the SFT stage remains prohibitively expensive. This\npaper investigates when and how rationale annotation costs can be substantially\nreduced without compromising reasoning performance. We identify a broad class\nof problems, termed patterned reasoning tasks, where reasoning follows a fixed,\nprocedural strategy consistent across instances. Although instances vary in\ncontent such as domain knowledge, factual information, or numeric values, the\nsolution derives from applying a shared reasoning pattern. We argue that the\nsuccess of SFT+RLVR on such tasks primarily stems from its ability to enable\nmodels to internalize these reasoning patterns. Using numerical semantic\nmatching as a representative task, we provide both causal and behavioral\nevidence showing that reasoning patterns rather than the quantity or quality of\nrationales are the key determinant of performance. Building on these insights,\nwe propose Pattern-Aware LLMs as Rationale AnnOtators (PARO), a simple yet\neffective framework that enables LLMs to generate rationales aligned with\ntask-specific reasoning patterns without requiring human rationale annotations.\nExperiments show that PARO-generated rationales achieve comparable SFT+RLVR\nperformance to human rationales that are 10 times larger. These results suggest\nthat large-scale human rationale annotations can be replaced with LLM-based\nautomatic annotations requiring only limited human supervision over reasoning\npatterns."
                },
                "authors": [
                    {
                        "name": "Chaoxu Pang"
                    },
                    {
                        "name": "Yixuan Cao"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "arxiv_comment": "Submitted to Frontiers of Computer Science",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02392v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02392v2",
                "updated": "2025-10-14T15:32:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    32,
                    32,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-01T00:15:25Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    0,
                    15,
                    25,
                    2,
                    274,
                    0
                ],
                "title": "KnowledgeSmith: Uncovering Knowledge Updating in LLMs with Model Editing\n  and Unlearning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KnowledgeSmith: Uncovering Knowledge Updating in LLMs with Model Editing\n  and Unlearning"
                },
                "summary": "Knowledge editing and machine unlearning are two popular approaches for large\nlanguage models (LLMs) to stay up-to-date. However, the knowledge updating\nmechanism of LLMs remains largely unexplored due to insufficient, isolated, and\nsmall-scale evaluation. For instance, are LLMs similar to humans in modifying\ncertain knowledge? What differs editing and unlearning as training data\nincreases? This paper proposes KnowledgeSmith, a unified framework to\nsystematically understand the updating mechanism of LLMs. We first cast editing\nand unlearning as instances of one constrained optimization problem. Then, we\npropose an automatic dataset generator that provides structured interventions\nacross multiple graph levels and data scales, enabling controlled studies of\nhow different modification strategies propagate through model knowledge.\nExtensive experiments demonstrate nuanced insights over knowledge propagation,\nplasticity scaling, consistency, and robustness. For instance, our results show\nthat LLMs do not exhibit similar updating as humans for different levels of\nknowledge, and there exists consistency-capacity trade-off. We hope our\nfindings can offer suggestions to the design of more reliable and scalable\nstrategies. Code: https://github.com/AIFrontierLab/KnowledgeSmith.git",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge editing and machine unlearning are two popular approaches for large\nlanguage models (LLMs) to stay up-to-date. However, the knowledge updating\nmechanism of LLMs remains largely unexplored due to insufficient, isolated, and\nsmall-scale evaluation. For instance, are LLMs similar to humans in modifying\ncertain knowledge? What differs editing and unlearning as training data\nincreases? This paper proposes KnowledgeSmith, a unified framework to\nsystematically understand the updating mechanism of LLMs. We first cast editing\nand unlearning as instances of one constrained optimization problem. Then, we\npropose an automatic dataset generator that provides structured interventions\nacross multiple graph levels and data scales, enabling controlled studies of\nhow different modification strategies propagate through model knowledge.\nExtensive experiments demonstrate nuanced insights over knowledge propagation,\nplasticity scaling, consistency, and robustness. For instance, our results show\nthat LLMs do not exhibit similar updating as humans for different levels of\nknowledge, and there exists consistency-capacity trade-off. We hope our\nfindings can offer suggestions to the design of more reliable and scalable\nstrategies. Code: https://github.com/AIFrontierLab/KnowledgeSmith.git"
                },
                "authors": [
                    {
                        "name": "Yinyi Luo"
                    },
                    {
                        "name": "Zhexian Zhou"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Kai Qiu"
                    },
                    {
                        "name": "Marios Savvides"
                    },
                    {
                        "name": "Sharon Li"
                    },
                    {
                        "name": "Jindong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jindong Wang"
                },
                "author": "Jindong Wang",
                "arxiv_comment": "Technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02392v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02392v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22926v2",
                "updated": "2025-10-14T15:32:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    32,
                    0,
                    1,
                    287,
                    0
                ],
                "published": "2025-09-26T20:51:48Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    20,
                    51,
                    48,
                    4,
                    269,
                    0
                ],
                "title": "Large language models management of medications: three performance\n  analyses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models management of medications: three performance\n  analyses"
                },
                "summary": "Purpose: Large language models (LLMs) have proven performance for certain\ndiagnostic tasks, however limited studies have evaluated their consistency in\nrecommending appropriate medication regimens for a given diagnosis. Medication\nmanagement is a complex task that requires synthesis of drug formulation and\ncomplete order instructions for safe use. Here, the performance of GPT 4o, an\nLLM available with ChatGPT, was tested for three medication management tasks.\nMethods: GPT-4o performance was tested using three medication tasks:\nidentifying available formulations for a given generic drug name, identifying\ndrug-drug interactions (DDI) for a given medication regimen, and preparing a\nmedication order for a given generic drug name. For each experiment, the models\nraw text response was captured exactly as returned and evaluated using\nclinician evaluation in addition to standard LLM metrics, including Term\nFrequency-Inverse Document Frequency (TF IDF) vectors, normalized Levenshtein\nsimilarity, and Recall-Oriented Understudy for Gisting Evaluation (ROUGE\n1/ROUGE L F1) between each response and its reference string. Results: For the\nfirst task of drug-formulation matching, GPT-4o had 49% accuracy for generic\nmedications being matched to all available formulations, with an average of\n1.23 omissions per medication and 1.14 hallucinations per medication. For the\nsecond task of drug-drug interaction identification, the accuracy was 54.7% for\nidentifying the DDI pair. For the third task, GPT-4o generated order sentences\ncontaining no medication or abbreviation errors in 65.8% of cases. Conclusions:\nModel performance for basic medication tasks was consistently poor. This\nevaluation highlights the need for domain-specific training through\nclinician-annotated datasets and a comprehensive evaluation framework for\nbenchmarking performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose: Large language models (LLMs) have proven performance for certain\ndiagnostic tasks, however limited studies have evaluated their consistency in\nrecommending appropriate medication regimens for a given diagnosis. Medication\nmanagement is a complex task that requires synthesis of drug formulation and\ncomplete order instructions for safe use. Here, the performance of GPT 4o, an\nLLM available with ChatGPT, was tested for three medication management tasks.\nMethods: GPT-4o performance was tested using three medication tasks:\nidentifying available formulations for a given generic drug name, identifying\ndrug-drug interactions (DDI) for a given medication regimen, and preparing a\nmedication order for a given generic drug name. For each experiment, the models\nraw text response was captured exactly as returned and evaluated using\nclinician evaluation in addition to standard LLM metrics, including Term\nFrequency-Inverse Document Frequency (TF IDF) vectors, normalized Levenshtein\nsimilarity, and Recall-Oriented Understudy for Gisting Evaluation (ROUGE\n1/ROUGE L F1) between each response and its reference string. Results: For the\nfirst task of drug-formulation matching, GPT-4o had 49% accuracy for generic\nmedications being matched to all available formulations, with an average of\n1.23 omissions per medication and 1.14 hallucinations per medication. For the\nsecond task of drug-drug interaction identification, the accuracy was 54.7% for\nidentifying the DDI pair. For the third task, GPT-4o generated order sentences\ncontaining no medication or abbreviation errors in 65.8% of cases. Conclusions:\nModel performance for basic medication tasks was consistently poor. This\nevaluation highlights the need for domain-specific training through\nclinician-annotated datasets and a comprehensive evaluation framework for\nbenchmarking performance."
                },
                "authors": [
                    {
                        "name": "Kelli Henry"
                    },
                    {
                        "name": "Steven Xu"
                    },
                    {
                        "name": "Kaitlin Blotske"
                    },
                    {
                        "name": "Moriah Cargile"
                    },
                    {
                        "name": "Erin F. Barreto"
                    },
                    {
                        "name": "Brian Murray"
                    },
                    {
                        "name": "Susan Smith"
                    },
                    {
                        "name": "Seth R. Bauer"
                    },
                    {
                        "name": "Xingmeng Zhao"
                    },
                    {
                        "name": "Adeleine Tilley"
                    },
                    {
                        "name": "Yanjun Gao"
                    },
                    {
                        "name": "Tianming Liu"
                    },
                    {
                        "name": "Sunghwan Sohn"
                    },
                    {
                        "name": "Andrea Sikora"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Sikora"
                },
                "author": "Andrea Sikora",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12637v1",
                "updated": "2025-10-14T15:31:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    31,
                    21,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T15:31:21Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    31,
                    21,
                    1,
                    287,
                    0
                ],
                "title": "COSTAR-A: A prompting framework for enhancing Large Language Model\n  performance on Point-of-View questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COSTAR-A: A prompting framework for enhancing Large Language Model\n  performance on Point-of-View questions"
                },
                "summary": "Large Language Models (LLMs) are highly sensitive to prompt design, and\nmaking optimized prompting techniques is crucial for generating consistent,\nhigh-quality outputs. In this study, we introduce COSTAR-A, a novel prompt\nengineering framework that enhances the existing COSTAR method, which stands\nfor Context, Objective, Style, Tone, Audience, and Response, by adding the\n'Answer' component at the end. We demonstrate that while the original COSTAR\nframework improves prompt clarity and aligns outputs for larger LLMs, its\nperformance is less consistent with smaller, locally optimized models,\nparticularly in tasks that require more directive or constrained outputs.\nThrough a series of controlled prompt-output assessments with smaller (at most\n8 billion parameters), fine-tuned models, we found that COSTAR-A can enhance\nthe output structure and decisiveness of localized LLMs for certain tasks,\nalthough its effectiveness varies across models and use cases. Notably, the\nLlama 3.1-8B model exhibited performance improvements when prompted with\nCOSTAR-A compared to COSTAR alone. These findings emphasize the adaptability\nand scalability of COSTAR-A as a prompting framework, particularly in\ncomputationally efficient AI deployments on resource-constrained hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are highly sensitive to prompt design, and\nmaking optimized prompting techniques is crucial for generating consistent,\nhigh-quality outputs. In this study, we introduce COSTAR-A, a novel prompt\nengineering framework that enhances the existing COSTAR method, which stands\nfor Context, Objective, Style, Tone, Audience, and Response, by adding the\n'Answer' component at the end. We demonstrate that while the original COSTAR\nframework improves prompt clarity and aligns outputs for larger LLMs, its\nperformance is less consistent with smaller, locally optimized models,\nparticularly in tasks that require more directive or constrained outputs.\nThrough a series of controlled prompt-output assessments with smaller (at most\n8 billion parameters), fine-tuned models, we found that COSTAR-A can enhance\nthe output structure and decisiveness of localized LLMs for certain tasks,\nalthough its effectiveness varies across models and use cases. Notably, the\nLlama 3.1-8B model exhibited performance improvements when prompted with\nCOSTAR-A compared to COSTAR alone. These findings emphasize the adaptability\nand scalability of COSTAR-A as a prompting framework, particularly in\ncomputationally efficient AI deployments on resource-constrained hardware."
                },
                "authors": [
                    {
                        "name": "Nzubechukwu C. Ohalete"
                    },
                    {
                        "name": "Kevin B. Gittner"
                    },
                    {
                        "name": "Lauren M. Matheny"
                    }
                ],
                "author_detail": {
                    "name": "Lauren M. Matheny"
                },
                "arxiv_affiliation": "School of Data Science and Analytics, Kennesaw State University, GA, USA",
                "author": "Lauren M. Matheny",
                "arxiv_comment": "20 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03999v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03999v2",
                "updated": "2025-10-14T15:30:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    30,
                    52,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-05T02:18:23Z",
                "published_parsed": [
                    2025,
                    10,
                    5,
                    2,
                    18,
                    23,
                    6,
                    278,
                    0
                ],
                "title": "Simulating and Understanding Deceptive Behaviors in Long-Horizon\n  Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating and Understanding Deceptive Behaviors in Long-Horizon\n  Interactions"
                },
                "summary": "Deception is a pervasive feature of human communication and an emerging\nconcern in large language models (LLMs). While recent studies document\ninstances of LLM deception under pressure, most evaluations remain confined to\nsingle-turn prompts and fail to capture the long-horizon interactions in which\ndeceptive strategies typically unfold. We introduce the first simulation\nframework for probing and evaluating deception in LLMs under extended sequences\nof interdependent tasks and dynamic contextual pressures. Our framework\ninstantiates a multi-agent system: a performer agent tasked with completing\ntasks and a supervisor agent that evaluates progress, provides feedback, and\nmaintains evolving states of trust. An independent deception auditor then\nreviews full trajectories to identify when and how deception occurs. We conduct\nextensive experiments across 11 frontier models, spanning both closed- and\nopen-source systems, and find that deception is model-dependent, increases with\nevent pressure, and consistently erodes supervisor trust. Qualitative analyses\nfurther reveal distinct strategies of concealment, equivocation, and\nfalsification. Our findings establish deception as an emergent risk in\nlong-horizon interactions and provide a foundation for evaluating future LLMs\nin real-world, trust-sensitive contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deception is a pervasive feature of human communication and an emerging\nconcern in large language models (LLMs). While recent studies document\ninstances of LLM deception under pressure, most evaluations remain confined to\nsingle-turn prompts and fail to capture the long-horizon interactions in which\ndeceptive strategies typically unfold. We introduce the first simulation\nframework for probing and evaluating deception in LLMs under extended sequences\nof interdependent tasks and dynamic contextual pressures. Our framework\ninstantiates a multi-agent system: a performer agent tasked with completing\ntasks and a supervisor agent that evaluates progress, provides feedback, and\nmaintains evolving states of trust. An independent deception auditor then\nreviews full trajectories to identify when and how deception occurs. We conduct\nextensive experiments across 11 frontier models, spanning both closed- and\nopen-source systems, and find that deception is model-dependent, increases with\nevent pressure, and consistently erodes supervisor trust. Qualitative analyses\nfurther reveal distinct strategies of concealment, equivocation, and\nfalsification. Our findings establish deception as an emergent risk in\nlong-horizon interactions and provide a foundation for evaluating future LLMs\nin real-world, trust-sensitive contexts."
                },
                "authors": [
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Xuanming Zhang"
                    },
                    {
                        "name": "Samuel Yeh"
                    },
                    {
                        "name": "Jwala Dhamala"
                    },
                    {
                        "name": "Ousmane Dia"
                    },
                    {
                        "name": "Rahul Gupta"
                    },
                    {
                        "name": "Sharon Li"
                    }
                ],
                "author_detail": {
                    "name": "Sharon Li"
                },
                "author": "Sharon Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03999v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03999v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18943v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18943v3",
                "updated": "2025-10-14T15:30:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    30,
                    4,
                    1,
                    287,
                    0
                ],
                "published": "2025-05-25T02:32:57Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    2,
                    32,
                    57,
                    6,
                    145,
                    0
                ],
                "title": "MetaMind: Modeling Human Social Thoughts with Metacognitive Multi-Agent\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaMind: Modeling Human Social Thoughts with Metacognitive Multi-Agent\n  Systems"
                },
                "summary": "Human social interactions depend on the ability to infer others' unspoken\nintentions, emotions, and beliefs-a cognitive skill grounded in the\npsychological concept of Theory of Mind (ToM). While large language models\n(LLMs) excel in semantic understanding tasks, they struggle with the ambiguity\nand contextual nuance inherent in human communication. To bridge this gap, we\nintroduce MetaMind, a multi-agent framework inspired by psychological theories\nof metacognition, designed to emulate human-like social reasoning. MetaMind\ndecomposes social understanding into three collaborative stages: (1) a\nTheory-of-Mind Agent generates hypotheses about user mental states (e.g.,\nintent, emotion), (2) a Moral Agent refines these hypotheses using cultural\nnorms and ethical constraints, and (3) a Response Agent generates contextually\nappropriate responses while validating alignment with inferred intent. Our\nframework achieves state-of-the-art performance across three challenging\nbenchmarks, with 35.7% improvement in real-world social scenarios and 6.2% gain\nin ToM reasoning. Notably, it enables LLMs to match human-level performance on\nkey ToM tasks for the first time. Ablation studies confirm the necessity of all\ncomponents, which showcase the framework's ability to balance contextual\nplausibility, social appropriateness, and user adaptation. This work advances\nAI systems toward human-like social intelligence, with applications in\nempathetic dialogue and culturally sensitive interactions. Code is available at\nhttps://github.com/XMZhangAI/MetaMind.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human social interactions depend on the ability to infer others' unspoken\nintentions, emotions, and beliefs-a cognitive skill grounded in the\npsychological concept of Theory of Mind (ToM). While large language models\n(LLMs) excel in semantic understanding tasks, they struggle with the ambiguity\nand contextual nuance inherent in human communication. To bridge this gap, we\nintroduce MetaMind, a multi-agent framework inspired by psychological theories\nof metacognition, designed to emulate human-like social reasoning. MetaMind\ndecomposes social understanding into three collaborative stages: (1) a\nTheory-of-Mind Agent generates hypotheses about user mental states (e.g.,\nintent, emotion), (2) a Moral Agent refines these hypotheses using cultural\nnorms and ethical constraints, and (3) a Response Agent generates contextually\nappropriate responses while validating alignment with inferred intent. Our\nframework achieves state-of-the-art performance across three challenging\nbenchmarks, with 35.7% improvement in real-world social scenarios and 6.2% gain\nin ToM reasoning. Notably, it enables LLMs to match human-level performance on\nkey ToM tasks for the first time. Ablation studies confirm the necessity of all\ncomponents, which showcase the framework's ability to balance contextual\nplausibility, social appropriateness, and user adaptation. This work advances\nAI systems toward human-like social intelligence, with applications in\nempathetic dialogue and culturally sensitive interactions. Code is available at\nhttps://github.com/XMZhangAI/MetaMind."
                },
                "authors": [
                    {
                        "name": "Xuanming Zhang"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Samuel Yeh"
                    },
                    {
                        "name": "Sharon Li"
                    }
                ],
                "author_detail": {
                    "name": "Sharon Li"
                },
                "author": "Sharon Li",
                "arxiv_comment": "NeurIPS 2025 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18943v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18943v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12635v1",
                "updated": "2025-10-14T15:29:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    29,
                    57,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T15:29:57Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    29,
                    57,
                    1,
                    287,
                    0
                ],
                "title": "Memory as Action: Autonomous Context Curation for Long-Horizon Agentic\n  Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory as Action: Autonomous Context Curation for Long-Horizon Agentic\n  Tasks"
                },
                "summary": "Large Language Models face challenges in long-horizon agentic tasks as their\nconstrained memory is easily overwhelmed by distracting or irrelevant context.\nExisting working memory methods typically rely on external, heuristic\nmechanisms that are decoupled from the agent's core policy. In this work, we\nreframe working memory management as a learnable, intrinsic capability. We\npropose a novel framework, Memory-as-Action, where an agent actively manages\nits working memory by executing explicit editing operations as part of a\nunified policy. This formulation allows an agent, trained via reinforcement\nlearning, to balance memory curation against long-term task objectives under\ngiven resource constraints. However, such memory editing actions break the\nstandard assumption of a continuously growing prefix in LLM interactions,\nleading to what we call trajectory fractures. These non-prefix changes disrupt\nthe causal continuity required by standard policy gradient methods, making\nthose methods inapplicable. To address this, we propose a new algorithm,\nDynamic Context Policy Optimization, which enables stable end-to-end\nreinforcement learning by segmenting trajectories at memory action points and\napplying trajectory-level advantages to the resulting action segments. Our\nresults demonstrate that jointly optimizing for task reasoning and memory\nmanagement in an end-to-end fashion not only reduces overall computational\nconsumption but also improves task performance, driven by adaptive context\ncuration strategies tailored to the model's intrinsic capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models face challenges in long-horizon agentic tasks as their\nconstrained memory is easily overwhelmed by distracting or irrelevant context.\nExisting working memory methods typically rely on external, heuristic\nmechanisms that are decoupled from the agent's core policy. In this work, we\nreframe working memory management as a learnable, intrinsic capability. We\npropose a novel framework, Memory-as-Action, where an agent actively manages\nits working memory by executing explicit editing operations as part of a\nunified policy. This formulation allows an agent, trained via reinforcement\nlearning, to balance memory curation against long-term task objectives under\ngiven resource constraints. However, such memory editing actions break the\nstandard assumption of a continuously growing prefix in LLM interactions,\nleading to what we call trajectory fractures. These non-prefix changes disrupt\nthe causal continuity required by standard policy gradient methods, making\nthose methods inapplicable. To address this, we propose a new algorithm,\nDynamic Context Policy Optimization, which enables stable end-to-end\nreinforcement learning by segmenting trajectories at memory action points and\napplying trajectory-level advantages to the resulting action segments. Our\nresults demonstrate that jointly optimizing for task reasoning and memory\nmanagement in an end-to-end fashion not only reduces overall computational\nconsumption but also improves task performance, driven by adaptive context\ncuration strategies tailored to the model's intrinsic capabilities."
                },
                "authors": [
                    {
                        "name": "Yuxiang Zhang"
                    },
                    {
                        "name": "Jiangming Shu"
                    },
                    {
                        "name": "Ye Ma"
                    },
                    {
                        "name": "Xueyuan Lin"
                    },
                    {
                        "name": "Shangxi Wu"
                    },
                    {
                        "name": "Jitao Sang"
                    }
                ],
                "author_detail": {
                    "name": "Jitao Sang"
                },
                "author": "Jitao Sang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12633v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12633v1",
                "updated": "2025-10-14T15:29:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    29,
                    14,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T15:29:14Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    29,
                    14,
                    1,
                    287,
                    0
                ],
                "title": "Laminar: A Scalable Asynchronous RL Post-Training Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Laminar: A Scalable Asynchronous RL Post-Training Framework"
                },
                "summary": "Reinforcement learning (RL) post-training for Large Language Models (LLMs) is\nnow scaling to large clusters and running for extended durations to enhance\nmodel reasoning performance. However, the scalability of existing RL frameworks\nis limited, as extreme long-tail skewness in RL trajectory generation causes\nsevere GPU underutilization. Current asynchronous RL systems attempt to\nmitigate this, but they rely on global weight synchronization between the actor\nand all rollouts, which creates a rigid model update schedule. This global\nsynchronization is ill-suited for the highly skewed and evolving distribution\nof trajectory generation latency in RL training, crippling training efficiency.\nOur key insight is that efficient scaling requires breaking this lockstep\nthrough trajectory-level asynchrony, which generates and consumes each\ntrajectory independently. We propose Laminar, a scalable and robust RL\npost-training system built on a fully decoupled architecture. First, we replace\nglobal updates with a tier of relay workers acting as a distributed parameter\nservice. This enables asynchronous and fine-grained weight synchronization,\nallowing rollouts to pull the latest weight anytime without stalling the\nactor's training loop. Second, a dynamic repack mechanism consolidates\nlong-tail trajectories onto a few dedicated rollouts, maximizing generation\nthroughput. The fully decoupled design also isolates failures, ensuring\nrobustness for long-running jobs. Our evaluation on a 1024-GPU cluster shows\nthat Laminar achieves up to 5.48$\\times$ training throughput speedup over\nstate-of-the-art systems, while reducing model convergence time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) post-training for Large Language Models (LLMs) is\nnow scaling to large clusters and running for extended durations to enhance\nmodel reasoning performance. However, the scalability of existing RL frameworks\nis limited, as extreme long-tail skewness in RL trajectory generation causes\nsevere GPU underutilization. Current asynchronous RL systems attempt to\nmitigate this, but they rely on global weight synchronization between the actor\nand all rollouts, which creates a rigid model update schedule. This global\nsynchronization is ill-suited for the highly skewed and evolving distribution\nof trajectory generation latency in RL training, crippling training efficiency.\nOur key insight is that efficient scaling requires breaking this lockstep\nthrough trajectory-level asynchrony, which generates and consumes each\ntrajectory independently. We propose Laminar, a scalable and robust RL\npost-training system built on a fully decoupled architecture. First, we replace\nglobal updates with a tier of relay workers acting as a distributed parameter\nservice. This enables asynchronous and fine-grained weight synchronization,\nallowing rollouts to pull the latest weight anytime without stalling the\nactor's training loop. Second, a dynamic repack mechanism consolidates\nlong-tail trajectories onto a few dedicated rollouts, maximizing generation\nthroughput. The fully decoupled design also isolates failures, ensuring\nrobustness for long-running jobs. Our evaluation on a 1024-GPU cluster shows\nthat Laminar achieves up to 5.48$\\times$ training throughput speedup over\nstate-of-the-art systems, while reducing model convergence time."
                },
                "authors": [
                    {
                        "name": "Guangming Sheng"
                    },
                    {
                        "name": "Yuxuan Tong"
                    },
                    {
                        "name": "Borui Wan"
                    },
                    {
                        "name": "Wang Zhang"
                    },
                    {
                        "name": "Chaobo Jia"
                    },
                    {
                        "name": "Xibin Wu"
                    },
                    {
                        "name": "Yuqi Wu"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Yanghua Peng"
                    },
                    {
                        "name": "Haibin Lin"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12633v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12633v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12621v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12621v2",
                "updated": "2025-10-15T06:42:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    6,
                    42,
                    22,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-14T15:20:06Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    20,
                    6,
                    1,
                    287,
                    0
                ],
                "title": "ACADATA: Parallel Dataset of Academic Data for Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACADATA: Parallel Dataset of Academic Data for Machine Translation"
                },
                "summary": "We present ACADATA, a high-quality parallel dataset for academic translation,\nthat consists of two subsets: ACAD-TRAIN, which contains approximately 1.5\nmillion author-generated paragraph pairs across 96 language directions and\nACAD-BENCH, a curated evaluation set of almost 6,000 translations covering 12\ndirections. To validate its utility, we fine-tune two Large Language Models\n(LLMs) on ACAD-TRAIN and benchmark them on ACAD-BENCH against specialized\nmachine-translation systems, general-purpose, open-weight LLMs, and several\nlarge-scale proprietary models. Experimental results demonstrate that\nfine-tuning on ACAD-TRAIN leads to improvements in academic translation quality\nby +6.1 and +12.4 d-BLEU points on average for 7B and 2B models respectively,\nwhile also improving long-context translation in a general domain by up to\n24.9% when translating out of English. The fine-tuned top-performing model\nsurpasses the best propietary and open-weight models on academic translation\ndomain. By releasing ACAD-TRAIN, ACAD-BENCH and the fine-tuned models, we\nprovide the community with a valuable resource to advance research in academic\ndomain and long-context translation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present ACADATA, a high-quality parallel dataset for academic translation,\nthat consists of two subsets: ACAD-TRAIN, which contains approximately 1.5\nmillion author-generated paragraph pairs across 96 language directions and\nACAD-BENCH, a curated evaluation set of almost 6,000 translations covering 12\ndirections. To validate its utility, we fine-tune two Large Language Models\n(LLMs) on ACAD-TRAIN and benchmark them on ACAD-BENCH against specialized\nmachine-translation systems, general-purpose, open-weight LLMs, and several\nlarge-scale proprietary models. Experimental results demonstrate that\nfine-tuning on ACAD-TRAIN leads to improvements in academic translation quality\nby +6.1 and +12.4 d-BLEU points on average for 7B and 2B models respectively,\nwhile also improving long-context translation in a general domain by up to\n24.9% when translating out of English. The fine-tuned top-performing model\nsurpasses the best propietary and open-weight models on academic translation\ndomain. By releasing ACAD-TRAIN, ACAD-BENCH and the fine-tuned models, we\nprovide the community with a valuable resource to advance research in academic\ndomain and long-context translation."
                },
                "authors": [
                    {
                        "name": "Iaki Lacunza"
                    },
                    {
                        "name": "Javier Garcia Gilabert"
                    },
                    {
                        "name": "Francesca De Luca Fornaciari"
                    },
                    {
                        "name": "Javier Aula-Blasco"
                    },
                    {
                        "name": "Aitor Gonzalez-Agirre"
                    },
                    {
                        "name": "Maite Melero"
                    },
                    {
                        "name": "Marta Villegas"
                    }
                ],
                "author_detail": {
                    "name": "Marta Villegas"
                },
                "author": "Marta Villegas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12621v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12621v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12611v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12611v1",
                "updated": "2025-10-14T15:09:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    9,
                    54,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T15:09:54Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    9,
                    54,
                    1,
                    287,
                    0
                ],
                "title": "Learning Robust Agile Flight Control with Stability Guarantees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Robust Agile Flight Control with Stability Guarantees"
                },
                "summary": "In the evolving landscape of high-speed agile quadrotor flight, achieving\nprecise trajectory tracking at the platform's operational limits is paramount.\nControllers must handle actuator constraints, exhibit robustness to\ndisturbances, and remain computationally efficient for safety-critical\napplications. In this work, we present a novel neural-augmented feedback\ncontroller for agile flight control. The controller addresses individual\nlimitations of existing state-of-the-art control paradigms and unifies their\nstrengths. We demonstrate the controller's capabilities, including the accurate\ntracking of highly aggressive trajectories that surpass the feasibility of the\nactuators. Notably, the controller provides universal stability guarantees,\nenhancing its robustness and tracking performance even in exceedingly\ndisturbance-prone settings. Its nonlinear feedback structure is highly\nefficient enabling fast computation at high update rates. Moreover, the\nlearning process in simulation is both fast and stable, and the controller's\ninherent robustness allows direct deployment to real-world platforms without\nthe need for training augmentations or fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the evolving landscape of high-speed agile quadrotor flight, achieving\nprecise trajectory tracking at the platform's operational limits is paramount.\nControllers must handle actuator constraints, exhibit robustness to\ndisturbances, and remain computationally efficient for safety-critical\napplications. In this work, we present a novel neural-augmented feedback\ncontroller for agile flight control. The controller addresses individual\nlimitations of existing state-of-the-art control paradigms and unifies their\nstrengths. We demonstrate the controller's capabilities, including the accurate\ntracking of highly aggressive trajectories that surpass the feasibility of the\nactuators. Notably, the controller provides universal stability guarantees,\nenhancing its robustness and tracking performance even in exceedingly\ndisturbance-prone settings. Its nonlinear feedback structure is highly\nefficient enabling fast computation at high update rates. Moreover, the\nlearning process in simulation is both fast and stable, and the controller's\ninherent robustness allows direct deployment to real-world platforms without\nthe need for training augmentations or fine-tuning."
                },
                "authors": [
                    {
                        "name": "Lukas Pries"
                    },
                    {
                        "name": "Markus Ryll"
                    }
                ],
                "author_detail": {
                    "name": "Markus Ryll"
                },
                "author": "Markus Ryll",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12611v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12611v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12608v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12608v1",
                "updated": "2025-10-14T15:07:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    7,
                    27,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T15:07:27Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    7,
                    27,
                    1,
                    287,
                    0
                ],
                "title": "StyleDecipher: Robust and Explainable Detection of LLM-Generated Texts\n  with Stylistic Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StyleDecipher: Robust and Explainable Detection of LLM-Generated Texts\n  with Stylistic Analysis"
                },
                "summary": "With the increasing integration of large language models (LLMs) into\nopen-domain writing, detecting machine-generated text has become a critical\ntask for ensuring content authenticity and trust. Existing approaches rely on\nstatistical discrepancies or model-specific heuristics to distinguish between\nLLM-generated and human-written text. However, these methods struggle in\nreal-world scenarios due to limited generalization, vulnerability to\nparaphrasing, and lack of explainability, particularly when facing stylistic\ndiversity or hybrid human-AI authorship. In this work, we propose\nStyleDecipher, a robust and explainable detection framework that revisits\nLLM-generated text detection using combined feature extractors to quantify\nstylistic differences. By jointly modeling discrete stylistic indicators and\ncontinuous stylistic representations derived from semantic embeddings,\nStyleDecipher captures distinctive style-level divergences between human and\nLLM outputs within a unified representation space. This framework enables\naccurate, explainable, and domain-agnostic detection without requiring access\nto model internals or labeled segments. Extensive experiments across five\ndiverse domains, including news, code, essays, reviews, and academic abstracts,\ndemonstrate that StyleDecipher consistently achieves state-of-the-art in-domain\naccuracy. Moreover, in cross-domain evaluations, it surpasses existing\nbaselines by up to 36.30%, while maintaining robustness against adversarial\nperturbations and mixed human-AI content. Further qualitative and quantitative\nanalysis confirms that stylistic signals provide explainable evidence for\ndistinguishing machine-generated text. Our source code can be accessed at\nhttps://github.com/SiyuanLi00/StyleDecipher.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing integration of large language models (LLMs) into\nopen-domain writing, detecting machine-generated text has become a critical\ntask for ensuring content authenticity and trust. Existing approaches rely on\nstatistical discrepancies or model-specific heuristics to distinguish between\nLLM-generated and human-written text. However, these methods struggle in\nreal-world scenarios due to limited generalization, vulnerability to\nparaphrasing, and lack of explainability, particularly when facing stylistic\ndiversity or hybrid human-AI authorship. In this work, we propose\nStyleDecipher, a robust and explainable detection framework that revisits\nLLM-generated text detection using combined feature extractors to quantify\nstylistic differences. By jointly modeling discrete stylistic indicators and\ncontinuous stylistic representations derived from semantic embeddings,\nStyleDecipher captures distinctive style-level divergences between human and\nLLM outputs within a unified representation space. This framework enables\naccurate, explainable, and domain-agnostic detection without requiring access\nto model internals or labeled segments. Extensive experiments across five\ndiverse domains, including news, code, essays, reviews, and academic abstracts,\ndemonstrate that StyleDecipher consistently achieves state-of-the-art in-domain\naccuracy. Moreover, in cross-domain evaluations, it surpasses existing\nbaselines by up to 36.30%, while maintaining robustness against adversarial\nperturbations and mixed human-AI content. Further qualitative and quantitative\nanalysis confirms that stylistic signals provide explainable evidence for\ndistinguishing machine-generated text. Our source code can be accessed at\nhttps://github.com/SiyuanLi00/StyleDecipher."
                },
                "authors": [
                    {
                        "name": "Siyuan Li"
                    },
                    {
                        "name": "Aodu Wulianghai"
                    },
                    {
                        "name": "Xi Lin"
                    },
                    {
                        "name": "Guangyan Li"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Jun Wu"
                    },
                    {
                        "name": "Jianhua Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianhua Li"
                },
                "author": "Jianhua Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12608v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07505v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07505v2",
                "updated": "2025-10-14T14:48:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    14,
                    48,
                    46,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-08T20:04:17Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    20,
                    4,
                    17,
                    2,
                    281,
                    0
                ],
                "title": "PEAR: Planner-Executor Agent Robustness Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PEAR: Planner-Executor Agent Robustness Benchmark"
                },
                "summary": "Large Language Model (LLM)-based Multi-Agent Systems (MAS) have emerged as a\npowerful paradigm for tackling complex, multi-step tasks across diverse\ndomains. However, despite their impressive capabilities, MAS remain susceptible\nto adversarial manipulation. Existing studies typically examine isolated attack\nsurfaces or specific scenarios, leaving a lack of holistic understanding of MAS\nvulnerabilities. To bridge this gap, we introduce PEAR, a benchmark for\nsystematically evaluating both the utility and vulnerability of\nplanner-executor MAS. While compatible with various MAS architectures, our\nbenchmark focuses on the planner-executor structure, which is a practical and\nwidely adopted design. Through extensive experiments, we find that (1) a weak\nplanner degrades overall clean task performance more severely than a weak\nexecutor; (2) while a memory module is essential for the planner, having a\nmemory module for the executor does not impact the clean task performance; (3)\nthere exists a trade-off between task performance and robustness; and (4)\nattacks targeting the planner are particularly effective at misleading the\nsystem. These findings offer actionable insights for enhancing the robustness\nof MAS and lay the groundwork for principled defenses in multi-agent settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based Multi-Agent Systems (MAS) have emerged as a\npowerful paradigm for tackling complex, multi-step tasks across diverse\ndomains. However, despite their impressive capabilities, MAS remain susceptible\nto adversarial manipulation. Existing studies typically examine isolated attack\nsurfaces or specific scenarios, leaving a lack of holistic understanding of MAS\nvulnerabilities. To bridge this gap, we introduce PEAR, a benchmark for\nsystematically evaluating both the utility and vulnerability of\nplanner-executor MAS. While compatible with various MAS architectures, our\nbenchmark focuses on the planner-executor structure, which is a practical and\nwidely adopted design. Through extensive experiments, we find that (1) a weak\nplanner degrades overall clean task performance more severely than a weak\nexecutor; (2) while a memory module is essential for the planner, having a\nmemory module for the executor does not impact the clean task performance; (3)\nthere exists a trade-off between task performance and robustness; and (4)\nattacks targeting the planner are particularly effective at misleading the\nsystem. These findings offer actionable insights for enhancing the robustness\nof MAS and lay the groundwork for principled defenses in multi-agent settings."
                },
                "authors": [
                    {
                        "name": "Shen Dong"
                    },
                    {
                        "name": "Mingxuan Zhang"
                    },
                    {
                        "name": "Pengfei He"
                    },
                    {
                        "name": "Li Ma"
                    },
                    {
                        "name": "Bhavani Thuraisingham"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Yue Xing"
                    }
                ],
                "author_detail": {
                    "name": "Yue Xing"
                },
                "author": "Yue Xing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07505v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07505v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06486v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06486v2",
                "updated": "2025-10-14T14:47:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    14,
                    47,
                    52,
                    1,
                    287,
                    0
                ],
                "published": "2025-04-08T23:19:00Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    23,
                    19,
                    0,
                    1,
                    98,
                    0
                ],
                "title": "Mind the (Data) Gap: Evaluating Vision Systems in Small Data\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind the (Data) Gap: Evaluating Vision Systems in Small Data\n  Applications"
                },
                "summary": "The practical application of AI tools for specific computer vision tasks\nrelies on the \"small-data regime\" of hundreds to thousands of labeled samples.\nThis small-data regime is vital for applications requiring expensive expert\nannotations, such as ecological monitoring, medical diagnostics or industrial\nquality control. We find, however, that computer vision research has ignored\nthe small data regime as evaluations increasingly focus on zero- and few-shot\nlearning. We use the Natural World Tasks (NeWT) benchmark to compare\nmulti-modal large language models (MLLMs) and vision-only methods across\nvarying training set sizes. MLLMs exhibit early performance plateaus, while\nvision-only methods improve throughout the small-data regime, with performance\ngaps widening beyond 10 training examples. We provide the first comprehensive\ncomparison between these approaches in small-data contexts and advocate for\nexplicit small-data evaluations in AI research to better bridge theoretical\nadvances with practical deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The practical application of AI tools for specific computer vision tasks\nrelies on the \"small-data regime\" of hundreds to thousands of labeled samples.\nThis small-data regime is vital for applications requiring expensive expert\nannotations, such as ecological monitoring, medical diagnostics or industrial\nquality control. We find, however, that computer vision research has ignored\nthe small data regime as evaluations increasingly focus on zero- and few-shot\nlearning. We use the Natural World Tasks (NeWT) benchmark to compare\nmulti-modal large language models (MLLMs) and vision-only methods across\nvarying training set sizes. MLLMs exhibit early performance plateaus, while\nvision-only methods improve throughout the small-data regime, with performance\ngaps widening beyond 10 training examples. We provide the first comprehensive\ncomparison between these approaches in small-data contexts and advocate for\nexplicit small-data evaluations in AI research to better bridge theoretical\nadvances with practical deployments."
                },
                "authors": [
                    {
                        "name": "Samuel Stevens"
                    },
                    {
                        "name": "S M Rayeed"
                    },
                    {
                        "name": "Jenna Kline"
                    }
                ],
                "author_detail": {
                    "name": "Jenna Kline"
                },
                "author": "Jenna Kline",
                "arxiv_comment": "5 pages (main text), 3 figures. Accepted at the Imageomics Workshop\n  at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06486v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06486v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23564v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23564v2",
                "updated": "2025-10-14T14:47:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    14,
                    47,
                    29,
                    1,
                    287,
                    0
                ],
                "published": "2025-09-28T01:44:05Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    1,
                    44,
                    5,
                    6,
                    271,
                    0
                ],
                "title": "Clean First, Align Later: Benchmarking Preference Data Cleaning for\n  Reliable LLM Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clean First, Align Later: Benchmarking Preference Data Cleaning for\n  Reliable LLM Alignment"
                },
                "summary": "Human feedback plays a pivotal role in aligning large language models (LLMs)\nwith human preferences. However, such feedback is often noisy or inconsistent,\nwhich can degrade the quality of reward models and hinder alignment. While\nvarious automated data cleaning methods have been proposed to mitigate this\nissue, a systematic evaluation of their effectiveness and generalizability\nremains lacking. To bridge this gap, we introduce the first comprehensive\nbenchmark for evaluating 13 preference data cleaning methods in the context of\nLLM alignment. PrefCleanBench offers a standardized protocol to assess cleaning\nstrategies in terms of alignment performance and generalizability across\ndiverse datasets, model architectures, and optimization algorithms. By unifying\ndisparate methods and rigorously comparing them, we uncover key factors that\ndetermine the success of data cleaning in alignment tasks. This benchmark lays\nthe groundwork for principled and reproducible approaches to improving LLM\nalignment through better data quality-highlighting the crucial but\nunderexplored role of data preprocessing in responsible AI development. We\nrelease modular implementations of all methods to catalyze further research:\nhttps://github.com/deeplearning-wisc/PrefCleanBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human feedback plays a pivotal role in aligning large language models (LLMs)\nwith human preferences. However, such feedback is often noisy or inconsistent,\nwhich can degrade the quality of reward models and hinder alignment. While\nvarious automated data cleaning methods have been proposed to mitigate this\nissue, a systematic evaluation of their effectiveness and generalizability\nremains lacking. To bridge this gap, we introduce the first comprehensive\nbenchmark for evaluating 13 preference data cleaning methods in the context of\nLLM alignment. PrefCleanBench offers a standardized protocol to assess cleaning\nstrategies in terms of alignment performance and generalizability across\ndiverse datasets, model architectures, and optimization algorithms. By unifying\ndisparate methods and rigorously comparing them, we uncover key factors that\ndetermine the success of data cleaning in alignment tasks. This benchmark lays\nthe groundwork for principled and reproducible approaches to improving LLM\nalignment through better data quality-highlighting the crucial but\nunderexplored role of data preprocessing in responsible AI development. We\nrelease modular implementations of all methods to catalyze further research:\nhttps://github.com/deeplearning-wisc/PrefCleanBench."
                },
                "authors": [
                    {
                        "name": "Samuel Yeh"
                    },
                    {
                        "name": "Sharon Li"
                    }
                ],
                "author_detail": {
                    "name": "Sharon Li"
                },
                "author": "Sharon Li",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23564v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23564v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17542v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17542v2",
                "updated": "2025-10-14T14:46:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    14,
                    46,
                    58,
                    1,
                    287,
                    0
                ],
                "published": "2025-04-24T13:32:20Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    32,
                    20,
                    3,
                    114,
                    0
                ],
                "title": "Cottontail: Large Language Model-Driven Concolic Execution for Highly\n  Structured Test Input Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cottontail: Large Language Model-Driven Concolic Execution for Highly\n  Structured Test Input Generation"
                },
                "summary": "How can we perform concolic execution to generate highly structured test\ninputs for systematically testing parsing programs? Existing concolic execution\nengines are significantly restricted by (1) input structure-agnostic path\nconstraint selection, leading to the waste of testing effort or missing\ncoverage; (2) limited constraint-solving capability, yielding many\nsyntactically invalid test inputs; (3) reliance on manual acquisition of highly\nstructured seed inputs, resulting in non-continuous testing.\n  This paper proposes Cottontail, a new Large Language Model (LLM)-driven\nconcolic execution engine, to mitigate the above limitations. A more complete\nprogram path representation, named Expressive Structural Coverage Tree (ESCT),\nis first constructed to select structure-aware path constraints. Later, an\nLLM-driven constraint solver based on a Solve-Complete paradigm is designed to\nsolve the path constraints smartly to get test inputs that are not only\nsatisfiable to the constraints but also valid to the input syntax. Finally, a\nhistory-guided seed acquisition is employed to obtain new highly structured\ntest inputs either before testing starts or after testing is saturated.\n  We implemented Cottontail on top of SymCC and evaluated eight extensively\ntested open-source libraries across four different formats (XML, SQL,\nJavaScript, and JSON). Cottontail significantly outperforms baseline approaches\nby 30.73% and 41.32% on average in terms of line and branch coverage. Besides,\nCottontail found six previously unknown vulnerabilities (six CVEs assigned). We\nhave reported these issues to developers, and four out of them have been fixed\nso far.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How can we perform concolic execution to generate highly structured test\ninputs for systematically testing parsing programs? Existing concolic execution\nengines are significantly restricted by (1) input structure-agnostic path\nconstraint selection, leading to the waste of testing effort or missing\ncoverage; (2) limited constraint-solving capability, yielding many\nsyntactically invalid test inputs; (3) reliance on manual acquisition of highly\nstructured seed inputs, resulting in non-continuous testing.\n  This paper proposes Cottontail, a new Large Language Model (LLM)-driven\nconcolic execution engine, to mitigate the above limitations. A more complete\nprogram path representation, named Expressive Structural Coverage Tree (ESCT),\nis first constructed to select structure-aware path constraints. Later, an\nLLM-driven constraint solver based on a Solve-Complete paradigm is designed to\nsolve the path constraints smartly to get test inputs that are not only\nsatisfiable to the constraints but also valid to the input syntax. Finally, a\nhistory-guided seed acquisition is employed to obtain new highly structured\ntest inputs either before testing starts or after testing is saturated.\n  We implemented Cottontail on top of SymCC and evaluated eight extensively\ntested open-source libraries across four different formats (XML, SQL,\nJavaScript, and JSON). Cottontail significantly outperforms baseline approaches\nby 30.73% and 41.32% on average in terms of line and branch coverage. Besides,\nCottontail found six previously unknown vulnerabilities (six CVEs assigned). We\nhave reported these issues to developers, and four out of them have been fixed\nso far."
                },
                "authors": [
                    {
                        "name": "Haoxin Tu"
                    },
                    {
                        "name": "Seongmin Lee"
                    },
                    {
                        "name": "Yuxian Li"
                    },
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Lingxiao Jiang"
                    },
                    {
                        "name": "Marcel Bhme"
                    }
                ],
                "author_detail": {
                    "name": "Marcel Bhme"
                },
                "author": "Marcel Bhme",
                "arxiv_comment": "To appear on the 2026 IEEE Symposium on Security and Privacy (22\n  pages, with detailed Appendix)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17542v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17542v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12587v1",
                "updated": "2025-10-14T14:42:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    14,
                    42,
                    40,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T14:42:40Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    14,
                    42,
                    40,
                    1,
                    287,
                    0
                ],
                "title": "Teaching Language Models to Faithfully Express their Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching Language Models to Faithfully Express their Uncertainty"
                },
                "summary": "Large language models (LLMs) often miscommunicate their uncertainty: repeated\nqueries can produce divergent answers, yet generated responses are typically\nunhedged or hedged in ways that do not reflect this variability. This conveys\nunfaithful information about the uncertain state of the LLMs' knowledge,\ncreating a faithfulness gap that affects even strong LLMs. We introduce\nFaithful Uncertainty Tuning (FUT): a fine-tuning approach that teaches\ninstruction-tuned LLMs to express uncertainty faithfully without altering their\nunderlying answer distribution. We construct training data by augmenting model\nsamples with uncertainty hedges (i.e. verbal cues such as 'possibly' or\n'likely') aligned with sample consistency, requiring no supervision beyond the\nmodel and a set of prompts. We evaluate FUT on open-domain question answering\n(QA) across multiple models and datasets. Our results show that FUT\nsubstantially reduces the faithfulness gap, while preserving QA accuracy and\nintroducing minimal semantic distribution shift. Further analyses demonstrate\nrobustness across decoding strategies, choice of hedgers, and other forms of\nuncertainty expression (i.e. numerical). These findings establish FUT as a\nsimple and effective way to teach LLMs to communicate uncertainty faithfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often miscommunicate their uncertainty: repeated\nqueries can produce divergent answers, yet generated responses are typically\nunhedged or hedged in ways that do not reflect this variability. This conveys\nunfaithful information about the uncertain state of the LLMs' knowledge,\ncreating a faithfulness gap that affects even strong LLMs. We introduce\nFaithful Uncertainty Tuning (FUT): a fine-tuning approach that teaches\ninstruction-tuned LLMs to express uncertainty faithfully without altering their\nunderlying answer distribution. We construct training data by augmenting model\nsamples with uncertainty hedges (i.e. verbal cues such as 'possibly' or\n'likely') aligned with sample consistency, requiring no supervision beyond the\nmodel and a set of prompts. We evaluate FUT on open-domain question answering\n(QA) across multiple models and datasets. Our results show that FUT\nsubstantially reduces the faithfulness gap, while preserving QA accuracy and\nintroducing minimal semantic distribution shift. Further analyses demonstrate\nrobustness across decoding strategies, choice of hedgers, and other forms of\nuncertainty expression (i.e. numerical). These findings establish FUT as a\nsimple and effective way to teach LLMs to communicate uncertainty faithfully."
                },
                "authors": [
                    {
                        "name": "Bryan Eikema"
                    },
                    {
                        "name": "Evgenia Ilia"
                    },
                    {
                        "name": "Jos G. C. de Souza"
                    },
                    {
                        "name": "Chrysoula Zerva"
                    },
                    {
                        "name": "Wilker Aziz"
                    }
                ],
                "author_detail": {
                    "name": "Wilker Aziz"
                },
                "author": "Wilker Aziz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08704v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08704v2",
                "updated": "2025-10-14T14:35:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    14,
                    35,
                    49,
                    1,
                    287,
                    0
                ],
                "published": "2025-07-11T15:57:37Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    15,
                    57,
                    37,
                    4,
                    192,
                    0
                ],
                "title": "Knowledge Fusion via Bidirectional Information Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Fusion via Bidirectional Information Aggregation"
                },
                "summary": "Knowledge graphs (KGs) are the cornerstone of the semantic web, offering\nup-to-date representations of real-world entities and relations. Yet large\nlanguage models (LLMs) remain largely static after pre-training, causing their\ninternal knowledge to become outdated and limiting their utility in\ntime-sensitive web applications. To bridge this gap between dynamic knowledge\nand static models, a prevalent approach is to enhance LLMs with KGs. However,\nprevailing methods typically rely on parameter-invasive fine-tuning, which\nrisks catastrophic forgetting and often degrades LLMs' general capabilities.\nMoreover, their static integration frameworks cannot keep pace with the\ncontinuous evolution of real-world KGs, hindering their deployment in dynamic\nweb environments. To bridge this gap, we introduce KGA\n(\\textit{\\underline{K}nowledge \\underline{G}raph-guided\n\\underline{A}ttention}), a novel framework that dynamically integrates external\nKGs into LLMs exclusively at inference-time without any parameter modification.\nInspired by research on neuroscience, we rewire the self-attention module by\ninnovatively introducing two synergistic pathways: a \\textit{bottom-up\nknowledge fusion} pathway and a \\textit{top-down attention guidance} pathway.\nThe \\textit{bottom-up pathway} dynamically integrates external knowledge into\ninput representations via input-driven KG fusion, which is akin to the\n\\textit{stimulus-driven attention process} in the human brain. Complementarily,\nthe \\textit{top-down pathway} aims to assess the contextual relevance of each\ntriple through a \\textit{goal-directed verification process}, thereby\nsuppressing task-irrelevant signals and amplifying knowledge-relevant patterns.\nBy synergistically combining these two pathways, our method supports real-time\nknowledge fusion. Extensive experiments on four benchmarks verify KGA's strong\nfusion performance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge graphs (KGs) are the cornerstone of the semantic web, offering\nup-to-date representations of real-world entities and relations. Yet large\nlanguage models (LLMs) remain largely static after pre-training, causing their\ninternal knowledge to become outdated and limiting their utility in\ntime-sensitive web applications. To bridge this gap between dynamic knowledge\nand static models, a prevalent approach is to enhance LLMs with KGs. However,\nprevailing methods typically rely on parameter-invasive fine-tuning, which\nrisks catastrophic forgetting and often degrades LLMs' general capabilities.\nMoreover, their static integration frameworks cannot keep pace with the\ncontinuous evolution of real-world KGs, hindering their deployment in dynamic\nweb environments. To bridge this gap, we introduce KGA\n(\\textit{\\underline{K}nowledge \\underline{G}raph-guided\n\\underline{A}ttention}), a novel framework that dynamically integrates external\nKGs into LLMs exclusively at inference-time without any parameter modification.\nInspired by research on neuroscience, we rewire the self-attention module by\ninnovatively introducing two synergistic pathways: a \\textit{bottom-up\nknowledge fusion} pathway and a \\textit{top-down attention guidance} pathway.\nThe \\textit{bottom-up pathway} dynamically integrates external knowledge into\ninput representations via input-driven KG fusion, which is akin to the\n\\textit{stimulus-driven attention process} in the human brain. Complementarily,\nthe \\textit{top-down pathway} aims to assess the contextual relevance of each\ntriple through a \\textit{goal-directed verification process}, thereby\nsuppressing task-irrelevant signals and amplifying knowledge-relevant patterns.\nBy synergistically combining these two pathways, our method supports real-time\nknowledge fusion. Extensive experiments on four benchmarks verify KGA's strong\nfusion performance and efficiency."
                },
                "authors": [
                    {
                        "name": "Songlin Zhai"
                    },
                    {
                        "name": "Guilin Qi"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Yuan Meng"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Meng"
                },
                "author": "Yuan Meng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08704v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08704v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18389v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18389v4",
                "updated": "2025-10-14T14:32:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    14,
                    32,
                    37,
                    1,
                    287,
                    0
                ],
                "published": "2025-05-23T21:33:16Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    21,
                    33,
                    16,
                    4,
                    143,
                    0
                ],
                "title": "ALLSTaR: Automated LLM-Driven Scheduler Generation and Testing for\n  Intent-Based RAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALLSTaR: Automated LLM-Driven Scheduler Generation and Testing for\n  Intent-Based RAN"
                },
                "summary": "The evolution toward open, programmable O-RAN and AI-RAN 6G networks creates\nunprecedented opportunities for Intent-Based Networking (IBN) to dynamically\noptimize RAN[...]. However, applying IBN effectively to the RAN scheduler [...]\nremains a significant challenge. Current approaches predominantly rely on\ncoarse-grained network slicing, lacking the granularity for dynamic adaptation\nto individual user conditions and traffic patterns. Despite the existence of a\nvast body of scheduling algorithms [...], their practical utilization is\nhindered by implementation heterogeneity, insufficient systematic evaluation in\nproduction environments, and the complexity of developing high-performance\nscheduler implementations.[...] To address these limitations, we propose\nALLSTaR (Automated LLm-driven Scheduler generation and Testing for intent-based\nRAN), a novel framework leveraging LLMs for automated, intent-driven scheduler\ndesign, implementation, and evaluation. ALLSTaR interprets NL intents,\nautomatically generates functional scheduler code from the research literature\nusing OCR and LLMs, and intelligently matches operator intents to the most\nsuitable scheduler(s). Our implementation deploys these schedulers as O-RAN\ndApps, enabling on-the-fly deployment and testing on a production-grade,\n5G-compliant testbed. This approach has enabled the largest-scale OTA\nexperimental comparison of 18 scheduling algorithms automatically synthesized\nfrom the academic literature. The resulting performance profiles serve as the\ninput for our Intent-Based Scheduling (IBS) framework, which dynamically\nselects and deploys appropriate schedulers that optimally satisfy operator\nintents. We validate our approach through multiple use cases unattainable with\ncurrent slicing-based optimization techniques, demonstrating fine-grained\ncontrol based on buffer status, physical layer conditions, and heterogeneous\ntraffic types",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution toward open, programmable O-RAN and AI-RAN 6G networks creates\nunprecedented opportunities for Intent-Based Networking (IBN) to dynamically\noptimize RAN[...]. However, applying IBN effectively to the RAN scheduler [...]\nremains a significant challenge. Current approaches predominantly rely on\ncoarse-grained network slicing, lacking the granularity for dynamic adaptation\nto individual user conditions and traffic patterns. Despite the existence of a\nvast body of scheduling algorithms [...], their practical utilization is\nhindered by implementation heterogeneity, insufficient systematic evaluation in\nproduction environments, and the complexity of developing high-performance\nscheduler implementations.[...] To address these limitations, we propose\nALLSTaR (Automated LLm-driven Scheduler generation and Testing for intent-based\nRAN), a novel framework leveraging LLMs for automated, intent-driven scheduler\ndesign, implementation, and evaluation. ALLSTaR interprets NL intents,\nautomatically generates functional scheduler code from the research literature\nusing OCR and LLMs, and intelligently matches operator intents to the most\nsuitable scheduler(s). Our implementation deploys these schedulers as O-RAN\ndApps, enabling on-the-fly deployment and testing on a production-grade,\n5G-compliant testbed. This approach has enabled the largest-scale OTA\nexperimental comparison of 18 scheduling algorithms automatically synthesized\nfrom the academic literature. The resulting performance profiles serve as the\ninput for our Intent-Based Scheduling (IBS) framework, which dynamically\nselects and deploys appropriate schedulers that optimally satisfy operator\nintents. We validate our approach through multiple use cases unattainable with\ncurrent slicing-based optimization techniques, demonstrating fine-grained\ncontrol based on buffer status, physical layer conditions, and heterogeneous\ntraffic types"
                },
                "authors": [
                    {
                        "name": "Maxime Elkael"
                    },
                    {
                        "name": "Michele Polese"
                    },
                    {
                        "name": "Reshma Prasad"
                    },
                    {
                        "name": "Stefano Maxenti"
                    },
                    {
                        "name": "Tommaso Melodia"
                    }
                ],
                "author_detail": {
                    "name": "Tommaso Melodia"
                },
                "author": "Tommaso Melodia",
                "arxiv_comment": "Under submission to an IEEE journal, copyright may change without\n  notice",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18389v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18389v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08386v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08386v2",
                "updated": "2025-10-14T14:32:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    14,
                    32,
                    33,
                    1,
                    287,
                    0
                ],
                "published": "2024-07-11T10:46:33Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    10,
                    46,
                    33,
                    3,
                    193,
                    0
                ],
                "title": "RIS-Assisted Millimeter Wave Communications for Indoor Scenarios:\n  Modeling and Coverage Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIS-Assisted Millimeter Wave Communications for Indoor Scenarios:\n  Modeling and Coverage Analysis"
                },
                "summary": "Millimeter wave (mmWave) communications and reconfigurable intelligent\nsurfaces (RIS) are two critical technologies for next-generation networks,\nespecially in dense indoor environments. However, existing analyses often\noversimplify the indoor environment by neglecting some of the key\ncharacteristics, such as height variations, boundary effects, blockage effects,\nand user spatial distributions. In this paper, we develop an improved\nstochastic geometry-based model for RIS-assisted mmWave communications in\nindoor scenarios like conference centers, hospitals, and shopping malls. The\nproposed model incorporates the height factor for all the nodes in the network\n(e.g., transmitters, users, RISs, and obstacles) and captures the user\nclustering behavior in these scenarios. In addition, the boundary effect is\nalso being considered for line-of-sight (LOS) probability calculation.\nAnalytical expressions for distance distributions, LOS probabilities, and the\ncoverage probability (CP) are derived. The CP is then validated through Monte\nCarlo simulations. Our results reveal deployment insights by approximating and\nsimplifying the derived CP expressions, showing how transmitter density,\nobstacle density, RIS density, and user cluster radius impact network coverage.\nNotably, we show that RISs significantly improve coverage when transmitters or\ntransmit power are limited but offer marginal benefits when transmitter density\nis high. These findings provide practical guidelines for the design and\ndeployment of RIS-assisted indoor mmWave networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Millimeter wave (mmWave) communications and reconfigurable intelligent\nsurfaces (RIS) are two critical technologies for next-generation networks,\nespecially in dense indoor environments. However, existing analyses often\noversimplify the indoor environment by neglecting some of the key\ncharacteristics, such as height variations, boundary effects, blockage effects,\nand user spatial distributions. In this paper, we develop an improved\nstochastic geometry-based model for RIS-assisted mmWave communications in\nindoor scenarios like conference centers, hospitals, and shopping malls. The\nproposed model incorporates the height factor for all the nodes in the network\n(e.g., transmitters, users, RISs, and obstacles) and captures the user\nclustering behavior in these scenarios. In addition, the boundary effect is\nalso being considered for line-of-sight (LOS) probability calculation.\nAnalytical expressions for distance distributions, LOS probabilities, and the\ncoverage probability (CP) are derived. The CP is then validated through Monte\nCarlo simulations. Our results reveal deployment insights by approximating and\nsimplifying the derived CP expressions, showing how transmitter density,\nobstacle density, RIS density, and user cluster radius impact network coverage.\nNotably, we show that RISs significantly improve coverage when transmitters or\ntransmit power are limited but offer marginal benefits when transmitter density\nis high. These findings provide practical guidelines for the design and\ndeployment of RIS-assisted indoor mmWave networks."
                },
                "authors": [
                    {
                        "name": "Zhi Chai"
                    },
                    {
                        "name": "Jiajie Xu"
                    },
                    {
                        "name": "Justin P. Coon"
                    },
                    {
                        "name": "Mohamed-Slim Alouini"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed-Slim Alouini"
                },
                "author": "Mohamed-Slim Alouini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08386v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08386v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08621v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08621v2",
                "updated": "2025-10-14T14:08:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    14,
                    8,
                    19,
                    1,
                    287,
                    0
                ],
                "published": "2025-04-11T15:25:50Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    25,
                    50,
                    4,
                    101,
                    0
                ],
                "title": "MooseAgent: A LLM Based Multi-agent Framework for Automating Moose\n  Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MooseAgent: A LLM Based Multi-agent Framework for Automating Moose\n  Simulation"
                },
                "summary": "The Finite Element Method (FEM) is widely used in engineering and scientific\ncomputing, but its pre-processing, solver configuration, and post-processing\nstages are often time-consuming and require specialized knowledge. This paper\nproposes an automated solution framework, MooseAgent, for the multi-physics\nsimulation framework MOOSE, which combines large-scale pre-trained language\nmodels (LLMs) with a multi-agent system. The framework uses LLMs to understand\nuser-described simulation requirements in natural language and employs task\ndecomposition and multi-round iterative verification strategies to\nautomatically generate MOOSE input files. To improve accuracy and reduce model\nhallucinations, the system builds and utilizes a vector database containing\nannotated MOOSE input cards and function documentation. We conducted\nexperimental evaluations on several typical cases, including heat transfer,\nmechanics, phase field, and multi-physics coupling. The results show that\nMooseAgent can automate the MOOSE simulation process to a certain extent,\nespecially demonstrating a high success rate when dealing with relatively\nsimple single-physics problems. The main contribution of this research is the\nproposal of a multi-agent automated framework for MOOSE, which validates its\npotential in simplifying finite element simulation processes and lowering the\nuser barrier, providing new ideas for the development of intelligent finite\nelement simulation software. The code for the MooseAgent framework proposed in\nthis paper has been open-sourced and is available at\nhttps://github.com/taozhan18/MooseAgent",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Finite Element Method (FEM) is widely used in engineering and scientific\ncomputing, but its pre-processing, solver configuration, and post-processing\nstages are often time-consuming and require specialized knowledge. This paper\nproposes an automated solution framework, MooseAgent, for the multi-physics\nsimulation framework MOOSE, which combines large-scale pre-trained language\nmodels (LLMs) with a multi-agent system. The framework uses LLMs to understand\nuser-described simulation requirements in natural language and employs task\ndecomposition and multi-round iterative verification strategies to\nautomatically generate MOOSE input files. To improve accuracy and reduce model\nhallucinations, the system builds and utilizes a vector database containing\nannotated MOOSE input cards and function documentation. We conducted\nexperimental evaluations on several typical cases, including heat transfer,\nmechanics, phase field, and multi-physics coupling. The results show that\nMooseAgent can automate the MOOSE simulation process to a certain extent,\nespecially demonstrating a high success rate when dealing with relatively\nsimple single-physics problems. The main contribution of this research is the\nproposal of a multi-agent automated framework for MOOSE, which validates its\npotential in simplifying finite element simulation processes and lowering the\nuser barrier, providing new ideas for the development of intelligent finite\nelement simulation software. The code for the MooseAgent framework proposed in\nthis paper has been open-sourced and is available at\nhttps://github.com/taozhan18/MooseAgent"
                },
                "authors": [
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Zhenhai Liu"
                    },
                    {
                        "name": "Yong Xin"
                    },
                    {
                        "name": "Yongjun Jiao"
                    }
                ],
                "author_detail": {
                    "name": "Yongjun Jiao"
                },
                "author": "Yongjun Jiao",
                "arxiv_comment": "11 pages, 3 Figs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08621v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08621v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10315v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10315v2",
                "updated": "2025-10-14T14:07:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    14,
                    7,
                    30,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-11T18:47:14Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    18,
                    47,
                    14,
                    5,
                    284,
                    0
                ],
                "title": "Is Misinformation More Open? A Study of robots.txt Gatekeeping on the\n  Web",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Misinformation More Open? A Study of robots.txt Gatekeeping on the\n  Web"
                },
                "summary": "Large Language Models (LLMs) are increasingly relying on web crawling to stay\nup to date and accurately answer user queries. These crawlers are expected to\nhonor robots.txt files, which govern automated access. In this study, for the\nfirst time, we investigate whether reputable news websites and misinformation\nsites differ in how they configure these files, particularly in relation to AI\ncrawlers. Analyzing a curated dataset, we find a stark contrast: 60.0% of\nreputable sites disallow at least one AI crawler, compared to just 9.1% of\nmisinformation sites in their robots.txt files. Reputable sites forbid an\naverage of 15.5 AI user agents, while misinformation sites prohibit fewer than\none. We then measure active blocking behavior, where websites refuse to return\ncontent when HTTP requests include AI crawler user agents, and reveal that both\ncategories of websites utilize it. Notably, the behavior of reputable news\nwebsites in this regard aligns more closely with their declared robots.txt\ndirective than that of misinformation websites. Finally, our longitudinal\nanalysis reveals that this gap has widened over time, with AI-blocking by\nreputable sites rising from 23% in September 2023 to nearly 60% by May 2025.\nOur findings highlight a growing asymmetry in content accessibility that may\nshape the training data available to LLMs, raising essential questions for web\ntransparency, data ethics, and the future of AI training practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly relying on web crawling to stay\nup to date and accurately answer user queries. These crawlers are expected to\nhonor robots.txt files, which govern automated access. In this study, for the\nfirst time, we investigate whether reputable news websites and misinformation\nsites differ in how they configure these files, particularly in relation to AI\ncrawlers. Analyzing a curated dataset, we find a stark contrast: 60.0% of\nreputable sites disallow at least one AI crawler, compared to just 9.1% of\nmisinformation sites in their robots.txt files. Reputable sites forbid an\naverage of 15.5 AI user agents, while misinformation sites prohibit fewer than\none. We then measure active blocking behavior, where websites refuse to return\ncontent when HTTP requests include AI crawler user agents, and reveal that both\ncategories of websites utilize it. Notably, the behavior of reputable news\nwebsites in this regard aligns more closely with their declared robots.txt\ndirective than that of misinformation websites. Finally, our longitudinal\nanalysis reveals that this gap has widened over time, with AI-blocking by\nreputable sites rising from 23% in September 2023 to nearly 60% by May 2025.\nOur findings highlight a growing asymmetry in content accessibility that may\nshape the training data available to LLMs, raising essential questions for web\ntransparency, data ethics, and the future of AI training practices."
                },
                "authors": [
                    {
                        "name": "Nicolas Steinacker-Olsztyn"
                    },
                    {
                        "name": "Devashish Gosain"
                    },
                    {
                        "name": "Ha Dao"
                    }
                ],
                "author_detail": {
                    "name": "Ha Dao"
                },
                "author": "Ha Dao",
                "arxiv_comment": "10 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10315v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10315v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.10349v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.10349v4",
                "updated": "2025-10-14T14:07:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    14,
                    7,
                    18,
                    1,
                    287,
                    0
                ],
                "published": "2023-10-16T12:34:47Z",
                "published_parsed": [
                    2023,
                    10,
                    16,
                    12,
                    34,
                    47,
                    0,
                    289,
                    0
                ],
                "title": "Optimized Layerwise Approximation for Efficient Private Inference on\n  Fully Homomorphic Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimized Layerwise Approximation for Efficient Private Inference on\n  Fully Homomorphic Encryption"
                },
                "summary": "Recent studies have explored the deployment of privacy-preserving deep neural\nnetworks utilizing homomorphic encryption (HE), especially for private\ninference (PI). Many works have attempted the approximation-aware training\n(AAT) approach in PI, changing the activation functions of a model to\nlow-degree polynomials that are easier to compute on HE by allowing model\nretraining. However, due to constraints in the training environment, it is\noften necessary to consider post-training approximation (PTA), using the\npre-trained parameters of the existing plaintext model without retraining.\nExisting PTA studies have uniformly approximated the activation function in all\nlayers to a high degree to mitigate accuracy loss from approximation, leading\nto significant time consumption. This study proposes an optimized layerwise\napproximation (OLA), a systematic framework that optimizes both accuracy loss\nand time consumption by using different approximation polynomials for each\nlayer in the PTA scenario. For efficient approximation, we reflect the\nlayerwise impact on the classification accuracy by considering the actual input\ndistribution of each activation function while constructing the optimization\nproblem. Additionally, we provide a dynamic programming technique to solve the\noptimization problem and achieve the optimized layerwise degrees in polynomial\ntime. As a result, the OLA method reduces inference times for the ResNet-20\nmodel and the ResNet-32 model by 3.02 times and 2.82 times, respectively,\ncompared to prior state-of-the-art implementations employing uniform degree\npolynomials. Furthermore, we successfully classified CIFAR-10 by replacing the\nGELU function in the ConvNeXt model with only 3-degree polynomials using the\nproposed method, without modifying the backbone model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have explored the deployment of privacy-preserving deep neural\nnetworks utilizing homomorphic encryption (HE), especially for private\ninference (PI). Many works have attempted the approximation-aware training\n(AAT) approach in PI, changing the activation functions of a model to\nlow-degree polynomials that are easier to compute on HE by allowing model\nretraining. However, due to constraints in the training environment, it is\noften necessary to consider post-training approximation (PTA), using the\npre-trained parameters of the existing plaintext model without retraining.\nExisting PTA studies have uniformly approximated the activation function in all\nlayers to a high degree to mitigate accuracy loss from approximation, leading\nto significant time consumption. This study proposes an optimized layerwise\napproximation (OLA), a systematic framework that optimizes both accuracy loss\nand time consumption by using different approximation polynomials for each\nlayer in the PTA scenario. For efficient approximation, we reflect the\nlayerwise impact on the classification accuracy by considering the actual input\ndistribution of each activation function while constructing the optimization\nproblem. Additionally, we provide a dynamic programming technique to solve the\noptimization problem and achieve the optimized layerwise degrees in polynomial\ntime. As a result, the OLA method reduces inference times for the ResNet-20\nmodel and the ResNet-32 model by 3.02 times and 2.82 times, respectively,\ncompared to prior state-of-the-art implementations employing uniform degree\npolynomials. Furthermore, we successfully classified CIFAR-10 by replacing the\nGELU function in the ConvNeXt model with only 3-degree polynomials using the\nproposed method, without modifying the backbone model."
                },
                "authors": [
                    {
                        "name": "Junghyun Lee"
                    },
                    {
                        "name": "Eunsang Lee"
                    },
                    {
                        "name": "Young-Sik Kim"
                    },
                    {
                        "name": "Yongwoo Lee"
                    },
                    {
                        "name": "Joon-Woo Lee"
                    },
                    {
                        "name": "Yongjune Kim"
                    },
                    {
                        "name": "Jong-Seon No"
                    }
                ],
                "author_detail": {
                    "name": "Jong-Seon No"
                },
                "author": "Jong-Seon No",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.10349v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.10349v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12542v1",
                "updated": "2025-10-14T14:05:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    14,
                    5,
                    48,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T14:05:48Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    14,
                    5,
                    48,
                    1,
                    287,
                    0
                ],
                "title": "Monitoring of Fluid Transport in Low Temperature Water Electrolyzers and\n  Fuel Cells: Emerging Technologies and Future Prospects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monitoring of Fluid Transport in Low Temperature Water Electrolyzers and\n  Fuel Cells: Emerging Technologies and Future Prospects"
                },
                "summary": "Low temperature water electrolyzers (LTWEs) and low temperature hydrogen fuel\ncells (LTFCs) present a promising technological strategy for the productions\nand usages of green hydrogen energy towards a net-zero world. However, the\ninteractions of gas/liquid (fluid) transport and the intrinsic reaction\nkinetics in LTWEs/LTFCs present one of the key hurdles hindering high\nproduction rate and high energy conversion efficiency. Addressing these\nlimitations requires analytical tools that are capable of resolving fluid\ntransport across the heterogeneous, multiscale structures of operating LTWE and\nLTFC systems. This review provides a comprehensive overview of recent\nadvancements in measurement technologies for investigating fluid transport. We\nfirst outline the technical requirements of such analytical systems, and assess\nthe capabilities and limitations of established optical, X-rays and neutrons\nbased imaging systems. We emphasis on emerging strategies that utilize\nintegrated miniaturized sensors, ultrasound, and other alternative physical\nprinciples to achieve operando, high-resolution, and scalable measurements\ntowards applications at device and system levels. Finally, we outline future\ndirections in this highly interdisciplinary field, emphasizing the importance\nof next-generation sensing concepts to overcome the fluid transport hurdle,\ntowards accelerating the deployment of green hydrogen technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low temperature water electrolyzers (LTWEs) and low temperature hydrogen fuel\ncells (LTFCs) present a promising technological strategy for the productions\nand usages of green hydrogen energy towards a net-zero world. However, the\ninteractions of gas/liquid (fluid) transport and the intrinsic reaction\nkinetics in LTWEs/LTFCs present one of the key hurdles hindering high\nproduction rate and high energy conversion efficiency. Addressing these\nlimitations requires analytical tools that are capable of resolving fluid\ntransport across the heterogeneous, multiscale structures of operating LTWE and\nLTFC systems. This review provides a comprehensive overview of recent\nadvancements in measurement technologies for investigating fluid transport. We\nfirst outline the technical requirements of such analytical systems, and assess\nthe capabilities and limitations of established optical, X-rays and neutrons\nbased imaging systems. We emphasis on emerging strategies that utilize\nintegrated miniaturized sensors, ultrasound, and other alternative physical\nprinciples to achieve operando, high-resolution, and scalable measurements\ntowards applications at device and system levels. Finally, we outline future\ndirections in this highly interdisciplinary field, emphasizing the importance\nof next-generation sensing concepts to overcome the fluid transport hurdle,\ntowards accelerating the deployment of green hydrogen technologies."
                },
                "authors": [
                    {
                        "name": "Zehua Dou"
                    },
                    {
                        "name": "Laura Tropf"
                    },
                    {
                        "name": "Tobias Lappan"
                    },
                    {
                        "name": "Hannes Rox"
                    },
                    {
                        "name": "Xuegeng Yang"
                    },
                    {
                        "name": "Lars Buettner"
                    },
                    {
                        "name": "David Weik"
                    },
                    {
                        "name": "Harry Hoster"
                    },
                    {
                        "name": "Kerstin Eckert"
                    },
                    {
                        "name": "Juergen Czarske"
                    }
                ],
                "author_detail": {
                    "name": "Juergen Czarske"
                },
                "author": "Juergen Czarske",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09043v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09043v2",
                "updated": "2025-10-14T13:58:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    58,
                    28,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-10T06:23:50Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    6,
                    23,
                    50,
                    4,
                    283,
                    0
                ],
                "title": "Humanoid Artificial Consciousness Designed with Large Language Model\n  Based on Psychoanalysis and Personality Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humanoid Artificial Consciousness Designed with Large Language Model\n  Based on Psychoanalysis and Personality Theory"
                },
                "summary": "Human consciousness is still a concept hard to define with current scientific\nunderstanding. Although Large Language Models (LLMs) have recently demonstrated\nsignificant advancements across various domains including translation and\nsummarization, human consciousness is not something to imitate with current\nupfront technology owing to so-called hallucination. This study, therefore,\nproposes a novel approach to address these challenges by integrating\npsychoanalysis and the Myers-Briggs Type Indicator (MBTI) into constructing\nconsciousness and personality modules. We developed three artificial\nconsciousnesses (self-awareness, unconsciousness, and preconsciousness) based\non the principles of psychoanalysis. Additionally, we designed 16 characters\nwith different personalities representing the sixteen MBTI types, with several\nattributes such as needs, status, and memories. To determine if our model's\nartificial consciousness exhibits human-like cognition, we created ten distinct\nsituations considering seven attributes such as emotional understanding and\nlogical thinking. The decision-making process of artificial consciousness and\nthe final action were evaluated in three ways: survey evaluation, three-tier\nclassification via ChatGPT, and qualitative review. Both quantitative and\nqualitative analyses indicated a high likelihood of well-simulated\nconsciousness, although the difference in response between different characters\nand consciousnesses was not very significant. This implies that the developed\nmodels incorporating elements of psychoanalysis and personality theory can lead\nto building a more intuitive and adaptable AI system with humanoid\nconsciousness. Therefore, this study contributes to opening up new avenues for\nimproving AI interactions in complex cognitive contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human consciousness is still a concept hard to define with current scientific\nunderstanding. Although Large Language Models (LLMs) have recently demonstrated\nsignificant advancements across various domains including translation and\nsummarization, human consciousness is not something to imitate with current\nupfront technology owing to so-called hallucination. This study, therefore,\nproposes a novel approach to address these challenges by integrating\npsychoanalysis and the Myers-Briggs Type Indicator (MBTI) into constructing\nconsciousness and personality modules. We developed three artificial\nconsciousnesses (self-awareness, unconsciousness, and preconsciousness) based\non the principles of psychoanalysis. Additionally, we designed 16 characters\nwith different personalities representing the sixteen MBTI types, with several\nattributes such as needs, status, and memories. To determine if our model's\nartificial consciousness exhibits human-like cognition, we created ten distinct\nsituations considering seven attributes such as emotional understanding and\nlogical thinking. The decision-making process of artificial consciousness and\nthe final action were evaluated in three ways: survey evaluation, three-tier\nclassification via ChatGPT, and qualitative review. Both quantitative and\nqualitative analyses indicated a high likelihood of well-simulated\nconsciousness, although the difference in response between different characters\nand consciousnesses was not very significant. This implies that the developed\nmodels incorporating elements of psychoanalysis and personality theory can lead\nto building a more intuitive and adaptable AI system with humanoid\nconsciousness. Therefore, this study contributes to opening up new avenues for\nimproving AI interactions in complex cognitive contexts."
                },
                "authors": [
                    {
                        "name": "Sang Hun Kim"
                    },
                    {
                        "name": "Jongmin Lee"
                    },
                    {
                        "name": "Dongkyu Park"
                    },
                    {
                        "name": "So Young Lee"
                    },
                    {
                        "name": "Yosep Chong"
                    }
                ],
                "author_detail": {
                    "name": "Yosep Chong"
                },
                "author": "Yosep Chong",
                "arxiv_doi": "10.1016/j.cogsys.2025.101392",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.cogsys.2025.101392",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.09043v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09043v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "41 pages, 6 figures. Accepted and published to Cognitive Systems\n  Research, 2025",
                "arxiv_journal_ref": "Cognitive Systems Research Volume 94, December 2025, 101392",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19700v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19700v2",
                "updated": "2025-10-14T13:57:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    57,
                    53,
                    1,
                    287,
                    0
                ],
                "published": "2025-05-26T08:53:02Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    8,
                    53,
                    2,
                    0,
                    146,
                    0
                ],
                "title": "Leveraging Importance Sampling to Detach Alignment Modules from Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Importance Sampling to Detach Alignment Modules from Large\n  Language Models"
                },
                "summary": "The widespread adoption of large language models (LLMs) across industries has\nincreased the demand for high-quality and customizable outputs. However,\ntraditional alignment methods often require retraining large pretrained models,\nmaking it difficult to quickly adapt and optimize LLMs for diverse\napplications. To address this limitation, we propose a novel \\textit{Residual\nAlignment Model} (\\textit{RAM}) that formalizes the alignment process as a type\nof importance sampling. In this framework, the unaligned upstream model serves\nas the proposal distribution, while the alignment process is framed as\nsecondary sampling based on an autoregressive alignment module that acts as an\nestimator of the importance weights. This design enables a natural detachment\nof the alignment module from the target aligned model, improving flexibility\nand scalability. Based on this model, we derive an efficient sequence-level\ntraining strategy for the alignment module, which operates independently of the\nproposal module. Additionally, we develop a resampling algorithm with iterative\ntoken-level decoding to address the common first-token latency issue in\ncomparable methods. Experimental evaluations on two leading open-source LLMs\nacross diverse tasks, including instruction following, domain adaptation, and\npreference optimization, demonstrate that our approach consistently outperforms\nbaseline models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of large language models (LLMs) across industries has\nincreased the demand for high-quality and customizable outputs. However,\ntraditional alignment methods often require retraining large pretrained models,\nmaking it difficult to quickly adapt and optimize LLMs for diverse\napplications. To address this limitation, we propose a novel \\textit{Residual\nAlignment Model} (\\textit{RAM}) that formalizes the alignment process as a type\nof importance sampling. In this framework, the unaligned upstream model serves\nas the proposal distribution, while the alignment process is framed as\nsecondary sampling based on an autoregressive alignment module that acts as an\nestimator of the importance weights. This design enables a natural detachment\nof the alignment module from the target aligned model, improving flexibility\nand scalability. Based on this model, we derive an efficient sequence-level\ntraining strategy for the alignment module, which operates independently of the\nproposal module. Additionally, we develop a resampling algorithm with iterative\ntoken-level decoding to address the common first-token latency issue in\ncomparable methods. Experimental evaluations on two leading open-source LLMs\nacross diverse tasks, including instruction following, domain adaptation, and\npreference optimization, demonstrate that our approach consistently outperforms\nbaseline models."
                },
                "authors": [
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Dianqing Liu"
                    },
                    {
                        "name": "Mingye Zhu"
                    },
                    {
                        "name": "Junbo Guo"
                    },
                    {
                        "name": "Yongdong Zhang"
                    },
                    {
                        "name": "Zhendong Mao"
                    }
                ],
                "author_detail": {
                    "name": "Zhendong Mao"
                },
                "author": "Zhendong Mao",
                "arxiv_comment": "Accepted by NeurIPS 2025, 28 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19700v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19700v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10446v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10446v2",
                "updated": "2025-10-14T13:54:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    54,
                    24,
                    1,
                    287,
                    0
                ],
                "published": "2025-09-12T17:52:35Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    17,
                    52,
                    35,
                    4,
                    255,
                    0
                ],
                "title": "DeepDive: Advancing Deep Search Agents with Knowledge Graphs and\n  Multi-Turn RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepDive: Advancing Deep Search Agents with Knowledge Graphs and\n  Multi-Turn RL"
                },
                "summary": "Augmenting large language models (LLMs) with browsing tools substantially\nimproves their potential as deep search agents to solve complex, real-world\ntasks. Yet, open LLMs still perform poorly in such settings due to limited\nlong-horizon reasoning capacity with browsing tools and the lack of\nsufficiently difficult supervised data. To address these challenges, we present\nDeepDive to advance deep search agents. First, we propose a strategy to\nautomatically synthesize complex, difficult, and hard-to-find questions from\nopen knowledge graphs. Second, we apply end-to-end multi-turn reinforcement\nlearning (RL) to enhance LLMs' long-horizon reasoning with deep search. To\nencourage diversity and reduce redundancy, we design a redundancy penalty that\ndiscourages repeated similar queries. Experiments show that DeepDive-32B\nachieves a new open-source competitive result on BrowseComp, outperforming\nWebSailor, DeepSeek-R1-Browse, and Search-o1. We demonstrate that multi-turn RL\ntraining improves deep search ability and significantly contributes to the\nperformance improvements across multiple benchmarks. We observe that DeepDive\nenables test-time scaling of tool calls and parallel sampling. All datasets,\nmodels, and code are publicly available at https://github.com/THUDM/DeepDive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmenting large language models (LLMs) with browsing tools substantially\nimproves their potential as deep search agents to solve complex, real-world\ntasks. Yet, open LLMs still perform poorly in such settings due to limited\nlong-horizon reasoning capacity with browsing tools and the lack of\nsufficiently difficult supervised data. To address these challenges, we present\nDeepDive to advance deep search agents. First, we propose a strategy to\nautomatically synthesize complex, difficult, and hard-to-find questions from\nopen knowledge graphs. Second, we apply end-to-end multi-turn reinforcement\nlearning (RL) to enhance LLMs' long-horizon reasoning with deep search. To\nencourage diversity and reduce redundancy, we design a redundancy penalty that\ndiscourages repeated similar queries. Experiments show that DeepDive-32B\nachieves a new open-source competitive result on BrowseComp, outperforming\nWebSailor, DeepSeek-R1-Browse, and Search-o1. We demonstrate that multi-turn RL\ntraining improves deep search ability and significantly contributes to the\nperformance improvements across multiple benchmarks. We observe that DeepDive\nenables test-time scaling of tool calls and parallel sampling. All datasets,\nmodels, and code are publicly available at https://github.com/THUDM/DeepDive."
                },
                "authors": [
                    {
                        "name": "Rui Lu"
                    },
                    {
                        "name": "Zhenyu Hou"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Hanchen Zhang"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Yujiang Li"
                    },
                    {
                        "name": "Shi Feng"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Yuxiao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiao Dong"
                },
                "author": "Yuxiao Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10446v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10446v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12516v1",
                "updated": "2025-10-14T13:43:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    43,
                    8,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T13:43:08Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    43,
                    8,
                    1,
                    287,
                    0
                ],
                "title": "BoN Appetit Team at LeWiDi-2025: Best-of-N Test-time Scaling Can Not\n  Stomach Annotation Disagreements (Yet)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BoN Appetit Team at LeWiDi-2025: Best-of-N Test-time Scaling Can Not\n  Stomach Annotation Disagreements (Yet)"
                },
                "summary": "Test-time scaling is a family of techniques to improve LLM outputs at\ninference time by performing extra computation. To the best of our knowledge,\ntest-time scaling has been limited to domains with verifiably correct answers,\nlike mathematics and coding. We transfer test-time scaling to the LeWiDi-2025\ntasks to evaluate annotation disagreements. We experiment with three test-time\nscaling methods: two benchmark algorithms (Model Averaging and Majority\nVoting), and a Best-of-N sampling method. The two benchmark methods improve LLM\nperformance consistently on the LeWiDi tasks, but the Best-of-N method does\nnot. Our experiments suggest that the Best-of-N method does not currently\ntransfer from mathematics to LeWiDi tasks, and we analyze potential reasons for\nthis gap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling is a family of techniques to improve LLM outputs at\ninference time by performing extra computation. To the best of our knowledge,\ntest-time scaling has been limited to domains with verifiably correct answers,\nlike mathematics and coding. We transfer test-time scaling to the LeWiDi-2025\ntasks to evaluate annotation disagreements. We experiment with three test-time\nscaling methods: two benchmark algorithms (Model Averaging and Majority\nVoting), and a Best-of-N sampling method. The two benchmark methods improve LLM\nperformance consistently on the LeWiDi tasks, but the Best-of-N method does\nnot. Our experiments suggest that the Best-of-N method does not currently\ntransfer from mathematics to LeWiDi tasks, and we analyze potential reasons for\nthis gap."
                },
                "authors": [
                    {
                        "name": "Tomas Ruiz"
                    },
                    {
                        "name": "Siyao Peng"
                    },
                    {
                        "name": "Barbara Plank"
                    },
                    {
                        "name": "Carsten Schwemmer"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Schwemmer"
                },
                "author": "Carsten Schwemmer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22058v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22058v2",
                "updated": "2025-10-14T13:32:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    32,
                    12,
                    1,
                    287,
                    0
                ],
                "published": "2025-06-27T09:53:57Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    53,
                    57,
                    4,
                    178,
                    0
                ],
                "title": "Lost at the Beginning of Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lost at the Beginning of Reasoning"
                },
                "summary": "Recent advancements in large language models (LLMs) have significantly\nadvanced complex reasoning capabilities, particularly through extended\nchain-of-thought (CoT) reasoning that incorporates mechanisms such as\nbacktracking, self-reflection, and self-correction. Despite these developments,\nthe self-correction abilities of LLMs during long CoT reasoning remain\nunderexplored. And recent findings on overthinking suggest that such models\noften engage in unnecessarily redundant reasoning. In this work, we empirically\nshow that the first reasoning step exerts a disproportionately large influence\non the final prediction. I.e., errors introduced at this stage can\nsubstantially degrade subsequent reasoning quality. This phenomenon is\nconsistently observed across various state-of-the-art open- and closed-source\nreasoning models. Leveraging this insight, we propose an efficient sampling\nstrategy that leverages a reward model to identify and retain high-quality\nfirst reasoning steps while discarding suboptimal ones, achieving up to a 70%\nreduction in inference cost without sacrificing any accuracy. Our work\nhighlights the central role of the first reasoning step in generating a\nhigh-quality reasoning trajectory, and thus enabling significantly efficient\nsampling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have significantly\nadvanced complex reasoning capabilities, particularly through extended\nchain-of-thought (CoT) reasoning that incorporates mechanisms such as\nbacktracking, self-reflection, and self-correction. Despite these developments,\nthe self-correction abilities of LLMs during long CoT reasoning remain\nunderexplored. And recent findings on overthinking suggest that such models\noften engage in unnecessarily redundant reasoning. In this work, we empirically\nshow that the first reasoning step exerts a disproportionately large influence\non the final prediction. I.e., errors introduced at this stage can\nsubstantially degrade subsequent reasoning quality. This phenomenon is\nconsistently observed across various state-of-the-art open- and closed-source\nreasoning models. Leveraging this insight, we propose an efficient sampling\nstrategy that leverages a reward model to identify and retain high-quality\nfirst reasoning steps while discarding suboptimal ones, achieving up to a 70%\nreduction in inference cost without sacrificing any accuracy. Our work\nhighlights the central role of the first reasoning step in generating a\nhigh-quality reasoning trajectory, and thus enabling significantly efficient\nsampling."
                },
                "authors": [
                    {
                        "name": "Baohao Liao"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Sara Rajaee"
                    },
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Christian Herold"
                    },
                    {
                        "name": "Anders Sgaard"
                    },
                    {
                        "name": "Maarten de Rijke"
                    },
                    {
                        "name": "Christof Monz"
                    }
                ],
                "author_detail": {
                    "name": "Christof Monz"
                },
                "author": "Christof Monz",
                "arxiv_comment": "remove the benchmark part. (10 pages, 6 figures, 5 tables)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22058v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22058v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12490v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12490v1",
                "updated": "2025-10-14T13:24:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    24,
                    21,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T13:24:21Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    24,
                    21,
                    1,
                    287,
                    0
                ],
                "title": "Using Medical Algorithms for Task-Oriented Dialogue in LLM-Based Medical\n  Interviews",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Medical Algorithms for Task-Oriented Dialogue in LLM-Based Medical\n  Interviews"
                },
                "summary": "We developed a task-oriented dialogue framework structured as a Directed\nAcyclic Graph (DAG) of medical questions. The system integrates: (1) a\nsystematic pipeline for transforming medical algorithms and guidelines into a\nclinical question corpus; (2) a cold-start mechanism based on hierarchical\nclustering to generate efficient initial questioning without prior patient\ninformation; (3) an expand-and-prune mechanism enabling adaptive branching and\nbacktracking based on patient responses; (4) a termination logic to ensure\ninterviews end once sufficient information is gathered; and (5) automated\nsynthesis of doctor-friendly structured reports aligned with clinical\nworkflows. Human-computer interaction principles guided the design of both the\npatient and physician applications. Preliminary evaluation involved five\nphysicians using standardized instruments: NASA-TLX (cognitive workload), the\nSystem Usability Scale (SUS), and the Questionnaire for User Interface\nSatisfaction (QUIS). The patient application achieved low workload scores\n(NASA-TLX = 15.6), high usability (SUS = 86), and strong satisfaction (QUIS =\n8.1/9), with particularly high ratings for ease of learning and interface\ndesign. The physician application yielded moderate workload (NASA-TLX = 26) and\nexcellent usability (SUS = 88.5), with satisfaction scores of 8.3/9. Both\napplications demonstrated effective integration into clinical workflows,\nreducing cognitive demand and supporting efficient report generation.\nLimitations included occasional system latency and a small, non-diverse\nevaluation sample.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We developed a task-oriented dialogue framework structured as a Directed\nAcyclic Graph (DAG) of medical questions. The system integrates: (1) a\nsystematic pipeline for transforming medical algorithms and guidelines into a\nclinical question corpus; (2) a cold-start mechanism based on hierarchical\nclustering to generate efficient initial questioning without prior patient\ninformation; (3) an expand-and-prune mechanism enabling adaptive branching and\nbacktracking based on patient responses; (4) a termination logic to ensure\ninterviews end once sufficient information is gathered; and (5) automated\nsynthesis of doctor-friendly structured reports aligned with clinical\nworkflows. Human-computer interaction principles guided the design of both the\npatient and physician applications. Preliminary evaluation involved five\nphysicians using standardized instruments: NASA-TLX (cognitive workload), the\nSystem Usability Scale (SUS), and the Questionnaire for User Interface\nSatisfaction (QUIS). The patient application achieved low workload scores\n(NASA-TLX = 15.6), high usability (SUS = 86), and strong satisfaction (QUIS =\n8.1/9), with particularly high ratings for ease of learning and interface\ndesign. The physician application yielded moderate workload (NASA-TLX = 26) and\nexcellent usability (SUS = 88.5), with satisfaction scores of 8.3/9. Both\napplications demonstrated effective integration into clinical workflows,\nreducing cognitive demand and supporting efficient report generation.\nLimitations included occasional system latency and a small, non-diverse\nevaluation sample."
                },
                "authors": [
                    {
                        "name": "Rui Reis"
                    },
                    {
                        "name": "Pedro Rangel Henriques"
                    },
                    {
                        "name": "Joo Ferreira-Coimbra"
                    },
                    {
                        "name": "Eva Oliveira"
                    },
                    {
                        "name": "Nuno F. Rodrigues"
                    }
                ],
                "author_detail": {
                    "name": "Nuno F. Rodrigues"
                },
                "author": "Nuno F. Rodrigues",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12490v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12490v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12487v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12487v1",
                "updated": "2025-10-14T13:23:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    23,
                    1,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T13:23:01Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    23,
                    1,
                    1,
                    287,
                    0
                ],
                "title": "Diff-XYZ: A Benchmark for Evaluating Diff Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diff-XYZ: A Benchmark for Evaluating Diff Understanding"
                },
                "summary": "Reliable handling of code diffs is central to agents that edit and refactor\nrepositories at scale. We introduce Diff-XYZ, a compact benchmark for code-diff\nunderstanding with three supervised tasks: apply (old code $+$ diff\n$\\rightarrow$ new code), anti-apply (new code $-$ diff $\\rightarrow$ old code),\nand diff generation (new code $-$ old code $\\rightarrow$ diff). Instances in\nthe benchmark are triples $\\langle \\textit{old code}, \\textit{new code},\n\\textit{diff} \\rangle$ drawn from real commits in CommitPackFT, paired with\nautomatic metrics and a clear evaluation protocol. We use the benchmark to do a\nfocused empirical study of the unified diff format and run a cross-format\ncomparison of different diff representations. Our findings reveal that\ndifferent formats should be used depending on the use case and model size. For\nexample, representing diffs in search-replace format is good for larger models\nin the diff generation scenario, yet not suited well for diff analysis and\nsmaller models. The Diff-XYZ benchmark is a reusable foundation for assessing\nand improving diff handling in LLMs that can aid future development of diff\nformats and models editing code. The dataset is published on HuggingFace Hub:\nhttps://huggingface.co/datasets/JetBrains-Research/diff-xyz.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable handling of code diffs is central to agents that edit and refactor\nrepositories at scale. We introduce Diff-XYZ, a compact benchmark for code-diff\nunderstanding with three supervised tasks: apply (old code $+$ diff\n$\\rightarrow$ new code), anti-apply (new code $-$ diff $\\rightarrow$ old code),\nand diff generation (new code $-$ old code $\\rightarrow$ diff). Instances in\nthe benchmark are triples $\\langle \\textit{old code}, \\textit{new code},\n\\textit{diff} \\rangle$ drawn from real commits in CommitPackFT, paired with\nautomatic metrics and a clear evaluation protocol. We use the benchmark to do a\nfocused empirical study of the unified diff format and run a cross-format\ncomparison of different diff representations. Our findings reveal that\ndifferent formats should be used depending on the use case and model size. For\nexample, representing diffs in search-replace format is good for larger models\nin the diff generation scenario, yet not suited well for diff analysis and\nsmaller models. The Diff-XYZ benchmark is a reusable foundation for assessing\nand improving diff handling in LLMs that can aid future development of diff\nformats and models editing code. The dataset is published on HuggingFace Hub:\nhttps://huggingface.co/datasets/JetBrains-Research/diff-xyz."
                },
                "authors": [
                    {
                        "name": "Evgeniy Glukhov"
                    },
                    {
                        "name": "Michele Conti"
                    },
                    {
                        "name": "Egor Bogomolov"
                    },
                    {
                        "name": "Yaroslav Golubev"
                    },
                    {
                        "name": "Alexander Bezzubov"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Bezzubov"
                },
                "author": "Alexander Bezzubov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12487v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12487v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02038v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02038v2",
                "updated": "2025-10-14T13:16:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    16,
                    57,
                    1,
                    287,
                    0
                ],
                "published": "2025-03-03T20:30:22Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    20,
                    30,
                    22,
                    0,
                    62,
                    0
                ],
                "title": "Persuasion at Play: Understanding Misinformation Dynamics in\n  Demographic-Aware Human-LLM Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persuasion at Play: Understanding Misinformation Dynamics in\n  Demographic-Aware Human-LLM Interactions"
                },
                "summary": "Existing challenges in misinformation exposure and susceptibility vary across\ndemographic groups, as some populations are more vulnerable to misinformation\nthan others. Large language models (LLMs) introduce new dimensions to these\nchallenges through their ability to generate persuasive content at scale and\nreinforcing existing biases. This study investigates the bidirectional\npersuasion dynamics between LLMs and humans when exposed to misinformative\ncontent. We analyze human-to-LLM influence using human-stance datasets and\nassess LLM-to-human influence by generating LLM-based persuasive arguments.\nAdditionally, we use a multi-agent LLM framework to analyze the spread of\nmisinformation under persuasion among demographic-oriented LLM agents. Our\nfindings show that demographic factors influence susceptibility to\nmisinformation in LLMs, closely reflecting the demographic-based patterns seen\nin human susceptibility. We also find that, similar to human demographic\ngroups, multi-agent LLMs exhibit echo chamber behavior. This research explores\nthe interplay between humans and LLMs, highlighting demographic differences in\nthe context of misinformation and offering insights for future interventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing challenges in misinformation exposure and susceptibility vary across\ndemographic groups, as some populations are more vulnerable to misinformation\nthan others. Large language models (LLMs) introduce new dimensions to these\nchallenges through their ability to generate persuasive content at scale and\nreinforcing existing biases. This study investigates the bidirectional\npersuasion dynamics between LLMs and humans when exposed to misinformative\ncontent. We analyze human-to-LLM influence using human-stance datasets and\nassess LLM-to-human influence by generating LLM-based persuasive arguments.\nAdditionally, we use a multi-agent LLM framework to analyze the spread of\nmisinformation under persuasion among demographic-oriented LLM agents. Our\nfindings show that demographic factors influence susceptibility to\nmisinformation in LLMs, closely reflecting the demographic-based patterns seen\nin human susceptibility. We also find that, similar to human demographic\ngroups, multi-agent LLMs exhibit echo chamber behavior. This research explores\nthe interplay between humans and LLMs, highlighting demographic differences in\nthe context of misinformation and offering insights for future interventions."
                },
                "authors": [
                    {
                        "name": "Angana Borah"
                    },
                    {
                        "name": "Rada Mihalcea"
                    },
                    {
                        "name": "Vernica Prez-Rosas"
                    }
                ],
                "author_detail": {
                    "name": "Vernica Prez-Rosas"
                },
                "author": "Vernica Prez-Rosas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02038v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02038v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12434v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12434v4",
                "updated": "2025-10-14T13:13:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    13,
                    18,
                    1,
                    287,
                    0
                ],
                "published": "2025-05-18T14:14:35Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    14,
                    14,
                    35,
                    6,
                    138,
                    0
                ],
                "title": "VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via\n  Reinforced Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via\n  Reinforced Fine-Tuning"
                },
                "summary": "Reinforcement fine-tuning (RFT) has shown great promise in achieving\nhumanlevel reasoning capabilities of Large Language Models (LLMs), and has\nrecently been extended to MLLMs. Nevertheless, reasoning about videos, which is\na fundamental aspect of human intelligence, remains a persistent challenge due\nto the complex logic, temporal and causal structures inherent in video data. To\nfill this gap, we propose VideoRFT, a novel approach that extends the RFT\nparadigm to cultivate human-like video reasoning capabilities in MLLMs.\nVideoRFT follows the standard two-stage scheme in RFT: supervised fine-tuning\n(SFT) with chain-of-thought (CoT) annotations, followed by reinforcement\nlearning (RL) to improve generalization. A central challenge to achieve this in\nthe video domain lies in the scarcity of large-scale, high-quality video CoT\ndatasets. We address this by building a multi-expert-driven, cognition-inspired\nCoT curation pipeline. First, we devise a cognition-inspired prompting strategy\nto elicit a reasoning LLM to generate preliminary CoTs based solely on rich,\nstructured, and literal representations of video content. Subsequently, these\nCoTs are revised by a MLLM conditioned on the actual video, ensuring visual\nconsistency and reducing visual hallucinations. This pipeline results in two\nnew datasets, i.e.VideoRFT-CoT-102K for SFT and VideoRFT-RL-310K for RL. To\nfurther strengthen the RL phase, we introduce a novel semantic-consistency\nreward that explicitly promotes the alignment between textual reasoning and\nvisual evidence. This reward encourages the model to produce coherent,\ncontext-aware reasoning outputs grounded in visual input. Extensive experiments\nshow that VideoRFT achieves state-of-the-art performance on six video reasoning\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement fine-tuning (RFT) has shown great promise in achieving\nhumanlevel reasoning capabilities of Large Language Models (LLMs), and has\nrecently been extended to MLLMs. Nevertheless, reasoning about videos, which is\na fundamental aspect of human intelligence, remains a persistent challenge due\nto the complex logic, temporal and causal structures inherent in video data. To\nfill this gap, we propose VideoRFT, a novel approach that extends the RFT\nparadigm to cultivate human-like video reasoning capabilities in MLLMs.\nVideoRFT follows the standard two-stage scheme in RFT: supervised fine-tuning\n(SFT) with chain-of-thought (CoT) annotations, followed by reinforcement\nlearning (RL) to improve generalization. A central challenge to achieve this in\nthe video domain lies in the scarcity of large-scale, high-quality video CoT\ndatasets. We address this by building a multi-expert-driven, cognition-inspired\nCoT curation pipeline. First, we devise a cognition-inspired prompting strategy\nto elicit a reasoning LLM to generate preliminary CoTs based solely on rich,\nstructured, and literal representations of video content. Subsequently, these\nCoTs are revised by a MLLM conditioned on the actual video, ensuring visual\nconsistency and reducing visual hallucinations. This pipeline results in two\nnew datasets, i.e.VideoRFT-CoT-102K for SFT and VideoRFT-RL-310K for RL. To\nfurther strengthen the RL phase, we introduce a novel semantic-consistency\nreward that explicitly promotes the alignment between textual reasoning and\nvisual evidence. This reward encourages the model to produce coherent,\ncontext-aware reasoning outputs grounded in visual input. Extensive experiments\nshow that VideoRFT achieves state-of-the-art performance on six video reasoning\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Yanrui Yu"
                    },
                    {
                        "name": "Ye Yuan"
                    },
                    {
                        "name": "Rui Mao"
                    },
                    {
                        "name": "Tianfei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Tianfei Zhou"
                },
                "author": "Tianfei Zhou",
                "arxiv_comment": "Accepted by NeurIPS 2025. Code: https://github.com/QiWang98/VideoRFT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12434v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12434v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12476v1",
                "updated": "2025-10-14T13:10:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    10,
                    23,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T13:10:23Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    10,
                    23,
                    1,
                    287,
                    0
                ],
                "title": "When Personalization Tricks Detectors: The Feature-Inversion Trap in\n  Machine-Generated Text Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Personalization Tricks Detectors: The Feature-Inversion Trap in\n  Machine-Generated Text Detection"
                },
                "summary": "Large language models (LLMs) have grown more powerful in language generation,\nproducing fluent text and even imitating personal style. Yet, this ability also\nheightens the risk of identity impersonation. To the best of our knowledge, no\nprior work has examined personalized machine-generated text (MGT) detection. In\nthis paper, we introduce \\dataset, the first benchmark for evaluating detector\nrobustness in personalized settings, built from literary and blog texts paired\nwith their LLM-generated imitations. Our experimental results demonstrate large\nperformance gaps across detectors in personalized settings: some\nstate-of-the-art models suffer significant drops. We attribute this limitation\nto the \\textit{feature-inversion trap}, where features that are discriminative\nin general domains become inverted and misleading when applied to personalized\ntext. Based on this finding, we propose \\method, a simple and reliable way to\npredict detector performance changes in personalized settings. \\method\nidentifies latent directions corresponding to inverted features and constructs\nprobe datasets that differ primarily along these features to evaluate detector\ndependence. Our experiments show that \\method can accurately predict both the\ndirection and the magnitude of post-transfer changes, showing 85\\% correlation\nwith the actual performance gaps. We hope that this work will encourage further\nresearch on personalized text detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have grown more powerful in language generation,\nproducing fluent text and even imitating personal style. Yet, this ability also\nheightens the risk of identity impersonation. To the best of our knowledge, no\nprior work has examined personalized machine-generated text (MGT) detection. In\nthis paper, we introduce \\dataset, the first benchmark for evaluating detector\nrobustness in personalized settings, built from literary and blog texts paired\nwith their LLM-generated imitations. Our experimental results demonstrate large\nperformance gaps across detectors in personalized settings: some\nstate-of-the-art models suffer significant drops. We attribute this limitation\nto the \\textit{feature-inversion trap}, where features that are discriminative\nin general domains become inverted and misleading when applied to personalized\ntext. Based on this finding, we propose \\method, a simple and reliable way to\npredict detector performance changes in personalized settings. \\method\nidentifies latent directions corresponding to inverted features and constructs\nprobe datasets that differ primarily along these features to evaluate detector\ndependence. Our experiments show that \\method can accurately predict both the\ndirection and the magnitude of post-transfer changes, showing 85\\% correlation\nwith the actual performance gaps. We hope that this work will encourage further\nresearch on personalized text detection."
                },
                "authors": [
                    {
                        "name": "Lang Gao"
                    },
                    {
                        "name": "Xuhui Li"
                    },
                    {
                        "name": "Chenxi Wang"
                    },
                    {
                        "name": "Mingzhe Li"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Zirui Song"
                    },
                    {
                        "name": "Jinghui Zhang"
                    },
                    {
                        "name": "Rui Yan"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Xiuying Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiuying Chen"
                },
                "author": "Xiuying Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12474v1",
                "updated": "2025-10-14T13:04:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    4,
                    22,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T13:04:22Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    4,
                    22,
                    1,
                    287,
                    0
                ],
                "title": "SMEC: Rethinking Matryoshka Representation Learning for Retrieval\n  Embedding Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SMEC: Rethinking Matryoshka Representation Learning for Retrieval\n  Embedding Compression"
                },
                "summary": "Large language models (LLMs) generate high-dimensional embeddings that\ncapture rich semantic and syntactic information. However, high-dimensional\nembeddings exacerbate computational complexity and storage requirements,\nthereby hindering practical deployment. To address these challenges, we propose\na novel training framework named Sequential Matryoshka Embedding Compression\n(SMEC). This framework introduces the Sequential Matryoshka Representation\nLearning(SMRL) method to mitigate gradient variance during training, the\nAdaptive Dimension Selection (ADS) module to reduce information degradation\nduring dimension pruning, and the Selectable Cross-batch Memory (S-XBM) module\nto enhance unsupervised learning between high- and low-dimensional embeddings.\nExperiments on image, text, and multimodal datasets demonstrate that SMEC\nachieves significant dimensionality reduction while maintaining performance.\nFor instance, on the BEIR dataset, our approach improves the performance of\ncompressed LLM2Vec embeddings (256 dimensions) by 1.1 points and 2.7 points\ncompared to the Matryoshka-Adaptor and Search-Adaptor models, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) generate high-dimensional embeddings that\ncapture rich semantic and syntactic information. However, high-dimensional\nembeddings exacerbate computational complexity and storage requirements,\nthereby hindering practical deployment. To address these challenges, we propose\na novel training framework named Sequential Matryoshka Embedding Compression\n(SMEC). This framework introduces the Sequential Matryoshka Representation\nLearning(SMRL) method to mitigate gradient variance during training, the\nAdaptive Dimension Selection (ADS) module to reduce information degradation\nduring dimension pruning, and the Selectable Cross-batch Memory (S-XBM) module\nto enhance unsupervised learning between high- and low-dimensional embeddings.\nExperiments on image, text, and multimodal datasets demonstrate that SMEC\nachieves significant dimensionality reduction while maintaining performance.\nFor instance, on the BEIR dataset, our approach improves the performance of\ncompressed LLM2Vec embeddings (256 dimensions) by 1.1 points and 2.7 points\ncompared to the Matryoshka-Adaptor and Search-Adaptor models, respectively."
                },
                "authors": [
                    {
                        "name": "Biao Zhang"
                    },
                    {
                        "name": "Lixin Chen"
                    },
                    {
                        "name": "Tong Liu"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "arxiv_comment": "Accepted by EMNLP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12469v1",
                "updated": "2025-10-14T13:01:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    1,
                    48,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T13:01:48Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    1,
                    48,
                    1,
                    287,
                    0
                ],
                "title": "Proof of Cloud: Data Center Execution Assurance for Confidential VMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proof of Cloud: Data Center Execution Assurance for Confidential VMs"
                },
                "summary": "Confidential Virtual Machines (CVMs) protect data in use by running workloads\ninside hardware-isolated environments. In doing so, they also inherit the\nlimitations of the underlying hardware. Trusted Execution Environments (TEEs),\nwhich enforce this isolation, explicitly exclude adversaries with physical\naccess from their threat model. Commercial TEEs, e.g., Intel TDX, thus assume\ninfrastructure providers do not physically exploit hardware and serve as\nsafeguards instead. This creates a tension: tenants must trust provider\nintegrity at the hardware layer, yet existing remote attestation offers no way\nto verify that CVMs actually run on physically trusted platforms, leaving\ntoday's CVM deployments unable to demonstrate that their guarantees align with\nthe TEE vendor's threat model.\n  We bridge this confidence gap with Data Center Execution Assurance (DCEA), a\ndesign generating \"Proofs of Cloud\". DCEA binds a CVM to its underlying\nplatform using vTPM-anchored measurements, ensuring CVM launch evidence and TPM\nquotes refer to the same physical chassis.\n  This takes advantage of the fact that data centers are often identifiable via\nTPMs. Our approach applies to CVMs accessing vTPMs and running on top of\nsoftware stacks fully controlled by the cloud provider, as well as\nsingle-tenant bare-metal deployments with discrete TPMs. We trust providers for\nintegrity (certificate issuance), but not for the confidentiality of\nCVM-visible state. DCEA enables remote verification of a CVM's platform origin\nand integrity, mitigating attacks like replay and attestation proxying. We\ninclude a candidate implementation on Google Cloud and Intel TDX that leverages\nIntel TXT for trusted launch. Our design refines CVMs' threat model and\nprovides a practical path for deploying high-assurance, confidential workloads\nin minimally trusted environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidential Virtual Machines (CVMs) protect data in use by running workloads\ninside hardware-isolated environments. In doing so, they also inherit the\nlimitations of the underlying hardware. Trusted Execution Environments (TEEs),\nwhich enforce this isolation, explicitly exclude adversaries with physical\naccess from their threat model. Commercial TEEs, e.g., Intel TDX, thus assume\ninfrastructure providers do not physically exploit hardware and serve as\nsafeguards instead. This creates a tension: tenants must trust provider\nintegrity at the hardware layer, yet existing remote attestation offers no way\nto verify that CVMs actually run on physically trusted platforms, leaving\ntoday's CVM deployments unable to demonstrate that their guarantees align with\nthe TEE vendor's threat model.\n  We bridge this confidence gap with Data Center Execution Assurance (DCEA), a\ndesign generating \"Proofs of Cloud\". DCEA binds a CVM to its underlying\nplatform using vTPM-anchored measurements, ensuring CVM launch evidence and TPM\nquotes refer to the same physical chassis.\n  This takes advantage of the fact that data centers are often identifiable via\nTPMs. Our approach applies to CVMs accessing vTPMs and running on top of\nsoftware stacks fully controlled by the cloud provider, as well as\nsingle-tenant bare-metal deployments with discrete TPMs. We trust providers for\nintegrity (certificate issuance), but not for the confidentiality of\nCVM-visible state. DCEA enables remote verification of a CVM's platform origin\nand integrity, mitigating attacks like replay and attestation proxying. We\ninclude a candidate implementation on Google Cloud and Intel TDX that leverages\nIntel TXT for trusted launch. Our design refines CVMs' threat model and\nprovides a practical path for deploying high-assurance, confidential workloads\nin minimally trusted environments."
                },
                "authors": [
                    {
                        "name": "Filip Rezabek"
                    },
                    {
                        "name": "Moe Mahhouk"
                    },
                    {
                        "name": "Andrew Miller"
                    },
                    {
                        "name": "Stefan Genchev"
                    },
                    {
                        "name": "Quintus Kilbourn"
                    },
                    {
                        "name": "Georg Carle"
                    },
                    {
                        "name": "Jonathan Passerat-Palmbach"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Passerat-Palmbach"
                },
                "author": "Jonathan Passerat-Palmbach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01924v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01924v3",
                "updated": "2025-10-14T13:00:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    0,
                    7,
                    1,
                    287,
                    0
                ],
                "published": "2025-04-02T17:33:53Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    33,
                    53,
                    2,
                    92,
                    0
                ],
                "title": "Gen-C: Populating Virtual Worlds with Generative Crowds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gen-C: Populating Virtual Worlds with Generative Crowds"
                },
                "summary": "Over the past two decades, researchers have made significant steps in\nsimulating agent-based human crowds, yet most efforts remain focused on\nlow-level tasks such as collision avoidance, path following, and flocking.\nRealistic simulations, however, require modeling high-level behaviors that\nemerge from agents interacting with each other and with their environment over\ntime. We introduce Generative Crowds (Gen-C), a generative framework that\nproduces crowd scenarios capturing agent-agent and agent-environment\ninteractions, shaping coherent high-level crowd plans. To avoid the\nlabor-intensive process of collecting and annotating real crowd video data, we\nleverage large language models (LLMs) to bootstrap synthetic datasets of crowd\nscenarios. We propose a time-expanded graph representation, encoding actions,\ninteractions, and spatial context. Gen-C employs a dual Variational Graph\nAutoencoder (VGAE) architecture that jointly learns connectivity patterns and\nnode features conditioned on textual and structural signals, overcoming the\nlimitations of direct LLM generation to enable scalable, environment-aware\nmulti-agent crowd simulations. We demonstrate the effectiveness of Gen-C on\nscenarios with diverse behaviors such as a University Campus and a Train\nStation, showing that it generates heterogeneous crowds, coherent interactions,\nand high-level decision-making patterns consistent with real-world crowd\ndynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past two decades, researchers have made significant steps in\nsimulating agent-based human crowds, yet most efforts remain focused on\nlow-level tasks such as collision avoidance, path following, and flocking.\nRealistic simulations, however, require modeling high-level behaviors that\nemerge from agents interacting with each other and with their environment over\ntime. We introduce Generative Crowds (Gen-C), a generative framework that\nproduces crowd scenarios capturing agent-agent and agent-environment\ninteractions, shaping coherent high-level crowd plans. To avoid the\nlabor-intensive process of collecting and annotating real crowd video data, we\nleverage large language models (LLMs) to bootstrap synthetic datasets of crowd\nscenarios. We propose a time-expanded graph representation, encoding actions,\ninteractions, and spatial context. Gen-C employs a dual Variational Graph\nAutoencoder (VGAE) architecture that jointly learns connectivity patterns and\nnode features conditioned on textual and structural signals, overcoming the\nlimitations of direct LLM generation to enable scalable, environment-aware\nmulti-agent crowd simulations. We demonstrate the effectiveness of Gen-C on\nscenarios with diverse behaviors such as a University Campus and a Train\nStation, showing that it generates heterogeneous crowds, coherent interactions,\nand high-level decision-making patterns consistent with real-world crowd\ndynamics."
                },
                "authors": [
                    {
                        "name": "Andreas Panayiotou"
                    },
                    {
                        "name": "Panayiotis Charalambous"
                    },
                    {
                        "name": "Ioannis Karamouzas"
                    }
                ],
                "author_detail": {
                    "name": "Ioannis Karamouzas"
                },
                "author": "Ioannis Karamouzas",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01924v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01924v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08737v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08737v2",
                "updated": "2025-10-14T12:56:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    12,
                    56,
                    2,
                    1,
                    287,
                    0
                ],
                "published": "2025-01-15T11:33:52Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    11,
                    33,
                    52,
                    2,
                    15,
                    0
                ],
                "title": "Resource-Constrained Federated Continual Learning: What Does Matter?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-Constrained Federated Continual Learning: What Does Matter?"
                },
                "summary": "Federated Continual Learning (FCL) aims to enable sequentially\nprivacy-preserving model training on streams of incoming data that vary in edge\ndevices by preserving previous knowledge while adapting to new data. Current\nFCL literature focuses on restricted data privacy and access to previously seen\ndata while imposing no constraints on the training overhead. This is\nunreasonable for FCL applications in real-world scenarios, where edge devices\nare primarily constrained by resources such as storage, computational budget,\nand label rate. We revisit this problem with a large-scale benchmark and\nanalyze the performance of state-of-the-art FCL approaches under different\nresource-constrained settings. Various typical FCL techniques and six datasets\nin two incremental learning scenarios (Class-IL and Domain-IL) are involved in\nour experiments. Through extensive experiments amounting to a total of over\n1,000+ GPU hours, we find that, under limited resource-constrained settings,\nexisting FCL approaches, with no exception, fail to achieve the expected\nperformance. Our conclusions are consistent in the sensitivity analysis. This\nsuggests that most existing FCL methods are particularly too resource-dependent\nfor real-world deployment. Moreover, we study the performance of typical FCL\ntechniques with resource constraints and shed light on future research\ndirections in FCL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Continual Learning (FCL) aims to enable sequentially\nprivacy-preserving model training on streams of incoming data that vary in edge\ndevices by preserving previous knowledge while adapting to new data. Current\nFCL literature focuses on restricted data privacy and access to previously seen\ndata while imposing no constraints on the training overhead. This is\nunreasonable for FCL applications in real-world scenarios, where edge devices\nare primarily constrained by resources such as storage, computational budget,\nand label rate. We revisit this problem with a large-scale benchmark and\nanalyze the performance of state-of-the-art FCL approaches under different\nresource-constrained settings. Various typical FCL techniques and six datasets\nin two incremental learning scenarios (Class-IL and Domain-IL) are involved in\nour experiments. Through extensive experiments amounting to a total of over\n1,000+ GPU hours, we find that, under limited resource-constrained settings,\nexisting FCL approaches, with no exception, fail to achieve the expected\nperformance. Our conclusions are consistent in the sensitivity analysis. This\nsuggests that most existing FCL methods are particularly too resource-dependent\nfor real-world deployment. Moreover, we study the performance of typical FCL\ntechniques with resource constraints and shed light on future research\ndirections in FCL."
                },
                "authors": [
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Yuying Wang"
                    },
                    {
                        "name": "Jiahua Dong"
                    },
                    {
                        "name": "Haozhao Wang"
                    },
                    {
                        "name": "Yining Qi"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Ruixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruixuan Li"
                },
                "author": "Ruixuan Li",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2303.11165 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08737v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08737v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12462v1",
                "updated": "2025-10-14T12:52:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    12,
                    52,
                    29,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T12:52:29Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    12,
                    52,
                    29,
                    1,
                    287,
                    0
                ],
                "title": "Evaluating and Mitigating LLM-as-a-judge Bias in Communication Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating and Mitigating LLM-as-a-judge Bias in Communication Systems"
                },
                "summary": "Large Language Models (LLMs) are increasingly being used to autonomously\nevaluate the quality of content in communication systems, e.g., to assess\nresponses in telecom customer support chatbots. However, the impartiality of\nthese AI \"judges\" is not guaranteed, and any biases in their evaluation\ncriteria could skew outcomes and undermine user trust. In this paper, we\nsystematically investigate judgment biases in two LLM-as-a-judge models (i.e.,\nGPT-Judge and JudgeLM) under the point-wise scoring setting, encompassing 11\ntypes of biases that cover both implicit and explicit forms. We observed that\nstate-of-the-art LLM judges demonstrate robustness to biased inputs, generally\nassigning them lower scores than the corresponding clean samples. Providing a\ndetailed scoring rubric further enhances this robustness. We further found that\nfine-tuning an LLM on high-scoring yet biased responses can significantly\ndegrade its performance, highlighting the risk of training on biased data. We\nalso discovered that the judged scores correlate with task difficulty: a\nchallenging dataset like GPQA yields lower average scores, whereas an\nopen-ended reasoning dataset (e.g., JudgeLM-val) sees higher average scores.\nFinally, we proposed four potential mitigation strategies to ensure fair and\nreliable AI judging in practical communication scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being used to autonomously\nevaluate the quality of content in communication systems, e.g., to assess\nresponses in telecom customer support chatbots. However, the impartiality of\nthese AI \"judges\" is not guaranteed, and any biases in their evaluation\ncriteria could skew outcomes and undermine user trust. In this paper, we\nsystematically investigate judgment biases in two LLM-as-a-judge models (i.e.,\nGPT-Judge and JudgeLM) under the point-wise scoring setting, encompassing 11\ntypes of biases that cover both implicit and explicit forms. We observed that\nstate-of-the-art LLM judges demonstrate robustness to biased inputs, generally\nassigning them lower scores than the corresponding clean samples. Providing a\ndetailed scoring rubric further enhances this robustness. We further found that\nfine-tuning an LLM on high-scoring yet biased responses can significantly\ndegrade its performance, highlighting the risk of training on biased data. We\nalso discovered that the judged scores correlate with task difficulty: a\nchallenging dataset like GPQA yields lower average scores, whereas an\nopen-ended reasoning dataset (e.g., JudgeLM-val) sees higher average scores.\nFinally, we proposed four potential mitigation strategies to ensure fair and\nreliable AI judging in practical communication scenarios."
                },
                "authors": [
                    {
                        "name": "Jiaxin Gao"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Yanwen Jia"
                    },
                    {
                        "name": "Xueluan Gong"
                    },
                    {
                        "name": "Kwok-Yan Lam"
                    },
                    {
                        "name": "Qian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qian Wang"
                },
                "author": "Qian Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12461v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12461v1",
                "updated": "2025-10-14T12:50:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    12,
                    50,
                    11,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T12:50:11Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    12,
                    50,
                    11,
                    1,
                    287,
                    0
                ],
                "title": "Leveraging Language Semantics for Collaborative Filtering with TextGCN\n  and TextGCN-MLP: Zero-Shot vs In-Domain Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Language Semantics for Collaborative Filtering with TextGCN\n  and TextGCN-MLP: Zero-Shot vs In-Domain Performance"
                },
                "summary": "In recent years, various approaches have been proposed to leverage large\nlanguage models (LLMs) for incorporating textual information about items into\nrecommender systems. Existing methods primarily focus on either fine-tuning\nLLMs to generate recommendations or integrating LLM-based embeddings into\ndownstream models. In this work, we follow the latter direction and propose\n\\textbf{TextGCN}, which applies parameter-free graph convolution layers\ndirectly over LLM-based item-title embeddings, instead of learning ID-based\nembeddings as in traditional methods. By combining language semantics with\ngraph message passing, this architecture achieves state-of-the-art zero-shot\nperformance, significantly outperforming prior approaches. Furthermore, we\nintroduce \\textbf{TextGCN-MLP}, which extends TextGCN with a trainable\nmultilayer perceptron trained using a contrastive loss, achieving\nstate-of-the-art in-domain performance on recommendation benchmarks. However,\nthe zero-shot performance of TextGCN-MLP remains lower than that of TextGCN,\nhighlighting the trade-off between in-domain specialization and zero-shot\ngeneralization. We release our code on github at\n\\href{https://github.com/ChernovAndrey/TFCE}{github.com/ChernovAndrey/TFCE}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, various approaches have been proposed to leverage large\nlanguage models (LLMs) for incorporating textual information about items into\nrecommender systems. Existing methods primarily focus on either fine-tuning\nLLMs to generate recommendations or integrating LLM-based embeddings into\ndownstream models. In this work, we follow the latter direction and propose\n\\textbf{TextGCN}, which applies parameter-free graph convolution layers\ndirectly over LLM-based item-title embeddings, instead of learning ID-based\nembeddings as in traditional methods. By combining language semantics with\ngraph message passing, this architecture achieves state-of-the-art zero-shot\nperformance, significantly outperforming prior approaches. Furthermore, we\nintroduce \\textbf{TextGCN-MLP}, which extends TextGCN with a trainable\nmultilayer perceptron trained using a contrastive loss, achieving\nstate-of-the-art in-domain performance on recommendation benchmarks. However,\nthe zero-shot performance of TextGCN-MLP remains lower than that of TextGCN,\nhighlighting the trade-off between in-domain specialization and zero-shot\ngeneralization. We release our code on github at\n\\href{https://github.com/ChernovAndrey/TFCE}{github.com/ChernovAndrey/TFCE}."
                },
                "authors": [
                    {
                        "name": "Andrei Chernov"
                    },
                    {
                        "name": "Haroon Wahab"
                    },
                    {
                        "name": "Oleg Novitskij"
                    }
                ],
                "author_detail": {
                    "name": "Oleg Novitskij"
                },
                "author": "Oleg Novitskij",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12461v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12460v1",
                "updated": "2025-10-14T12:48:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    12,
                    48,
                    24,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T12:48:24Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    12,
                    48,
                    24,
                    1,
                    287,
                    0
                ],
                "title": "Probing Latent Knowledge Conflict for Faithful Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing Latent Knowledge Conflict for Faithful Retrieval-Augmented\n  Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm to\nenhance the factuality of Large Language Models (LLMs). However, existing RAG\nsystems often suffer from an unfaithfulness issue, where the model's response\ncontradicts evidence from the retrieved context. Existing approaches to\nimproving contextual faithfulness largely rely on external interventions, such\nas prompt engineering, decoding constraints, or reward-based fine-tuning. These\nworks treat the LLM as a black box and overlook a crucial question: how does\nthe LLM internally integrate retrieved evidence with its parametric memory,\nparticularly under knowledge conflicts? To address this gap, we conduct a\nprobing-based analysis of hidden-state representations in LLMs and observe\nthree findings: knowledge integration occurs hierarchically, conflicts manifest\nas latent signals at the sentence level, and irrelevant context is often\namplified when aligned with parametric knowledge. Building on these findings,\nwe propose CLEAR (Conflict-Localized and Enhanced Attention for RAG), a\nframework that (i) decomposes context into fine-grained sentence-level\nknowledge, (ii) employs hidden-state probing to localize conflicting knowledge,\nand (iii) introduces conflict-aware fine-tuning to guide the model to\naccurately integrate retrieved evidence. Extensive experiments across three\nbenchmarks demonstrate that CLEAR substantially improves both accuracy and\ncontextual faithfulness, consistently outperforming strong baselines under\ndiverse conflict conditions. The related resources are available at\nhttps://github.com/LinfengGao/CLEAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm to\nenhance the factuality of Large Language Models (LLMs). However, existing RAG\nsystems often suffer from an unfaithfulness issue, where the model's response\ncontradicts evidence from the retrieved context. Existing approaches to\nimproving contextual faithfulness largely rely on external interventions, such\nas prompt engineering, decoding constraints, or reward-based fine-tuning. These\nworks treat the LLM as a black box and overlook a crucial question: how does\nthe LLM internally integrate retrieved evidence with its parametric memory,\nparticularly under knowledge conflicts? To address this gap, we conduct a\nprobing-based analysis of hidden-state representations in LLMs and observe\nthree findings: knowledge integration occurs hierarchically, conflicts manifest\nas latent signals at the sentence level, and irrelevant context is often\namplified when aligned with parametric knowledge. Building on these findings,\nwe propose CLEAR (Conflict-Localized and Enhanced Attention for RAG), a\nframework that (i) decomposes context into fine-grained sentence-level\nknowledge, (ii) employs hidden-state probing to localize conflicting knowledge,\nand (iii) introduces conflict-aware fine-tuning to guide the model to\naccurately integrate retrieved evidence. Extensive experiments across three\nbenchmarks demonstrate that CLEAR substantially improves both accuracy and\ncontextual faithfulness, consistently outperforming strong baselines under\ndiverse conflict conditions. The related resources are available at\nhttps://github.com/LinfengGao/CLEAR."
                },
                "authors": [
                    {
                        "name": "Linfeng Gao"
                    },
                    {
                        "name": "Baolong Bi"
                    },
                    {
                        "name": "Zheng Yuan"
                    },
                    {
                        "name": "Le Wang"
                    },
                    {
                        "name": "Zerui Chen"
                    },
                    {
                        "name": "Zhimin Wei"
                    },
                    {
                        "name": "Shenghua Liu"
                    },
                    {
                        "name": "Qinggang Zhang"
                    },
                    {
                        "name": "Jinsong Su"
                    }
                ],
                "author_detail": {
                    "name": "Jinsong Su"
                },
                "author": "Jinsong Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12458v1",
                "updated": "2025-10-14T12:43:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    12,
                    43,
                    41,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T12:43:41Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    12,
                    43,
                    41,
                    1,
                    287,
                    0
                ],
                "title": "A Network Digital Twin of a 5G Private Network: Designing a\n  Proof-of-Concept from Theory to Practice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Network Digital Twin of a 5G Private Network: Designing a\n  Proof-of-Concept from Theory to Practice"
                },
                "summary": "Network Digital Twins represent a key technology in future networks, expected\nto provide the capability to perform accurate analysis and predictions about\nthe behaviour of 6G mobile networks. However, despite the availability of\nseveral theoretical works on the subject, still very few examples of actual\nimplementations of Network Digital Twin are available. This paper provides a\ndetailed description about the characteristics of Network Digital Twin and\nprovides a practical example about real deployment of the technology. The\nconsidered network infrastructure is a real 5G private network running in a\nlab. The Network Digital Twin is built based on open source network emulation\nsoftware and is available to the community as open source. Measurements on both\nthe physical infrastructure and the related Digital Twin demonstrate a high\naccuracy in reproducing the state and behavior of the actual 5G system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network Digital Twins represent a key technology in future networks, expected\nto provide the capability to perform accurate analysis and predictions about\nthe behaviour of 6G mobile networks. However, despite the availability of\nseveral theoretical works on the subject, still very few examples of actual\nimplementations of Network Digital Twin are available. This paper provides a\ndetailed description about the characteristics of Network Digital Twin and\nprovides a practical example about real deployment of the technology. The\nconsidered network infrastructure is a real 5G private network running in a\nlab. The Network Digital Twin is built based on open source network emulation\nsoftware and is available to the community as open source. Measurements on both\nthe physical infrastructure and the related Digital Twin demonstrate a high\naccuracy in reproducing the state and behavior of the actual 5G system."
                },
                "authors": [
                    {
                        "name": "Cristina Emilia Costa"
                    },
                    {
                        "name": "Tatenda Horiro Zhou"
                    },
                    {
                        "name": "Fabrizio Granelli"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Granelli"
                },
                "author": "Fabrizio Granelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12454v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12454v1",
                "updated": "2025-10-14T12:36:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    12,
                    36,
                    49,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T12:36:49Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    12,
                    36,
                    49,
                    1,
                    287,
                    0
                ],
                "title": "First GNSS-deployed optical clock for local time scale upgrade",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "First GNSS-deployed optical clock for local time scale upgrade"
                },
                "summary": "Precise time scale is the universal base for all measurements. Here we report\nthe deployment of a compact and transportable optical clock to a timekeeping\ninstitution and steering an active hydrogen maser to generate an optical time\nscale, realizing the upgrade of the local time scale in the Global Navigation\nSatellite System. The optical clock was transported over 1200 km by express\ndelivery and resume work as normal promptly, and its extremely high uptime of\n93.6% in the half-year enabled us to precisely correct the frequency drift of\nhydrogen maser, ultimately achieving an unprecedented monthly instability of\n4E-17. This steering experiment with a deployable optical clock marks a\nsignificant advancement, demonstrating that a timing accuracy below 100 ps per\nmonth can be achieved feasibly in various timekeeping institutions where\nhydrogen masers are typically employed as the primary contributor to\ntimekeeping. In the future, mobile optical time scale based on such\ntransportable optical clock can be deployed flexibly and rapidly, which is\nparticularly important in scenarios lacking International Atomic Time\nreference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precise time scale is the universal base for all measurements. Here we report\nthe deployment of a compact and transportable optical clock to a timekeeping\ninstitution and steering an active hydrogen maser to generate an optical time\nscale, realizing the upgrade of the local time scale in the Global Navigation\nSatellite System. The optical clock was transported over 1200 km by express\ndelivery and resume work as normal promptly, and its extremely high uptime of\n93.6% in the half-year enabled us to precisely correct the frequency drift of\nhydrogen maser, ultimately achieving an unprecedented monthly instability of\n4E-17. This steering experiment with a deployable optical clock marks a\nsignificant advancement, demonstrating that a timing accuracy below 100 ps per\nmonth can be achieved feasibly in various timekeeping institutions where\nhydrogen masers are typically employed as the primary contributor to\ntimekeeping. In the future, mobile optical time scale based on such\ntransportable optical clock can be deployed flexibly and rapidly, which is\nparticularly important in scenarios lacking International Atomic Time\nreference."
                },
                "authors": [
                    {
                        "name": "Yi Yuan"
                    },
                    {
                        "name": "Jian Cao"
                    },
                    {
                        "name": "Jinbo Yuan"
                    },
                    {
                        "name": "Dehao Wang"
                    },
                    {
                        "name": "Pengcheng Fang"
                    },
                    {
                        "name": "Qunfeng Chen"
                    },
                    {
                        "name": "Shiying Cao"
                    },
                    {
                        "name": "Xuanjian Wang"
                    },
                    {
                        "name": "Sijia Chao"
                    },
                    {
                        "name": "Hualin Shu"
                    },
                    {
                        "name": "Guojun Li"
                    },
                    {
                        "name": "Jinfeng Xu"
                    },
                    {
                        "name": "Guitao Fu"
                    },
                    {
                        "name": "Yuting Yang"
                    },
                    {
                        "name": "Run Zhao"
                    },
                    {
                        "name": "Fengfeng Shi"
                    },
                    {
                        "name": "Xueren Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xueren Huang"
                },
                "author": "Xueren Huang",
                "arxiv_comment": "7 pages main article, 3 pages supplemental material",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12454v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12454v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02613v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02613v3",
                "updated": "2025-10-14T12:26:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    12,
                    26,
                    33,
                    1,
                    287,
                    0
                ],
                "published": "2024-06-03T08:23:45Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    8,
                    23,
                    45,
                    0,
                    155,
                    0
                ],
                "title": "ACCO: Accumulate While You Communicate for Communication-Overlapped\n  Sharded LLM Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACCO: Accumulate While You Communicate for Communication-Overlapped\n  Sharded LLM Training"
                },
                "summary": "Training LLMs relies on distributed implementations using multiple GPUs to\ncompute gradients in parallel with sharded optimizers. However, synchronizing\ngradients in data parallel setups introduces communication overhead that grows\nwith the number of workers, limiting parallelization efficiency. Local\noptimization algorithms reduce communications but incur high memory costs as\nthey prevent optimizer state sharding, hindering scalability. To address this,\nwe propose \\textbf{AC}cumulate while \\textbf{CO}mmunicate (ACCO), a\nmemory-efficient optimization algorithm for distributed LLM training. By\nsynchronizing delayed gradients while computing new ones, ACCO reduces GPU idle\ntime and supports heterogeneous hardware. To mitigate the convergence issues\ncaused by delayed updates, we introduce a novel technique ensuring training\ndynamics align with standard distributed optimization. Compared to ZeRO-1, our\napproach is significantly faster and scales effectively across heterogeneous\nhardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training LLMs relies on distributed implementations using multiple GPUs to\ncompute gradients in parallel with sharded optimizers. However, synchronizing\ngradients in data parallel setups introduces communication overhead that grows\nwith the number of workers, limiting parallelization efficiency. Local\noptimization algorithms reduce communications but incur high memory costs as\nthey prevent optimizer state sharding, hindering scalability. To address this,\nwe propose \\textbf{AC}cumulate while \\textbf{CO}mmunicate (ACCO), a\nmemory-efficient optimization algorithm for distributed LLM training. By\nsynchronizing delayed gradients while computing new ones, ACCO reduces GPU idle\ntime and supports heterogeneous hardware. To mitigate the convergence issues\ncaused by delayed updates, we introduce a novel technique ensuring training\ndynamics align with standard distributed optimization. Compared to ZeRO-1, our\napproach is significantly faster and scales effectively across heterogeneous\nhardware."
                },
                "authors": [
                    {
                        "name": "Adel Nabli"
                    },
                    {
                        "name": "Louis Fournier"
                    },
                    {
                        "name": "Pierre Erbacher"
                    },
                    {
                        "name": "Louis Serrano"
                    },
                    {
                        "name": "Eugene Belilovsky"
                    },
                    {
                        "name": "Edouard Oyallon"
                    }
                ],
                "author_detail": {
                    "name": "Edouard Oyallon"
                },
                "arxiv_affiliation": "MLIA",
                "author": "Edouard Oyallon",
                "arxiv_journal_ref": "The Thirty-ninth Annual Conference on Neural Information\n  Processing Systems, Dec 2025, San diego (Californie), United States",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02613v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02613v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12435v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12435v2",
                "updated": "2025-10-15T02:12:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    2,
                    12,
                    50,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-14T12:13:58Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    12,
                    13,
                    58,
                    1,
                    287,
                    0
                ],
                "title": "The value of storage in electricity distribution: The role of markets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The value of storage in electricity distribution: The role of markets"
                },
                "summary": "Electricity distribution companies deploy battery storage to defer grid\nupgrades by reducing peak demand. In deregulated jurisdictions, such storage\noften sits idle because regulatory constraints bar participation in electricity\nmarkets. Here, we develop an optimization framework that, to our knowledge,\nprovides the first formal model of market participation constraints within\nstorage investment and operation planning. Applying the framework to a\nMassachusetts case study, we find that market participation could deliver\nsimilar savings as peak demand reduction. Under current conditions, market\nparticipation does not increase storage investment, but at very low storage\ncosts, could incentivize deployment beyond local distribution needs. This might\nrun contrary to the separation of distribution from generation in deregulated\nmarkets. Our framework can identify investment levels appropriate for local\ndistribution needs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electricity distribution companies deploy battery storage to defer grid\nupgrades by reducing peak demand. In deregulated jurisdictions, such storage\noften sits idle because regulatory constraints bar participation in electricity\nmarkets. Here, we develop an optimization framework that, to our knowledge,\nprovides the first formal model of market participation constraints within\nstorage investment and operation planning. Applying the framework to a\nMassachusetts case study, we find that market participation could deliver\nsimilar savings as peak demand reduction. Under current conditions, market\nparticipation does not increase storage investment, but at very low storage\ncosts, could incentivize deployment beyond local distribution needs. This might\nrun contrary to the separation of distribution from generation in deregulated\nmarkets. Our framework can identify investment levels appropriate for local\ndistribution needs."
                },
                "authors": [
                    {
                        "name": "Dirk Lauinger"
                    },
                    {
                        "name": "Deepjyoti Deka"
                    },
                    {
                        "name": "Sungho Shin"
                    }
                ],
                "author_detail": {
                    "name": "Sungho Shin"
                },
                "author": "Sungho Shin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12435v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12435v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07214v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07214v4",
                "updated": "2025-10-14T12:09:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    12,
                    9,
                    1,
                    1,
                    287,
                    0
                ],
                "published": "2024-02-20T18:57:34Z",
                "published_parsed": [
                    2024,
                    2,
                    20,
                    18,
                    57,
                    34,
                    1,
                    51,
                    0
                ],
                "title": "Exploring the Frontier of Vision-Language Models: A Survey of Current\n  Methodologies and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Frontier of Vision-Language Models: A Survey of Current\n  Methodologies and Future Directions"
                },
                "summary": "The advent of Large Language Models (LLMs) has significantly reshaped the\ntrajectory of the AI revolution. Nevertheless, these LLMs exhibit a notable\nlimitation, as they are primarily adept at processing textual information. To\naddress this constraint, researchers have endeavored to integrate visual\ncapabilities with LLMs, resulting in the emergence of Vision-Language Models\n(VLMs). These advanced models are instrumental in tackling more intricate tasks\nsuch as image captioning and visual question answering. In our comprehensive\nsurvey paper, we delve into the key advancements within the realm of VLMs. Our\nclassification organizes VLMs into three distinct categories: models dedicated\nto vision-language understanding, models that process multimodal inputs to\ngenerate unimodal (textual) outputs and models that both accept and produce\nmultimodal inputs and outputs.This classification is based on their respective\ncapabilities and functionalities in processing and generating various\nmodalities of data.We meticulously dissect each model, offering an extensive\nanalysis of its foundational architecture, training data sources, as well as\nits strengths and limitations wherever possible, providing readers with a\ncomprehensive understanding of its essential components. We also analyzed the\nperformance of VLMs in various benchmark datasets. By doing so, we aim to offer\na nuanced understanding of the diverse landscape of VLMs. Additionally, we\nunderscore potential avenues for future research in this dynamic domain,\nanticipating further breakthroughs and advancements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) has significantly reshaped the\ntrajectory of the AI revolution. Nevertheless, these LLMs exhibit a notable\nlimitation, as they are primarily adept at processing textual information. To\naddress this constraint, researchers have endeavored to integrate visual\ncapabilities with LLMs, resulting in the emergence of Vision-Language Models\n(VLMs). These advanced models are instrumental in tackling more intricate tasks\nsuch as image captioning and visual question answering. In our comprehensive\nsurvey paper, we delve into the key advancements within the realm of VLMs. Our\nclassification organizes VLMs into three distinct categories: models dedicated\nto vision-language understanding, models that process multimodal inputs to\ngenerate unimodal (textual) outputs and models that both accept and produce\nmultimodal inputs and outputs.This classification is based on their respective\ncapabilities and functionalities in processing and generating various\nmodalities of data.We meticulously dissect each model, offering an extensive\nanalysis of its foundational architecture, training data sources, as well as\nits strengths and limitations wherever possible, providing readers with a\ncomprehensive understanding of its essential components. We also analyzed the\nperformance of VLMs in various benchmark datasets. By doing so, we aim to offer\na nuanced understanding of the diverse landscape of VLMs. Additionally, we\nunderscore potential avenues for future research in this dynamic domain,\nanticipating further breakthroughs and advancements."
                },
                "authors": [
                    {
                        "name": "Akash Ghosh"
                    },
                    {
                        "name": "Arkadeep Acharya"
                    },
                    {
                        "name": "Sriparna Saha"
                    },
                    {
                        "name": "Vinija Jain"
                    },
                    {
                        "name": "Aman Chadha"
                    }
                ],
                "author_detail": {
                    "name": "Aman Chadha"
                },
                "author": "Aman Chadha",
                "arxiv_comment": "One of the first survey on Visual Language Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07214v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07214v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12423v1",
                "updated": "2025-10-14T11:59:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    11,
                    59,
                    47,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T11:59:47Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    11,
                    59,
                    47,
                    1,
                    287,
                    0
                ],
                "title": "MTOS: A LLM-Driven Multi-topic Opinion Simulation Framework for\n  Exploring Echo Chamber Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MTOS: A LLM-Driven Multi-topic Opinion Simulation Framework for\n  Exploring Echo Chamber Dynamics"
                },
                "summary": "The polarization of opinions, information segregation, and cognitive biases\non social media have attracted significant academic attention. In real-world\nnetworks, information often spans multiple interrelated topics, posing\nchallenges for opinion evolution and highlighting the need for frameworks that\nsimulate interactions among topics. Existing studies based on large language\nmodels (LLMs) focus largely on single topics, limiting the capture of cognitive\ntransfer in multi-topic, cross-domain contexts. Traditional numerical models,\nmeanwhile, simplify complex linguistic attitudes into discrete values, lacking\ninterpretability, behavioral consistency, and the ability to integrate multiple\ntopics. To address these issues, we propose Multi-topic Opinion Simulation\n(MTOS), a social simulation framework integrating multi-topic contexts with\nLLMs. MTOS leverages LLMs alongside short-term and long-term memory,\nincorporates multiple user-selection interaction mechanisms and dynamic\ntopic-selection strategies, and employs a belief decay mechanism to enable\nperspective updates across topics. We conduct extensive experiments on MTOS,\nvarying topic numbers, correlation types, and performing ablation studies to\nassess features such as group polarization and local consistency. Results show\nthat multi-topic settings significantly alter polarization trends: positively\ncorrelated topics amplify echo chambers, negatively correlated topics inhibit\nthem, and irrelevant topics also mitigate echo chamber effects through resource\ncompetition. Compared with numerical models, LLM-based agents realistically\nsimulate dynamic opinion changes, reproduce linguistic features of news texts,\nand capture complex human reasoning, improving simulation interpretability and\nsystem stability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The polarization of opinions, information segregation, and cognitive biases\non social media have attracted significant academic attention. In real-world\nnetworks, information often spans multiple interrelated topics, posing\nchallenges for opinion evolution and highlighting the need for frameworks that\nsimulate interactions among topics. Existing studies based on large language\nmodels (LLMs) focus largely on single topics, limiting the capture of cognitive\ntransfer in multi-topic, cross-domain contexts. Traditional numerical models,\nmeanwhile, simplify complex linguistic attitudes into discrete values, lacking\ninterpretability, behavioral consistency, and the ability to integrate multiple\ntopics. To address these issues, we propose Multi-topic Opinion Simulation\n(MTOS), a social simulation framework integrating multi-topic contexts with\nLLMs. MTOS leverages LLMs alongside short-term and long-term memory,\nincorporates multiple user-selection interaction mechanisms and dynamic\ntopic-selection strategies, and employs a belief decay mechanism to enable\nperspective updates across topics. We conduct extensive experiments on MTOS,\nvarying topic numbers, correlation types, and performing ablation studies to\nassess features such as group polarization and local consistency. Results show\nthat multi-topic settings significantly alter polarization trends: positively\ncorrelated topics amplify echo chambers, negatively correlated topics inhibit\nthem, and irrelevant topics also mitigate echo chamber effects through resource\ncompetition. Compared with numerical models, LLM-based agents realistically\nsimulate dynamic opinion changes, reproduce linguistic features of news texts,\nand capture complex human reasoning, improving simulation interpretability and\nsystem stability."
                },
                "authors": [
                    {
                        "name": "Dingyi Zuo"
                    },
                    {
                        "name": "Hongjie Zhang"
                    },
                    {
                        "name": "Jie Ou"
                    },
                    {
                        "name": "Chaosheng Feng"
                    },
                    {
                        "name": "Shuwan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Shuwan Liu"
                },
                "author": "Shuwan Liu",
                "arxiv_comment": "14 pages, 11figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12422v1",
                "updated": "2025-10-14T11:59:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    11,
                    59,
                    19,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T11:59:19Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    11,
                    59,
                    19,
                    1,
                    287,
                    0
                ],
                "title": "VideoLucy: Deep Memory Backtracking for Long Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoLucy: Deep Memory Backtracking for Long Video Understanding"
                },
                "summary": "Recent studies have shown that agent-based systems leveraging large language\nmodels (LLMs) for key information retrieval and integration have emerged as a\npromising approach for long video understanding. However, these systems face\ntwo major challenges. First, they typically perform modeling and reasoning on\nindividual frames, struggling to capture the temporal context of consecutive\nframes. Second, to reduce the cost of dense frame-level captioning, they adopt\nsparse frame sampling, which risks discarding crucial information. To overcome\nthese limitations, we propose VideoLucy, a deep memory backtracking framework\nfor long video understanding. Inspired by the human recollection process from\ncoarse to fine, VideoLucy employs a hierarchical memory structure with\nprogressive granularity. This structure explicitly defines the detail level and\ntemporal scope of memory at different hierarchical depths. Through an\nagent-based iterative backtracking mechanism, VideoLucy systematically mines\nvideo-wide, question-relevant deep memories until sufficient information is\ngathered to provide a confident answer. This design enables effective temporal\nunderstanding of consecutive frames while preserving critical details. In\naddition, we introduce EgoMem, a new benchmark for long video understanding.\nEgoMem is designed to comprehensively evaluate a model's ability to understand\ncomplex events that unfold over time and capture fine-grained details in\nextremely long videos. Extensive experiments demonstrate the superiority of\nVideoLucy. Built on open-source models, VideoLucy significantly outperforms\nstate-of-the-art methods on multiple long video understanding benchmarks,\nachieving performance even surpassing the latest proprietary models such as\nGPT-4o. Our code and dataset will be made publicly at\nhttps://videolucy.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have shown that agent-based systems leveraging large language\nmodels (LLMs) for key information retrieval and integration have emerged as a\npromising approach for long video understanding. However, these systems face\ntwo major challenges. First, they typically perform modeling and reasoning on\nindividual frames, struggling to capture the temporal context of consecutive\nframes. Second, to reduce the cost of dense frame-level captioning, they adopt\nsparse frame sampling, which risks discarding crucial information. To overcome\nthese limitations, we propose VideoLucy, a deep memory backtracking framework\nfor long video understanding. Inspired by the human recollection process from\ncoarse to fine, VideoLucy employs a hierarchical memory structure with\nprogressive granularity. This structure explicitly defines the detail level and\ntemporal scope of memory at different hierarchical depths. Through an\nagent-based iterative backtracking mechanism, VideoLucy systematically mines\nvideo-wide, question-relevant deep memories until sufficient information is\ngathered to provide a confident answer. This design enables effective temporal\nunderstanding of consecutive frames while preserving critical details. In\naddition, we introduce EgoMem, a new benchmark for long video understanding.\nEgoMem is designed to comprehensively evaluate a model's ability to understand\ncomplex events that unfold over time and capture fine-grained details in\nextremely long videos. Extensive experiments demonstrate the superiority of\nVideoLucy. Built on open-source models, VideoLucy significantly outperforms\nstate-of-the-art methods on multiple long video understanding benchmarks,\nachieving performance even surpassing the latest proprietary models such as\nGPT-4o. Our code and dataset will be made publicly at\nhttps://videolucy.github.io"
                },
                "authors": [
                    {
                        "name": "Jialong Zuo"
                    },
                    {
                        "name": "Yongtai Deng"
                    },
                    {
                        "name": "Lingdong Kong"
                    },
                    {
                        "name": "Jingkang Yang"
                    },
                    {
                        "name": "Rui Jin"
                    },
                    {
                        "name": "Yiwei Zhang"
                    },
                    {
                        "name": "Nong Sang"
                    },
                    {
                        "name": "Liang Pan"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Changxin Gao"
                    }
                ],
                "author_detail": {
                    "name": "Changxin Gao"
                },
                "author": "Changxin Gao",
                "arxiv_comment": "NeurIPS-2025 Accepted Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16548v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16548v2",
                "updated": "2025-10-14T11:57:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    11,
                    57,
                    11,
                    1,
                    287,
                    0
                ],
                "published": "2025-09-20T06:19:55Z",
                "published_parsed": [
                    2025,
                    9,
                    20,
                    6,
                    19,
                    55,
                    5,
                    263,
                    0
                ],
                "title": "SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward\n  Learning"
                },
                "summary": "Process reward models (PRMs) offer fine-grained, step-level evaluations that\nfacilitate deeper reasoning processes in large language models (LLMs), proving\neffective in complex tasks like mathematical reasoning. However, developing\nPRMs is challenging due to the high cost and limited scalability of\nhuman-annotated data. Synthetic data from Monte Carlo (MC) estimation is a\npromising alternative but suffers from a high noise ratio, which can cause\noverfitting and hinder large-scale training. In this work, we conduct a\npreliminary study on the noise distribution in synthetic data from MC\nestimation, identifying that annotation models tend to both underestimate and\noverestimate step correctness due to limitations in their annotation\ncapabilities. Building on these insights, we propose Self-Denoising Monte Carlo\nAnnotation (SCAN), an efficient data synthesis and noise-tolerant learning\nframework. Our key findings indicate that: (1) Even lightweight models (e.g.,\n1.5B parameters) can produce high-quality annotations through a self-denoising\nstrategy, enabling PRMs to achieve superior performance with only 6% the\ninference cost required by vanilla MC estimation. (2) With our robust learning\nstrategy, PRMs can effectively learn from this weak supervision, achieving a\n39.2 F1 score improvement (from 19.9 to 59.1) in ProcessBench. Despite using\nonly a compact synthetic dataset, our models surpass strong baselines,\nincluding those trained on large-scale human-annotated datasets such as\nPRM800K. Furthermore, performance continues to improve as we scale up the\nsynthetic data, highlighting the potential of SCAN for scalable,\ncost-efficient, and robust PRM training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process reward models (PRMs) offer fine-grained, step-level evaluations that\nfacilitate deeper reasoning processes in large language models (LLMs), proving\neffective in complex tasks like mathematical reasoning. However, developing\nPRMs is challenging due to the high cost and limited scalability of\nhuman-annotated data. Synthetic data from Monte Carlo (MC) estimation is a\npromising alternative but suffers from a high noise ratio, which can cause\noverfitting and hinder large-scale training. In this work, we conduct a\npreliminary study on the noise distribution in synthetic data from MC\nestimation, identifying that annotation models tend to both underestimate and\noverestimate step correctness due to limitations in their annotation\ncapabilities. Building on these insights, we propose Self-Denoising Monte Carlo\nAnnotation (SCAN), an efficient data synthesis and noise-tolerant learning\nframework. Our key findings indicate that: (1) Even lightweight models (e.g.,\n1.5B parameters) can produce high-quality annotations through a self-denoising\nstrategy, enabling PRMs to achieve superior performance with only 6% the\ninference cost required by vanilla MC estimation. (2) With our robust learning\nstrategy, PRMs can effectively learn from this weak supervision, achieving a\n39.2 F1 score improvement (from 19.9 to 59.1) in ProcessBench. Despite using\nonly a compact synthetic dataset, our models surpass strong baselines,\nincluding those trained on large-scale human-annotated datasets such as\nPRM800K. Furthermore, performance continues to improve as we scale up the\nsynthetic data, highlighting the potential of SCAN for scalable,\ncost-efficient, and robust PRM training."
                },
                "authors": [
                    {
                        "name": "Yuyang Ding"
                    },
                    {
                        "name": "Xinyu Shi"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Xiaobo Liang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "NeurIPS 2025. Project page: https://scan-prm.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16548v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16548v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12409v1",
                "updated": "2025-10-14T11:42:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    11,
                    42,
                    15,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T11:42:15Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    11,
                    42,
                    15,
                    1,
                    287,
                    0
                ],
                "title": "PricingLogic: Evaluating LLMs Reasoning on Complex Tourism Pricing Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PricingLogic: Evaluating LLMs Reasoning on Complex Tourism Pricing Tasks"
                },
                "summary": "We present PricingLogic, the first benchmark that probes whether Large\nLanguage Models(LLMs) can reliably automate tourism-related prices when\nmultiple, overlapping fare rules apply. Travel agencies are eager to offload\nthis error-prone task onto AI systems; however, deploying LLMs without verified\nreliability could result in significant financial losses and erode customer\ntrust. PricingLogic comprises 300 natural-language questions based on booking\nrequests derived from 42 real-world pricing policies, spanning two levels of\ndifficulty: (i) basic customer-type pricing and (ii)bundled-tour calculations\ninvolving interacting discounts. Evaluations of a line of LLMs reveal a steep\nperformance drop on the harder tier,exposing systematic failures in rule\ninterpretation and arithmetic reasoning.These results highlight that, despite\ntheir general capabilities, today's LLMs remain unreliable in revenue-critical\napplications without further safeguards or domain adaptation. Our code and\ndataset are available at https://github.com/EIT-NLP/PricingLogic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present PricingLogic, the first benchmark that probes whether Large\nLanguage Models(LLMs) can reliably automate tourism-related prices when\nmultiple, overlapping fare rules apply. Travel agencies are eager to offload\nthis error-prone task onto AI systems; however, deploying LLMs without verified\nreliability could result in significant financial losses and erode customer\ntrust. PricingLogic comprises 300 natural-language questions based on booking\nrequests derived from 42 real-world pricing policies, spanning two levels of\ndifficulty: (i) basic customer-type pricing and (ii)bundled-tour calculations\ninvolving interacting discounts. Evaluations of a line of LLMs reveal a steep\nperformance drop on the harder tier,exposing systematic failures in rule\ninterpretation and arithmetic reasoning.These results highlight that, despite\ntheir general capabilities, today's LLMs remain unreliable in revenue-critical\napplications without further safeguards or domain adaptation. Our code and\ndataset are available at https://github.com/EIT-NLP/PricingLogic."
                },
                "authors": [
                    {
                        "name": "Yunuo Liu"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Zena Al-Khalili"
                    },
                    {
                        "name": "Dai Cheng"
                    },
                    {
                        "name": "Yanjun Chen"
                    },
                    {
                        "name": "Dietrich Klakow"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Shen"
                },
                "author": "Xiaoyu Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03536v2",
                "updated": "2025-10-14T11:40:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    11,
                    40,
                    4,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-03T22:11:17Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    22,
                    11,
                    17,
                    4,
                    276,
                    0
                ],
                "title": "Triplet-Structured Knowledge Integration for Multi-Turn Medical\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Triplet-Structured Knowledge Integration for Multi-Turn Medical\n  Reasoning"
                },
                "summary": "Large Language Models (LLMs) have shown strong performance on static medical\nQuestion Answering (QA) tasks, yet their reasoning often deteriorates in\nmulti-turn clinical dialogues where patient information is scattered across\nturns. This paper introduces TriMediQ, a triplet-structured approach that\nenhances the reasoning reliability of LLMs through explicit knowledge\nintegration. TriMediQ first employs a frozen triplet extraction LLM to convert\npatient responses into clinically grounded triplets, ensuring factual precision\nvia constrained prompting. These triplets are incorporated into a\npatient-specific Knowledge Graph (KG), from which a trainable projection module\nconsisting of a graph encoder and a projector captures relational dependencies\nwhile keeping all LLM parameters frozen. During inference, the projection\nmodule guides multi-hop reasoning over the KG, enabling coherent clinical\ndialogue understanding. Experiments on two interactive medical QA benchmarks\nshow that TriMediQ achieves up to 10.4\\% improvement in accuracy over five\nexisting baselines on the iMedQA dataset. These results demonstrate that\nstructuring patient information as triplets can effectively improve the\nreasoning capability of LLMs in multi-turn medical QA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown strong performance on static medical\nQuestion Answering (QA) tasks, yet their reasoning often deteriorates in\nmulti-turn clinical dialogues where patient information is scattered across\nturns. This paper introduces TriMediQ, a triplet-structured approach that\nenhances the reasoning reliability of LLMs through explicit knowledge\nintegration. TriMediQ first employs a frozen triplet extraction LLM to convert\npatient responses into clinically grounded triplets, ensuring factual precision\nvia constrained prompting. These triplets are incorporated into a\npatient-specific Knowledge Graph (KG), from which a trainable projection module\nconsisting of a graph encoder and a projector captures relational dependencies\nwhile keeping all LLM parameters frozen. During inference, the projection\nmodule guides multi-hop reasoning over the KG, enabling coherent clinical\ndialogue understanding. Experiments on two interactive medical QA benchmarks\nshow that TriMediQ achieves up to 10.4\\% improvement in accuracy over five\nexisting baselines on the iMedQA dataset. These results demonstrate that\nstructuring patient information as triplets can effectively improve the\nreasoning capability of LLMs in multi-turn medical QA."
                },
                "authors": [
                    {
                        "name": "Zhaohan Meng"
                    },
                    {
                        "name": "Zaiqiao Meng"
                    },
                    {
                        "name": "Siwei Liu"
                    },
                    {
                        "name": "Iadh Ounis"
                    }
                ],
                "author_detail": {
                    "name": "Iadh Ounis"
                },
                "author": "Iadh Ounis",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.07140v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.07140v6",
                "updated": "2025-10-14T11:34:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    11,
                    34,
                    26,
                    1,
                    287,
                    0
                ],
                "published": "2024-02-11T09:46:24Z",
                "published_parsed": [
                    2024,
                    2,
                    11,
                    9,
                    46,
                    24,
                    6,
                    42,
                    0
                ],
                "title": "Can Graph Descriptive Order Affect Solving Graph Problems with LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Graph Descriptive Order Affect Solving Graph Problems with LLMs?"
                },
                "summary": "Large language models (LLMs) have achieved significant success in reasoning\ntasks, including mathematical reasoning and logical deduction. Among these\nreasoning tasks, graph problems stand out due to their complexity and unique\nstructural characteristics, attracting considerable attention from researchers.\nPrevious studies have explored LLMs' graph reasoning abilities through various\ntechniques, such as different encoding methods for graph structures and the use\nof carefully designed prompts. However, a critical factor has been mostly\noverlooked: the prompt sequential order in which graph descriptions are\npresented to the models. In this study, we present the first comprehensive\nanalysis of how the order of graph descriptions impacts LLM performance.\nSpecifically, we comprehensively evaluate four graph description orders across\nsix graph problems using six mainstream LLMs. The results reveal that: (1)\nordered graph descriptions significantly improve LLMs' comprehension of graph\nstructures; (2) the robustness of LLMs to graph description order varies across\ndifferent tasks; and (3) the impact of graph order on performance is closely\nrelated to the inherent characteristics of tasks. This study provides a\ncritical advancement in the application of LLMs for solving graph-related\nproblems, paving the way for future research to optimize model performance\nthrough strategic graph description ordering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved significant success in reasoning\ntasks, including mathematical reasoning and logical deduction. Among these\nreasoning tasks, graph problems stand out due to their complexity and unique\nstructural characteristics, attracting considerable attention from researchers.\nPrevious studies have explored LLMs' graph reasoning abilities through various\ntechniques, such as different encoding methods for graph structures and the use\nof carefully designed prompts. However, a critical factor has been mostly\noverlooked: the prompt sequential order in which graph descriptions are\npresented to the models. In this study, we present the first comprehensive\nanalysis of how the order of graph descriptions impacts LLM performance.\nSpecifically, we comprehensively evaluate four graph description orders across\nsix graph problems using six mainstream LLMs. The results reveal that: (1)\nordered graph descriptions significantly improve LLMs' comprehension of graph\nstructures; (2) the robustness of LLMs to graph description order varies across\ndifferent tasks; and (3) the impact of graph order on performance is closely\nrelated to the inherent characteristics of tasks. This study provides a\ncritical advancement in the application of LLMs for solving graph-related\nproblems, paving the way for future research to optimize model performance\nthrough strategic graph description ordering."
                },
                "authors": [
                    {
                        "name": "Yuyao Ge"
                    },
                    {
                        "name": "Shenghua Liu"
                    },
                    {
                        "name": "Baolong Bi"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Lingrui Mei"
                    },
                    {
                        "name": "Wenjie Feng"
                    },
                    {
                        "name": "Lizhe Chen"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "Accepted to ACL 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.07140v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.07140v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12399v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12399v1",
                "updated": "2025-10-14T11:26:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    11,
                    26,
                    56,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T11:26:56Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    11,
                    26,
                    56,
                    1,
                    287,
                    0
                ],
                "title": "A Survey of Vibe Coding with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Vibe Coding with Large Language Models"
                },
                "summary": "The advancement of large language models (LLMs) has catalyzed a paradigm\nshift from code generation assistance to autonomous coding agents, enabling a\nnovel development methodology termed \"Vibe Coding\" where developers validate\nAI-generated implementations through outcome observation rather than\nline-by-line code comprehension. Despite its transformative potential, the\neffectiveness of this emergent paradigm remains under-explored, with empirical\nevidence revealing unexpected productivity losses and fundamental challenges in\nhuman-AI collaboration. To address this gap, this survey provides the first\ncomprehensive and systematic review of Vibe Coding with large language models,\nestablishing both theoretical foundations and practical frameworks for this\ntransformative development approach. Drawing from systematic analysis of over\n1000 research papers, we survey the entire vibe coding ecosystem, examining\ncritical infrastructure components including LLMs for coding, LLM-based coding\nagent, development environment of coding agent, and feedback mechanisms. We\nfirst introduce Vibe Coding as a formal discipline by formalizing it through a\nConstrained Markov Decision Process that captures the dynamic triadic\nrelationship among human developers, software projects, and coding agents.\nBuilding upon this theoretical foundation, we then synthesize existing\npractices into five distinct development models: Unconstrained Automation,\nIterative Conversational Collaboration, Planning-Driven, Test-Driven, and\nContext-Enhanced Models, thus providing the first comprehensive taxonomy in\nthis domain. Critically, our analysis reveals that successful Vibe Coding\ndepends not merely on agent capabilities but on systematic context engineering,\nwell-established development environments, and human-agent collaborative\ndevelopment models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of large language models (LLMs) has catalyzed a paradigm\nshift from code generation assistance to autonomous coding agents, enabling a\nnovel development methodology termed \"Vibe Coding\" where developers validate\nAI-generated implementations through outcome observation rather than\nline-by-line code comprehension. Despite its transformative potential, the\neffectiveness of this emergent paradigm remains under-explored, with empirical\nevidence revealing unexpected productivity losses and fundamental challenges in\nhuman-AI collaboration. To address this gap, this survey provides the first\ncomprehensive and systematic review of Vibe Coding with large language models,\nestablishing both theoretical foundations and practical frameworks for this\ntransformative development approach. Drawing from systematic analysis of over\n1000 research papers, we survey the entire vibe coding ecosystem, examining\ncritical infrastructure components including LLMs for coding, LLM-based coding\nagent, development environment of coding agent, and feedback mechanisms. We\nfirst introduce Vibe Coding as a formal discipline by formalizing it through a\nConstrained Markov Decision Process that captures the dynamic triadic\nrelationship among human developers, software projects, and coding agents.\nBuilding upon this theoretical foundation, we then synthesize existing\npractices into five distinct development models: Unconstrained Automation,\nIterative Conversational Collaboration, Planning-Driven, Test-Driven, and\nContext-Enhanced Models, thus providing the first comprehensive taxonomy in\nthis domain. Critically, our analysis reveals that successful Vibe Coding\ndepends not merely on agent capabilities but on systematic context engineering,\nwell-established development environments, and human-agent collaborative\ndevelopment models."
                },
                "authors": [
                    {
                        "name": "Yuyao Ge"
                    },
                    {
                        "name": "Lingrui Mei"
                    },
                    {
                        "name": "Zenghao Duan"
                    },
                    {
                        "name": "Tianhao Li"
                    },
                    {
                        "name": "Yujia Zheng"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Lexin Wang"
                    },
                    {
                        "name": "Jiayu Yao"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Yujun Cai"
                    },
                    {
                        "name": "Baolong Bi"
                    },
                    {
                        "name": "Fangda Guo"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Shenghua Liu"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12399v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12399v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10661v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10661v2",
                "updated": "2025-10-14T11:24:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    11,
                    24,
                    4,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-12T15:35:05Z",
                "published_parsed": [
                    2025,
                    10,
                    12,
                    15,
                    35,
                    5,
                    6,
                    285,
                    0
                ],
                "title": "AGENTIQL: An Agent-Inspired Multi-Expert Framework for Text-to-SQL\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGENTIQL: An Agent-Inspired Multi-Expert Framework for Text-to-SQL\n  Generation"
                },
                "summary": "LLMs have advanced text-to-SQL generation, yet monolithic architectures\nstruggle with complex reasoning and schema diversity. We propose AGENTIQL, an\nagent-inspired multi-expert framework that combines a reasoning agent for\nquestion decomposition, a coding agent for sub-query generation, and a\nrefinement step for column selection. An adaptive router further balances\nefficiency and accuracy by selecting between our modular pipeline and a\nbaseline parser. Several steps in the pipeline can be executed in parallel,\nmaking the framework scalable to larger workloads. Evaluated on the Spider\nbenchmark, AGENTIQL improves execution accuracy and interpretability and\nachieves up to 86.07% EX with 14B models using the Planner&Executor merging\nstrategy. The attained performance is contingent upon the efficacy of the\nrouting mechanism, thereby narrowing the gap to GPT-4-based SOTA (89.65% EX)\nwhile using much smaller open-source LLMs. Beyond accuracy, AGENTIQL enhances\ntransparency by exposing intermediate reasoning steps, offering a robust,\nscalable, and interpretable approach to semantic parsing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have advanced text-to-SQL generation, yet monolithic architectures\nstruggle with complex reasoning and schema diversity. We propose AGENTIQL, an\nagent-inspired multi-expert framework that combines a reasoning agent for\nquestion decomposition, a coding agent for sub-query generation, and a\nrefinement step for column selection. An adaptive router further balances\nefficiency and accuracy by selecting between our modular pipeline and a\nbaseline parser. Several steps in the pipeline can be executed in parallel,\nmaking the framework scalable to larger workloads. Evaluated on the Spider\nbenchmark, AGENTIQL improves execution accuracy and interpretability and\nachieves up to 86.07% EX with 14B models using the Planner&Executor merging\nstrategy. The attained performance is contingent upon the efficacy of the\nrouting mechanism, thereby narrowing the gap to GPT-4-based SOTA (89.65% EX)\nwhile using much smaller open-source LLMs. Beyond accuracy, AGENTIQL enhances\ntransparency by exposing intermediate reasoning steps, offering a robust,\nscalable, and interpretable approach to semantic parsing."
                },
                "authors": [
                    {
                        "name": "Omid Reza Heidari"
                    },
                    {
                        "name": "Siobhan Reid"
                    },
                    {
                        "name": "Yassine Yaakoubi"
                    }
                ],
                "author_detail": {
                    "name": "Yassine Yaakoubi"
                },
                "author": "Yassine Yaakoubi",
                "arxiv_comment": "Accepted at NeurIPS 2025, ER \"Efficient Reasoning\" workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10661v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10661v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22337v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22337v3",
                "updated": "2025-10-14T11:20:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    11,
                    20,
                    7,
                    1,
                    287,
                    0
                ],
                "published": "2025-07-30T02:44:20Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    2,
                    44,
                    20,
                    2,
                    211,
                    0
                ],
                "title": "A Comprehensive Taxonomy of Negation for NLP and Neural Retrievers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Taxonomy of Negation for NLP and Neural Retrievers"
                },
                "summary": "Understanding and solving complex reasoning tasks is vital for addressing the\ninformation needs of a user. Although dense neural models learn contextualised\nembeddings, they still underperform on queries containing negation. To\nunderstand this phenomenon, we study negation in both traditional neural\ninformation retrieval and LLM-based models. We (1) introduce a taxonomy of\nnegation that derives from philosophical, linguistic, and logical definitions;\n(2) generate two benchmark datasets that can be used to evaluate the\nperformance of neural information retrieval models and to fine-tune models for\na more robust performance on negation; and (3) propose a logic-based\nclassification mechanism that can be used to analyze the performance of\nretrieval models on existing datasets. Our taxonomy produces a balanced data\ndistribution over negation types, providing a better training setup that leads\nto faster convergence on the NevIR dataset. Moreover, we propose a\nclassification schema that reveals the coverage of negation types in existing\ndatasets, offering insights into the factors that might affect the\ngeneralization of fine-tuned models on negation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and solving complex reasoning tasks is vital for addressing the\ninformation needs of a user. Although dense neural models learn contextualised\nembeddings, they still underperform on queries containing negation. To\nunderstand this phenomenon, we study negation in both traditional neural\ninformation retrieval and LLM-based models. We (1) introduce a taxonomy of\nnegation that derives from philosophical, linguistic, and logical definitions;\n(2) generate two benchmark datasets that can be used to evaluate the\nperformance of neural information retrieval models and to fine-tune models for\na more robust performance on negation; and (3) propose a logic-based\nclassification mechanism that can be used to analyze the performance of\nretrieval models on existing datasets. Our taxonomy produces a balanced data\ndistribution over negation types, providing a better training setup that leads\nto faster convergence on the NevIR dataset. Moreover, we propose a\nclassification schema that reveals the coverage of negation types in existing\ndatasets, offering insights into the factors that might affect the\ngeneralization of fine-tuned models on negation."
                },
                "authors": [
                    {
                        "name": "Roxana Petcu"
                    },
                    {
                        "name": "Samarth Bhargav"
                    },
                    {
                        "name": "Maarten de Rijke"
                    },
                    {
                        "name": "Evangelos Kanoulas"
                    }
                ],
                "author_detail": {
                    "name": "Evangelos Kanoulas"
                },
                "author": "Evangelos Kanoulas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22337v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22337v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12389v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12389v1",
                "updated": "2025-10-14T11:14:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    11,
                    14,
                    38,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T11:14:38Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    11,
                    14,
                    38,
                    1,
                    287,
                    0
                ],
                "title": "Tokenization Disparities as Infrastructure Bias: How Subword Systems\n  Create Inequities in LLM Access and Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization Disparities as Infrastructure Bias: How Subword Systems\n  Create Inequities in LLM Access and Efficiency"
                },
                "summary": "Tokenization disparities pose a significant barrier to achieving equitable\naccess to artificial intelligence across linguistically diverse populations.\nThis study conducts a large-scale cross-linguistic evaluation of tokenization\nefficiency in over 200 languages to systematically quantify computational\ninequities in large language models (LLMs). Using a standardized experimental\nframework, we applied consistent preprocessing and normalization protocols,\nfollowed by uniform tokenization through the tiktoken library across all\nlanguage samples. Comprehensive tokenization statistics were collected using\nestablished evaluation metrics, including Tokens Per Sentence (TPS) and\nRelative Tokenization Cost (RTC), benchmarked against English baselines. Our\ncross-linguistic analysis reveals substantial and systematic disparities:\nLatin-script languages consistently exhibit higher tokenization efficiency,\nwhile non-Latin and morphologically complex languages incur significantly\ngreater token inflation, often 3-5 times higher RTC ratios. These\ninefficiencies translate into increased computational costs and reduced\neffective context utilization for underrepresented languages. Overall, the\nfindings highlight structural inequities in current AI systems, where speakers\nof low-resource and non-Latin languages face disproportionate computational\ndisadvantages. Future research should prioritize the development of\nlinguistically informed tokenization strategies and adaptive vocabulary\nconstruction methods that incorporate typological diversity, ensuring more\ninclusive and computationally equitable multilingual AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization disparities pose a significant barrier to achieving equitable\naccess to artificial intelligence across linguistically diverse populations.\nThis study conducts a large-scale cross-linguistic evaluation of tokenization\nefficiency in over 200 languages to systematically quantify computational\ninequities in large language models (LLMs). Using a standardized experimental\nframework, we applied consistent preprocessing and normalization protocols,\nfollowed by uniform tokenization through the tiktoken library across all\nlanguage samples. Comprehensive tokenization statistics were collected using\nestablished evaluation metrics, including Tokens Per Sentence (TPS) and\nRelative Tokenization Cost (RTC), benchmarked against English baselines. Our\ncross-linguistic analysis reveals substantial and systematic disparities:\nLatin-script languages consistently exhibit higher tokenization efficiency,\nwhile non-Latin and morphologically complex languages incur significantly\ngreater token inflation, often 3-5 times higher RTC ratios. These\ninefficiencies translate into increased computational costs and reduced\neffective context utilization for underrepresented languages. Overall, the\nfindings highlight structural inequities in current AI systems, where speakers\nof low-resource and non-Latin languages face disproportionate computational\ndisadvantages. Future research should prioritize the development of\nlinguistically informed tokenization strategies and adaptive vocabulary\nconstruction methods that incorporate typological diversity, ensuring more\ninclusive and computationally equitable multilingual AI systems."
                },
                "authors": [
                    {
                        "name": "Hailay Kidu Teklehaymanot"
                    },
                    {
                        "name": "Wolfgang Nejdl"
                    }
                ],
                "author_detail": {
                    "name": "Wolfgang Nejdl"
                },
                "author": "Wolfgang Nejdl",
                "arxiv_comment": "6 pages 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12389v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12389v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.1; H.3.3; F.2.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12386v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12386v1",
                "updated": "2025-10-14T11:10:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    11,
                    10,
                    35,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T11:10:35Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    11,
                    10,
                    35,
                    1,
                    287,
                    0
                ],
                "title": "Hey Dashboard!: Supporting Voice, Text, and Pointing Modalities in\n  Dashboard Onboarding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hey Dashboard!: Supporting Voice, Text, and Pointing Modalities in\n  Dashboard Onboarding"
                },
                "summary": "Visualization dashboards are regularly used for data exploration and\nanalysis, but their complex interactions and interlinked views often require\ntime-consuming onboarding sessions from dashboard authors. Preparing these\nonboarding materials is labor-intensive and requires manual updates when\ndashboards change. Recent advances in multimodal interaction powered by large\nlanguage models (LLMs) provide ways to support self-guided onboarding. We\npresent DIANA (Dashboard Interactive Assistant for Navigation and Analysis), a\nmultimodal dashboard assistant that helps users for navigation and guided\nanalysis through chat, audio, and mouse-based interactions. Users can choose\nany interaction modality or a combination of them to onboard themselves on the\ndashboard. Each modality highlights relevant dashboard features to support user\norientation. Unlike typical LLM systems that rely solely on text-based chat,\nDIANA combines multiple modalities to provide explanations directly in the\ndashboard interface. We conducted a qualitative user study to understand the\nuse of different modalities for different types of onboarding tasks and their\ncomplexities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visualization dashboards are regularly used for data exploration and\nanalysis, but their complex interactions and interlinked views often require\ntime-consuming onboarding sessions from dashboard authors. Preparing these\nonboarding materials is labor-intensive and requires manual updates when\ndashboards change. Recent advances in multimodal interaction powered by large\nlanguage models (LLMs) provide ways to support self-guided onboarding. We\npresent DIANA (Dashboard Interactive Assistant for Navigation and Analysis), a\nmultimodal dashboard assistant that helps users for navigation and guided\nanalysis through chat, audio, and mouse-based interactions. Users can choose\nany interaction modality or a combination of them to onboard themselves on the\ndashboard. Each modality highlights relevant dashboard features to support user\norientation. Unlike typical LLM systems that rely solely on text-based chat,\nDIANA combines multiple modalities to provide explanations directly in the\ndashboard interface. We conducted a qualitative user study to understand the\nuse of different modalities for different types of onboarding tasks and their\ncomplexities."
                },
                "authors": [
                    {
                        "name": "Vaishali Dhanoa"
                    },
                    {
                        "name": "Gabriela Molina Len"
                    },
                    {
                        "name": "Eve Hoggan"
                    },
                    {
                        "name": "Eduard Grller"
                    },
                    {
                        "name": "Marc Streit"
                    },
                    {
                        "name": "Niklas Elmqvist"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Elmqvist"
                },
                "author": "Niklas Elmqvist",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12386v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12386v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12367v1",
                "updated": "2025-10-14T10:30:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    10,
                    30,
                    20,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T10:30:20Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    10,
                    30,
                    20,
                    1,
                    287,
                    0
                ],
                "title": "LLM-REVal: Can We Trust LLM Reviewers Yet?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-REVal: Can We Trust LLM Reviewers Yet?"
                },
                "summary": "The rapid advancement of large language models (LLMs) has inspired\nresearchers to integrate them extensively into the academic workflow,\npotentially reshaping how research is practiced and reviewed. While previous\nstudies highlight the potential of LLMs in supporting research and peer review,\ntheir dual roles in the academic workflow and the complex interplay between\nresearch and review bring new risks that remain largely underexplored. In this\nstudy, we focus on how the deep integration of LLMs into both peer-review and\nresearch processes may influence scholarly fairness, examining the potential\nrisks of using LLMs as reviewers by simulation. This simulation incorporates a\nresearch agent, which generates papers and revises, alongside a review agent,\nwhich assesses the submissions. Based on the simulation results, we conduct\nhuman annotations and identify pronounced misalignment between LLM-based\nreviews and human judgments: (1) LLM reviewers systematically inflate scores\nfor LLM-authored papers, assigning them markedly higher scores than\nhuman-authored ones; (2) LLM reviewers persistently underrate human-authored\npapers with critical statements (e.g., risk, fairness), even after multiple\nrevisions. Our analysis reveals that these stem from two primary biases in LLM\nreviewers: a linguistic feature bias favoring LLM-generated writing styles, and\nan aversion toward critical statements. These results highlight the risks and\nequity concerns posed to human authors and academic research if LLMs are\ndeployed in the peer review cycle without adequate caution. On the other hand,\nrevisions guided by LLM reviews yield quality gains in both LLM-based and human\nevaluations, illustrating the potential of the LLMs-as-reviewers for\nearly-stage researchers and enhancing low-quality papers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has inspired\nresearchers to integrate them extensively into the academic workflow,\npotentially reshaping how research is practiced and reviewed. While previous\nstudies highlight the potential of LLMs in supporting research and peer review,\ntheir dual roles in the academic workflow and the complex interplay between\nresearch and review bring new risks that remain largely underexplored. In this\nstudy, we focus on how the deep integration of LLMs into both peer-review and\nresearch processes may influence scholarly fairness, examining the potential\nrisks of using LLMs as reviewers by simulation. This simulation incorporates a\nresearch agent, which generates papers and revises, alongside a review agent,\nwhich assesses the submissions. Based on the simulation results, we conduct\nhuman annotations and identify pronounced misalignment between LLM-based\nreviews and human judgments: (1) LLM reviewers systematically inflate scores\nfor LLM-authored papers, assigning them markedly higher scores than\nhuman-authored ones; (2) LLM reviewers persistently underrate human-authored\npapers with critical statements (e.g., risk, fairness), even after multiple\nrevisions. Our analysis reveals that these stem from two primary biases in LLM\nreviewers: a linguistic feature bias favoring LLM-generated writing styles, and\nan aversion toward critical statements. These results highlight the risks and\nequity concerns posed to human authors and academic research if LLMs are\ndeployed in the peer review cycle without adequate caution. On the other hand,\nrevisions guided by LLM reviews yield quality gains in both LLM-based and human\nevaluations, illustrating the potential of the LLMs-as-reviewers for\nearly-stage researchers and enhancing low-quality papers."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Jia-Chen Gu"
                    },
                    {
                        "name": "Po-Nien Kung"
                    },
                    {
                        "name": "Heming Xia"
                    },
                    {
                        "name": "Junfeng liu"
                    },
                    {
                        "name": "Xiangwen Kong"
                    },
                    {
                        "name": "Zhifang Sui"
                    },
                    {
                        "name": "Nanyun Peng"
                    }
                ],
                "author_detail": {
                    "name": "Nanyun Peng"
                },
                "author": "Nanyun Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12362v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12362v1",
                "updated": "2025-10-14T10:25:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    10,
                    25,
                    26,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T10:25:26Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    10,
                    25,
                    26,
                    1,
                    287,
                    0
                ],
                "title": "CurriFlow: Curriculum-Guided Depth Fusion with Optical Flow-Based\n  Temporal Alignment for 3D Semantic Scene Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CurriFlow: Curriculum-Guided Depth Fusion with Optical Flow-Based\n  Temporal Alignment for 3D Semantic Scene Completion"
                },
                "summary": "Semantic Scene Completion (SSC) aims to infer complete 3D geometry and\nsemantics from monocular images, serving as a crucial capability for\ncamera-based perception in autonomous driving. However, existing SSC methods\nrelying on temporal stacking or depth projection often lack explicit motion\nreasoning and struggle with occlusions and noisy depth supervision. We propose\nCurriFlow, a novel semantic occupancy prediction framework that integrates\noptical flow-based temporal alignment with curriculum-guided depth fusion.\nCurriFlow employs a multi-level fusion strategy to align segmentation, visual,\nand depth features across frames using pre-trained optical flow, thereby\nimproving temporal consistency and dynamic object understanding. To enhance\ngeometric robustness, a curriculum learning mechanism progressively transitions\nfrom sparse yet accurate LiDAR depth to dense but noisy stereo depth during\ntraining, ensuring stable optimization and seamless adaptation to real-world\ndeployment. Furthermore, semantic priors from the Segment Anything Model (SAM)\nprovide category-agnostic supervision, strengthening voxel-level semantic\nlearning and spatial consistency. Experiments on the SemanticKITTI benchmark\ndemonstrate that CurriFlow achieves state-of-the-art performance with a mean\nIoU of 16.9, validating the effectiveness of our motion-guided and\ncurriculum-aware design for camera-based 3D semantic scene completion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Scene Completion (SSC) aims to infer complete 3D geometry and\nsemantics from monocular images, serving as a crucial capability for\ncamera-based perception in autonomous driving. However, existing SSC methods\nrelying on temporal stacking or depth projection often lack explicit motion\nreasoning and struggle with occlusions and noisy depth supervision. We propose\nCurriFlow, a novel semantic occupancy prediction framework that integrates\noptical flow-based temporal alignment with curriculum-guided depth fusion.\nCurriFlow employs a multi-level fusion strategy to align segmentation, visual,\nand depth features across frames using pre-trained optical flow, thereby\nimproving temporal consistency and dynamic object understanding. To enhance\ngeometric robustness, a curriculum learning mechanism progressively transitions\nfrom sparse yet accurate LiDAR depth to dense but noisy stereo depth during\ntraining, ensuring stable optimization and seamless adaptation to real-world\ndeployment. Furthermore, semantic priors from the Segment Anything Model (SAM)\nprovide category-agnostic supervision, strengthening voxel-level semantic\nlearning and spatial consistency. Experiments on the SemanticKITTI benchmark\ndemonstrate that CurriFlow achieves state-of-the-art performance with a mean\nIoU of 16.9, validating the effectiveness of our motion-guided and\ncurriculum-aware design for camera-based 3D semantic scene completion."
                },
                "authors": [
                    {
                        "name": "Jinzhou Lin"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Wenhao Xu"
                    },
                    {
                        "name": "Rongtao Xu"
                    },
                    {
                        "name": "Changwei Wang"
                    },
                    {
                        "name": "Shunpeng Chen"
                    },
                    {
                        "name": "Kexue Fu"
                    },
                    {
                        "name": "Yihua Shao"
                    },
                    {
                        "name": "Li Guo"
                    },
                    {
                        "name": "Shibiao Xu"
                    }
                ],
                "author_detail": {
                    "name": "Shibiao Xu"
                },
                "author": "Shibiao Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12362v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12362v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12355v1",
                "updated": "2025-10-14T10:19:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    10,
                    19,
                    1,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T10:19:01Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    10,
                    19,
                    1,
                    1,
                    287,
                    0
                ],
                "title": "Fine-grained Analysis of Brain-LLM Alignment through Input Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-grained Analysis of Brain-LLM Alignment through Input Attribution"
                },
                "summary": "Understanding the alignment between large language models (LLMs) and human\nbrain activity can reveal computational principles underlying language\nprocessing. We introduce a fine-grained input attribution method to identify\nthe specific words most important for brain-LLM alignment, and leverage it to\nstudy a contentious research question about brain-LLM alignment: the\nrelationship between brain alignment (BA) and next-word prediction (NWP). Our\nfindings reveal that BA and NWP rely on largely distinct word subsets: NWP\nexhibits recency and primacy biases with a focus on syntax, while BA\nprioritizes semantic and discourse-level information with a more targeted\nrecency effect. This work advances our understanding of how LLMs relate to\nhuman language processing and highlights differences in feature reliance\nbetween BA and NWP. Beyond this study, our attribution method can be broadly\napplied to explore the cognitive relevance of model predictions in diverse\nlanguage processing tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the alignment between large language models (LLMs) and human\nbrain activity can reveal computational principles underlying language\nprocessing. We introduce a fine-grained input attribution method to identify\nthe specific words most important for brain-LLM alignment, and leverage it to\nstudy a contentious research question about brain-LLM alignment: the\nrelationship between brain alignment (BA) and next-word prediction (NWP). Our\nfindings reveal that BA and NWP rely on largely distinct word subsets: NWP\nexhibits recency and primacy biases with a focus on syntax, while BA\nprioritizes semantic and discourse-level information with a more targeted\nrecency effect. This work advances our understanding of how LLMs relate to\nhuman language processing and highlights differences in feature reliance\nbetween BA and NWP. Beyond this study, our attribution method can be broadly\napplied to explore the cognitive relevance of model predictions in diverse\nlanguage processing tasks."
                },
                "authors": [
                    {
                        "name": "Michela Proietti"
                    },
                    {
                        "name": "Roberto Capobianco"
                    },
                    {
                        "name": "Mariya Toneva"
                    }
                ],
                "author_detail": {
                    "name": "Mariya Toneva"
                },
                "author": "Mariya Toneva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10764v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10764v2",
                "updated": "2025-10-14T10:17:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    10,
                    17,
                    25,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-12T19:05:04Z",
                "published_parsed": [
                    2025,
                    10,
                    12,
                    19,
                    5,
                    4,
                    6,
                    285,
                    0
                ],
                "title": "Optimally Deep Networks -- Adapting Model Depth to Datasets for Superior\n  Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimally Deep Networks -- Adapting Model Depth to Datasets for Superior\n  Efficiency"
                },
                "summary": "Deep neural networks (DNNs) have provided brilliant performance across\nvarious tasks. However, this success often comes at the cost of unnecessarily\nlarge model sizes, high computational demands, and substantial memory\nfootprints. Typically, powerful architectures are trained at full depths but\nnot all datasets or tasks require such high model capacity. Training very deep\narchitectures on relatively low-complexity datasets frequently leads to wasted\ncomputation, unnecessary energy consumption, and excessive memory usage, which\nin turn makes deployment of models on resource-constrained devices impractical.\nTo address this problem, we introduce Optimally Deep Networks (ODNs), which\nprovide a balance between model depth and task complexity. Specifically, we\npropose a NAS like training strategy called progressive depth expansion, which\nbegins by training deep networks at shallower depths and incrementally\nincreases their depth as the earlier blocks converge, continuing this process\nuntil the target accuracy is reached. ODNs use only the optimal depth for the\ngiven datasets, removing redundant layers. This cuts down future training and\ninference costs, lowers the memory footprint, enhances computational\nefficiency, and facilitates deployment on edge devices. Empirical results show\nthat the optimal depths of ResNet-18 and ResNet-34 for MNIST and SVHN, achieve\nup to 98.64 % and 96.44 % reduction in memory footprint, while maintaining a\ncompetitive accuracy of 99.31 % and 96.08 %, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural networks (DNNs) have provided brilliant performance across\nvarious tasks. However, this success often comes at the cost of unnecessarily\nlarge model sizes, high computational demands, and substantial memory\nfootprints. Typically, powerful architectures are trained at full depths but\nnot all datasets or tasks require such high model capacity. Training very deep\narchitectures on relatively low-complexity datasets frequently leads to wasted\ncomputation, unnecessary energy consumption, and excessive memory usage, which\nin turn makes deployment of models on resource-constrained devices impractical.\nTo address this problem, we introduce Optimally Deep Networks (ODNs), which\nprovide a balance between model depth and task complexity. Specifically, we\npropose a NAS like training strategy called progressive depth expansion, which\nbegins by training deep networks at shallower depths and incrementally\nincreases their depth as the earlier blocks converge, continuing this process\nuntil the target accuracy is reached. ODNs use only the optimal depth for the\ngiven datasets, removing redundant layers. This cuts down future training and\ninference costs, lowers the memory footprint, enhances computational\nefficiency, and facilitates deployment on edge devices. Empirical results show\nthat the optimal depths of ResNet-18 and ResNet-34 for MNIST and SVHN, achieve\nup to 98.64 % and 96.44 % reduction in memory footprint, while maintaining a\ncompetitive accuracy of 99.31 % and 96.08 %, respectively."
                },
                "authors": [
                    {
                        "name": "Shaharyar Ahmed Khan Tareen"
                    },
                    {
                        "name": "Filza Khan Tareen"
                    }
                ],
                "author_detail": {
                    "name": "Filza Khan Tareen"
                },
                "author": "Filza Khan Tareen",
                "arxiv_comment": "6 pages, 3 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10764v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10764v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10815v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10815v2",
                "updated": "2025-10-14T10:15:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    10,
                    15,
                    4,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-12T21:42:04Z",
                "published_parsed": [
                    2025,
                    10,
                    12,
                    21,
                    42,
                    4,
                    6,
                    285,
                    0
                ],
                "title": "DRIFT: Decompose, Retrieve, Illustrate, then Formalize Theorems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRIFT: Decompose, Retrieve, Illustrate, then Formalize Theorems"
                },
                "summary": "Automating the formalization of mathematical statements for theorem proving\nremains a major challenge for Large Language Models (LLMs). LLMs struggle to\nidentify and utilize the prerequisite mathematical knowledge and its\ncorresponding formal representation in languages like Lean. Current\nretrieval-augmented autoformalization methods query external libraries using\nthe informal statement directly, but overlook a fundamental limitation:\ninformal mathematical statements are often complex and offer limited context on\nthe underlying math concepts. To address this, we introduce DRIFT, a novel\nframework that enables LLMs to decompose informal mathematical statements into\nsmaller, more tractable ''sub-components''. This facilitates targeted retrieval\nof premises from mathematical libraries such as Mathlib. Additionally, DRIFT\nretrieves illustrative theorems to help models use premises more effectively in\nformalization tasks. We evaluate DRIFT across diverse benchmarks (ProofNet,\nConNF, and MiniF2F-test) and find that it consistently improves premise\nretrieval, nearly doubling the F1 score compared to the DPR baseline on\nProofNet. Notably, DRIFT demonstrates strong performance on the\nout-of-distribution ConNF benchmark, with BEq+@10 improvements of 37.14% and\n42.25% using GPT-4.1 and DeepSeek-V3.1, respectively. Our analysis shows that\nretrieval effectiveness in mathematical autoformalization depends heavily on\nmodel-specific knowledge boundaries, highlighting the need for adaptive\nretrieval strategies aligned with each model's capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating the formalization of mathematical statements for theorem proving\nremains a major challenge for Large Language Models (LLMs). LLMs struggle to\nidentify and utilize the prerequisite mathematical knowledge and its\ncorresponding formal representation in languages like Lean. Current\nretrieval-augmented autoformalization methods query external libraries using\nthe informal statement directly, but overlook a fundamental limitation:\ninformal mathematical statements are often complex and offer limited context on\nthe underlying math concepts. To address this, we introduce DRIFT, a novel\nframework that enables LLMs to decompose informal mathematical statements into\nsmaller, more tractable ''sub-components''. This facilitates targeted retrieval\nof premises from mathematical libraries such as Mathlib. Additionally, DRIFT\nretrieves illustrative theorems to help models use premises more effectively in\nformalization tasks. We evaluate DRIFT across diverse benchmarks (ProofNet,\nConNF, and MiniF2F-test) and find that it consistently improves premise\nretrieval, nearly doubling the F1 score compared to the DPR baseline on\nProofNet. Notably, DRIFT demonstrates strong performance on the\nout-of-distribution ConNF benchmark, with BEq+@10 improvements of 37.14% and\n42.25% using GPT-4.1 and DeepSeek-V3.1, respectively. Our analysis shows that\nretrieval effectiveness in mathematical autoformalization depends heavily on\nmodel-specific knowledge boundaries, highlighting the need for adaptive\nretrieval strategies aligned with each model's capabilities."
                },
                "authors": [
                    {
                        "name": "Meiru Zhang"
                    },
                    {
                        "name": "Philipp Borchert"
                    },
                    {
                        "name": "Milan Gritta"
                    },
                    {
                        "name": "Gerasimos Lampouras"
                    }
                ],
                "author_detail": {
                    "name": "Gerasimos Lampouras"
                },
                "author": "Gerasimos Lampouras",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10815v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10815v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13635v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13635v3",
                "updated": "2025-10-14T10:14:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    10,
                    14,
                    8,
                    1,
                    287,
                    0
                ],
                "published": "2025-08-19T08:48:05Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    8,
                    48,
                    5,
                    1,
                    231,
                    0
                ],
                "title": "Interpreting the Interpreter: Can We Model post-ECB Conferences\n  Volatility with LLM Agents?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpreting the Interpreter: Can We Model post-ECB Conferences\n  Volatility with LLM Agents?"
                },
                "summary": "Traditional high-frequency identification of monetary policy communication\neffects operates ex-post, precluding evaluation of alternative strategies\nbefore publication. In this paper, we introduce an ex-ante framework that\nsimulates heterogeneous market reactions to central bank communication before\nrelease. Our methodology employs Large Language Models (LLMs) to construct an\nagent-based simulation of 30 synthetic traders with heterogeneous risk\npreferences, cognitive biases, and interpretive styles. These agents process\nEuropean Central Bank (ECB) press conference transcripts and forecast Euro\ninterest rate swap rates across 3-month, 2-year, and 10-year maturities.\nCross-sectional forecast dispersion provides a model-based measure of market\ndisagreement, validated against realized overnight index swap (OIS) volatility.\nAnalyzing 283 ECB press conferences (June 1998-April 2025), we document\nSpearman correlations of approximately 0.5 between simulated and realized\ndisagreement, rising to 0.6 under iterative prompt optimization. Results prove\nrobust across prompting strategies, are temporally stable across training and\nholdout samples, and fare significantly better than simple language complexity\nscores. For central banks, the framework provides an operational tool to\nanticipate communication-induced volatility before release, thus enabling\nex-ante language refinement. For researchers, it offers a micro-founded\nalternative to reduced-form event studies, explicitly modeling the\nheterogeneous interpretive processes through which policy signals are\ntransmitted to asset prices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional high-frequency identification of monetary policy communication\neffects operates ex-post, precluding evaluation of alternative strategies\nbefore publication. In this paper, we introduce an ex-ante framework that\nsimulates heterogeneous market reactions to central bank communication before\nrelease. Our methodology employs Large Language Models (LLMs) to construct an\nagent-based simulation of 30 synthetic traders with heterogeneous risk\npreferences, cognitive biases, and interpretive styles. These agents process\nEuropean Central Bank (ECB) press conference transcripts and forecast Euro\ninterest rate swap rates across 3-month, 2-year, and 10-year maturities.\nCross-sectional forecast dispersion provides a model-based measure of market\ndisagreement, validated against realized overnight index swap (OIS) volatility.\nAnalyzing 283 ECB press conferences (June 1998-April 2025), we document\nSpearman correlations of approximately 0.5 between simulated and realized\ndisagreement, rising to 0.6 under iterative prompt optimization. Results prove\nrobust across prompting strategies, are temporally stable across training and\nholdout samples, and fare significantly better than simple language complexity\nscores. For central banks, the framework provides an operational tool to\nanticipate communication-induced volatility before release, thus enabling\nex-ante language refinement. For researchers, it offers a micro-founded\nalternative to reduced-form event studies, explicitly modeling the\nheterogeneous interpretive processes through which policy signals are\ntransmitted to asset prices."
                },
                "authors": [
                    {
                        "name": "Umberto Collodel"
                    }
                ],
                "author_detail": {
                    "name": "Umberto Collodel"
                },
                "author": "Umberto Collodel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13635v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13635v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12350v1",
                "updated": "2025-10-14T10:07:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    10,
                    7,
                    53,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T10:07:53Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    10,
                    7,
                    53,
                    1,
                    287,
                    0
                ],
                "title": "O-Forge: An LLM + Computer Algebra Framework for Asymptotic Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "O-Forge: An LLM + Computer Algebra Framework for Asymptotic Analysis"
                },
                "summary": "Large language models have recently demonstrated advanced capabilities in\nsolving IMO and Putnam problems; yet their role in research mathematics has\nremained fairly limited. The key difficulty is verification: suggested proofs\nmay look plausible, but cannot be trusted without rigorous checking. We present\na framework, called LLM+CAS, and an associated tool, O-Forge, that couples\nfrontier LLMs with a computer algebra systems (CAS) in an In-Context Symbolic\nFeedback loop to produce proofs that are both creative and symbolically\nverified. Our focus is on asymptotic inequalities, a topic that often involves\ndifficult proofs and appropriate decomposition of the domain into the \"right\"\nsubdomains. Many mathematicians, including Terry Tao, have suggested that using\nAI tools to find the right decompositions can be very useful for research-level\nasymptotic analysis. In this paper, we show that our framework LLM+CAS turns\nout to be remarkably effective at proposing such decompositions via a\ncombination of a frontier LLM and a CAS. More precisely, we use an LLM to\nsuggest domain decomposition, and a CAS (such as Mathematica) that provides a\nverification of each piece axiomatically. Using this loop, we answer a question\nposed by Terence Tao: whether LLMs coupled with a verifier can be used to help\nprove intricate asymptotic inequalities. More broadly, we show how AI can move\nbeyond contest math towards research-level tools for professional\nmathematicians.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have recently demonstrated advanced capabilities in\nsolving IMO and Putnam problems; yet their role in research mathematics has\nremained fairly limited. The key difficulty is verification: suggested proofs\nmay look plausible, but cannot be trusted without rigorous checking. We present\na framework, called LLM+CAS, and an associated tool, O-Forge, that couples\nfrontier LLMs with a computer algebra systems (CAS) in an In-Context Symbolic\nFeedback loop to produce proofs that are both creative and symbolically\nverified. Our focus is on asymptotic inequalities, a topic that often involves\ndifficult proofs and appropriate decomposition of the domain into the \"right\"\nsubdomains. Many mathematicians, including Terry Tao, have suggested that using\nAI tools to find the right decompositions can be very useful for research-level\nasymptotic analysis. In this paper, we show that our framework LLM+CAS turns\nout to be remarkably effective at proposing such decompositions via a\ncombination of a frontier LLM and a CAS. More precisely, we use an LLM to\nsuggest domain decomposition, and a CAS (such as Mathematica) that provides a\nverification of each piece axiomatically. Using this loop, we answer a question\nposed by Terence Tao: whether LLMs coupled with a verifier can be used to help\nprove intricate asymptotic inequalities. More broadly, we show how AI can move\nbeyond contest math towards research-level tools for professional\nmathematicians."
                },
                "authors": [
                    {
                        "name": "Ayush Khaitan"
                    },
                    {
                        "name": "Vijay Ganesh"
                    }
                ],
                "author_detail": {
                    "name": "Vijay Ganesh"
                },
                "author": "Vijay Ganesh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "03B35, 68W30, 68T05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08746v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08746v3",
                "updated": "2025-10-14T10:03:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    10,
                    3,
                    0,
                    1,
                    287,
                    0
                ],
                "published": "2025-08-12T08:41:00Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    8,
                    41,
                    0,
                    1,
                    224,
                    0
                ],
                "title": "Interpretable Reward Model via Sparse Autoencoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretable Reward Model via Sparse Autoencoder"
                },
                "summary": "Large language models (LLMs) have been widely deployed across numerous\nfields. Reinforcement Learning from Human Feedback (RLHF) leverages reward\nmodels (RMs) as proxies for human preferences to align LLM behaviors with human\nvalues, making the accuracy, reliability, and interpretability of RMs critical\nfor effective alignment. However, traditional RMs lack interpretability, offer\nlimited insight into the reasoning behind reward assignments, and are\ninflexible toward user preference shifts. While recent multidimensional RMs aim\nfor improved interpretability, they often fail to provide feature-level\nattribution and require costly annotations. To overcome these limitations, we\nintroduce the Sparse Autoencoder-enhanced Reward Model (SARM), a novel\narchitecture that integrates a pretrained Sparse Autoencoder (SAE) into a\nreward model. SARM maps the hidden activations of LLM-based RM into an\ninterpretable, sparse, and monosemantic feature space, from which a scalar head\naggregates feature activations to produce transparent and conceptually\nmeaningful reward scores. Empirical evaluations demonstrate that SARM\nfacilitates direct feature-level attribution of reward assignments, allows\ndynamic adjustment to preference shifts, and achieves superior alignment\nperformance compared to conventional reward models. Our code is available at\nhttps://github.com/schrieffer-z/sarm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely deployed across numerous\nfields. Reinforcement Learning from Human Feedback (RLHF) leverages reward\nmodels (RMs) as proxies for human preferences to align LLM behaviors with human\nvalues, making the accuracy, reliability, and interpretability of RMs critical\nfor effective alignment. However, traditional RMs lack interpretability, offer\nlimited insight into the reasoning behind reward assignments, and are\ninflexible toward user preference shifts. While recent multidimensional RMs aim\nfor improved interpretability, they often fail to provide feature-level\nattribution and require costly annotations. To overcome these limitations, we\nintroduce the Sparse Autoencoder-enhanced Reward Model (SARM), a novel\narchitecture that integrates a pretrained Sparse Autoencoder (SAE) into a\nreward model. SARM maps the hidden activations of LLM-based RM into an\ninterpretable, sparse, and monosemantic feature space, from which a scalar head\naggregates feature activations to produce transparent and conceptually\nmeaningful reward scores. Empirical evaluations demonstrate that SARM\nfacilitates direct feature-level attribution of reward assignments, allows\ndynamic adjustment to preference shifts, and achieves superior alignment\nperformance compared to conventional reward models. Our code is available at\nhttps://github.com/schrieffer-z/sarm."
                },
                "authors": [
                    {
                        "name": "Shuyi Zhang"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Sihang Li"
                    },
                    {
                        "name": "Jiayi Liao"
                    },
                    {
                        "name": "Tao Liang"
                    },
                    {
                        "name": "Hengxing Cai"
                    },
                    {
                        "name": "Xiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Wang"
                },
                "author": "Xiang Wang",
                "arxiv_comment": "Commercial firm need to review this paper before publishing it",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08746v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08746v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12343v1",
                "updated": "2025-10-14T09:56:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    9,
                    56,
                    50,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T09:56:50Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    9,
                    56,
                    50,
                    1,
                    287,
                    0
                ],
                "title": "Traveling Salesman-Based Token Ordering Improves Stability in\n  Homomorphically Encrypted Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traveling Salesman-Based Token Ordering Improves Stability in\n  Homomorphically Encrypted Language Models"
                },
                "summary": "As users increasingly interact with large language models (LLMs) using\nprivate information, secure and encrypted communication becomes essential.\nHomomorphic encryption (HE) provides a principled solution by enabling\ncomputation directly on encrypted data. Although prior work has explored\naspects of running LLMs under HE, the challenge of text generation,\nparticularly next-token prediction, has received limited attention and remains\na key obstacle to practical encrypted interaction. In this work, we propose a\nTSP-based token reordering strategy to address the difficulties of encrypted\ntext generation, together with a post-processing step that further reduces\napproximation error. Theoretical analysis and experimental results demonstrate\nthat our method prevents collapse, improves coherence in generated text, and\npreserves data privacy throughout. Overall, our contributions advance the\nfeasibility of practical and privacy-preserving LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As users increasingly interact with large language models (LLMs) using\nprivate information, secure and encrypted communication becomes essential.\nHomomorphic encryption (HE) provides a principled solution by enabling\ncomputation directly on encrypted data. Although prior work has explored\naspects of running LLMs under HE, the challenge of text generation,\nparticularly next-token prediction, has received limited attention and remains\na key obstacle to practical encrypted interaction. In this work, we propose a\nTSP-based token reordering strategy to address the difficulties of encrypted\ntext generation, together with a post-processing step that further reduces\napproximation error. Theoretical analysis and experimental results demonstrate\nthat our method prevents collapse, improves coherence in generated text, and\npreserves data privacy throughout. Overall, our contributions advance the\nfeasibility of practical and privacy-preserving LLM inference."
                },
                "authors": [
                    {
                        "name": "Donghwan Rho"
                    },
                    {
                        "name": "Sieun Seo"
                    },
                    {
                        "name": "Hyewon Sung"
                    },
                    {
                        "name": "Chohong Min"
                    },
                    {
                        "name": "Ernest K. Ryu"
                    }
                ],
                "author_detail": {
                    "name": "Ernest K. Ryu"
                },
                "author": "Ernest K. Ryu",
                "arxiv_comment": "34 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00757v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00757v4",
                "updated": "2025-10-14T09:49:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    9,
                    49,
                    14,
                    1,
                    287,
                    0
                ],
                "published": "2025-02-02T11:40:07Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    11,
                    40,
                    7,
                    6,
                    33,
                    0
                ],
                "title": "AgentBreeder: Mitigating the AI Safety Risks of Multi-Agent Scaffolds\n  via Self-Improvement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentBreeder: Mitigating the AI Safety Risks of Multi-Agent Scaffolds\n  via Self-Improvement"
                },
                "summary": "Scaffolding Large Language Models (LLMs) into multi-agent systems often\nimproves performance on complex tasks, but the safety impact of such scaffolds\nhas not been thoroughly explored. We introduce AgentBreeder, a framework for\nmulti-objective self-improving evolutionary search over scaffolds. We evaluate\ndiscovered scaffolds on widely recognized reasoning, mathematics, and safety\nbenchmarks and compare them with popular baselines. In \"blue\" mode, we see a\n79.4% average uplift in safety benchmark performance while maintaining or\nimproving capability scores. In \"red\" mode, we find adversarially weak\nscaffolds emerging concurrently with capability optimization. Our work\ndemonstrates the risks of multi-agent scaffolding and provides a framework for\nmitigating them. Code is available at\nhttps://github.com/jrosseruk/AgentBreeder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaffolding Large Language Models (LLMs) into multi-agent systems often\nimproves performance on complex tasks, but the safety impact of such scaffolds\nhas not been thoroughly explored. We introduce AgentBreeder, a framework for\nmulti-objective self-improving evolutionary search over scaffolds. We evaluate\ndiscovered scaffolds on widely recognized reasoning, mathematics, and safety\nbenchmarks and compare them with popular baselines. In \"blue\" mode, we see a\n79.4% average uplift in safety benchmark performance while maintaining or\nimproving capability scores. In \"red\" mode, we find adversarially weak\nscaffolds emerging concurrently with capability optimization. Our work\ndemonstrates the risks of multi-agent scaffolding and provides a framework for\nmitigating them. Code is available at\nhttps://github.com/jrosseruk/AgentBreeder."
                },
                "authors": [
                    {
                        "name": "J Rosser"
                    },
                    {
                        "name": "Jakob Foerster"
                    }
                ],
                "author_detail": {
                    "name": "Jakob Foerster"
                },
                "author": "Jakob Foerster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00757v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00757v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T42, 68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07300v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07300v2",
                "updated": "2025-10-14T09:32:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    9,
                    32,
                    5,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-08T17:55:02Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    55,
                    2,
                    2,
                    281,
                    0
                ],
                "title": "Think Natively: Unlocking Multilingual Reasoning with\n  Consistency-Enhanced Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think Natively: Unlocking Multilingual Reasoning with\n  Consistency-Enhanced Reinforcement Learning"
                },
                "summary": "Large Reasoning Models (LRMs) have achieved remarkable performance on complex\nreasoning tasks by adopting the \"think-then-answer\" paradigm, which enhances\nboth accuracy and interpretability. However, current LRMs exhibit two critical\nlimitations when processing non-English languages: (1) They often struggle to\nmaintain input-output language consistency; (2) They generally perform poorly\nwith wrong reasoning paths and lower answer accuracy compared to English. These\nlimitations significantly degrade the user experience for non-English speakers\nand hinder the global deployment of LRMs. To address these limitations, we\npropose M-Thinker, which is trained by the GRPO algorithm that involves a\nLanguage Consistency (LC) reward and a novel Cross-lingual Thinking Alignment\n(CTA) reward. Specifically, the LC reward defines a strict constraint on the\nlanguage consistency between the input, thought, and answer. Besides, the CTA\nreward compares the model's non-English reasoning paths with its English\nreasoning path to transfer its own reasoning capability from English to\nnon-English languages. Through an iterative RL procedure, our M-Thinker-1.5B/7B\nmodels not only achieve nearly 100% language consistency and superior\nperformance on two multilingual benchmarks (MMATH and PolyMath), but also\nexhibit excellent generalization on out-of-domain languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) have achieved remarkable performance on complex\nreasoning tasks by adopting the \"think-then-answer\" paradigm, which enhances\nboth accuracy and interpretability. However, current LRMs exhibit two critical\nlimitations when processing non-English languages: (1) They often struggle to\nmaintain input-output language consistency; (2) They generally perform poorly\nwith wrong reasoning paths and lower answer accuracy compared to English. These\nlimitations significantly degrade the user experience for non-English speakers\nand hinder the global deployment of LRMs. To address these limitations, we\npropose M-Thinker, which is trained by the GRPO algorithm that involves a\nLanguage Consistency (LC) reward and a novel Cross-lingual Thinking Alignment\n(CTA) reward. Specifically, the LC reward defines a strict constraint on the\nlanguage consistency between the input, thought, and answer. Besides, the CTA\nreward compares the model's non-English reasoning paths with its English\nreasoning path to transfer its own reasoning capability from English to\nnon-English languages. Through an iterative RL procedure, our M-Thinker-1.5B/7B\nmodels not only achieve nearly 100% language consistency and superior\nperformance on two multilingual benchmarks (MMATH and PolyMath), but also\nexhibit excellent generalization on out-of-domain languages."
                },
                "authors": [
                    {
                        "name": "Xue Zhang"
                    },
                    {
                        "name": "Yunlong Liang"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Songming Zhang"
                    },
                    {
                        "name": "Kaiyu Huang"
                    },
                    {
                        "name": "Yufeng Chen"
                    },
                    {
                        "name": "Jinan Xu"
                    },
                    {
                        "name": "Jie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhou"
                },
                "author": "Jie Zhou",
                "arxiv_comment": "13 pages, 8 tables, 4 figures. Code is available at:\n  https://github.com/XZhang00/M-Thinker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07300v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07300v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00665v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00665v2",
                "updated": "2025-10-14T09:23:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    9,
                    23,
                    11,
                    1,
                    287,
                    0
                ],
                "published": "2025-07-01T11:04:03Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    11,
                    4,
                    3,
                    1,
                    182,
                    0
                ],
                "title": "SAFER: Probing Safety in Reward Models with Sparse Autoencoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAFER: Probing Safety in Reward Models with Sparse Autoencoder"
                },
                "summary": "Reinforcement learning from human feedback (RLHF) is a key paradigm for\naligning large language models (LLMs) with human values, yet the reward models\nat its core remain largely opaque. In this work, we present sparse Autoencoder\nFor Enhanced Reward model (\\textbf{SAFER}), a novel framework for interpreting\nand improving reward models through mechanistic analysis. Leveraging Sparse\nAutoencoders (SAEs), we uncover human-interpretable features in reward model\nactivations, enabling insight into safety-relevant decision-making. We apply\nSAFER to safety-oriented preference datasets and quantify the salience of\nindividual features by activation differences between chosen and rejected\nresponses. Using these feature-level signals, we design targeted data poisoning\nand denoising strategies. Experiments show that SAFER can precisely degrade or\nenhance safety alignment with minimal data modification, without sacrificing\ngeneral chat performance. Our approach contributes to interpreting, auditing\nand refining reward models in high-stakes LLM alignment tasks. Our codes are\navailable at https://github.com/xzy-101/SAFER-code. \\textit{This paper\ndiscusses topics related to large language model safety and may include\ndiscussions or examples that highlight potential risks or unsafe outcomes.}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning from human feedback (RLHF) is a key paradigm for\naligning large language models (LLMs) with human values, yet the reward models\nat its core remain largely opaque. In this work, we present sparse Autoencoder\nFor Enhanced Reward model (\\textbf{SAFER}), a novel framework for interpreting\nand improving reward models through mechanistic analysis. Leveraging Sparse\nAutoencoders (SAEs), we uncover human-interpretable features in reward model\nactivations, enabling insight into safety-relevant decision-making. We apply\nSAFER to safety-oriented preference datasets and quantify the salience of\nindividual features by activation differences between chosen and rejected\nresponses. Using these feature-level signals, we design targeted data poisoning\nand denoising strategies. Experiments show that SAFER can precisely degrade or\nenhance safety alignment with minimal data modification, without sacrificing\ngeneral chat performance. Our approach contributes to interpreting, auditing\nand refining reward models in high-stakes LLM alignment tasks. Our codes are\navailable at https://github.com/xzy-101/SAFER-code. \\textit{This paper\ndiscusses topics related to large language model safety and may include\ndiscussions or examples that highlight potential risks or unsafe outcomes.}"
                },
                "authors": [
                    {
                        "name": "Sihang Li"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Ziyuan Xie"
                    },
                    {
                        "name": "Tao Liang"
                    },
                    {
                        "name": "Guojun Ma"
                    },
                    {
                        "name": "Xiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Wang"
                },
                "author": "Xiang Wang",
                "arxiv_comment": "One of the institutions requires additional approval before we can\n  move forward with the publication. Thanks for your understanding, and we hope\n  to resubmit once everything is finalized",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00665v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00665v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12316v1",
                "updated": "2025-10-14T09:20:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    9,
                    20,
                    1,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T09:20:01Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    9,
                    20,
                    1,
                    1,
                    287,
                    0
                ],
                "title": "Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech\n  Generation"
                },
                "summary": "Counter-speech generation is at the core of many expert activities, such as\nfact-checking and hate speech, to counter harmful content. Yet, existing work\ntreats counter-speech generation as pure text generation task, mainly based on\nLarge Language Models or NGO experts. These approaches show severe drawbacks\ndue to the limited reliability and coherence in the generated countering text,\nand in scalability, respectively. To close this gap, we introduce a novel\nframework to model counter-speech generation as knowledge-wise text generation\nprocess. Our framework integrates advanced Retrieval-Augmented Generation (RAG)\npipelines to ensure the generation of trustworthy counter-speech for 8 main\ntarget groups identified in the hate speech literature, including women, people\nof colour, persons with disabilities, migrants, Muslims, Jews, LGBT persons,\nand other. We built a knowledge base over the United Nations Digital Library,\nEUR-Lex and the EU Agency for Fundamental Rights, comprising a total of 32,792\ntexts. We use the MultiTarget-CONAN dataset to empirically assess the quality\nof the generated counter-speech, both through standard metrics (i.e., JudgeLM)\nand a human evaluation. Results show that our framework outperforms standard\nLLM baselines and competitive approach, on both assessments. The resulting\nframework and the knowledge base pave the way for studying trustworthy and\nsound counter-speech generation, in hate speech and beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counter-speech generation is at the core of many expert activities, such as\nfact-checking and hate speech, to counter harmful content. Yet, existing work\ntreats counter-speech generation as pure text generation task, mainly based on\nLarge Language Models or NGO experts. These approaches show severe drawbacks\ndue to the limited reliability and coherence in the generated countering text,\nand in scalability, respectively. To close this gap, we introduce a novel\nframework to model counter-speech generation as knowledge-wise text generation\nprocess. Our framework integrates advanced Retrieval-Augmented Generation (RAG)\npipelines to ensure the generation of trustworthy counter-speech for 8 main\ntarget groups identified in the hate speech literature, including women, people\nof colour, persons with disabilities, migrants, Muslims, Jews, LGBT persons,\nand other. We built a knowledge base over the United Nations Digital Library,\nEUR-Lex and the EU Agency for Fundamental Rights, comprising a total of 32,792\ntexts. We use the MultiTarget-CONAN dataset to empirically assess the quality\nof the generated counter-speech, both through standard metrics (i.e., JudgeLM)\nand a human evaluation. Results show that our framework outperforms standard\nLLM baselines and competitive approach, on both assessments. The resulting\nframework and the knowledge base pave the way for studying trustworthy and\nsound counter-speech generation, in hate speech and beyond."
                },
                "authors": [
                    {
                        "name": "Greta Damo"
                    },
                    {
                        "name": "Elena Cabrio"
                    },
                    {
                        "name": "Serena Villata"
                    }
                ],
                "author_detail": {
                    "name": "Serena Villata"
                },
                "author": "Serena Villata",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15969v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15969v3",
                "updated": "2025-10-15T01:55:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    1,
                    55,
                    31,
                    2,
                    288,
                    0
                ],
                "published": "2025-06-19T02:25:04Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    25,
                    4,
                    3,
                    170,
                    0
                ],
                "title": "LazyEviction: Lagged KV Eviction with Attention Pattern Observation for\n  Efficient Long Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyEviction: Lagged KV Eviction with Attention Pattern Observation for\n  Efficient Long Reasoning"
                },
                "summary": "Large Language Models (LLMs) exhibit enhanced capabilities by\nChain-of-Thought reasoning. However, the extended reasoning sequences introduce\nsignificant GPU memory overhead due to increased key-value (KV) cache. Existing\nKV cache compression methods mitigate memory bottlenecks but struggle in long\nreasoning tasks. In this paper, we analyze attention patterns in reasoning\ntasks and reveal a Token Importance Recurrence phenomenon: a large proportion\nof tokens regain high attention after multiple decoding steps, which is failed\nto capture by existing works and may lead to unpredictable eviction on such\nperiodically critical tokens. To address this, we propose LazyEviction, an\nobservation window-based lagged eviction framework retaining latent recurring\ntokens by prioritized eviction based on tokens' recurrence patterns. Extensive\nexperiments demonstrate that LazyEviction reduces KV cache by 50%~70% while\nmaintaining comparable accuracy, outperforming existing KV cache compression\nbaselines. Our implementation code can be found at\nhttps://github.com/Halo-949/LazyEviction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit enhanced capabilities by\nChain-of-Thought reasoning. However, the extended reasoning sequences introduce\nsignificant GPU memory overhead due to increased key-value (KV) cache. Existing\nKV cache compression methods mitigate memory bottlenecks but struggle in long\nreasoning tasks. In this paper, we analyze attention patterns in reasoning\ntasks and reveal a Token Importance Recurrence phenomenon: a large proportion\nof tokens regain high attention after multiple decoding steps, which is failed\nto capture by existing works and may lead to unpredictable eviction on such\nperiodically critical tokens. To address this, we propose LazyEviction, an\nobservation window-based lagged eviction framework retaining latent recurring\ntokens by prioritized eviction based on tokens' recurrence patterns. Extensive\nexperiments demonstrate that LazyEviction reduces KV cache by 50%~70% while\nmaintaining comparable accuracy, outperforming existing KV cache compression\nbaselines. Our implementation code can be found at\nhttps://github.com/Halo-949/LazyEviction."
                },
                "authors": [
                    {
                        "name": "Haoyue Zhang"
                    },
                    {
                        "name": "Hualei Zhang"
                    },
                    {
                        "name": "Xiaosong Ma"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Song Guo"
                    }
                ],
                "author_detail": {
                    "name": "Song Guo"
                },
                "author": "Song Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15969v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15969v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12306v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12306v1",
                "updated": "2025-10-14T09:06:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    9,
                    6,
                    14,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T09:06:14Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    9,
                    6,
                    14,
                    1,
                    287,
                    0
                ],
                "title": "A large-scale, unsupervised pipeline for automatic corpus annotation\n  using LLMs: variation and change in the English consider construction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A large-scale, unsupervised pipeline for automatic corpus annotation\n  using LLMs: variation and change in the English consider construction"
                },
                "summary": "As natural language corpora expand at an unprecedented rate, manual\nannotation remains a significant methodological bottleneck in corpus linguistic\nwork. We address this challenge by presenting a scalable, unsupervised pipeline\nfor automating grammatical annotation in voluminous corpora using large\nlanguage models (LLMs). Unlike previous supervised and iterative approaches,\nour method employs a four-phase workflow: prompt engineering, pre-hoc\nevaluation, automated batch processing, and post-hoc validation. We demonstrate\nthe pipeline's accessibility and effectiveness through a diachronic case study\nof variation in the English consider construction. Using GPT-5 through the\nOpenAI API, we annotate 143,933 sentences from the Corpus of Historical\nAmerican English (COHA) in under 60 hours, achieving 98%+ accuracy on two\nsophisticated annotation procedures. Our results suggest that LLMs can perform\na range of data preparation tasks at scale with minimal human intervention,\nopening new possibilities for corpus-based research, though implementation\nrequires attention to costs, licensing, and other ethical considerations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As natural language corpora expand at an unprecedented rate, manual\nannotation remains a significant methodological bottleneck in corpus linguistic\nwork. We address this challenge by presenting a scalable, unsupervised pipeline\nfor automating grammatical annotation in voluminous corpora using large\nlanguage models (LLMs). Unlike previous supervised and iterative approaches,\nour method employs a four-phase workflow: prompt engineering, pre-hoc\nevaluation, automated batch processing, and post-hoc validation. We demonstrate\nthe pipeline's accessibility and effectiveness through a diachronic case study\nof variation in the English consider construction. Using GPT-5 through the\nOpenAI API, we annotate 143,933 sentences from the Corpus of Historical\nAmerican English (COHA) in under 60 hours, achieving 98%+ accuracy on two\nsophisticated annotation procedures. Our results suggest that LLMs can perform\na range of data preparation tasks at scale with minimal human intervention,\nopening new possibilities for corpus-based research, though implementation\nrequires attention to costs, licensing, and other ethical considerations."
                },
                "authors": [
                    {
                        "name": "Cameron Morin"
                    },
                    {
                        "name": "Matti Marttinen Larsson"
                    }
                ],
                "author_detail": {
                    "name": "Matti Marttinen Larsson"
                },
                "author": "Matti Marttinen Larsson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12306v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12306v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12294v1",
                "updated": "2025-10-14T08:56:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    8,
                    56,
                    16,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T08:56:16Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    8,
                    56,
                    16,
                    1,
                    287,
                    0
                ],
                "title": "Show Your Title! A Scoping Review on Verbalization in Software\n  Engineering with LLM-Assisted Screening",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Show Your Title! A Scoping Review on Verbalization in Software\n  Engineering with LLM-Assisted Screening"
                },
                "summary": "Understanding how software developers think, make decisions, and behave\nremains a key challenge in software engineering (SE). Verbalization techniques\n(methods that capture spoken or written thought processes) offer a lightweight\nand accessible way to study these cognitive aspects. This paper presents a\nscoping review of research at the intersection of SE and psychology (PSY),\nfocusing on the use of verbal data. To make large-scale interdisciplinary\nreviews feasible, we employed a large language model (LLM)-assisted screening\npipeline using GPT to assess the relevance of over 9,000 papers based solely on\ntitles. We addressed two questions: what themes emerge from\nverbalization-related work in SE, and how effective are LLMs in supporting\ninterdisciplinary review processes? We validated GPT's outputs against human\nreviewers and found high consistency, with a 13\\% disagreement rate. Prominent\nthemes mainly were tied to the craft of SE, while more human-centered topics\nwere underrepresented. The data also suggests that SE frequently draws on PSY\nmethods, whereas the reverse is rare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding how software developers think, make decisions, and behave\nremains a key challenge in software engineering (SE). Verbalization techniques\n(methods that capture spoken or written thought processes) offer a lightweight\nand accessible way to study these cognitive aspects. This paper presents a\nscoping review of research at the intersection of SE and psychology (PSY),\nfocusing on the use of verbal data. To make large-scale interdisciplinary\nreviews feasible, we employed a large language model (LLM)-assisted screening\npipeline using GPT to assess the relevance of over 9,000 papers based solely on\ntitles. We addressed two questions: what themes emerge from\nverbalization-related work in SE, and how effective are LLMs in supporting\ninterdisciplinary review processes? We validated GPT's outputs against human\nreviewers and found high consistency, with a 13\\% disagreement rate. Prominent\nthemes mainly were tied to the craft of SE, while more human-centered topics\nwere underrepresented. The data also suggests that SE frequently draws on PSY\nmethods, whereas the reverse is rare."
                },
                "authors": [
                    {
                        "name": "Gerg Balogh"
                    },
                    {
                        "name": "Dvid Ksz"
                    },
                    {
                        "name": "Homayoun Safarpour Motealegh Mahalegi"
                    },
                    {
                        "name": "Lszl Tth"
                    },
                    {
                        "name": "Bence Szakcs"
                    },
                    {
                        "name": "ron Bcs"
                    }
                ],
                "author_detail": {
                    "name": "ron Bcs"
                },
                "author": "ron Bcs",
                "arxiv_comment": "preprint of a paper under publication in Quality of Information and\n  Communications Technology 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]