[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.19755v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19755v2",
                "updated": "2025-10-23T09:09:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    9,
                    9,
                    15,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-22T16:46:05Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    46,
                    5,
                    2,
                    295,
                    0
                ],
                "title": "A Survey on Cache Methods in Diffusion Models: Toward Efficient\n  Multi-Modal Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Cache Methods in Diffusion Models: Toward Efficient\n  Multi-Modal Generation"
                },
                "summary": "Diffusion Models have become a cornerstone of modern generative AI for their\nexceptional generation quality and controllability. However, their inherent\n\\textit{multi-step iterations} and \\textit{complex backbone networks} lead to\nprohibitive computational overhead and generation latency, forming a major\nbottleneck for real-time applications. Although existing acceleration\ntechniques have made progress, they still face challenges such as limited\napplicability, high training costs, or quality degradation.\n  Against this backdrop, \\textbf{Diffusion Caching} offers a promising\ntraining-free, architecture-agnostic, and efficient inference paradigm. Its\ncore mechanism identifies and reuses intrinsic computational redundancies in\nthe diffusion process. By enabling feature-level cross-step reuse and\ninter-layer scheduling, it reduces computation without modifying model\nparameters. This paper systematically reviews the theoretical foundations and\nevolution of Diffusion Caching and proposes a unified framework for its\nclassification and analysis.\n  Through comparative analysis of representative methods, we show that\nDiffusion Caching evolves from \\textit{static reuse} to \\textit{dynamic\nprediction}. This trend enhances caching flexibility across diverse tasks and\nenables integration with other acceleration techniques such as sampling\noptimization and model distillation, paving the way for a unified, efficient\ninference framework for future multimodal and interactive applications. We\nargue that this paradigm will become a key enabler of real-time and efficient\ngenerative AI, injecting new vitality into both theory and practice of\n\\textit{Efficient Generative Intelligence}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Models have become a cornerstone of modern generative AI for their\nexceptional generation quality and controllability. However, their inherent\n\\textit{multi-step iterations} and \\textit{complex backbone networks} lead to\nprohibitive computational overhead and generation latency, forming a major\nbottleneck for real-time applications. Although existing acceleration\ntechniques have made progress, they still face challenges such as limited\napplicability, high training costs, or quality degradation.\n  Against this backdrop, \\textbf{Diffusion Caching} offers a promising\ntraining-free, architecture-agnostic, and efficient inference paradigm. Its\ncore mechanism identifies and reuses intrinsic computational redundancies in\nthe diffusion process. By enabling feature-level cross-step reuse and\ninter-layer scheduling, it reduces computation without modifying model\nparameters. This paper systematically reviews the theoretical foundations and\nevolution of Diffusion Caching and proposes a unified framework for its\nclassification and analysis.\n  Through comparative analysis of representative methods, we show that\nDiffusion Caching evolves from \\textit{static reuse} to \\textit{dynamic\nprediction}. This trend enhances caching flexibility across diverse tasks and\nenables integration with other acceleration techniques such as sampling\noptimization and model distillation, paving the way for a unified, efficient\ninference framework for future multimodal and interactive applications. We\nargue that this paradigm will become a key enabler of real-time and efficient\ngenerative AI, injecting new vitality into both theory and practice of\n\\textit{Efficient Generative Intelligence}."
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Yuqi Lin"
                    },
                    {
                        "name": "Zhikai Wang"
                    },
                    {
                        "name": "Peiru Wang"
                    },
                    {
                        "name": "Peiliang Cai"
                    },
                    {
                        "name": "Qinming Zhou"
                    },
                    {
                        "name": "Zhengan Yan"
                    },
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Zhengyi Shi"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "22 pages,2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19755v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19755v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19670v1",
                "updated": "2025-10-22T15:16:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    16,
                    56,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T15:16:56Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    16,
                    56,
                    2,
                    295,
                    0
                ],
                "title": "CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware\n  Cloud-Edge Cooperation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware\n  Cloud-Edge Cooperation"
                },
                "summary": "We present CoSense-LLM, an edge-first framework that turns continuous\nmultimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and\nlightweight vision) into compact, verifiable semantic tokens and coordinates\nwith large language models under explicit latency, energy, bandwidth, and\nprivacy constraints. CoSense-LLM has four parts: (i) SenseFusion, a lightweight\nencoder that aligns sensor embeddings with language and compresses them into\nshort discrete code sequences; (ii) Edge-RAG, a local hybrid retrieval layer\nthat grounds generation in site specific policies and notes; (iii)\nPromptRouter, a cost and uncertainty aware policy that selects edge only\ngeneration, edge plus retrieval, or compact cloud escalation; and (iv) Secure\nExecution, an auditable redaction path that enforces data minimization so raw\nwaveforms never leave the device. The system works with modern serving\noptimizations, including paged or streaming KV caches, FlashAttention style\nkernels, speculative decoding, and quantized LoRA adapters, and supports on\ndevice personalization and federated updates under non IID drift. Across home,\noffice, and clinic deployments, CoSense-LLM delivers grounded explanations\nwhile meeting tight service level objectives: it sustains sub second (p95) end\nto end latency on edge dominant paths, reduces inter tier token and bandwidth\ncosts by preferring local retrieval grounded responses, and preserves privacy\nby transmitting only discrete codes and redacted metadata. Ablations show that\nEdge-RAG improves factual consistency and reduces contradictions, calibrated\nuncertainty enables selective abstention and controlled escalations, and KV\nplus decoding accelerators lower energy per decision. The results support an\nedge first design that treats semantics, privacy, and predictable latency as co\nequal goals for large model deployments in interference prone environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present CoSense-LLM, an edge-first framework that turns continuous\nmultimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and\nlightweight vision) into compact, verifiable semantic tokens and coordinates\nwith large language models under explicit latency, energy, bandwidth, and\nprivacy constraints. CoSense-LLM has four parts: (i) SenseFusion, a lightweight\nencoder that aligns sensor embeddings with language and compresses them into\nshort discrete code sequences; (ii) Edge-RAG, a local hybrid retrieval layer\nthat grounds generation in site specific policies and notes; (iii)\nPromptRouter, a cost and uncertainty aware policy that selects edge only\ngeneration, edge plus retrieval, or compact cloud escalation; and (iv) Secure\nExecution, an auditable redaction path that enforces data minimization so raw\nwaveforms never leave the device. The system works with modern serving\noptimizations, including paged or streaming KV caches, FlashAttention style\nkernels, speculative decoding, and quantized LoRA adapters, and supports on\ndevice personalization and federated updates under non IID drift. Across home,\noffice, and clinic deployments, CoSense-LLM delivers grounded explanations\nwhile meeting tight service level objectives: it sustains sub second (p95) end\nto end latency on edge dominant paths, reduces inter tier token and bandwidth\ncosts by preferring local retrieval grounded responses, and preserves privacy\nby transmitting only discrete codes and redacted metadata. Ablations show that\nEdge-RAG improves factual consistency and reduces contradictions, calibrated\nuncertainty enables selective abstention and controlled escalations, and KV\nplus decoding accelerators lower energy per decision. The results support an\nedge first design that treats semantics, privacy, and predictable latency as co\nequal goals for large model deployments in interference prone environments."
                },
                "authors": [
                    {
                        "name": "Hasan Akgul"
                    },
                    {
                        "name": "Mari Eplik"
                    },
                    {
                        "name": "Javier Rojas"
                    },
                    {
                        "name": "Aina Binti Abdullah"
                    },
                    {
                        "name": "Pieter van der Merwe"
                    }
                ],
                "author_detail": {
                    "name": "Pieter van der Merwe"
                },
                "author": "Pieter van der Merwe",
                "arxiv_comment": "19 pages,8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; C.2.4; C.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08666v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08666v3",
                "updated": "2025-10-22T14:33:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    33,
                    49,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-09T16:19:42Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    19,
                    42,
                    3,
                    282,
                    0
                ],
                "title": "dInfer: An Efficient Inference Framework for Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "dInfer: An Efficient Inference Framework for Diffusion Language Models"
                },
                "summary": "Diffusion-based large language models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs, leveraging denoising-based generation\nto enable inherent parallelism. Even more and more open-sourced dLLM models\nemerge, yet their widespread adoption remains constrained by the lack of a\nstandardized and efficient inference framework. We present dInfer, an efficient\nand extensible framework for dLLM inference. dInfer decomposes the inference\npipeline into four modular components--model, diffusion iteration manager,\ndecoding strategy, and KV-cache manager--and integrates novel algorithms for\neach component alongside system-level optimizations. Through this combination\nof algorithmic innovations and system enhancements, dInfer achieves substantial\nefficiency gains without compromising output quality on LLaDA-MoE. At batch\nsize 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800\ntokens per second across six benchmarks on $8\\times$ H800 GPUs. Compared to\nprior systems, dInfer delivers a $10\\times$ speedup over Fast-dLLM while\nmaintaining similar model performance. Even compared to the AR model (with a\ncomparable number of activation parameters and performance) QWen2.5-3B, which\nis highly optimized with the latest vLLM inference engine, dInfer still\ndelivers a $2$-$3\\times$ speedup. The implementation of dInfer is open-sourced\nat https://github.com/inclusionAI/dInfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs, leveraging denoising-based generation\nto enable inherent parallelism. Even more and more open-sourced dLLM models\nemerge, yet their widespread adoption remains constrained by the lack of a\nstandardized and efficient inference framework. We present dInfer, an efficient\nand extensible framework for dLLM inference. dInfer decomposes the inference\npipeline into four modular components--model, diffusion iteration manager,\ndecoding strategy, and KV-cache manager--and integrates novel algorithms for\neach component alongside system-level optimizations. Through this combination\nof algorithmic innovations and system enhancements, dInfer achieves substantial\nefficiency gains without compromising output quality on LLaDA-MoE. At batch\nsize 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800\ntokens per second across six benchmarks on $8\\times$ H800 GPUs. Compared to\nprior systems, dInfer delivers a $10\\times$ speedup over Fast-dLLM while\nmaintaining similar model performance. Even compared to the AR model (with a\ncomparable number of activation parameters and performance) QWen2.5-3B, which\nis highly optimized with the latest vLLM inference engine, dInfer still\ndelivers a $2$-$3\\times$ speedup. The implementation of dInfer is open-sourced\nat https://github.com/inclusionAI/dInfer."
                },
                "authors": [
                    {
                        "name": "Yuxin Ma"
                    },
                    {
                        "name": "Lun Du"
                    },
                    {
                        "name": "Lanning Wei"
                    },
                    {
                        "name": "Kun Chen"
                    },
                    {
                        "name": "Qian Xu"
                    },
                    {
                        "name": "Kangyu Wang"
                    },
                    {
                        "name": "Guofeng Feng"
                    },
                    {
                        "name": "Guoshan Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Xiaojing Qi"
                    },
                    {
                        "name": "Xinyuan Zhang"
                    },
                    {
                        "name": "Zhen Tao"
                    },
                    {
                        "name": "Haibo Feng"
                    },
                    {
                        "name": "Ziyun Jiang"
                    },
                    {
                        "name": "Ying Xu"
                    },
                    {
                        "name": "Zenan Huang"
                    },
                    {
                        "name": "Yihong Zhuang"
                    },
                    {
                        "name": "Haokai Xu"
                    },
                    {
                        "name": "Jiaqi Hu"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    },
                    {
                        "name": "Junbo Zhao"
                    },
                    {
                        "name": "Jianguo Li"
                    },
                    {
                        "name": "Da Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Da Zheng"
                },
                "author": "Da Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08666v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08666v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19264v1",
                "updated": "2025-10-22T05:47:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    5,
                    47,
                    41,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T05:47:41Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    5,
                    47,
                    41,
                    2,
                    295,
                    0
                ],
                "title": "LAPRAD: LLM-Assisted PRotocol Attack Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAPRAD: LLM-Assisted PRotocol Attack Discovery"
                },
                "summary": "With the goal of improving the security of Internet protocols, we seek\nfaster, semi-automatic methods to discover new vulnerabilities in protocols\nsuch as DNS, BGP, and others. To this end, we introduce the LLM-Assisted\nProtocol Attack Discovery (LAPRAD) methodology, enabling security researchers\nwith some DNS knowledge to efficiently uncover vulnerabilities that would\notherwise be hard to detect.\n  LAPRAD follows a three-stage process. In the first, we consult an LLM\n(GPT-o1) that has been trained on a broad corpus of DNS-related sources and\nprevious DDoS attacks to identify potential exploits. In the second stage, a\ndifferent LLM automatically constructs the corresponding attack configurations\nusing the ReACT approach implemented via LangChain (DNS zone file generation).\nFinally, in the third stage, we validate the attack's functionality and\neffectiveness.\n  Using LAPRAD, we uncovered three new DDoS attacks on the DNS protocol and\nrediscovered two recently reported ones that were not included in the LLM's\ntraining data. The first new attack employs a bait-and-switch technique to\ntrick resolvers into caching large, bogus DNSSEC RRSIGs, reducing their serving\ncapacity to as little as 6%. The second exploits large DNSSEC encryption\nalgorithms (RSA-4096) with multiple keys, thereby bypassing a recently\nimplemented default RRSet limit. The third leverages ANY-type responses to\nproduce a similar effect.\n  These variations of a cache-flushing DDoS attack, called SigCacheFlush,\ncircumvent existing patches, severely degrade resolver query capacity, and\nimpact the latest versions of major DNS resolver implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the goal of improving the security of Internet protocols, we seek\nfaster, semi-automatic methods to discover new vulnerabilities in protocols\nsuch as DNS, BGP, and others. To this end, we introduce the LLM-Assisted\nProtocol Attack Discovery (LAPRAD) methodology, enabling security researchers\nwith some DNS knowledge to efficiently uncover vulnerabilities that would\notherwise be hard to detect.\n  LAPRAD follows a three-stage process. In the first, we consult an LLM\n(GPT-o1) that has been trained on a broad corpus of DNS-related sources and\nprevious DDoS attacks to identify potential exploits. In the second stage, a\ndifferent LLM automatically constructs the corresponding attack configurations\nusing the ReACT approach implemented via LangChain (DNS zone file generation).\nFinally, in the third stage, we validate the attack's functionality and\neffectiveness.\n  Using LAPRAD, we uncovered three new DDoS attacks on the DNS protocol and\nrediscovered two recently reported ones that were not included in the LLM's\ntraining data. The first new attack employs a bait-and-switch technique to\ntrick resolvers into caching large, bogus DNSSEC RRSIGs, reducing their serving\ncapacity to as little as 6%. The second exploits large DNSSEC encryption\nalgorithms (RSA-4096) with multiple keys, thereby bypassing a recently\nimplemented default RRSet limit. The third leverages ANY-type responses to\nproduce a similar effect.\n  These variations of a cache-flushing DDoS attack, called SigCacheFlush,\ncircumvent existing patches, severely degrade resolver query capacity, and\nimpact the latest versions of major DNS resolver implementations."
                },
                "authors": [
                    {
                        "name": "R. Can Aygun"
                    },
                    {
                        "name": "Yehuda Afek"
                    },
                    {
                        "name": "Anat Bremler-Barr"
                    },
                    {
                        "name": "Leonard Kleinrock"
                    }
                ],
                "author_detail": {
                    "name": "Leonard Kleinrock"
                },
                "arxiv_affiliation": "UCLA",
                "author": "Leonard Kleinrock",
                "arxiv_comment": "IFIP Networking 2025 Proceedings (Accepted on 05.05.2025)",
                "arxiv_journal_ref": "Published in IFIP Networking 2025 Proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19240v1",
                "updated": "2025-10-22T04:48:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    4,
                    48,
                    41,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T04:48:41Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    4,
                    48,
                    41,
                    2,
                    295,
                    0
                ],
                "title": "A General Solution for the Implementation of CI/CD in Embedded Linux\n  Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A General Solution for the Implementation of CI/CD in Embedded Linux\n  Development"
                },
                "summary": "With the growing use of embedded systems in various industries, the need for\nautomated platforms for the development and deployment of customized\nLinux-based operating systems has become more important. This research was\nconducted with the aim of designing and implementing an integrated and\nreproducible infrastructure for the development, building, and testing of a\nLinux-based operating system using the Yocto Project. The proposed structure\nwas implemented based on a three-layer architecture consisting of the main\nYocto repositories, a custom layer (meta-custom), and a coordinating manifest\nlayer to ensure version synchronization, scalability, and reproducibility.\nThree sample projects, including libhelloworld, helloworld, and the kernel\nmodule hello mod, were developed and integrated into the build process.\nContinuous Integration and Continuous Deployment pipelines were implemented\nwith GitLab CI and combined with an isolated Docker environment to automate and\nstreamline the build and testing workflows. Using a local cache server\ncontaining hashserv, downloads and sstate cache significantly reduced the build\ntime. The functionality and stability of the system were verified through six\nboot test scenarios in the QEMU simulator. The results show that the proposed\ndesign not only ensures reproducibility but also can be extended to advanced\napplications such as continuous deployment of real-time Linux versions. Future\nrecommendations include expanding automated tests, implementing system\nmonitoring with Prometheus and Grafana, using distributed builds, optimizing\nwith Docker multi-stage builds, and enabling continuous deployment of real-time\nLinux changes to provide a stable and scalable model for industrial and\nresearch projects in embedded systems with a rapid and reliable development\ncycle.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing use of embedded systems in various industries, the need for\nautomated platforms for the development and deployment of customized\nLinux-based operating systems has become more important. This research was\nconducted with the aim of designing and implementing an integrated and\nreproducible infrastructure for the development, building, and testing of a\nLinux-based operating system using the Yocto Project. The proposed structure\nwas implemented based on a three-layer architecture consisting of the main\nYocto repositories, a custom layer (meta-custom), and a coordinating manifest\nlayer to ensure version synchronization, scalability, and reproducibility.\nThree sample projects, including libhelloworld, helloworld, and the kernel\nmodule hello mod, were developed and integrated into the build process.\nContinuous Integration and Continuous Deployment pipelines were implemented\nwith GitLab CI and combined with an isolated Docker environment to automate and\nstreamline the build and testing workflows. Using a local cache server\ncontaining hashserv, downloads and sstate cache significantly reduced the build\ntime. The functionality and stability of the system were verified through six\nboot test scenarios in the QEMU simulator. The results show that the proposed\ndesign not only ensures reproducibility but also can be extended to advanced\napplications such as continuous deployment of real-time Linux versions. Future\nrecommendations include expanding automated tests, implementing system\nmonitoring with Prometheus and Grafana, using distributed builds, optimizing\nwith Docker multi-stage builds, and enabling continuous deployment of real-time\nLinux changes to provide a stable and scalable model for industrial and\nresearch projects in embedded systems with a rapid and reliable development\ncycle."
                },
                "authors": [
                    {
                        "name": "Behnam Agahi"
                    },
                    {
                        "name": "Hamed Farbeh"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Farbeh"
                },
                "author": "Hamed Farbeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19183v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19183v1",
                "updated": "2025-10-22T02:41:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    2,
                    41,
                    7,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T02:41:07Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    2,
                    41,
                    7,
                    2,
                    295,
                    0
                ],
                "title": "PruneHal: Reducing Hallucinations in Multi-modal Large Language Models\n  through Adaptive KV Cache Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PruneHal: Reducing Hallucinations in Multi-modal Large Language Models\n  through Adaptive KV Cache Pruning"
                },
                "summary": "While multi-modal large language models (MLLMs) have made significant\nprogress in recent years, the issue of hallucinations remains a major\nchallenge. To mitigate this phenomenon, existing solutions either introduce\nadditional data for further training or incorporate external or internal\ninformation during inference. However, these approaches inevitably introduce\nextra computational costs. In this paper, we observe that hallucinations in\nMLLMs are strongly associated with insufficient attention allocated to visual\ntokens. In particular, the presence of redundant visual tokens disperses the\nmodel's attention, preventing it from focusing on the most informative ones. As\na result, critical visual cues are often under-attended, which in turn\nexacerbates the occurrence of hallucinations. Building on this observation, we\npropose \\textbf{PruneHal}, a training-free, simple yet effective method that\nleverages adaptive KV cache pruning to enhance the model's focus on critical\nvisual information, thereby mitigating hallucinations. To the best of our\nknowledge, we are the first to apply token pruning for hallucination mitigation\nin MLLMs. Notably, our method don't require additional training and incurs\nnearly no extra inference cost. Moreover, PruneHal is model-agnostic and can be\nseamlessly integrated with different decoding strategies, including those\nspecifically designed for hallucination mitigation. We evaluate PruneHal on\nseveral widely used hallucination evaluation benchmarks using four mainstream\nMLLMs, achieving robust and outstanding results that highlight the\neffectiveness and superiority of our method. Our code will be publicly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While multi-modal large language models (MLLMs) have made significant\nprogress in recent years, the issue of hallucinations remains a major\nchallenge. To mitigate this phenomenon, existing solutions either introduce\nadditional data for further training or incorporate external or internal\ninformation during inference. However, these approaches inevitably introduce\nextra computational costs. In this paper, we observe that hallucinations in\nMLLMs are strongly associated with insufficient attention allocated to visual\ntokens. In particular, the presence of redundant visual tokens disperses the\nmodel's attention, preventing it from focusing on the most informative ones. As\na result, critical visual cues are often under-attended, which in turn\nexacerbates the occurrence of hallucinations. Building on this observation, we\npropose \\textbf{PruneHal}, a training-free, simple yet effective method that\nleverages adaptive KV cache pruning to enhance the model's focus on critical\nvisual information, thereby mitigating hallucinations. To the best of our\nknowledge, we are the first to apply token pruning for hallucination mitigation\nin MLLMs. Notably, our method don't require additional training and incurs\nnearly no extra inference cost. Moreover, PruneHal is model-agnostic and can be\nseamlessly integrated with different decoding strategies, including those\nspecifically designed for hallucination mitigation. We evaluate PruneHal on\nseveral widely used hallucination evaluation benchmarks using four mainstream\nMLLMs, achieving robust and outstanding results that highlight the\neffectiveness and superiority of our method. Our code will be publicly\navailable."
                },
                "authors": [
                    {
                        "name": "Fengyuan Sun"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Xinhao Xu"
                    },
                    {
                        "name": "Dandan Zheng"
                    },
                    {
                        "name": "Jingdong Chen"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19183v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19183v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19171v1",
                "updated": "2025-10-22T02:09:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    2,
                    9,
                    23,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T02:09:23Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    2,
                    9,
                    23,
                    2,
                    295,
                    0
                ],
                "title": "Think Straight, Stop Smart: Structured Reasoning for Efficient Multi-Hop\n  RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think Straight, Stop Smart: Structured Reasoning for Efficient Multi-Hop\n  RAG"
                },
                "summary": "Multi-hop retrieval-augmented generation (RAG) is a promising strategy for\ncomplex reasoning, yet existing iterative prompting approaches remain\ninefficient. They often regenerate predictable token sequences at every step\nand rely on stochastic stopping, leading to excessive token usage and unstable\ntermination. We propose TSSS (Think Straight, Stop Smart), a structured\nmulti-hop RAG framework designed for efficiency. TSSS introduces (i) a\ntemplate-based reasoning that caches recurring prefixes and anchors sub-queries\nto the main question, reducing token generation cost while promoting stable\nreasoning, and (ii) a retriever-based terminator, which deterministically halts\nreasoning once additional sub-queries collapse into repetition. This separation\nof structured reasoning and termination control enables both faster inference\nand more reliable answers. On HotpotQA, 2WikiMultiHop, and MuSiQue, TSSS\nachieves state-of-the-art accuracy and competitive efficiency among RAG-CoT\napproaches, highlighting its effectiveness in efficiency-constrained scenarios\nsuch as on-device inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-hop retrieval-augmented generation (RAG) is a promising strategy for\ncomplex reasoning, yet existing iterative prompting approaches remain\ninefficient. They often regenerate predictable token sequences at every step\nand rely on stochastic stopping, leading to excessive token usage and unstable\ntermination. We propose TSSS (Think Straight, Stop Smart), a structured\nmulti-hop RAG framework designed for efficiency. TSSS introduces (i) a\ntemplate-based reasoning that caches recurring prefixes and anchors sub-queries\nto the main question, reducing token generation cost while promoting stable\nreasoning, and (ii) a retriever-based terminator, which deterministically halts\nreasoning once additional sub-queries collapse into repetition. This separation\nof structured reasoning and termination control enables both faster inference\nand more reliable answers. On HotpotQA, 2WikiMultiHop, and MuSiQue, TSSS\nachieves state-of-the-art accuracy and competitive efficiency among RAG-CoT\napproaches, highlighting its effectiveness in efficiency-constrained scenarios\nsuch as on-device inference."
                },
                "authors": [
                    {
                        "name": "Jihwan Bang"
                    },
                    {
                        "name": "Juntae Lee"
                    },
                    {
                        "name": "Seunghan Yang"
                    },
                    {
                        "name": "Sungha Choi"
                    }
                ],
                "author_detail": {
                    "name": "Sungha Choi"
                },
                "author": "Sungha Choi",
                "arxiv_comment": "Accepted at NeurIPS 2025 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11857v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11857v3",
                "updated": "2025-10-21T22:37:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    22,
                    37,
                    11,
                    1,
                    294,
                    0
                ],
                "published": "2024-10-04T15:23:28Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    15,
                    23,
                    28,
                    4,
                    278,
                    0
                ],
                "title": "LLMBridge: Reducing Costs to Access LLMs in a Prompt-Centric Internet",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMBridge: Reducing Costs to Access LLMs in a Prompt-Centric Internet"
                },
                "summary": "Today's Internet infrastructure is centered around content retrieval over\nHTTP, with middleboxes (e.g., HTTP proxies) playing a crucial role in\nperformance, security, and cost-effectiveness. We envision a future where\nInternet communication will be dominated by \"prompts\" sent to generative AI\nmodels. For this, we will need proxies that provide similar functions to HTTP\nproxies (e.g., caching, routing, compression) while dealing with unique\nchallenges and opportunities of prompt-based communication. As a first step\ntoward supporting prompt-based communication, we present LLMBridge, an LLM\nproxy designed for cost-conscious users, such as those in developing regions\nand education (e.g., students, instructors). LLMBridge supports three key\noptimizations: model selection (routing prompts to the most suitable model),\ncontext management (intelligently reducing the amount of context), and semantic\ncaching (serving prompts using local models and vector databases). These\noptimizations introduce trade-offs between cost and quality, which applications\nnavigate through a high-level, bidirectional interface. As case studies, we\ndeploy LLMBridge in two cost-sensitive settings: a WhatsApp-based Q&A service\nand a university classroom environment. The WhatsApp service has been live for\nover twelve months, serving 100+ users and handling more than 14.7K requests.\nIn parallel, we exposed LLMBridge to students across three computer science\ncourses over a semester, where it supported diverse LLM-powered applications -\nsuch as reasoning agents and chatbots - and handled an average of 500 requests\nper day. We report on deployment experiences across both settings and use the\ncollected workloads to benchmark the effectiveness of various cost-optimization\nstrategies, analyzing their trade-offs in cost, latency, and response quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Today's Internet infrastructure is centered around content retrieval over\nHTTP, with middleboxes (e.g., HTTP proxies) playing a crucial role in\nperformance, security, and cost-effectiveness. We envision a future where\nInternet communication will be dominated by \"prompts\" sent to generative AI\nmodels. For this, we will need proxies that provide similar functions to HTTP\nproxies (e.g., caching, routing, compression) while dealing with unique\nchallenges and opportunities of prompt-based communication. As a first step\ntoward supporting prompt-based communication, we present LLMBridge, an LLM\nproxy designed for cost-conscious users, such as those in developing regions\nand education (e.g., students, instructors). LLMBridge supports three key\noptimizations: model selection (routing prompts to the most suitable model),\ncontext management (intelligently reducing the amount of context), and semantic\ncaching (serving prompts using local models and vector databases). These\noptimizations introduce trade-offs between cost and quality, which applications\nnavigate through a high-level, bidirectional interface. As case studies, we\ndeploy LLMBridge in two cost-sensitive settings: a WhatsApp-based Q&A service\nand a university classroom environment. The WhatsApp service has been live for\nover twelve months, serving 100+ users and handling more than 14.7K requests.\nIn parallel, we exposed LLMBridge to students across three computer science\ncourses over a semester, where it supported diverse LLM-powered applications -\nsuch as reasoning agents and chatbots - and handled an average of 500 requests\nper day. We report on deployment experiences across both settings and use the\ncollected workloads to benchmark the effectiveness of various cost-optimization\nstrategies, analyzing their trade-offs in cost, latency, and response quality."
                },
                "authors": [
                    {
                        "name": "Noah Martin"
                    },
                    {
                        "name": "Abdullah Bin Faisal"
                    },
                    {
                        "name": "Hiba Eltigani"
                    },
                    {
                        "name": "Rukhshan Haroon"
                    },
                    {
                        "name": "Swaminathan Lamelas"
                    },
                    {
                        "name": "Fahad Dogar"
                    }
                ],
                "author_detail": {
                    "name": "Fahad Dogar"
                },
                "author": "Fahad Dogar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11857v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11857v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18191v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18191v3",
                "updated": "2025-10-21T21:07:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    21,
                    7,
                    17,
                    1,
                    294,
                    0
                ],
                "published": "2025-03-23T20:18:16Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    20,
                    18,
                    16,
                    6,
                    82,
                    0
                ],
                "title": "DFUSE: Strongly Consistent Write-Back Kernel Caching for Distributed\n  Userspace File Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DFUSE: Strongly Consistent Write-Back Kernel Caching for Distributed\n  Userspace File Systems"
                },
                "summary": "Cloud platforms host thousands of tenants that demand POSIX semantics, high\nthroughput, and rapid evolution from their storage layer. Kernel-native\ndistributed file systems supply raw speed, but their privileged code base\ncouples every release to the kernel, widens the blast radius of crashes, and\nslows innovation. FUSE-based distributed file systems flip those trade-offs:\nthey run in user space for fast deployment and strong fault isolation, yet the\nFUSE interface disables the kernel's write-back page cache whenever strong\nconsistency is required. Practitioners must therefore choose between (i) weak\nconsistency with fast write-back caching or (ii) strong consistency with slow\nwrite-through I/O, a limitation that has kept FUSE distributed file systems out\nof write-intensive cloud workloads.\n  To this end, we present DFUSE, the first distributed FUSE file system that\ndelivers write-back kernel caching and strong consistency. DFUSE achieves this\nby offloading userspace consistency control to the kernel driver, allowing\ncoordinated access to the kernel's page cache across nodes. This design\neliminates blind local cache updates and ensures cluster-wide strong\nconsistency without compromising performance. In our evaluation, DFUSE achieves\nup to 68.0% higher throughput and 40.4% lower latency than the existing\nwrite-through design of FUSE-based distributed file systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud platforms host thousands of tenants that demand POSIX semantics, high\nthroughput, and rapid evolution from their storage layer. Kernel-native\ndistributed file systems supply raw speed, but their privileged code base\ncouples every release to the kernel, widens the blast radius of crashes, and\nslows innovation. FUSE-based distributed file systems flip those trade-offs:\nthey run in user space for fast deployment and strong fault isolation, yet the\nFUSE interface disables the kernel's write-back page cache whenever strong\nconsistency is required. Practitioners must therefore choose between (i) weak\nconsistency with fast write-back caching or (ii) strong consistency with slow\nwrite-through I/O, a limitation that has kept FUSE distributed file systems out\nof write-intensive cloud workloads.\n  To this end, we present DFUSE, the first distributed FUSE file system that\ndelivers write-back kernel caching and strong consistency. DFUSE achieves this\nby offloading userspace consistency control to the kernel driver, allowing\ncoordinated access to the kernel's page cache across nodes. This design\neliminates blind local cache updates and ensures cluster-wide strong\nconsistency without compromising performance. In our evaluation, DFUSE achieves\nup to 68.0% higher throughput and 40.4% lower latency than the existing\nwrite-through design of FUSE-based distributed file systems."
                },
                "authors": [
                    {
                        "name": "Haoyu Li"
                    },
                    {
                        "name": "Jingkai Fu"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Windsor Hsu"
                    },
                    {
                        "name": "Asaf Cidon"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Cidon"
                },
                "author": "Asaf Cidon",
                "arxiv_doi": "10.1145/3772052.3772208",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3772052.3772208",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.18191v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18191v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This is the version accepted to ACM SoCC 2025. The title has been\n  updated to match the published version",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15878v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15878v2",
                "updated": "2025-10-21T16:32:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    32,
                    50,
                    1,
                    294,
                    0
                ],
                "published": "2025-08-21T16:10:26Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    16,
                    10,
                    26,
                    3,
                    233,
                    0
                ],
                "title": "Putting the Context back into Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Putting the Context back into Memory"
                },
                "summary": "Requests arriving at main memory are often different from what programmers\ncan observe or estimate by using CPU-based monitoring. Hardware cache\nprefetching, memory request scheduling and interleaving cause a loss of\nobservability that limits potential data movement and tiering optimizations. In\nresponse, memory-side telemetry hardware like page access heat map units (HMU)\nand page prefetchers were proposed to inform Operating Systems with accurate\nusage data. However, it is still hard to map memory activity to software\nprogram functions and objects because of the decoupled nature of host\nprocessors and memory devices. Valuable program context is stripped out from\nthe memory bus, leaving only commands, addresses and data. Programmers have\nexpert knowledge of future data accesses, priorities, and access to processor\nstate, which could be useful hints for runtime memory device optimization. This\npaper makes context visible at memory devices by encoding any user-visible\nstate as detectable packets in the memory read address stream, in a\nnondestructive manner without significant capacity overhead, drivers or special\naccess privileges. We prototyped an end-to-end system with metadata injection\nthat can be reliably detected and decoded from a memory address trace, either\nby a host processor, or a memory module. We illustrate a use case with precise\ncode execution markers and object address range tracking. In the future, real\ntime metadata decoding with near-memory computing (NMC) could provide\ncustomized telemetry and statistics to users, or act on application hints to\nperform functions like prioritizing requests, remapping data and reconfiguring\ndevices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Requests arriving at main memory are often different from what programmers\ncan observe or estimate by using CPU-based monitoring. Hardware cache\nprefetching, memory request scheduling and interleaving cause a loss of\nobservability that limits potential data movement and tiering optimizations. In\nresponse, memory-side telemetry hardware like page access heat map units (HMU)\nand page prefetchers were proposed to inform Operating Systems with accurate\nusage data. However, it is still hard to map memory activity to software\nprogram functions and objects because of the decoupled nature of host\nprocessors and memory devices. Valuable program context is stripped out from\nthe memory bus, leaving only commands, addresses and data. Programmers have\nexpert knowledge of future data accesses, priorities, and access to processor\nstate, which could be useful hints for runtime memory device optimization. This\npaper makes context visible at memory devices by encoding any user-visible\nstate as detectable packets in the memory read address stream, in a\nnondestructive manner without significant capacity overhead, drivers or special\naccess privileges. We prototyped an end-to-end system with metadata injection\nthat can be reliably detected and decoded from a memory address trace, either\nby a host processor, or a memory module. We illustrate a use case with precise\ncode execution markers and object address range tracking. In the future, real\ntime metadata decoding with near-memory computing (NMC) could provide\ncustomized telemetry and statistics to users, or act on application hints to\nperform functions like prioritizing requests, remapping data and reconfiguring\ndevices."
                },
                "authors": [
                    {
                        "name": "David A. Roberts"
                    }
                ],
                "author_detail": {
                    "name": "David A. Roberts"
                },
                "author": "David A. Roberts",
                "arxiv_comment": "Fixed errors in paragraph numbering",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15878v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15878v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18716v1",
                "updated": "2025-10-21T15:17:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    17,
                    37,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T15:17:37Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    17,
                    37,
                    1,
                    294,
                    0
                ],
                "title": "SSD: Spatial-Semantic Head Decoupling for Efficient Autoregressive Image\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SSD: Spatial-Semantic Head Decoupling for Efficient Autoregressive Image\n  Generation"
                },
                "summary": "Autoregressive image generation models like Janus-Pro produce high-quality\nimages, but at the significant cost of high memory and ever-growing\ncomputational demands due to the large number of visual tokens. While KV cache\ncompression has been extensively studied in language modeling, it still remains\nlargely unexplored for the image generation domain. In this work, we begin by\nidentifying a distinct and prominent attention phenomenon, which we term\nspatial locality and emergent semantic sink. To leverage this key insight, we\nintroduce a novel KV cache compression framework. Specifically, we compress the\nKV cache for all visual tokens by adaptively decoupling attention heads into\ntwo separate types: for spatial-locality heads, our method maintains a short\nrecent token window; for semantic-sink heads, it strategically preserves a\ncompact set of highly-attended tokens. Our extensive experiments demonstrate\nthat the proposed method achieves a 5$\\times$ reduction in memory usage and a\nnotable 6.6$\\times$ speedup in overall throughput with only minimal visual\nquality loss, thereby enabling highly efficient native autoregressive image\ngeneration on resource-constrained hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive image generation models like Janus-Pro produce high-quality\nimages, but at the significant cost of high memory and ever-growing\ncomputational demands due to the large number of visual tokens. While KV cache\ncompression has been extensively studied in language modeling, it still remains\nlargely unexplored for the image generation domain. In this work, we begin by\nidentifying a distinct and prominent attention phenomenon, which we term\nspatial locality and emergent semantic sink. To leverage this key insight, we\nintroduce a novel KV cache compression framework. Specifically, we compress the\nKV cache for all visual tokens by adaptively decoupling attention heads into\ntwo separate types: for spatial-locality heads, our method maintains a short\nrecent token window; for semantic-sink heads, it strategically preserves a\ncompact set of highly-attended tokens. Our extensive experiments demonstrate\nthat the proposed method achieves a 5$\\times$ reduction in memory usage and a\nnotable 6.6$\\times$ speedup in overall throughput with only minimal visual\nquality loss, thereby enabling highly efficient native autoregressive image\ngeneration on resource-constrained hardware."
                },
                "authors": [
                    {
                        "name": "Siyong Jian"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14576v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14576v3",
                "updated": "2025-10-21T15:13:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    13,
                    47,
                    1,
                    294,
                    0
                ],
                "published": "2024-01-26T00:27:00Z",
                "published_parsed": [
                    2024,
                    1,
                    26,
                    0,
                    27,
                    0,
                    4,
                    26,
                    0
                ],
                "title": "ParaLog: Consistent Host-side Logging for Parallel Checkpoints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParaLog: Consistent Host-side Logging for Parallel Checkpoints"
                },
                "summary": "Output-intensive scientific applications are highly sensitive to low storage\nthroughput. While existing scientific application stacks are optimized for\ntraditional High-Performance Computing (HPC) environments with high remote\nstorage and network bandwidth, these assumptions often fail in modern settings\nlike cloud deployment. This is because the existing scientific application I/O\nstack fails to leverage the available resources. At the same time, scientific\napplications exhibit special synchronization and data output requirements that\nare difficult to satisfy using traditional approaches such as block-level or\nfilesystem-level caching. We introduce ParaLog, a distributed host-side logging\napproach designed to accelerate scientific applications transparently. ParaLog\nemphasizes deployability, enabling support for unmodified message passing\ninterface (MPI) applications and implementations while preserving crash\nconsistency semantics. We evaluate ParaLog across traditional HPC, cloud HPC,\nlocal clusters, and hybrid environments, demonstrating its capability to reduce\nend-to-end execution time by 13-26% for popular scientific applications in\ncloud settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Output-intensive scientific applications are highly sensitive to low storage\nthroughput. While existing scientific application stacks are optimized for\ntraditional High-Performance Computing (HPC) environments with high remote\nstorage and network bandwidth, these assumptions often fail in modern settings\nlike cloud deployment. This is because the existing scientific application I/O\nstack fails to leverage the available resources. At the same time, scientific\napplications exhibit special synchronization and data output requirements that\nare difficult to satisfy using traditional approaches such as block-level or\nfilesystem-level caching. We introduce ParaLog, a distributed host-side logging\napproach designed to accelerate scientific applications transparently. ParaLog\nemphasizes deployability, enabling support for unmodified message passing\ninterface (MPI) applications and implementations while preserving crash\nconsistency semantics. We evaluate ParaLog across traditional HPC, cloud HPC,\nlocal clusters, and hybrid environments, demonstrating its capability to reduce\nend-to-end execution time by 13-26% for popular scientific applications in\ncloud settings."
                },
                "authors": [
                    {
                        "name": "Steven W. D. Chien"
                    },
                    {
                        "name": "Kento Sato"
                    },
                    {
                        "name": "Artur Podobas"
                    },
                    {
                        "name": "Niclas Jansson"
                    },
                    {
                        "name": "Stefano Markidis"
                    },
                    {
                        "name": "Michio Honda"
                    }
                ],
                "author_detail": {
                    "name": "Michio Honda"
                },
                "author": "Michio Honda",
                "arxiv_doi": "10.1145/3772052.3772212",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3772052.3772212",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.14576v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14576v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to SoCC 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18672v1",
                "updated": "2025-10-21T14:25:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    25,
                    51,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T14:25:51Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    25,
                    51,
                    1,
                    294,
                    0
                ],
                "title": "Reasoning Language Model Inference Serving Unveiled: An Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Language Model Inference Serving Unveiled: An Empirical Study"
                },
                "summary": "The reasoning large language model (RLLM) has been proven competitive in\nsolving complex reasoning tasks such as mathematics, coding, compared to\ngeneral LLM. However, the serving performance and behavior of RLLM remains\nunexplored, which may undermine the deployment and utilization of RLLM in\nreal-world scenario. To close this gap, in this paper, we conduct a\ncomprehensive study of RLLM service. We first perform a pilot study on\ncomparing the serving performance between RLLM and traditional LLM and reveal\nthat there are several distinct differences regarding serving behavior: (1)\nsignificant memory usage and fluctuations; (2) straggler requests; (3) adaptive\nrunning time; (4) domain preference. Then we further investigate whether\nexisting inference optimization techniques are valid for RLLM. Our main\ntakeaways are that model quantization methods and speculative decoding can\nimprove service system efficiency with small compromise to RLLM accuracy, while\nprefix caching, KV cache quantization may even degrade accuracy or serving\nperformance for small RLLM. Lastly, we conduct evaluation under real world\nworkload modeled by Gamma distribution to verify our findings. Empirical\nresults of real world workload evaluation across different dataset are aligned\nwith our main findings regarding RLLM serving. We hope our work can provide the\nresearch community and industry with insights to advance RLLM inference\nserving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reasoning large language model (RLLM) has been proven competitive in\nsolving complex reasoning tasks such as mathematics, coding, compared to\ngeneral LLM. However, the serving performance and behavior of RLLM remains\nunexplored, which may undermine the deployment and utilization of RLLM in\nreal-world scenario. To close this gap, in this paper, we conduct a\ncomprehensive study of RLLM service. We first perform a pilot study on\ncomparing the serving performance between RLLM and traditional LLM and reveal\nthat there are several distinct differences regarding serving behavior: (1)\nsignificant memory usage and fluctuations; (2) straggler requests; (3) adaptive\nrunning time; (4) domain preference. Then we further investigate whether\nexisting inference optimization techniques are valid for RLLM. Our main\ntakeaways are that model quantization methods and speculative decoding can\nimprove service system efficiency with small compromise to RLLM accuracy, while\nprefix caching, KV cache quantization may even degrade accuracy or serving\nperformance for small RLLM. Lastly, we conduct evaluation under real world\nworkload modeled by Gamma distribution to verify our findings. Empirical\nresults of real world workload evaluation across different dataset are aligned\nwith our main findings regarding RLLM serving. We hope our work can provide the\nresearch community and industry with insights to advance RLLM inference\nserving."
                },
                "authors": [
                    {
                        "name": "Qi Li"
                    },
                    {
                        "name": "Junpan Wu"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Yuhan Chen"
                    },
                    {
                        "name": "Shaohuai Shi"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18586v1",
                "updated": "2025-10-21T12:39:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    39,
                    32,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T12:39:32Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    39,
                    32,
                    1,
                    294,
                    0
                ],
                "title": "Tokencake: A KV-Cache-centric Serving Framework for LLM-based\n  Multi-Agent Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokencake: A KV-Cache-centric Serving Framework for LLM-based\n  Multi-Agent Applications"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in complex multi-agent\napplications that use external function calls. This workload creates severe\nperformance challenges for the KV Cache: space contention leads to the eviction\nof critical agents' caches and time underutilization leaves the cache of agents\nstalled on long-running tool calls idling in GPU memory. We present Tokencake,\na KV-Cache-centric serving framework that co-optimizes scheduling and memory\nmanagement with an agent-aware design. Tokencake's Space Scheduler uses dynamic\nmemory partitioning to shield critical agents from contention, while its Time\nScheduler employs a proactive offload and predictive upload mechanism to\nrepurpose GPU memory during function call stalls. Our evaluation on\nrepresentative multi-agent benchmarks shows that Tokencake can reduce\nend-to-end latency by over 47.06%, improve effective GPU memory utilization by\nup to 16.9% compared to vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in complex multi-agent\napplications that use external function calls. This workload creates severe\nperformance challenges for the KV Cache: space contention leads to the eviction\nof critical agents' caches and time underutilization leaves the cache of agents\nstalled on long-running tool calls idling in GPU memory. We present Tokencake,\na KV-Cache-centric serving framework that co-optimizes scheduling and memory\nmanagement with an agent-aware design. Tokencake's Space Scheduler uses dynamic\nmemory partitioning to shield critical agents from contention, while its Time\nScheduler employs a proactive offload and predictive upload mechanism to\nrepurpose GPU memory during function call stalls. Our evaluation on\nrepresentative multi-agent benchmarks shows that Tokencake can reduce\nend-to-end latency by over 47.06%, improve effective GPU memory utilization by\nup to 16.9% compared to vLLM."
                },
                "authors": [
                    {
                        "name": "Zhuohang Bian"
                    },
                    {
                        "name": "Feiyang Wu"
                    },
                    {
                        "name": "Teng Ma"
                    },
                    {
                        "name": "Youwei Zhuo"
                    }
                ],
                "author_detail": {
                    "name": "Youwei Zhuo"
                },
                "author": "Youwei Zhuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18546v1",
                "updated": "2025-10-21T11:52:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    11,
                    52,
                    44,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T11:52:44Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    11,
                    52,
                    44,
                    1,
                    294,
                    0
                ],
                "title": "EfficientNav: Towards On-Device Object-Goal Navigation with Navigation\n  Map Caching and Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EfficientNav: Towards On-Device Object-Goal Navigation with Navigation\n  Map Caching and Retrieval"
                },
                "summary": "Object-goal navigation (ObjNav) tasks an agent with navigating to the\nlocation of a specific object in an unseen environment. Embodied agents\nequipped with large language models (LLMs) and online constructed navigation\nmaps can perform ObjNav in a zero-shot manner. However, existing agents heavily\nrely on giant LLMs on the cloud, e.g., GPT-4, while directly switching to small\nLLMs, e.g., LLaMA3.2-11b, suffer from significant success rate drops due to\nlimited model capacity for understanding complex navigation maps, which\nprevents deploying ObjNav on local devices. At the same time, the long prompt\nintroduced by the navigation map description will cause high planning latency\non local devices. In this paper, we propose EfficientNav to enable on-device\nefficient LLM-based zero-shot ObjNav. To help the smaller LLMs better\nunderstand the environment, we propose semantics-aware memory retrieval to\nprune redundant information in navigation maps. To reduce planning latency, we\npropose discrete memory caching and attention-based memory clustering to\nefficiently save and re-use the KV cache. Extensive experimental results\ndemonstrate that EfficientNav achieves 11.1% improvement in success rate on\nHM3D benchmark over GPT-4-based baselines, and demonstrates 6.7x real-time\nlatency reduction and 4.7x end-to-end latency reduction over GPT-4 planner. Our\ncode will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object-goal navigation (ObjNav) tasks an agent with navigating to the\nlocation of a specific object in an unseen environment. Embodied agents\nequipped with large language models (LLMs) and online constructed navigation\nmaps can perform ObjNav in a zero-shot manner. However, existing agents heavily\nrely on giant LLMs on the cloud, e.g., GPT-4, while directly switching to small\nLLMs, e.g., LLaMA3.2-11b, suffer from significant success rate drops due to\nlimited model capacity for understanding complex navigation maps, which\nprevents deploying ObjNav on local devices. At the same time, the long prompt\nintroduced by the navigation map description will cause high planning latency\non local devices. In this paper, we propose EfficientNav to enable on-device\nefficient LLM-based zero-shot ObjNav. To help the smaller LLMs better\nunderstand the environment, we propose semantics-aware memory retrieval to\nprune redundant information in navigation maps. To reduce planning latency, we\npropose discrete memory caching and attention-based memory clustering to\nefficiently save and re-use the KV cache. Extensive experimental results\ndemonstrate that EfficientNav achieves 11.1% improvement in success rate on\nHM3D benchmark over GPT-4-based baselines, and demonstrates 6.7x real-time\nlatency reduction and 4.7x end-to-end latency reduction over GPT-4 planner. Our\ncode will be released soon."
                },
                "authors": [
                    {
                        "name": "Zebin Yang"
                    },
                    {
                        "name": "Sunjian Zheng"
                    },
                    {
                        "name": "Tong Xie"
                    },
                    {
                        "name": "Tianshi Xu"
                    },
                    {
                        "name": "Bo Yu"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Shaoshan Liu"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02175v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02175v2",
                "updated": "2025-10-21T10:33:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    33,
                    29,
                    1,
                    294,
                    0
                ],
                "published": "2025-02-04T09:48:14Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    48,
                    14,
                    1,
                    35,
                    0
                ],
                "title": "VLA-Cache: Efficient Vision-Language-Action Manipulation via Adaptive\n  Token Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLA-Cache: Efficient Vision-Language-Action Manipulation via Adaptive\n  Token Caching"
                },
                "summary": "Vision-Language-Action (VLA) models have demonstrated strong multi-modal\nreasoning capabilities, enabling direct action generation from visual\nperception and language instructions in an end-to-end manner. However, their\nsubstantial computational cost poses a challenge for real-time robotic control,\nwhere rapid decision-making is essential. This paper introduces VLA-Cache, a\ntraining-free inference acceleration method that reduces computational overhead\nby adaptively caching and reusing static visual tokens across frames.\nExploiting the temporal continuity in robotic manipulation, VLA-Cache\nidentifies minimally changed tokens between adjacent frames and reuses their\ncached key-value representations, thereby circumventing redundant computations.\nAdditionally, to maintain action precision, VLA-Cache selectively re-computes\ntask-relevant tokens that are environmentally sensitive, ensuring the fidelity\nof critical visual information. To further optimize efficiency, we introduce a\nlayer adaptive token reusing strategy that dynamically adjusts the reuse ratio\nbased on attention concentration across decoder layers, prioritizing critical\ntokens for recomputation. Extensive experiments on two simulation platforms\n(LIBERO and SIMPLER) and a real-world robotic system demonstrate that VLA-Cache\nachieves up to 1.7x speedup in CUDA latency and a 15% increase in control\nfrequency, with negligible loss on task success rate. The code and videos can\nbe found at our project page: https://vla-cache.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have demonstrated strong multi-modal\nreasoning capabilities, enabling direct action generation from visual\nperception and language instructions in an end-to-end manner. However, their\nsubstantial computational cost poses a challenge for real-time robotic control,\nwhere rapid decision-making is essential. This paper introduces VLA-Cache, a\ntraining-free inference acceleration method that reduces computational overhead\nby adaptively caching and reusing static visual tokens across frames.\nExploiting the temporal continuity in robotic manipulation, VLA-Cache\nidentifies minimally changed tokens between adjacent frames and reuses their\ncached key-value representations, thereby circumventing redundant computations.\nAdditionally, to maintain action precision, VLA-Cache selectively re-computes\ntask-relevant tokens that are environmentally sensitive, ensuring the fidelity\nof critical visual information. To further optimize efficiency, we introduce a\nlayer adaptive token reusing strategy that dynamically adjusts the reuse ratio\nbased on attention concentration across decoder layers, prioritizing critical\ntokens for recomputation. Extensive experiments on two simulation platforms\n(LIBERO and SIMPLER) and a real-world robotic system demonstrate that VLA-Cache\nachieves up to 1.7x speedup in CUDA latency and a 15% increase in control\nfrequency, with negligible loss on task success rate. The code and videos can\nbe found at our project page: https://vla-cache.github.io."
                },
                "authors": [
                    {
                        "name": "Siyu Xu"
                    },
                    {
                        "name": "Yunke Wang"
                    },
                    {
                        "name": "Chenghao Xia"
                    },
                    {
                        "name": "Dihao Zhu"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02175v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02175v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06436v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06436v2",
                "updated": "2025-10-21T10:08:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    8,
                    33,
                    1,
                    294,
                    0
                ],
                "published": "2025-09-08T08:34:02Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    34,
                    2,
                    0,
                    251,
                    0
                ],
                "title": "Tree of Agents: Improving Long-Context Capabilities of Large Language\n  Models through Multi-Perspective Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Agents: Improving Long-Context Capabilities of Large Language\n  Models through Multi-Perspective Reasoning"
                },
                "summary": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents."
                },
                "authors": [
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Xiaofei Xu"
                    },
                    {
                        "name": "Ke Deng"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Lin Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lin Tian"
                },
                "author": "Lin Tian",
                "arxiv_comment": "19 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06436v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06436v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18480v1",
                "updated": "2025-10-21T10:00:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    0,
                    32,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T10:00:32Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    0,
                    32,
                    1,
                    294,
                    0
                ],
                "title": "How Efficient Are Diffusion Language Models? A Critical Examination of\n  Efficiency Evaluation Practices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Efficient Are Diffusion Language Models? A Critical Examination of\n  Efficiency Evaluation Practices"
                },
                "summary": "Diffusion language models (DLMs) have emerged as a promising alternative to\nthe long-dominant autoregressive (AR) paradigm, offering a parallelable\ndecoding process that could yield greater efficiency. Yet, in practice, current\nopen-source DLMs often underperform their AR counterparts in speed, limiting\ntheir real-world utility. This work presents a systematic study of DLM\nefficiency, identifying key issues in prior evaluation methods. Through\nempirical benchmarking and a roofline-based theoretical analysis, we\ndemonstrate that AR models generally achieve higher throughput, while DLMs\nconsistently lag. We also investigate acceleration strategies, finding that\ntechniques like dual cache and parallel decoding mainly offer gains at small\nbatch sizes, with their benefits diminishing upon scaling. Our findings\nunderscore the necessity of robust evaluation methods and improved acceleration\nstrategies to advance research on DLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models (DLMs) have emerged as a promising alternative to\nthe long-dominant autoregressive (AR) paradigm, offering a parallelable\ndecoding process that could yield greater efficiency. Yet, in practice, current\nopen-source DLMs often underperform their AR counterparts in speed, limiting\ntheir real-world utility. This work presents a systematic study of DLM\nefficiency, identifying key issues in prior evaluation methods. Through\nempirical benchmarking and a roofline-based theoretical analysis, we\ndemonstrate that AR models generally achieve higher throughput, while DLMs\nconsistently lag. We also investigate acceleration strategies, finding that\ntechniques like dual cache and parallel decoding mainly offer gains at small\nbatch sizes, with their benefits diminishing upon scaling. Our findings\nunderscore the necessity of robust evaluation methods and improved acceleration\nstrategies to advance research on DLMs."
                },
                "authors": [
                    {
                        "name": "Han Peng"
                    },
                    {
                        "name": "Peiyu Liu"
                    },
                    {
                        "name": "Zican Dong"
                    },
                    {
                        "name": "Daixuan Cheng"
                    },
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Yiru Tang"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wayne Xin Zhao"
                },
                "author": "Wayne Xin Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18413v1",
                "updated": "2025-10-21T08:44:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    8,
                    44,
                    47,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T08:44:47Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    8,
                    44,
                    47,
                    1,
                    294,
                    0
                ],
                "title": "Adamas: Hadamard Sparse Attention for Efficient Long-Context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adamas: Hadamard Sparse Attention for Efficient Long-Context Inference"
                },
                "summary": "Large language models (LLMs) now support context windows of hundreds of\nthousands to millions of tokens, enabling applications such as long-document\nsummarization, large-scale code synthesis, multi-document question answering\nand persistent multi-turn dialogue. However, such extended contexts exacerbate\nthe quadratic cost of self-attention, leading to severe latency in\nautoregressive decoding. Existing sparse attention methods alleviate these\ncosts but rely on heuristic patterns that struggle to recall critical key-value\n(KV) pairs for each query, resulting in accuracy degradation. We introduce\nAdamas, a lightweight yet highly accurate sparse attention mechanism designed\nfor long-context inference. Adamas applies the Hadamard transform,\nbucketization and 2-bit compression to produce compact representations, and\nleverages Manhattan-distance estimation for efficient top-k selections.\nExperiments show that Adamas matches the accuracy of full attention with only a\n64-token budget, achieves near-lossless performance at 128, and supports up to\n8x higher sparsity than prior state-of-the-art (SOTA) methods while delivering\nup to 4.4x self-attention and 1.5x end-to-end speedups on 32K-length sequences.\nRemarkably, Adamas attains comparable or even lower perplexity than full\nattention, underscoring its effectiveness in maintaining accuracy under\naggressive sparsity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) now support context windows of hundreds of\nthousands to millions of tokens, enabling applications such as long-document\nsummarization, large-scale code synthesis, multi-document question answering\nand persistent multi-turn dialogue. However, such extended contexts exacerbate\nthe quadratic cost of self-attention, leading to severe latency in\nautoregressive decoding. Existing sparse attention methods alleviate these\ncosts but rely on heuristic patterns that struggle to recall critical key-value\n(KV) pairs for each query, resulting in accuracy degradation. We introduce\nAdamas, a lightweight yet highly accurate sparse attention mechanism designed\nfor long-context inference. Adamas applies the Hadamard transform,\nbucketization and 2-bit compression to produce compact representations, and\nleverages Manhattan-distance estimation for efficient top-k selections.\nExperiments show that Adamas matches the accuracy of full attention with only a\n64-token budget, achieves near-lossless performance at 128, and supports up to\n8x higher sparsity than prior state-of-the-art (SOTA) methods while delivering\nup to 4.4x self-attention and 1.5x end-to-end speedups on 32K-length sequences.\nRemarkably, Adamas attains comparable or even lower perplexity than full\nattention, underscoring its effectiveness in maintaining accuracy under\naggressive sparsity."
                },
                "authors": [
                    {
                        "name": "Siyuan Yan"
                    },
                    {
                        "name": "Guo-Qing Jiang"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Xiaoxing Ma"
                    },
                    {
                        "name": "Ran Zhu"
                    },
                    {
                        "name": "Chun Cao"
                    },
                    {
                        "name": "Jingwei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jingwei Xu"
                },
                "author": "Jingwei Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v5",
                "updated": "2025-10-21T06:47:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    6,
                    47,
                    29,
                    1,
                    294,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was first submitted for review on Sept. 5, 2024, and the\n  initial version was uploaded to Arxiv on Sept. 30, 2024. The latest version\n  has accepted for publication by IEEE Transactions on Information Forensics\n  and Security (TIFS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14374v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14374v3",
                "updated": "2025-10-21T06:30:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    6,
                    30,
                    21,
                    1,
                    294,
                    0
                ],
                "published": "2025-04-19T18:25:20Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "title": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation"
                },
                "summary": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model."
                },
                "authors": [
                    {
                        "name": "Max Lübke"
                    },
                    {
                        "name": "Marco De Lucia"
                    },
                    {
                        "name": "Steffen Christgau"
                    },
                    {
                        "name": "Stefan Petri"
                    },
                    {
                        "name": "Bettina Schnor"
                    }
                ],
                "author_detail": {
                    "name": "Bettina Schnor"
                },
                "author": "Bettina Schnor",
                "arxiv_doi": "10.1007/978-3-031-97635-3_28",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-97635-3_28",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.14374v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14374v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Long version, 15 pages, 6 figures; Short version (8 pages) included\n  in the proceedings of \"25th International Conference on Computational\n  Science\" (ICCS25)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18269v1",
                "updated": "2025-10-21T03:39:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    3,
                    39,
                    41,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T03:39:41Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    3,
                    39,
                    41,
                    1,
                    294,
                    0
                ],
                "title": "StreamingTOM: Streaming Token Compression for Efficient Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamingTOM: Streaming Token Compression for Efficient Video\n  Understanding"
                },
                "summary": "Unlike offline processing, streaming video vision-language models face two\nfundamental constraints: causality and accumulation. Causality prevents access\nto future frames that offline methods exploit, while accumulation causes tokens\nto grow unbounded, creating efficiency bottlenecks. However, existing\napproaches only regulate post-LLM kv-cache, leaving costly pre-LLM prefill\nunchanged. We introduce StreamingTOM, a training-free, plug-and-play two-stage\nframework that addresses both pre-LLM and post-LLM bottlenecks with predictable\nlatency. Causal Temporal Reduction imposes a fixed per-frame budget and selects\ntokens based on adjacent-frame changes and token saliency, drastically reducing\nper-frame prefill cost by processing only a compact subset of visual tokens per\nframe instead of all visual tokens. Online Quantized Memory stores tokens in\n4-bit format, retrieves relevant groups on demand, and dequantizes them,\nkeeping the active kv-cache bounded regardless of stream length. Experiments\ndemonstrate our method achieves $15.7\\times$ kv-cache compression, $1.2\\times$\nlower peak memory and $2\\times$ faster TTFT compared to prior SOTA.\nStreamingTOM maintains state-of-the-art accuracy among training-free methods\nwith an average of $63.8\\%$ on offline benchmarks and $55.8\\%/3.7$ on RVS.\nThese results highlight the practical benefits of our two-stage approach for\nefficient streaming video understanding with bounded growth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlike offline processing, streaming video vision-language models face two\nfundamental constraints: causality and accumulation. Causality prevents access\nto future frames that offline methods exploit, while accumulation causes tokens\nto grow unbounded, creating efficiency bottlenecks. However, existing\napproaches only regulate post-LLM kv-cache, leaving costly pre-LLM prefill\nunchanged. We introduce StreamingTOM, a training-free, plug-and-play two-stage\nframework that addresses both pre-LLM and post-LLM bottlenecks with predictable\nlatency. Causal Temporal Reduction imposes a fixed per-frame budget and selects\ntokens based on adjacent-frame changes and token saliency, drastically reducing\nper-frame prefill cost by processing only a compact subset of visual tokens per\nframe instead of all visual tokens. Online Quantized Memory stores tokens in\n4-bit format, retrieves relevant groups on demand, and dequantizes them,\nkeeping the active kv-cache bounded regardless of stream length. Experiments\ndemonstrate our method achieves $15.7\\times$ kv-cache compression, $1.2\\times$\nlower peak memory and $2\\times$ faster TTFT compared to prior SOTA.\nStreamingTOM maintains state-of-the-art accuracy among training-free methods\nwith an average of $63.8\\%$ on offline benchmarks and $55.8\\%/3.7$ on RVS.\nThese results highlight the practical benefits of our two-stage approach for\nefficient streaming video understanding with bounded growth."
                },
                "authors": [
                    {
                        "name": "Xueyi Chen"
                    },
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Kele Shao"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17777v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17777v1",
                "updated": "2025-10-20T17:35:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    35,
                    47,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T17:35:47Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    35,
                    47,
                    0,
                    293,
                    0
                ],
                "title": "SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference"
                },
                "summary": "Vision Language Models (VLMs) have rapidly advanced in integrating visual and\ntextual reasoning, powering applications across high-resolution image\nunderstanding, long-video analysis, and multi-turn conversation. However, their\nscalability remains limited by the growing number of visual tokens that\ndominate inference latency. We present SparseVILA, a new paradigm for efficient\nVLM inference that decouples visual sparsity across the prefilling and decoding\nstages. SparseVILA distributes sparsity across stages by pruning redundant\nvisual tokens during prefill and retrieving only query-relevant tokens during\ndecoding. This decoupled design matches leading prefill pruning methods while\npreserving multi-turn fidelity by retaining most of the visual cache so that\nquery-aware tokens can be retrieved at each conversation round. Built on an\nAWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster\nprefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end\nspeedup on long-context video tasks -- while improving accuracy on\ndocument-understanding and reasoning tasks. By decoupling query-agnostic\npruning and query-aware retrieval, SparseVILA establishes a new direction for\nefficient multimodal inference, offering a training-free, architecture-agnostic\nframework for accelerating large VLMs without sacrificing capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) have rapidly advanced in integrating visual and\ntextual reasoning, powering applications across high-resolution image\nunderstanding, long-video analysis, and multi-turn conversation. However, their\nscalability remains limited by the growing number of visual tokens that\ndominate inference latency. We present SparseVILA, a new paradigm for efficient\nVLM inference that decouples visual sparsity across the prefilling and decoding\nstages. SparseVILA distributes sparsity across stages by pruning redundant\nvisual tokens during prefill and retrieving only query-relevant tokens during\ndecoding. This decoupled design matches leading prefill pruning methods while\npreserving multi-turn fidelity by retaining most of the visual cache so that\nquery-aware tokens can be retrieved at each conversation round. Built on an\nAWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster\nprefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end\nspeedup on long-context video tasks -- while improving accuracy on\ndocument-understanding and reasoning tasks. By decoupling query-agnostic\npruning and query-aware retrieval, SparseVILA establishes a new direction for\nefficient multimodal inference, offering a training-free, architecture-agnostic\nframework for accelerating large VLMs without sacrificing capability."
                },
                "authors": [
                    {
                        "name": "Samir Khaki"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Konstantinos N. Plataniotis"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Zhijian Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijian Liu"
                },
                "author": "Zhijian Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17777v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17777v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17238v1",
                "updated": "2025-10-20T07:27:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    7,
                    27,
                    37,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T07:27:37Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    7,
                    27,
                    37,
                    0,
                    293,
                    0
                ],
                "title": "StreamingThinker: Large Language Models Can Think While Reading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamingThinker: Large Language Models Can Think While Reading"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nchain of thought (CoT) reasoning. However, the current LLM reasoning paradigm\ninitiates thinking only after the entire input is available, which introduces\nunnecessary latency and weakens attention to earlier information in dynamic\nscenarios. Inspired by human cognition of thinking while reading, we first\ndesign a \\textit{\\textbf{streaming thinking}} paradigm for LLMs, where\nreasoning unfolds in the order of input and further adjusts its depth once\nreading is complete. We instantiate this paradigm with\n\\textit{StreamingThinker}, a framework that enables LLMs to think while reading\nthrough the integration of streaming CoT generation, streaming-constraint\ntraining, and streaming parallel inference. Specifically, StreamingThinker\nemploys streaming reasoning units with quality control for CoT generation,\nenforces order-preserving reasoning through streaming attention masks and\nposition encoding, and leverages parallel KV caches that decouple input\nencoding from reasoning generation, thereby ensuring alignment and enabling\ntrue concurrency. We evaluate StreamingThinker on the Qwen3 model family across\nmath reasoning, logical reasoning, and context-based QA reasoning tasks.\nExperimental results show that the StreamingThinker preserves performance\ncomparable to batch thinking, while yielding an 80\\% reduction in token waiting\nbefore the onset of reasoning and a more than 60\\% reduction in time-level\nlatency for producing the final answer, demonstrating the effectiveness of the\nstreaming paradigm for LLM reasoning. Code will be released at\n\\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this\nrepository.}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in\nchain of thought (CoT) reasoning. However, the current LLM reasoning paradigm\ninitiates thinking only after the entire input is available, which introduces\nunnecessary latency and weakens attention to earlier information in dynamic\nscenarios. Inspired by human cognition of thinking while reading, we first\ndesign a \\textit{\\textbf{streaming thinking}} paradigm for LLMs, where\nreasoning unfolds in the order of input and further adjusts its depth once\nreading is complete. We instantiate this paradigm with\n\\textit{StreamingThinker}, a framework that enables LLMs to think while reading\nthrough the integration of streaming CoT generation, streaming-constraint\ntraining, and streaming parallel inference. Specifically, StreamingThinker\nemploys streaming reasoning units with quality control for CoT generation,\nenforces order-preserving reasoning through streaming attention masks and\nposition encoding, and leverages parallel KV caches that decouple input\nencoding from reasoning generation, thereby ensuring alignment and enabling\ntrue concurrency. We evaluate StreamingThinker on the Qwen3 model family across\nmath reasoning, logical reasoning, and context-based QA reasoning tasks.\nExperimental results show that the StreamingThinker preserves performance\ncomparable to batch thinking, while yielding an 80\\% reduction in token waiting\nbefore the onset of reasoning and a more than 60\\% reduction in time-level\nlatency for producing the final answer, demonstrating the effectiveness of the\nstreaming paradigm for LLM reasoning. Code will be released at\n\\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this\nrepository.}"
                },
                "authors": [
                    {
                        "name": "Junlong Tong"
                    },
                    {
                        "name": "Yingqi Fan"
                    },
                    {
                        "name": "Anhao Zhao"
                    },
                    {
                        "name": "Yunpu Ma"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Shen"
                },
                "author": "Xiaoyu Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17098v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17098v1",
                "updated": "2025-10-20T02:04:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    2,
                    4,
                    18,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T02:04:18Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    2,
                    4,
                    18,
                    0,
                    293,
                    0
                ],
                "title": "Can Transformer Memory Be Corrupted? Investigating Cache-Side\n  Vulnerabilities in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Transformer Memory Be Corrupted? Investigating Cache-Side\n  Vulnerabilities in Large Language Models"
                },
                "summary": "Even when prompts and parameters are secured, transformer language models\nremain vulnerable because their key-value (KV) cache during inference\nconstitutes an overlooked attack surface. This paper introduces Malicious Token\nInjection (MTI), a modular framework that systematically perturbs cached key\nvectors at selected layers and timesteps through controlled magnitude and\nfrequency, using additive Gaussian noise, zeroing, and orthogonal rotations. A\ntheoretical analysis quantifies how these perturbations propagate through\nattention, linking logit deviations to the Frobenius norm of corruption and\nsoftmax Lipschitz dynamics. Empirical results show that MTI significantly\nalters next-token distributions and downstream task performance across GPT-2\nand LLaMA-2/7B, as well as destabilizes retrieval-augmented and agentic\nreasoning pipelines. These findings identify cache integrity as a critical yet\nunderexplored vulnerability in current LLM deployments, positioning cache\ncorruption as a reproducible and theoretically grounded threat model for future\nrobustness and security research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Even when prompts and parameters are secured, transformer language models\nremain vulnerable because their key-value (KV) cache during inference\nconstitutes an overlooked attack surface. This paper introduces Malicious Token\nInjection (MTI), a modular framework that systematically perturbs cached key\nvectors at selected layers and timesteps through controlled magnitude and\nfrequency, using additive Gaussian noise, zeroing, and orthogonal rotations. A\ntheoretical analysis quantifies how these perturbations propagate through\nattention, linking logit deviations to the Frobenius norm of corruption and\nsoftmax Lipschitz dynamics. Empirical results show that MTI significantly\nalters next-token distributions and downstream task performance across GPT-2\nand LLaMA-2/7B, as well as destabilizes retrieval-augmented and agentic\nreasoning pipelines. These findings identify cache integrity as a critical yet\nunderexplored vulnerability in current LLM deployments, positioning cache\ncorruption as a reproducible and theoretically grounded threat model for future\nrobustness and security research."
                },
                "authors": [
                    {
                        "name": "Elias Hossain"
                    },
                    {
                        "name": "Swayamjit Saha"
                    },
                    {
                        "name": "Somshubhra Roy"
                    },
                    {
                        "name": "Ravi Prasad"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Prasad"
                },
                "author": "Ravi Prasad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17098v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17045v1",
                "updated": "2025-10-19T23:17:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    19,
                    23,
                    17,
                    13,
                    6,
                    292,
                    0
                ],
                "published": "2025-10-19T23:17:13Z",
                "published_parsed": [
                    2025,
                    10,
                    19,
                    23,
                    17,
                    13,
                    6,
                    292,
                    0
                ],
                "title": "Video Reasoning without Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Reasoning without Training"
                },
                "summary": "Video reasoning using Large Multimodal Models (LMMs) relies on costly\nreinforcement learning (RL) and verbose chain-of-thought, resulting in\nsubstantial computational overhead during both training and inference.\nMoreover, the mechanisms that control the thinking process in these reasoning\nmodels are very limited. In this paper, using entropy of the model's output as\na signal, we discover that the high-quality models go through a series of\nmicro-explorations and micro-exploitations which keep the reasoning process\ngrounded (i.e., avoid excessive randomness while the model is exploring or\nthinking through an answer). We further observe that once this \"thinking\"\nprocess is over, more accurate models demonstrate a better convergence by\nreducing the entropy significantly via a final exploitation phase (i.e., a more\ncertain convergence towards a solution trajectory). We then use these novel,\ntheoretically-grounded insights to tune the model's behavior directly at\ninference, without using any RL or supervised fine-tuning. Specifically, during\ninference, our proposed approach called V-Reason (Video-Reason) adapts the\nvalue cache of the LMM via a few optimization steps on a small, trainable\ncontroller using an entropy-based objective, i.e., no supervision from any\ndataset or RL is necessary. This tuning improves the model's micro-exploration\nand exploitation behavior during inference. Our experiments show that our\nproposed method achieves significant improvements over the base\ninstruction-tuned models across several video reasoning datasets, narrowing the\ngap with RL-trained models to within 0.6% average accuracy without any\ntraining, while offering massive efficiency benefits: output tokens are reduced\nby 58.6% compared to the RL model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video reasoning using Large Multimodal Models (LMMs) relies on costly\nreinforcement learning (RL) and verbose chain-of-thought, resulting in\nsubstantial computational overhead during both training and inference.\nMoreover, the mechanisms that control the thinking process in these reasoning\nmodels are very limited. In this paper, using entropy of the model's output as\na signal, we discover that the high-quality models go through a series of\nmicro-explorations and micro-exploitations which keep the reasoning process\ngrounded (i.e., avoid excessive randomness while the model is exploring or\nthinking through an answer). We further observe that once this \"thinking\"\nprocess is over, more accurate models demonstrate a better convergence by\nreducing the entropy significantly via a final exploitation phase (i.e., a more\ncertain convergence towards a solution trajectory). We then use these novel,\ntheoretically-grounded insights to tune the model's behavior directly at\ninference, without using any RL or supervised fine-tuning. Specifically, during\ninference, our proposed approach called V-Reason (Video-Reason) adapts the\nvalue cache of the LMM via a few optimization steps on a small, trainable\ncontroller using an entropy-based objective, i.e., no supervision from any\ndataset or RL is necessary. This tuning improves the model's micro-exploration\nand exploitation behavior during inference. Our experiments show that our\nproposed method achieves significant improvements over the base\ninstruction-tuned models across several video reasoning datasets, narrowing the\ngap with RL-trained models to within 0.6% average accuracy without any\ntraining, while offering massive efficiency benefits: output tokens are reduced\nby 58.6% compared to the RL model."
                },
                "authors": [
                    {
                        "name": "Deepak Sridhar"
                    },
                    {
                        "name": "Kartikeya Bhardwaj"
                    },
                    {
                        "name": "Jeya Pradha Jeyaraj"
                    },
                    {
                        "name": "Nuno Vasconcelos"
                    },
                    {
                        "name": "Ankita Nayak"
                    },
                    {
                        "name": "Harris Teague"
                    }
                ],
                "author_detail": {
                    "name": "Harris Teague"
                },
                "author": "Harris Teague",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16871v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16871v1",
                "updated": "2025-10-19T15:13:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    19,
                    15,
                    13,
                    25,
                    6,
                    292,
                    0
                ],
                "published": "2025-10-19T15:13:25Z",
                "published_parsed": [
                    2025,
                    10,
                    19,
                    15,
                    13,
                    25,
                    6,
                    292,
                    0
                ],
                "title": "Addendum: Systematic Evaluation of Randomized Cache Designs against\n  Cache Occupancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addendum: Systematic Evaluation of Randomized Cache Designs against\n  Cache Occupancy"
                },
                "summary": "In the main text published at USENIX Security 2025, we presented a systematic\nanalysis of the role of cache occupancy in the design considerations for\nrandomized caches (from the perspectives of performance and security). On the\nperformance front, we presented a uniform benchmarking strategy that allows for\na fair comparison among different randomized cache designs. Likewise, from the\nsecurity perspective, we presented three threat assumptions: (1) covert\nchannels; (2) process fingerprinting side-channel; and (3) AES key recovery.\nThe main takeaway of our work is an open problem of designing a randomized\ncache of comparable efficiency with modern set-associative LLCs, while still\nresisting both contention-based and occupancy-based attacks. This note is meant\nas an addendum to the main text in light of the observations made in [2]. To\nsummarize, the authors in [2] argue that (1) L1d cache size plays a role in\nadversarial success, and that (2) a patched version of MIRAGE with randomized\ninitial seeding of global eviction map prevents leakage of AES key. We discuss\nthe same in this addendum.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the main text published at USENIX Security 2025, we presented a systematic\nanalysis of the role of cache occupancy in the design considerations for\nrandomized caches (from the perspectives of performance and security). On the\nperformance front, we presented a uniform benchmarking strategy that allows for\na fair comparison among different randomized cache designs. Likewise, from the\nsecurity perspective, we presented three threat assumptions: (1) covert\nchannels; (2) process fingerprinting side-channel; and (3) AES key recovery.\nThe main takeaway of our work is an open problem of designing a randomized\ncache of comparable efficiency with modern set-associative LLCs, while still\nresisting both contention-based and occupancy-based attacks. This note is meant\nas an addendum to the main text in light of the observations made in [2]. To\nsummarize, the authors in [2] argue that (1) L1d cache size plays a role in\nadversarial success, and that (2) a patched version of MIRAGE with randomized\ninitial seeding of global eviction map prevents leakage of AES key. We discuss\nthe same in this addendum."
                },
                "authors": [
                    {
                        "name": "Anirban Chakraborty"
                    },
                    {
                        "name": "Nimish Mishra"
                    },
                    {
                        "name": "Sayandeep Saha"
                    },
                    {
                        "name": "Sarani Bhattacharya"
                    },
                    {
                        "name": "Debdeep Mukhopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Debdeep Mukhopadhyay"
                },
                "author": "Debdeep Mukhopadhyay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16871v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16871v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16807v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16807v2",
                "updated": "2025-10-23T08:29:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    8,
                    29,
                    11,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-19T12:17:42Z",
                "published_parsed": [
                    2025,
                    10,
                    19,
                    12,
                    17,
                    42,
                    6,
                    292,
                    0
                ],
                "title": "Improving Model Representation and Reducing KV Cache via Skip\n  Connections with First Value Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Model Representation and Reducing KV Cache via Skip\n  Connections with First Value Heads"
                },
                "summary": "Transformer models have driven breakthroughs across various language tasks by\ntheir strong capability to learn rich contextual representations. Scaling them\nto improve representation, however, often demands substantial memory and\ncompute costs, such as the Key-Value (KV) cache used during auto-regressive\ndecoding. Skip connections offer a promising way to improve representation\nwithout bloating resource usage, yet most prior works either improve\nexpressivity while leaving KV costs unchanged, or reduce memory at the cost of\nweaker representation. In this work, we propose SkipV1Former, a Transformer\nvariant that uses skip connections from the first layer's Value heads to\nstrengthen model representation and reduce KV cache. Specifically, from the\nsecond block onward, each layer reuses half of its Value heads from the very\nfirst layer, while computing the other half as usual-cutting Value projections\nand V cache by nearly 50 \\%. Theoretically, we show that routing uncompressed\nfirst-layer Values into deeper layers restores information lost to compression\nand accelerates the model's implicit mesa-optimization-a key pattern of\nTransformer in auto-regressive tasks. Empirically, across different model\nscales, SkipV1Former delivers consistent reductions of approximately 25 \\% in\nKV cache while improving perplexity relative to standard Multi-Head Attention\n(MHA) Transformers and some advanced variants. Moreover, we propose a recipe\nfor uptraining existing MHA Transformer checkpoints to SkipV1Former with only\n10-15\\% additional compute. Finally, SkipV1Former can seamlessly combine\nadvanced methods like Group-Query Attention and Multi-Latent Attention to\nachieve further KV cache savings and performance improvement. When combined\nwith YOCO, it cuts KV cache size by nearly 50 \\% while still improving\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer models have driven breakthroughs across various language tasks by\ntheir strong capability to learn rich contextual representations. Scaling them\nto improve representation, however, often demands substantial memory and\ncompute costs, such as the Key-Value (KV) cache used during auto-regressive\ndecoding. Skip connections offer a promising way to improve representation\nwithout bloating resource usage, yet most prior works either improve\nexpressivity while leaving KV costs unchanged, or reduce memory at the cost of\nweaker representation. In this work, we propose SkipV1Former, a Transformer\nvariant that uses skip connections from the first layer's Value heads to\nstrengthen model representation and reduce KV cache. Specifically, from the\nsecond block onward, each layer reuses half of its Value heads from the very\nfirst layer, while computing the other half as usual-cutting Value projections\nand V cache by nearly 50 \\%. Theoretically, we show that routing uncompressed\nfirst-layer Values into deeper layers restores information lost to compression\nand accelerates the model's implicit mesa-optimization-a key pattern of\nTransformer in auto-regressive tasks. Empirically, across different model\nscales, SkipV1Former delivers consistent reductions of approximately 25 \\% in\nKV cache while improving perplexity relative to standard Multi-Head Attention\n(MHA) Transformers and some advanced variants. Moreover, we propose a recipe\nfor uptraining existing MHA Transformer checkpoints to SkipV1Former with only\n10-15\\% additional compute. Finally, SkipV1Former can seamlessly combine\nadvanced methods like Group-Query Attention and Multi-Latent Attention to\nachieve further KV cache savings and performance improvement. When combined\nwith YOCO, it cuts KV cache size by nearly 50 \\% while still improving\nperformance."
                },
                "authors": [
                    {
                        "name": "Zhoutong Wu"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Yiming Dong"
                    },
                    {
                        "name": "Chenheng Zhang"
                    },
                    {
                        "name": "Cong Fang"
                    },
                    {
                        "name": "Kun Yuan"
                    },
                    {
                        "name": "Zhouchen Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhouchen Lin"
                },
                "author": "Zhouchen Lin",
                "arxiv_comment": "The code is available at:\n  \\url{https://github.com/Zhoutong-Wu/SkipV1Former}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16807v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16807v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16805v1",
                "updated": "2025-10-19T12:16:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    19,
                    12,
                    16,
                    40,
                    6,
                    292,
                    0
                ],
                "published": "2025-10-19T12:16:40Z",
                "published_parsed": [
                    2025,
                    10,
                    19,
                    12,
                    16,
                    40,
                    6,
                    292,
                    0
                ],
                "title": "Mixed-Precision Quantization for Language Models: Techniques and\n  Prospects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixed-Precision Quantization for Language Models: Techniques and\n  Prospects"
                },
                "summary": "The rapid scaling of language models (LMs) has resulted in unprecedented\ncomputational, memory, and energy requirements, making their training and\ndeployment increasingly unsustainable. Quantization has emerged as an essential\ncompression technique to reduce model size, alleviate memory bottlenecks, and\naccelerate inference. However, while uniform low-bit quantization (e.g., INT8,\nINT4) provides significant efficiency gains, it can degrade accuracy in\nsensitive components of transformer-based LMs. Mixed-precision quantization\noffers a promising alternative by selectively allocating precision across\nlayers or within tensors to balance efficiency and accuracy. This survey\nprovides a comprehensive overview of Mixed-Precision quantization frameworks\nfor LMs (MXPLMs). We first review quantization fundamentals, including uniform\nand non-uniform quantizers, quantization granularity, and methods widely used\nin post-training quantization. We then categorize and compare recent MXPLM\nframeworks according to their bit allocation strategies and precision\nconfigurations across weights, activations, and key-value caches. A comparative\nanalysis highlights differences in perplexity, zero-shot task performance, and\ndeployment trade-offs. Furthermore, we contrast MXPLMs with earlier\nmixed-precision quantization methods for deep neural networks, identifying\nstrategies that transfer and those that face challenges in the LM setting.\nFinally, we summarize open issues and future directions, including\nhardware-aware design, activation quantization, and scalable optimization\nmethods for billion-parameter models. By consolidating recent advances, this\nwork serves as a reference for understanding the current landscape and research\nprospects of mixed-precision quantization for large-scale language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid scaling of language models (LMs) has resulted in unprecedented\ncomputational, memory, and energy requirements, making their training and\ndeployment increasingly unsustainable. Quantization has emerged as an essential\ncompression technique to reduce model size, alleviate memory bottlenecks, and\naccelerate inference. However, while uniform low-bit quantization (e.g., INT8,\nINT4) provides significant efficiency gains, it can degrade accuracy in\nsensitive components of transformer-based LMs. Mixed-precision quantization\noffers a promising alternative by selectively allocating precision across\nlayers or within tensors to balance efficiency and accuracy. This survey\nprovides a comprehensive overview of Mixed-Precision quantization frameworks\nfor LMs (MXPLMs). We first review quantization fundamentals, including uniform\nand non-uniform quantizers, quantization granularity, and methods widely used\nin post-training quantization. We then categorize and compare recent MXPLM\nframeworks according to their bit allocation strategies and precision\nconfigurations across weights, activations, and key-value caches. A comparative\nanalysis highlights differences in perplexity, zero-shot task performance, and\ndeployment trade-offs. Furthermore, we contrast MXPLMs with earlier\nmixed-precision quantization methods for deep neural networks, identifying\nstrategies that transfer and those that face challenges in the LM setting.\nFinally, we summarize open issues and future directions, including\nhardware-aware design, activation quantization, and scalable optimization\nmethods for billion-parameter models. By consolidating recent advances, this\nwork serves as a reference for understanding the current landscape and research\nprospects of mixed-precision quantization for large-scale language models."
                },
                "authors": [
                    {
                        "name": "Mariam Rakka"
                    },
                    {
                        "name": "Marios Fournarakis"
                    },
                    {
                        "name": "Olga Krestinskaya"
                    },
                    {
                        "name": "Jinane Bazzi"
                    },
                    {
                        "name": "Khaled N. Salama"
                    },
                    {
                        "name": "Fadi Kurdahi"
                    },
                    {
                        "name": "Ahmed M. Eltawil"
                    },
                    {
                        "name": "Mohammed E. Fouda"
                    }
                ],
                "author_detail": {
                    "name": "Mohammed E. Fouda"
                },
                "author": "Mohammed E. Fouda",
                "arxiv_comment": "46 pages, 6 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19505v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19505v2",
                "updated": "2025-10-18T11:29:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    18,
                    11,
                    29,
                    52,
                    5,
                    291,
                    0
                ],
                "published": "2025-06-24T10:45:48Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    45,
                    48,
                    1,
                    175,
                    0
                ],
                "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in\n  Large Language Models"
                },
                "summary": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models. Nevertheless,\nminimizing the accuracy degradation caused by ultra-low-bit KV cache\nquantization remains a significant challenge. While scalar quantization is\nconstrained by 1-bit bound, vector quantization exploits intra-vector\ncorrelations and enables sub-bit regimes, making it more suitable for\nultra-low-bit quantization. To further mitigate quantization-induced\ndegradation, we reveal that the degradation is highly uneven across tokens in\nattention quality. To investigate this unevenness, we introduce anchor score to\nmeasure each token's sensitivity to quantization. Our analysis and experiments\nshow that preserving a small subset (1\\%) of tokens with the highest Anchor\nScore significantly mitigates accuracy loss under aggressive quantization.\n  We propose AnTKV, a dual-stage framework that leverages anchor token-aware\nvector quantization to compress the KV cache. It combines offline token-aware\ncentroids learning and online anchor token selection to balance compression and\naccuracy. To enable efficient deployment, we design an online anchor token\nselection kernel compatible with FlashAttention. It allows LLaMA3-8B to scale\nto 840K tokens on a single 80GB A100, while delivering up to $3.5\\times$ higher\ndecoding throughput over the FP16 baseline. Experiments demonstrate that AnTKV\nmatches or surpasses prior methods at 4-bit, and significantly reduce\nperplexity under ultra-low-bit quantization, achieving 6.32 at 1-bit on\nMistral-7B, compared to 7.25 for CQ and 15.36 for KVQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models. Nevertheless,\nminimizing the accuracy degradation caused by ultra-low-bit KV cache\nquantization remains a significant challenge. While scalar quantization is\nconstrained by 1-bit bound, vector quantization exploits intra-vector\ncorrelations and enables sub-bit regimes, making it more suitable for\nultra-low-bit quantization. To further mitigate quantization-induced\ndegradation, we reveal that the degradation is highly uneven across tokens in\nattention quality. To investigate this unevenness, we introduce anchor score to\nmeasure each token's sensitivity to quantization. Our analysis and experiments\nshow that preserving a small subset (1\\%) of tokens with the highest Anchor\nScore significantly mitigates accuracy loss under aggressive quantization.\n  We propose AnTKV, a dual-stage framework that leverages anchor token-aware\nvector quantization to compress the KV cache. It combines offline token-aware\ncentroids learning and online anchor token selection to balance compression and\naccuracy. To enable efficient deployment, we design an online anchor token\nselection kernel compatible with FlashAttention. It allows LLaMA3-8B to scale\nto 840K tokens on a single 80GB A100, while delivering up to $3.5\\times$ higher\ndecoding throughput over the FP16 baseline. Experiments demonstrate that AnTKV\nmatches or surpasses prior methods at 4-bit, and significantly reduce\nperplexity under ultra-low-bit quantization, achieving 6.32 at 1-bit on\nMistral-7B, compared to 7.25 for CQ and 15.36 for KVQuant."
                },
                "authors": [
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Chuanfu Xiao"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Mao Yang"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19505v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19505v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06807v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06807v2",
                "updated": "2025-10-18T06:04:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    18,
                    6,
                    4,
                    53,
                    5,
                    291,
                    0
                ],
                "published": "2025-01-12T13:18:04Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    13,
                    18,
                    4,
                    6,
                    12,
                    0
                ],
                "title": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private LLM\n  Inference"
                },
                "summary": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) achieves formal data privacy protection but suffers from\nsignificant latency overhead, especially for long input sequences. While\nkey-value (KV) cache eviction and sparse attention algorithms have been\nproposed for efficient LLM inference in plaintext, they are not designed for\nMPC and cannot benefit private LLM inference directly. In this paper, we\npropose an accurate and MPC-friendly KV cache eviction framework, dubbed\nMPCache, building on the observation that historical tokens in a long sequence\nmay have different effects on the downstream decoding. Hence, MPCache combines\na look-once static eviction algorithm to discard unimportant KV cache and a\nquery-aware dynamic selection algorithm to activate only a small subset of KV\ncache for attention computation. MPCache further incorporates a series of\noptimizations for efficient dynamic KV cache selection, including MPC-friendly\nsimilarity approximation, hierarchical KV cache clustering, and cross-layer\nindex-sharing strategy. Extensive experiments demonstrate that MPCache\nconsistently outperforms prior-art KV cache eviction baselines across different\ngeneration tasks and achieves 1.8 ~ 2.01x and 3.39 ~ 8.37x decoding latency and\ncommunication reduction on different sequence lengths, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) achieves formal data privacy protection but suffers from\nsignificant latency overhead, especially for long input sequences. While\nkey-value (KV) cache eviction and sparse attention algorithms have been\nproposed for efficient LLM inference in plaintext, they are not designed for\nMPC and cannot benefit private LLM inference directly. In this paper, we\npropose an accurate and MPC-friendly KV cache eviction framework, dubbed\nMPCache, building on the observation that historical tokens in a long sequence\nmay have different effects on the downstream decoding. Hence, MPCache combines\na look-once static eviction algorithm to discard unimportant KV cache and a\nquery-aware dynamic selection algorithm to activate only a small subset of KV\ncache for attention computation. MPCache further incorporates a series of\noptimizations for efficient dynamic KV cache selection, including MPC-friendly\nsimilarity approximation, hierarchical KV cache clustering, and cross-layer\nindex-sharing strategy. Extensive experiments demonstrate that MPCache\nconsistently outperforms prior-art KV cache eviction baselines across different\ngeneration tasks and achieves 1.8 ~ 2.01x and 3.39 ~ 8.37x decoding latency and\ncommunication reduction on different sequence lengths, respectively."
                },
                "authors": [
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Ye Dong"
                    },
                    {
                        "name": "Jinjin Zhou"
                    },
                    {
                        "name": "Jin Tan"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Tao Wei"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06807v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06807v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08907v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08907v3",
                "updated": "2025-10-18T02:48:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    18,
                    2,
                    48,
                    35,
                    5,
                    291,
                    0
                ],
                "published": "2025-10-10T01:42:14Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    1,
                    42,
                    14,
                    4,
                    283,
                    0
                ],
                "title": "Autoencoding-Free Context Compression for LLMs via Contextual Semantic\n  Anchors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoencoding-Free Context Compression for LLMs via Contextual Semantic\n  Anchors"
                },
                "summary": "Context compression presents a promising approach for accelerating large\nlanguage model (LLM) inference by compressing long contexts into compact\nrepresentations. Current context compression methods predominantly rely on\nautoencoding tasks to train context-agnostic compression tokens to compress\ncontextual semantics. While autoencoding tasks enable compression tokens to\nacquire compression capabilities, compression via autoencoding tasks creates a\nfundamental mismatch: the models are optimized for reconstruction that diverge\nfrom actual downstream tasks, thereby weakening the features more beneficial\nfor real-world usage. We propose Semantic-Anchor Compression (SAC), a novel\nmethod that shifts from autoencoding task based compression to an architecture\nthat is equipped with this compression capability \\textit{a priori}. Instead of\ntraining models to compress contexts through autoencoding tasks, SAC directly\nselects so-called anchor tokens from the original context and aggregates\ncontextual information into their key-value (KV) representations. By deriving\nrepresentations directly from the contextual tokens, SAC eliminates the need\nfor autoencoding training. To ensure compression performance while directly\nleveraging anchor tokens, SAC incorporates two key designs: (1) anchor\nembeddings that enable the compressor to identify critical tokens, and (2)\nbidirectional attention modification that allows anchor tokens to capture\ninformation from the entire context. Experimental results demonstrate that SAC\nconsistently outperforms existing context compression methods across various\ncompression ratios. On out-of-distribution evaluation using MRQA, SAC achieves\n1 EM improvement at 5x compression over strong baselines, with increasing\nadvantages at higher compression ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context compression presents a promising approach for accelerating large\nlanguage model (LLM) inference by compressing long contexts into compact\nrepresentations. Current context compression methods predominantly rely on\nautoencoding tasks to train context-agnostic compression tokens to compress\ncontextual semantics. While autoencoding tasks enable compression tokens to\nacquire compression capabilities, compression via autoencoding tasks creates a\nfundamental mismatch: the models are optimized for reconstruction that diverge\nfrom actual downstream tasks, thereby weakening the features more beneficial\nfor real-world usage. We propose Semantic-Anchor Compression (SAC), a novel\nmethod that shifts from autoencoding task based compression to an architecture\nthat is equipped with this compression capability \\textit{a priori}. Instead of\ntraining models to compress contexts through autoencoding tasks, SAC directly\nselects so-called anchor tokens from the original context and aggregates\ncontextual information into their key-value (KV) representations. By deriving\nrepresentations directly from the contextual tokens, SAC eliminates the need\nfor autoencoding training. To ensure compression performance while directly\nleveraging anchor tokens, SAC incorporates two key designs: (1) anchor\nembeddings that enable the compressor to identify critical tokens, and (2)\nbidirectional attention modification that allows anchor tokens to capture\ninformation from the entire context. Experimental results demonstrate that SAC\nconsistently outperforms existing context compression methods across various\ncompression ratios. On out-of-distribution evaluation using MRQA, SAC achieves\n1 EM improvement at 5x compression over strong baselines, with increasing\nadvantages at higher compression ratios."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Runsong Zhao"
                    },
                    {
                        "name": "Pengcheng Huang"
                    },
                    {
                        "name": "Xinyu Liu"
                    },
                    {
                        "name": "Junyi Xiao"
                    },
                    {
                        "name": "Chunyang Xiao"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Shengxiang Gao"
                    },
                    {
                        "name": "Zhengtao Yu"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "18 pages,9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08907v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08907v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16292v1",
                "updated": "2025-10-18T01:31:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    18,
                    1,
                    31,
                    14,
                    5,
                    291,
                    0
                ],
                "published": "2025-10-18T01:31:14Z",
                "published_parsed": [
                    2025,
                    10,
                    18,
                    1,
                    31,
                    14,
                    5,
                    291,
                    0
                ],
                "title": "QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value\n  Weight Compression in Low-Precision Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value\n  Weight Compression in Low-Precision Vision-Language Models"
                },
                "summary": "Vision-Language Models (VLMs) are integral to tasks such as image captioning\nand visual question answering, but their high computational cost, driven by\nlarge memory footprints and processing time, limits their scalability and\nreal-time applicability. In this work, we propose leveraging Singular-Value\nDecomposition (SVD) over the joint query (Q), key (K), and value (V) weight\nmatrices to reduce KV cache size and computational overhead. We in addition\nintroduce an efficient rank allocation strategy that dynamically adjusts the\nSVD rank based on its impact on VLM accuracy, achieving a significant reduction\nin both memory usage and computational cost. Finally, we extend this approach\nby applying quantization to both VLM weights and activations, resulting in a\nhighly efficient VLM. Our method outperforms previous approaches that rely\nsolely on quantization or SVD by achieving more than $10\\%$ accuracy\nimprovement while consuming less hardware cost, making it better for real-time\ndeployment on resource-constrained devices. We open source our code at\n\\href{https://github.com/SAI-Lab-NYU/QSVD}{\\texttt{https://github.com/SAI-Lab-NYU/QSVD}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) are integral to tasks such as image captioning\nand visual question answering, but their high computational cost, driven by\nlarge memory footprints and processing time, limits their scalability and\nreal-time applicability. In this work, we propose leveraging Singular-Value\nDecomposition (SVD) over the joint query (Q), key (K), and value (V) weight\nmatrices to reduce KV cache size and computational overhead. We in addition\nintroduce an efficient rank allocation strategy that dynamically adjusts the\nSVD rank based on its impact on VLM accuracy, achieving a significant reduction\nin both memory usage and computational cost. Finally, we extend this approach\nby applying quantization to both VLM weights and activations, resulting in a\nhighly efficient VLM. Our method outperforms previous approaches that rely\nsolely on quantization or SVD by achieving more than $10\\%$ accuracy\nimprovement while consuming less hardware cost, making it better for real-time\ndeployment on resource-constrained devices. We open source our code at\n\\href{https://github.com/SAI-Lab-NYU/QSVD}{\\texttt{https://github.com/SAI-Lab-NYU/QSVD}}."
                },
                "authors": [
                    {
                        "name": "Yutong Wang"
                    },
                    {
                        "name": "Haiyu Wang"
                    },
                    {
                        "name": "Sai Qian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Sai Qian Zhang"
                },
                "author": "Sai Qian Zhang",
                "arxiv_comment": "Accepted as Spotlight paper by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16276v1",
                "updated": "2025-10-18T00:21:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    18,
                    0,
                    21,
                    45,
                    5,
                    291,
                    0
                ],
                "published": "2025-10-18T00:21:45Z",
                "published_parsed": [
                    2025,
                    10,
                    18,
                    0,
                    21,
                    45,
                    5,
                    291,
                    0
                ],
                "title": "What Limits Agentic Systems Efficiency?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Limits Agentic Systems Efficiency?"
                },
                "summary": "Large Language Models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have\ndemonstrated strong reasoning capabilities. To further enhance LLM\ncapabilities, recent agentic systems, such as Deep Research, incorporate web\ninteractions into LLM reasoning to mitigate uncertainties and reduce potential\nerrors. However, existing research predominantly focuses on reasoning\nperformance, often neglecting the efficiency of agentic systems. In this work,\nwe present a comprehensive empirical study that identifies efficiency\nbottlenecks in web-interactive agentic systems. We decompose end-to-end latency\ninto two primary components: LLM API latency and web environment latency. We\nconduct a comprehensive empirical study across 15 models and 5 providers to\ndemonstrate high variability in API-based agentic systems. We observe that web\nenvironment latency can contribute as much as 53.7% to the overall latency in a\nweb-based agentic system. To improve latency, we propose SpecCache, a caching\nframework augmented with speculative execution that can reduce web environment\noverhead. Extensive evaluations on two standard benchmarks show that our\napproach improves the cache hit rate by up to 58x compared to a random caching\nstrategy, while reducing web environment overhead by up to 3.2x, without\ndegrading agentic system performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have\ndemonstrated strong reasoning capabilities. To further enhance LLM\ncapabilities, recent agentic systems, such as Deep Research, incorporate web\ninteractions into LLM reasoning to mitigate uncertainties and reduce potential\nerrors. However, existing research predominantly focuses on reasoning\nperformance, often neglecting the efficiency of agentic systems. In this work,\nwe present a comprehensive empirical study that identifies efficiency\nbottlenecks in web-interactive agentic systems. We decompose end-to-end latency\ninto two primary components: LLM API latency and web environment latency. We\nconduct a comprehensive empirical study across 15 models and 5 providers to\ndemonstrate high variability in API-based agentic systems. We observe that web\nenvironment latency can contribute as much as 53.7% to the overall latency in a\nweb-based agentic system. To improve latency, we propose SpecCache, a caching\nframework augmented with speculative execution that can reduce web environment\noverhead. Extensive evaluations on two standard benchmarks show that our\napproach improves the cache hit rate by up to 58x compared to a random caching\nstrategy, while reducing web environment overhead by up to 3.2x, without\ndegrading agentic system performance."
                },
                "authors": [
                    {
                        "name": "Song Bian"
                    },
                    {
                        "name": "Minghao Yan"
                    },
                    {
                        "name": "Anand Jayarajan"
                    },
                    {
                        "name": "Gennady Pekhimenko"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "arxiv_comment": "27 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15590v1",
                "updated": "2025-10-17T12:38:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    17,
                    12,
                    38,
                    25,
                    4,
                    290,
                    0
                ],
                "published": "2025-10-17T12:38:25Z",
                "published_parsed": [
                    2025,
                    10,
                    17,
                    12,
                    38,
                    25,
                    4,
                    290,
                    0
                ],
                "title": "A single optically detectable tumbling spin in silicon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A single optically detectable tumbling spin in silicon"
                },
                "summary": "Electron spin resonance spectroscopy is a widely used technique for analyzing\nthe microscopic structure, local environment and reorientation of atomic and\nmolecular systems. Conventional inductive detection methods typically require\nto probe more than a billion of electron spins such that single atom motion is\nhidden through ensemble averaging. While several single spin spectroscopy\nmethods are currently available, they have been so far limited to static\nsystems. Here we demonstrate single spin spectroscopy of a fluorescent tumbling\ndefect in silicon called the G center, behaving as a pseudo-molecule randomly\nreorienting itself in the crystalline matrix. Using high-resolution spin\nspectroscopy, we reveal a fine magnetic structure resulting from the spin\nprincipal axes jumping between discrete orientations in the crystal. By\nmodeling the atomic reorientation of the defect, we demonstrate that spin\ntumbling induces variations in the coupling to the microwave magnetic field,\nenabling position-dependent Rabi frequencies to be detected in coherent spin\ncontrol experiments. By virtue of its pseudo-molecule configuration, the G\ncenter in silicon is a unique quantum system to investigate the mutual\ninteraction between optical, spin and rotation properties in a highly versatile\nmaterial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron spin resonance spectroscopy is a widely used technique for analyzing\nthe microscopic structure, local environment and reorientation of atomic and\nmolecular systems. Conventional inductive detection methods typically require\nto probe more than a billion of electron spins such that single atom motion is\nhidden through ensemble averaging. While several single spin spectroscopy\nmethods are currently available, they have been so far limited to static\nsystems. Here we demonstrate single spin spectroscopy of a fluorescent tumbling\ndefect in silicon called the G center, behaving as a pseudo-molecule randomly\nreorienting itself in the crystalline matrix. Using high-resolution spin\nspectroscopy, we reveal a fine magnetic structure resulting from the spin\nprincipal axes jumping between discrete orientations in the crystal. By\nmodeling the atomic reorientation of the defect, we demonstrate that spin\ntumbling induces variations in the coupling to the microwave magnetic field,\nenabling position-dependent Rabi frequencies to be detected in coherent spin\ncontrol experiments. By virtue of its pseudo-molecule configuration, the G\ncenter in silicon is a unique quantum system to investigate the mutual\ninteraction between optical, spin and rotation properties in a highly versatile\nmaterial."
                },
                "authors": [
                    {
                        "name": "Félix Cache"
                    },
                    {
                        "name": "Yoann Baron"
                    },
                    {
                        "name": "Baptiste Lefaucher"
                    },
                    {
                        "name": "Jean-Baptiste Jager"
                    },
                    {
                        "name": "Frédéric Mazen"
                    },
                    {
                        "name": "Frédéric Milési"
                    },
                    {
                        "name": "Sébastien Kerdilès"
                    },
                    {
                        "name": "Isabelle Robert-Philip"
                    },
                    {
                        "name": "Jean-Michel Gérard"
                    },
                    {
                        "name": "Guillaume Cassabois"
                    },
                    {
                        "name": "Vincent Jacques"
                    },
                    {
                        "name": "Anaïs Dréau"
                    }
                ],
                "author_detail": {
                    "name": "Anaïs Dréau"
                },
                "author": "Anaïs Dréau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15413v1",
                "updated": "2025-10-17T08:07:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    17,
                    8,
                    7,
                    27,
                    4,
                    290,
                    0
                ],
                "published": "2025-10-17T08:07:27Z",
                "published_parsed": [
                    2025,
                    10,
                    17,
                    8,
                    7,
                    27,
                    4,
                    290,
                    0
                ],
                "title": "FHE-SQL: Fully Homomorphic Encrypted SQL Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FHE-SQL: Fully Homomorphic Encrypted SQL Database"
                },
                "summary": "FHE-SQL is a privacy-preserving database system that enables secure query\nprocessing on encrypted data using Fully Homomorphic Encryption (FHE),\nproviding privacy guaranties where an untrusted server can execute encrypted\nqueries without learning either the query contents or the underlying data.\nUnlike property-preserving encryption-based systems such as CryptDB, which rely\non deterministic or order-preserving encryption and are vulnerable to\nfrequency, order, and equality-pattern inference attacks, FHE-SQL performs\ncomputations entirely under encryption, eliminating these leakage channels.\nCompared to trusted-hardware approaches such as TrustedDB, which depend on a\nhardware security module and thus inherit its trust and side-channel\nlimitations, our design achieves end-to-end cryptographic protection without\nrequiring trusted execution environments. In contrast to high-performance\nFHE-based engines-Hermes, which target specialized workloads such as vector\nsearch, FHE-SQL supports general SQL query semantics with schema-aware,\ntype-safe definitions suitable for relational data management. FHE-SQL\nmitigates the high cost of ciphertext space by using an indirection\narchitecture that separates metadata in RocksDB from large ciphertexts in blob\nstorage. It supports oblivious selection via homomorphic boolean masks,\nmulti-tier caching, and garbage collection, with security proven under the\nUniversal Composability framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FHE-SQL is a privacy-preserving database system that enables secure query\nprocessing on encrypted data using Fully Homomorphic Encryption (FHE),\nproviding privacy guaranties where an untrusted server can execute encrypted\nqueries without learning either the query contents or the underlying data.\nUnlike property-preserving encryption-based systems such as CryptDB, which rely\non deterministic or order-preserving encryption and are vulnerable to\nfrequency, order, and equality-pattern inference attacks, FHE-SQL performs\ncomputations entirely under encryption, eliminating these leakage channels.\nCompared to trusted-hardware approaches such as TrustedDB, which depend on a\nhardware security module and thus inherit its trust and side-channel\nlimitations, our design achieves end-to-end cryptographic protection without\nrequiring trusted execution environments. In contrast to high-performance\nFHE-based engines-Hermes, which target specialized workloads such as vector\nsearch, FHE-SQL supports general SQL query semantics with schema-aware,\ntype-safe definitions suitable for relational data management. FHE-SQL\nmitigates the high cost of ciphertext space by using an indirection\narchitecture that separates metadata in RocksDB from large ciphertexts in blob\nstorage. It supports oblivious selection via homomorphic boolean masks,\nmulti-tier caching, and garbage collection, with security proven under the\nUniversal Composability framework."
                },
                "authors": [
                    {
                        "name": "Po-Yu Tseng"
                    },
                    {
                        "name": "Po-Chu Hsu"
                    },
                    {
                        "name": "Shih-Wei Liao"
                    }
                ],
                "author_detail": {
                    "name": "Shih-Wei Liao"
                },
                "author": "Shih-Wei Liao",
                "arxiv_comment": "12 pages, 1 figures, Keywords: Fully Homomorphic Encryption, Private\n  Information Retrieval, Encrypted Databases, Privacy-Preserving Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03409v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03409v4",
                "updated": "2025-10-17T06:54:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    17,
                    6,
                    54,
                    10,
                    4,
                    290,
                    0
                ],
                "published": "2024-12-04T15:48:59Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation"
                },
                "summary": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV, where\n\"Prefix\" means the top-ranked KV based on importance rather than position in\nthe original sequence. It reframes the challenge of determining KV cache sizes\nfor all layers into the task of searching for the optimal global prefix\nconfiguration. With an adaptive layer-wise KV retention recipe based on binary\nsearch, the maximum contextual information can thus be preserved in each layer,\nfacilitating the generation. Extensive experiments demonstrate that our method\nachieves the state-of-the-art performance compared with others. It exhibits\nsuperior inference efficiency and generation quality trade-offs, showing\npromising potential for practical applications. Code is available at\nhttps://github.com/THU-MIG/PrefixKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV, where\n\"Prefix\" means the top-ranked KV based on importance rather than position in\nthe original sequence. It reframes the challenge of determining KV cache sizes\nfor all layers into the task of searching for the optimal global prefix\nconfiguration. With an adaptive layer-wise KV retention recipe based on binary\nsearch, the maximum contextual information can thus be preserved in each layer,\nfacilitating the generation. Extensive experiments demonstrate that our method\nachieves the state-of-the-art performance compared with others. It exhibits\nsuperior inference efficiency and generation quality trade-offs, showing\npromising potential for practical applications. Code is available at\nhttps://github.com/THU-MIG/PrefixKV."
                },
                "authors": [
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jiaxin Li"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "NeurIPS 2025 Camera-ready Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03409v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03409v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01890v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01890v2",
                "updated": "2025-10-17T06:45:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    17,
                    6,
                    45,
                    17,
                    4,
                    290,
                    0
                ],
                "published": "2024-08-04T00:38:34Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "title": "Cross-layer Attention Sharing for Pre-trained Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-layer Attention Sharing for Pre-trained Large Language Models"
                },
                "summary": "To enhance the efficiency of the attention mechanism within large language\nmodels (LLMs), previous works primarily compress the KV cache or group\nattention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to reduce the redundancy by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights.\n  Driven by these insights, we introduce LISA, a lightweight substitute for\nself-attention in well-trained LLMs. LISA employs tiny feed-forward networks to\nalign attention heads between adjacent layers and low-rank matrices to\napproximate differences in layer-wise attention weights. Evaluations\nencompassing 13 typical benchmarks demonstrate that LISA maintains high\nresponse quality in terms of accuracy and perplexity while reducing redundant\nattention calculations within 53%-84% of the total layers. Our implementations\nof LISA achieve a 6x compression of Q and K matrices within the attention\nmechanism, with maximum throughput improvements 19.5%, 32.3%, and 40.1% for\nLLaMA3-8B, LLaMA2-7B, and LLaMA2-13B, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To enhance the efficiency of the attention mechanism within large language\nmodels (LLMs), previous works primarily compress the KV cache or group\nattention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to reduce the redundancy by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights.\n  Driven by these insights, we introduce LISA, a lightweight substitute for\nself-attention in well-trained LLMs. LISA employs tiny feed-forward networks to\nalign attention heads between adjacent layers and low-rank matrices to\napproximate differences in layer-wise attention weights. Evaluations\nencompassing 13 typical benchmarks demonstrate that LISA maintains high\nresponse quality in terms of accuracy and perplexity while reducing redundant\nattention calculations within 53%-84% of the total layers. Our implementations\nof LISA achieve a 6x compression of Q and K matrices within the attention\nmechanism, with maximum throughput improvements 19.5%, 32.3%, and 40.1% for\nLLaMA3-8B, LLaMA2-7B, and LLaMA2-13B, respectively."
                },
                "authors": [
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Yuzhang Wu"
                    },
                    {
                        "name": "Yuchun Fan"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Hengyu Li"
                    },
                    {
                        "name": "Jiali Zeng"
                    },
                    {
                        "name": "Qiaozhi He"
                    },
                    {
                        "name": "Murun Yang"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "A version accepted by TACL, prior to its publication by MIT Press",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01890v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01890v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15152v1",
                "updated": "2025-10-16T21:22:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    21,
                    22,
                    16,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T21:22:16Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    21,
                    22,
                    16,
                    3,
                    289,
                    0
                ],
                "title": "Tail-Optimized Caching for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tail-Optimized Caching for LLM Inference"
                },
                "summary": "Prompt caching is critical for reducing latency and cost in LLM inference:\nOpenAI and Anthropic report up to 50-90% cost savings through prompt reuse.\nDespite its widespread success, little is known about what constitutes an\noptimal prompt caching policy, particularly when optimizing tail latency, a\nmetric of central importance to practitioners. The widely used Least Recently\nUsed (LRU) policy can perform arbitrarily poor on this metric, as it is\noblivious to the heterogeneity of conversation lengths. To address this gap, we\npropose Tail-Optimized LRU, a simple two-line modification that reallocates KV\ncache capacity to prioritize high-latency conversations by evicting cache\nentries that are unlikely to affect future turns. Though the implementation is\nsimple, we prove its optimality under a natural stochastic model of\nconversation dynamics, providing the first theoretical justification for LRU in\nthis setting, a result that may be of independent interest to the caching\ncommunity. Experimentally, on real conversation data WildChat, Tail-Optimized\nLRU achieves up to 27.5% reduction in P90 tail Time to First Token latency and\n23.9% in P95 tail latency compared to LRU, along with up to 38.9% decrease in\nSLO violations of 200ms. We believe this provides a practical and theoretically\ngrounded option for practitioners seeking to optimize tail latency in\nreal-world LLM deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt caching is critical for reducing latency and cost in LLM inference:\nOpenAI and Anthropic report up to 50-90% cost savings through prompt reuse.\nDespite its widespread success, little is known about what constitutes an\noptimal prompt caching policy, particularly when optimizing tail latency, a\nmetric of central importance to practitioners. The widely used Least Recently\nUsed (LRU) policy can perform arbitrarily poor on this metric, as it is\noblivious to the heterogeneity of conversation lengths. To address this gap, we\npropose Tail-Optimized LRU, a simple two-line modification that reallocates KV\ncache capacity to prioritize high-latency conversations by evicting cache\nentries that are unlikely to affect future turns. Though the implementation is\nsimple, we prove its optimality under a natural stochastic model of\nconversation dynamics, providing the first theoretical justification for LRU in\nthis setting, a result that may be of independent interest to the caching\ncommunity. Experimentally, on real conversation data WildChat, Tail-Optimized\nLRU achieves up to 27.5% reduction in P90 tail Time to First Token latency and\n23.9% in P95 tail latency compared to LRU, along with up to 38.9% decrease in\nSLO violations of 200ms. We believe this provides a practical and theoretically\ngrounded option for practitioners seeking to optimize tail latency in\nreal-world LLM deployments."
                },
                "authors": [
                    {
                        "name": "Wenxin Zhang"
                    },
                    {
                        "name": "Yueying Li"
                    },
                    {
                        "name": "Ciamac C. Moallemi"
                    },
                    {
                        "name": "Tianyi Peng"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Peng"
                },
                "author": "Tianyi Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15095v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15095v1",
                "updated": "2025-10-16T19:28:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    19,
                    28,
                    30,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T19:28:30Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    19,
                    28,
                    30,
                    3,
                    289,
                    0
                ],
                "title": "Hive Hash Table: A Warp-Cooperative, Dynamically Resizable Hash Table\n  for GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hive Hash Table: A Warp-Cooperative, Dynamically Resizable Hash Table\n  for GPUs"
                },
                "summary": "Hash tables are essential building blocks in data-intensive applications, yet\nexisting GPU implementations often struggle with concurrent updates, high load\nfactors, and irregular memory access patterns. We present Hive hash table, a\nhigh-performance, warp-cooperative and dynamically resizable GPU hash table\nthat adapts to varying workloads without global rehashing.\n  Hive hash table makes three key contributions. First, a cache-aligned packed\nbucket layout stores key-value pairs as 64-bit words, enabling coalesced memory\naccess and atomic updates via single-CAS operations. Second, warp-synchronous\nconcurrency protocols - Warp-Aggregated-Bitmask-Claim (WABC) and\nWarp-Cooperative Match-and-Elect (WCME) - reduce contention to one atomic\noperation per warp while ensuring lock-free progress. Third, a\nload-factor-aware dynamic resizing strategy expands or contracts capacity in\nwarp-parallel K-bucket batches using linear hashing, maintaining balanced\noccupancy. To handle insertions under heavy contention, Hive hash table employs\na four-step strategy: replace, claim-and-commit, bounded cuckoo eviction, and\noverflow-stash fallback. This design provides lock-free fast paths and bounded\nrecovery cost under contention determined by a fixed eviction depth, while\neliminating ABA hazards during concurrent updates.\n  Experimental evaluation on an NVIDIA RTX 4090 shows Hive hash table sustains\nload factors up to 95% while delivering 1.5-2x higher throughput than\nstate-of-the-art GPU hash tables (Slab-Hash, DyCuckoo, WarpCore) under mixed\ninsert-delete-lookup workloads. On balanced workload, Hive hash table reaches\n3.5 billion updates/s and nearly 4 billion lookups/s, demonstrating scalability\nand efficiency for GPU-accelerated data processing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hash tables are essential building blocks in data-intensive applications, yet\nexisting GPU implementations often struggle with concurrent updates, high load\nfactors, and irregular memory access patterns. We present Hive hash table, a\nhigh-performance, warp-cooperative and dynamically resizable GPU hash table\nthat adapts to varying workloads without global rehashing.\n  Hive hash table makes three key contributions. First, a cache-aligned packed\nbucket layout stores key-value pairs as 64-bit words, enabling coalesced memory\naccess and atomic updates via single-CAS operations. Second, warp-synchronous\nconcurrency protocols - Warp-Aggregated-Bitmask-Claim (WABC) and\nWarp-Cooperative Match-and-Elect (WCME) - reduce contention to one atomic\noperation per warp while ensuring lock-free progress. Third, a\nload-factor-aware dynamic resizing strategy expands or contracts capacity in\nwarp-parallel K-bucket batches using linear hashing, maintaining balanced\noccupancy. To handle insertions under heavy contention, Hive hash table employs\na four-step strategy: replace, claim-and-commit, bounded cuckoo eviction, and\noverflow-stash fallback. This design provides lock-free fast paths and bounded\nrecovery cost under contention determined by a fixed eviction depth, while\neliminating ABA hazards during concurrent updates.\n  Experimental evaluation on an NVIDIA RTX 4090 shows Hive hash table sustains\nload factors up to 95% while delivering 1.5-2x higher throughput than\nstate-of-the-art GPU hash tables (Slab-Hash, DyCuckoo, WarpCore) under mixed\ninsert-delete-lookup workloads. On balanced workload, Hive hash table reaches\n3.5 billion updates/s and nearly 4 billion lookups/s, demonstrating scalability\nand efficiency for GPU-accelerated data processing."
                },
                "authors": [
                    {
                        "name": "Md Sabbir Hossain Polak"
                    },
                    {
                        "name": "David Troendle"
                    },
                    {
                        "name": "Byunghyun Jang"
                    }
                ],
                "author_detail": {
                    "name": "Byunghyun Jang"
                },
                "author": "Byunghyun Jang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15095v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15095v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14973v1",
                "updated": "2025-10-16T17:59:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    48,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:59:48Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    48,
                    3,
                    289,
                    0
                ],
                "title": "Attention Is All You Need for KV Cache in Diffusion LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Is All You Need for KV Cache in Diffusion LLMs"
                },
                "summary": "This work studies how to adaptively recompute key-value (KV) caches for\ndiffusion large language models (DLMs) to maximize prediction accuracy while\nminimizing decoding latency. Prior methods' decoders recompute QKV for all\ntokens at every denoising step and layer, despite KV states changing little\nacross most steps, especially in shallow layers, leading to substantial\nredundancy. We make three observations: (1) distant ${\\bf MASK}$ tokens\nprimarily act as a length-bias and can be cached block-wise beyond the active\nprediction window; (2) KV dynamics increase with depth, suggesting that\nselective refresh starting from deeper layers is sufficient; and (3) the\nmost-attended token exhibits the smallest KV drift, providing a conservative\nlower bound on cache change for other tokens. Building on these, we propose\n${\\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that\njointly decides ${when}$ to refresh (via an attention-aware drift test on the\nmost-attended token) and ${where}$ to refresh (via a depth-aware schedule that\nrecomputes from a chosen layer onward while reusing shallow-layer caches and\noff-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs\nadaptive, layer-aware cache updates for diffusion LLMs, reducing redundant\ncomputation and accelerating decoding with negligible loss in generation\nquality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across\nmathematical reasoning and code generation tasks demonstrate consistent\nspeedups: $8.7\\times$ on GSM8K (256 tokens), $45.1\\times$ on longer sequences,\nand $4.8\\times$ on HumanEval, while consistently maintaining higher accuracy\nthan the baseline. Our method achieves significantly higher throughput\n($6.8\\times$ on GSM8K) than existing confidence-based approaches while\npreserving generation quality, enabling practical deployment of diffusion LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work studies how to adaptively recompute key-value (KV) caches for\ndiffusion large language models (DLMs) to maximize prediction accuracy while\nminimizing decoding latency. Prior methods' decoders recompute QKV for all\ntokens at every denoising step and layer, despite KV states changing little\nacross most steps, especially in shallow layers, leading to substantial\nredundancy. We make three observations: (1) distant ${\\bf MASK}$ tokens\nprimarily act as a length-bias and can be cached block-wise beyond the active\nprediction window; (2) KV dynamics increase with depth, suggesting that\nselective refresh starting from deeper layers is sufficient; and (3) the\nmost-attended token exhibits the smallest KV drift, providing a conservative\nlower bound on cache change for other tokens. Building on these, we propose\n${\\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that\njointly decides ${when}$ to refresh (via an attention-aware drift test on the\nmost-attended token) and ${where}$ to refresh (via a depth-aware schedule that\nrecomputes from a chosen layer onward while reusing shallow-layer caches and\noff-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs\nadaptive, layer-aware cache updates for diffusion LLMs, reducing redundant\ncomputation and accelerating decoding with negligible loss in generation\nquality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across\nmathematical reasoning and code generation tasks demonstrate consistent\nspeedups: $8.7\\times$ on GSM8K (256 tokens), $45.1\\times$ on longer sequences,\nand $4.8\\times$ on HumanEval, while consistently maintaining higher accuracy\nthan the baseline. Our method achieves significantly higher throughput\n($6.8\\times$ on GSM8K) than existing confidence-based approaches while\npreserving generation quality, enabling practical deployment of diffusion LLMs."
                },
                "authors": [
                    {
                        "name": "Quan Nguyen-Tri"
                    },
                    {
                        "name": "Mukul Ranjan"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "arxiv_comment": "https://vila-lab.github.io/elastic-cache-webpage/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14891v1",
                "updated": "2025-10-16T17:10:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    10,
                    3,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:10:03Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    10,
                    3,
                    3,
                    289,
                    0
                ],
                "title": "A Performance Portable Matrix Free Dense MTTKRP in GenTen",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Performance Portable Matrix Free Dense MTTKRP in GenTen"
                },
                "summary": "We extend the GenTen tensor decomposition package by introducing an\naccelerated dense matricized tensor times Khatri-Rao product (MTTKRP), the\nworkhorse kernel for canonical polyadic (CP) tensor decompositions, that is\nportable and performant on modern CPU and GPU architectures. In contrast to the\nstate-of-the-art matrix multiply based MTTKRP kernels used by Tensor Toolbox,\nTensorLy, etc., that explicitly form Khatri-Rao matrices, we develop a\nmatrix-free element-wise parallelization approach whose memory cost grows with\nthe rank R like the sum of the tensor shape O(R(n+m+k)), compared to\nmatrix-based methods whose memory cost grows like the product of the tensor\nshape O(R(mnk)). For the largest problem we study, a rank 2000 MTTKRP, the\nsmaller growth rate yields a matrix-free memory cost of just 2% of the\nmatrix-based methods, a 50x improvement. In practice, the reduced memory impact\nmeans our matrix-free MTTKRP can compute a rank 2000 tensor decomposition on a\nsingle NVIDIA H100 instead of six H100s using a matrix-based MTTKRP. We also\ncompare our optimized matrix-free MTTKRP to baseline matrix-free\nimplementations on different devices, showing a 3x single-device speedup on an\nIntel 8480+ CPU and an 11x speedup on a H100 GPU. In addition to numerical\nresults, we provide fine grained performance models for an ideal multi-level\ncache machine, compare analytical performance predictions to empirical results,\nand provide a motivated heuristic selection for selecting an algorithmic\nhyperparameter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We extend the GenTen tensor decomposition package by introducing an\naccelerated dense matricized tensor times Khatri-Rao product (MTTKRP), the\nworkhorse kernel for canonical polyadic (CP) tensor decompositions, that is\nportable and performant on modern CPU and GPU architectures. In contrast to the\nstate-of-the-art matrix multiply based MTTKRP kernels used by Tensor Toolbox,\nTensorLy, etc., that explicitly form Khatri-Rao matrices, we develop a\nmatrix-free element-wise parallelization approach whose memory cost grows with\nthe rank R like the sum of the tensor shape O(R(n+m+k)), compared to\nmatrix-based methods whose memory cost grows like the product of the tensor\nshape O(R(mnk)). For the largest problem we study, a rank 2000 MTTKRP, the\nsmaller growth rate yields a matrix-free memory cost of just 2% of the\nmatrix-based methods, a 50x improvement. In practice, the reduced memory impact\nmeans our matrix-free MTTKRP can compute a rank 2000 tensor decomposition on a\nsingle NVIDIA H100 instead of six H100s using a matrix-based MTTKRP. We also\ncompare our optimized matrix-free MTTKRP to baseline matrix-free\nimplementations on different devices, showing a 3x single-device speedup on an\nIntel 8480+ CPU and an 11x speedup on a H100 GPU. In addition to numerical\nresults, we provide fine grained performance models for an ideal multi-level\ncache machine, compare analytical performance predictions to empirical results,\nand provide a motivated heuristic selection for selecting an algorithmic\nhyperparameter."
                },
                "authors": [
                    {
                        "name": "Gabriel Kosmacher"
                    },
                    {
                        "name": "Eric T. Phipps"
                    },
                    {
                        "name": "Sivasankaran Rajamanickam"
                    }
                ],
                "author_detail": {
                    "name": "Sivasankaran Rajamanickam"
                },
                "author": "Sivasankaran Rajamanickam",
                "arxiv_comment": "10 pages, 5 figures, 4 tables, for implementation see\n  https://github.com/sandialabs/GenTen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14686v1",
                "updated": "2025-10-16T13:53:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    53,
                    47,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T13:53:47Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    53,
                    47,
                    3,
                    289,
                    0
                ],
                "title": "xLLM Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xLLM Technical Report"
                },
                "summary": "We introduce xLLM, an intelligent and efficient Large Language Model (LLM)\ninference framework designed for high-performance, large-scale enterprise-grade\nserving, with deep optimizations for diverse AI accelerators. To address these\nchallenges, xLLM builds a novel decoupled service-engine architecture. At the\nservice layer, xLLM-Service features an intelligent scheduling module that\nefficiently processes multimodal requests and co-locates online and offline\ntasks through unified elastic scheduling to maximize cluster utilization. This\nmodule also relies on a workload-adaptive dynamic Prefill-Decode (PD)\ndisaggregation policy and a novel Encode-Prefill-Decode (EPD) disaggregation\npolicy designed for multimodal inputs. Furthermore, it incorporates a\ndistributed architecture to provide global KV Cache management and robust\nfault-tolerant capabilities for high availability. At the engine layer,\nxLLM-Engine co-optimizes system and algorithm designs to fully saturate\ncomputing resources. This is achieved through comprehensive multi-layer\nexecution pipeline optimizations, an adaptive graph mode and an xTensor memory\nmanagement. xLLM-Engine also further integrates algorithmic enhancements such\nas optimized speculative decoding and dynamic EPLB, collectively serving to\nsubstantially boost throughput and inference efficiency. Extensive evaluations\ndemonstrate that xLLM delivers significantly superior performance and resource\nefficiency. Under identical TPOT constraints, xLLM achieves throughput up to\n1.7x that of MindIE and 2.2x that of vLLM-Ascend with Qwen-series models, while\nmaintaining an average throughput of 1.7x that of MindIE with Deepseek-series\nmodels. xLLM framework is publicly available at\nhttps://github.com/jd-opensource/xllm and\nhttps://github.com/jd-opensource/xllm-service.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce xLLM, an intelligent and efficient Large Language Model (LLM)\ninference framework designed for high-performance, large-scale enterprise-grade\nserving, with deep optimizations for diverse AI accelerators. To address these\nchallenges, xLLM builds a novel decoupled service-engine architecture. At the\nservice layer, xLLM-Service features an intelligent scheduling module that\nefficiently processes multimodal requests and co-locates online and offline\ntasks through unified elastic scheduling to maximize cluster utilization. This\nmodule also relies on a workload-adaptive dynamic Prefill-Decode (PD)\ndisaggregation policy and a novel Encode-Prefill-Decode (EPD) disaggregation\npolicy designed for multimodal inputs. Furthermore, it incorporates a\ndistributed architecture to provide global KV Cache management and robust\nfault-tolerant capabilities for high availability. At the engine layer,\nxLLM-Engine co-optimizes system and algorithm designs to fully saturate\ncomputing resources. This is achieved through comprehensive multi-layer\nexecution pipeline optimizations, an adaptive graph mode and an xTensor memory\nmanagement. xLLM-Engine also further integrates algorithmic enhancements such\nas optimized speculative decoding and dynamic EPLB, collectively serving to\nsubstantially boost throughput and inference efficiency. Extensive evaluations\ndemonstrate that xLLM delivers significantly superior performance and resource\nefficiency. Under identical TPOT constraints, xLLM achieves throughput up to\n1.7x that of MindIE and 2.2x that of vLLM-Ascend with Qwen-series models, while\nmaintaining an average throughput of 1.7x that of MindIE with Deepseek-series\nmodels. xLLM framework is publicly available at\nhttps://github.com/jd-opensource/xllm and\nhttps://github.com/jd-opensource/xllm-service."
                },
                "authors": [
                    {
                        "name": "Tongxuan Liu"
                    },
                    {
                        "name": "Tao Peng"
                    },
                    {
                        "name": "Peijun Yang"
                    },
                    {
                        "name": "Xiaoyang Zhao"
                    },
                    {
                        "name": "Xiusheng Lu"
                    },
                    {
                        "name": "Weizhe Huang"
                    },
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Xiaoyu Chen"
                    },
                    {
                        "name": "Zhiwei Liang"
                    },
                    {
                        "name": "Jun Xiong"
                    },
                    {
                        "name": "Donghe Jin"
                    },
                    {
                        "name": "Minchao Zhang"
                    },
                    {
                        "name": "Jinrong Guo"
                    },
                    {
                        "name": "Yingxu Deng"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Xianzhe Dong"
                    },
                    {
                        "name": "Siqi Wang"
                    },
                    {
                        "name": "Siyu Wu"
                    },
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Zihan Tang"
                    },
                    {
                        "name": "Yuting Zeng"
                    },
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Jinguang Liu"
                    },
                    {
                        "name": "Meng Kang"
                    },
                    {
                        "name": "Menxin Li"
                    },
                    {
                        "name": "Yunlong Wang"
                    },
                    {
                        "name": "Yiming Liu"
                    },
                    {
                        "name": "Xiaolong Ma"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Yichen Zhang"
                    },
                    {
                        "name": "Jinrun Yin"
                    },
                    {
                        "name": "Keyang Zheng"
                    },
                    {
                        "name": "Jiawei Yin"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Ziyue Wang"
                    },
                    {
                        "name": "Xiaobo Lin"
                    },
                    {
                        "name": "Liangyu Liu"
                    },
                    {
                        "name": "Liwei Lan"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Chunhua Peng"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Songcheng Ren"
                    },
                    {
                        "name": "Xuezhu Wang"
                    },
                    {
                        "name": "Yunheng Shen"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Guyue Liu"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Tong Yang"
                    },
                    {
                        "name": "Hailong Yang"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Guiguang Ding"
                    },
                    {
                        "name": "Ke Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ke Zhang"
                },
                "author": "Ke Zhang",
                "arxiv_comment": "39 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v5",
                "updated": "2025-10-16T13:25:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    25,
                    38,
                    3,
                    289,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods. Our code is available at\nhttps://github.com/FFY0/AdaKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods. Our code is available at\nhttps://github.com/FFY0/AdaKV."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14622v1",
                "updated": "2025-10-16T12:32:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    12,
                    32,
                    51,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T12:32:51Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    12,
                    32,
                    51,
                    3,
                    289,
                    0
                ],
                "title": "MPI-over-CXL: Enhancing Communication Efficiency in Distributed HPC\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPI-over-CXL: Enhancing Communication Efficiency in Distributed HPC\n  Systems"
                },
                "summary": "MPI implementations commonly rely on explicit memory-copy operations,\nincurring overhead from redundant data movement and buffer management. This\noverhead notably impacts HPC workloads involving intensive inter-processor\ncommunication. In response, we introduce MPI-over-CXL, a novel MPI\ncommunication paradigm leveraging CXL, which provides cache-coherent shared\nmemory across multiple hosts. MPI-over-CXL replaces traditional data-copy\nmethods with direct shared memory access, significantly reducing communication\nlatency and memory bandwidth usage. By mapping shared memory regions directly\ninto the virtual address spaces of MPI processes, our design enables efficient\npointer-based communication, eliminating redundant copying operations. To\nvalidate this approach, we implement a comprehensive hardware and software\nenvironment, including a custom CXL 3.2 controller, FPGA-based multi-host\nemulation, and dedicated software stack. Our evaluations using representative\nbenchmarks demonstrate substantial performance improvements over conventional\nMPI systems, underscoring MPI-over-CXL's potential to enhance efficiency and\nscalability in large-scale HPC environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPI implementations commonly rely on explicit memory-copy operations,\nincurring overhead from redundant data movement and buffer management. This\noverhead notably impacts HPC workloads involving intensive inter-processor\ncommunication. In response, we introduce MPI-over-CXL, a novel MPI\ncommunication paradigm leveraging CXL, which provides cache-coherent shared\nmemory across multiple hosts. MPI-over-CXL replaces traditional data-copy\nmethods with direct shared memory access, significantly reducing communication\nlatency and memory bandwidth usage. By mapping shared memory regions directly\ninto the virtual address spaces of MPI processes, our design enables efficient\npointer-based communication, eliminating redundant copying operations. To\nvalidate this approach, we implement a comprehensive hardware and software\nenvironment, including a custom CXL 3.2 controller, FPGA-based multi-host\nemulation, and dedicated software stack. Our evaluations using representative\nbenchmarks demonstrate substantial performance improvements over conventional\nMPI systems, underscoring MPI-over-CXL's potential to enhance efficiency and\nscalability in large-scale HPC environments."
                },
                "authors": [
                    {
                        "name": "Miryeong Kwon"
                    },
                    {
                        "name": "Donghyun Gouk"
                    },
                    {
                        "name": "Hyein Woo"
                    },
                    {
                        "name": "Junhee Kim"
                    },
                    {
                        "name": "Jinwoo Baek"
                    },
                    {
                        "name": "Kyungkuk Nam"
                    },
                    {
                        "name": "Sangyoon Ji"
                    },
                    {
                        "name": "Jiseon Kim"
                    },
                    {
                        "name": "Hanyeoreum Bae"
                    },
                    {
                        "name": "Junhyeok Jang"
                    },
                    {
                        "name": "Hyunwoo You"
                    },
                    {
                        "name": "Junseok Moon"
                    },
                    {
                        "name": "Myoungsoo Jung"
                    }
                ],
                "author_detail": {
                    "name": "Myoungsoo Jung"
                },
                "author": "Myoungsoo Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14531v1",
                "updated": "2025-10-16T10:21:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    10,
                    21,
                    35,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T10:21:35Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    10,
                    21,
                    35,
                    3,
                    289,
                    0
                ],
                "title": "Design and simulation of a 4H-SiC low gain avalanche diode with\n  trench-isolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and simulation of a 4H-SiC low gain avalanche diode with\n  trench-isolation"
                },
                "summary": "We present the design and simulation of a 30 $\\mathrm{\\mu m}$ thick 4H-SiC\nLow Gain Avalanche Diode (LGAD) optimized for high-voltage operation. A 2.4\n$\\mathrm{\\mu m}$ thick epitaxially grown gain layer enables controlled internal\namplification up to 1 kV reverse bias, while maintaining full depletion below\n500 V. Electrical characteristics, including I-V, C-V, and gain behavior, were\nsimulated in Synopsys Sentaurus Technology Computer-Aided Design (TCAD) using a\nquasi-1D geometry and verified across process-related variations in gain layer\nparameters. To ensure high-voltage stability and proper edge termination, a\nguard structure combining deep etched trenches and deep $p^+$ junction\ntermination extension (JTE) implants was designed. TCAD simulations varying the\nguard structure dimensions yielded an optimized design with a breakdown voltage\nabove 2.4 kV. A corresponding wafer run is currently processed at IMB-CNM,\nBarcelona.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the design and simulation of a 30 $\\mathrm{\\mu m}$ thick 4H-SiC\nLow Gain Avalanche Diode (LGAD) optimized for high-voltage operation. A 2.4\n$\\mathrm{\\mu m}$ thick epitaxially grown gain layer enables controlled internal\namplification up to 1 kV reverse bias, while maintaining full depletion below\n500 V. Electrical characteristics, including I-V, C-V, and gain behavior, were\nsimulated in Synopsys Sentaurus Technology Computer-Aided Design (TCAD) using a\nquasi-1D geometry and verified across process-related variations in gain layer\nparameters. To ensure high-voltage stability and proper edge termination, a\nguard structure combining deep etched trenches and deep $p^+$ junction\ntermination extension (JTE) implants was designed. TCAD simulations varying the\nguard structure dimensions yielded an optimized design with a breakdown voltage\nabove 2.4 kV. A corresponding wafer run is currently processed at IMB-CNM,\nBarcelona."
                },
                "authors": [
                    {
                        "name": "Sebastian Onder"
                    },
                    {
                        "name": "Philipp Gaggl"
                    },
                    {
                        "name": "Jürgen Burin"
                    },
                    {
                        "name": "Andreas Gsponer"
                    },
                    {
                        "name": "Matthias Knopf"
                    },
                    {
                        "name": "Simon Waid"
                    },
                    {
                        "name": "Neil Moffat"
                    },
                    {
                        "name": "Giulio Pellegrini"
                    },
                    {
                        "name": "Thomas Bergauer"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Bergauer"
                },
                "author": "Thomas Bergauer",
                "arxiv_doi": "10.1016/j.nima.2025.170740",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.nima.2025.170740",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.14531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16040v1",
                "updated": "2025-10-16T07:12:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    7,
                    12,
                    8,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T07:12:08Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    7,
                    12,
                    8,
                    3,
                    289,
                    0
                ],
                "title": "Kelle: Co-design KV Caching and eDRAM for Efficient LLM Serving in Edge\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kelle: Co-design KV Caching and eDRAM for Efficient LLM Serving in Edge\n  Computing"
                },
                "summary": "Running Large Language Models (LLMs) on edge devices is crucial for reducing\nlatency, improving real-time processing, and enhancing privacy. By performing\ninference directly on the device, data does not need to be sent to the cloud,\nensuring faster responses and reducing reliance on network connectivity.\nHowever, implementing LLMs on edge devices presents challenges, particularly\nwith managing key-value (KV) caches, which plays a pivotal role in LLM serving.\nAs the input text lengthens, the size of the KV cache increases linearly with\nthe sequence length, leading to a significant memory footprint and data access\ncosts. On the other hand, edge devices have limited memory and computational\npower, making it hard to store and efficiently access the large caches needed\nfor LLM inference.\n  To mitigate the substantial overhead caused by KV cache, we propose using\nembedded DRAM (eDRAM) as the primary storage for LLM serving in edge device,\nwhich offers higher storage density compared to SRAM. However, to ensure data\nintegrity, eDRAM needs periodic refresh operations, which are power-intensive.\nTo reduce eDRAM costs and improve overall system performance, we\npropose~\\textit{Kelle}, a software-hardware co-design solution optimized for\ndeploying LLMs on eDRAM-based edge systems. Combined with our fine-grained\nmemory eviction, recomputation, and refresh control algorithms, the\n\\textit{Kelle} accelerator delivers a $3.9\\times$ speedup and $4.5\\times$\nenergy savings compared to existing baseline solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Running Large Language Models (LLMs) on edge devices is crucial for reducing\nlatency, improving real-time processing, and enhancing privacy. By performing\ninference directly on the device, data does not need to be sent to the cloud,\nensuring faster responses and reducing reliance on network connectivity.\nHowever, implementing LLMs on edge devices presents challenges, particularly\nwith managing key-value (KV) caches, which plays a pivotal role in LLM serving.\nAs the input text lengthens, the size of the KV cache increases linearly with\nthe sequence length, leading to a significant memory footprint and data access\ncosts. On the other hand, edge devices have limited memory and computational\npower, making it hard to store and efficiently access the large caches needed\nfor LLM inference.\n  To mitigate the substantial overhead caused by KV cache, we propose using\nembedded DRAM (eDRAM) as the primary storage for LLM serving in edge device,\nwhich offers higher storage density compared to SRAM. However, to ensure data\nintegrity, eDRAM needs periodic refresh operations, which are power-intensive.\nTo reduce eDRAM costs and improve overall system performance, we\npropose~\\textit{Kelle}, a software-hardware co-design solution optimized for\ndeploying LLMs on eDRAM-based edge systems. Combined with our fine-grained\nmemory eviction, recomputation, and refresh control algorithms, the\n\\textit{Kelle} accelerator delivers a $3.9\\times$ speedup and $4.5\\times$\nenergy savings compared to existing baseline solutions."
                },
                "authors": [
                    {
                        "name": "Tianhua Xia"
                    },
                    {
                        "name": "Sai Qian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Sai Qian Zhang"
                },
                "author": "Sai Qian Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14126v1",
                "updated": "2025-10-15T21:49:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    21,
                    49,
                    38,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T21:49:38Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    21,
                    49,
                    38,
                    2,
                    288,
                    0
                ],
                "title": "Cortex: Workflow-Aware Resource Pooling and Scheduling for Agentic\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cortex: Workflow-Aware Resource Pooling and Scheduling for Agentic\n  Serving"
                },
                "summary": "We introduce Cortex, a prototype workflow-aware serving platform designed for\nagentic workloads. The core principle of Cortex is stage isolation: it\nprovisions dedicated resource pools for each distinct stage of an agentic\nworkflow. This simple yet powerful strategy mitigates inter-stage interference\nin compute and memory, leading to better KV cache utilization, higher\nthroughput, and more predictable performance. By customizing resource\nallocation and scheduling within each distinct stage of agentic workflows,\nCortex lays the groundwork for more advanced, agent-native serving paradigms,\nincluding malleable resource management, speculative execution of workflow\nbranches, and a shared, multi-tiered cache for \"agentic state.\"",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Cortex, a prototype workflow-aware serving platform designed for\nagentic workloads. The core principle of Cortex is stage isolation: it\nprovisions dedicated resource pools for each distinct stage of an agentic\nworkflow. This simple yet powerful strategy mitigates inter-stage interference\nin compute and memory, leading to better KV cache utilization, higher\nthroughput, and more predictable performance. By customizing resource\nallocation and scheduling within each distinct stage of agentic workflows,\nCortex lays the groundwork for more advanced, agent-native serving paradigms,\nincluding malleable resource management, speculative execution of workflow\nbranches, and a shared, multi-tiered cache for \"agentic state.\""
                },
                "authors": [
                    {
                        "name": "Nikos Pagonas"
                    },
                    {
                        "name": "Yeounoh Chung"
                    },
                    {
                        "name": "Kostis Kaffes"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    }
                ],
                "author_detail": {
                    "name": "Arvind Krishnamurthy"
                },
                "author": "Arvind Krishnamurthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13940v1",
                "updated": "2025-10-15T17:59:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    59,
                    45,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T17:59:45Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    59,
                    45,
                    2,
                    288,
                    0
                ],
                "title": "Less is More: Improving LLM Reasoning with Minimal Test-Time\n  Intervention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less is More: Improving LLM Reasoning with Minimal Test-Time\n  Intervention"
                },
                "summary": "Recent progress in large language models (LLMs) has focused on test-time\nscaling to improve reasoning via increased inference computation, but often at\nthe cost of efficiency. We revisit test-time behavior and uncover a simple yet\nunderexplored phenomenon: reasoning uncertainty is highly localized-only a\nsmall subset of high-entropy tokens dominantly affects output correctness.\nMotivated by this, we propose Minimal Test-Time Intervention (MTI), a\ntraining-free framework that enhances reasoning accuracy and stability with\nminimal overhead. MTI includes: (i) Selective CFG intervention, applying\nclassifier-free guidance only at uncertain positions; and (ii) Lightweight\nnegative-prompt guidance, reusing the main model's KV cache to approximate\nunconditional decoding efficiently. MTI yields consistent gains across general,\ncoding, and STEM tasks-e.g., +1.35% average improvement on eight benchmarks for\nQwen3-8B-Base and +5% on AIME2024 using Qwen3-32B-Reasoning-while remaining\nhighly efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in large language models (LLMs) has focused on test-time\nscaling to improve reasoning via increased inference computation, but often at\nthe cost of efficiency. We revisit test-time behavior and uncover a simple yet\nunderexplored phenomenon: reasoning uncertainty is highly localized-only a\nsmall subset of high-entropy tokens dominantly affects output correctness.\nMotivated by this, we propose Minimal Test-Time Intervention (MTI), a\ntraining-free framework that enhances reasoning accuracy and stability with\nminimal overhead. MTI includes: (i) Selective CFG intervention, applying\nclassifier-free guidance only at uncertain positions; and (ii) Lightweight\nnegative-prompt guidance, reusing the main model's KV cache to approximate\nunconditional decoding efficiently. MTI yields consistent gains across general,\ncoding, and STEM tasks-e.g., +1.35% average improvement on eight benchmarks for\nQwen3-8B-Base and +5% on AIME2024 using Qwen3-32B-Reasoning-while remaining\nhighly efficient."
                },
                "authors": [
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Mingyang Zhang"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Ganggui Ding"
                    },
                    {
                        "name": "Liang Hou"
                    },
                    {
                        "name": "Xin Tao"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Ying-Cong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ying-Cong Chen"
                },
                "author": "Ying-Cong Chen",
                "arxiv_comment": "Code: https://github.com/EnVision-Research/MTI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13797v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13797v1",
                "updated": "2025-10-15T17:57:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    57,
                    21,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T17:57:21Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    57,
                    21,
                    2,
                    288,
                    0
                ],
                "title": "Breadcrumbs Reasoning: Memory-Efficient Reasoning with Compression\n  Beacons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breadcrumbs Reasoning: Memory-Efficient Reasoning with Compression\n  Beacons"
                },
                "summary": "The scalability of large language models for long-context reasoning is\nseverely constrained by the linear growth of their Transformer key-value cache,\nwhich incurs significant memory and computational costs. We posit that as a\nmodel generates reasoning tokens, the informational value of past generated\ntokens diminishes, creating an opportunity for compression. In this work, we\npropose to periodically compress the generation KV cache with a learned,\nspecial-purpose token and evict compressed entries. We train the model to\nperform this compression via a modified joint distillation and reinforcement\nlearning (RL) framework. Our training method minimizes overhead over the\nconventional RL process, as it leverages RL outputs for distillation.\nEmpirically, our method achieves a superior memory-accuracy Pareto frontier\ncompared to both the model without cache compression and training-free\ncompression techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scalability of large language models for long-context reasoning is\nseverely constrained by the linear growth of their Transformer key-value cache,\nwhich incurs significant memory and computational costs. We posit that as a\nmodel generates reasoning tokens, the informational value of past generated\ntokens diminishes, creating an opportunity for compression. In this work, we\npropose to periodically compress the generation KV cache with a learned,\nspecial-purpose token and evict compressed entries. We train the model to\nperform this compression via a modified joint distillation and reinforcement\nlearning (RL) framework. Our training method minimizes overhead over the\nconventional RL process, as it leverages RL outputs for distillation.\nEmpirically, our method achieves a superior memory-accuracy Pareto frontier\ncompared to both the model without cache compression and training-free\ncompression techniques."
                },
                "authors": [
                    {
                        "name": "Giovanni Monea"
                    },
                    {
                        "name": "Yair Feldman"
                    },
                    {
                        "name": "Shankar Padmanabhan"
                    },
                    {
                        "name": "Kianté Brantley"
                    },
                    {
                        "name": "Yoav Artzi"
                    }
                ],
                "author_detail": {
                    "name": "Yoav Artzi"
                },
                "author": "Yoav Artzi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13797v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13797v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15075v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15075v3",
                "updated": "2025-10-15T16:03:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    3,
                    13,
                    2,
                    288,
                    0
                ],
                "published": "2025-02-20T22:24:27Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    22,
                    24,
                    27,
                    3,
                    51,
                    0
                ],
                "title": "Quantize What Counts: More for Keys, Less for Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantize What Counts: More for Keys, Less for Values"
                },
                "summary": "Large Language Models (LLMs) suffer inference-time memory bottlenecks\ndominated by the attention Key-Value (KV) cache, which scales with model size\nand context length. While KV-cache quantization alleviates this cost, bit\nallocation between keys and values is often tuned heuristically, lacking\ntheoretical grounding and generalizability. This paper proposes two theorems\nthat anchor mixed-precision KV quantization in the intrinsic geometry of\nTransformer models. First, key projections systematically have larger spectral\nand Frobenius norms than value matrices, implying higher information density\nalong the key path. Second, for any given memory budget, prioritizing precision\nfor keys over values strictly reduces quantization error and better preserves\naccuracy. Empirical evaluations across various prominent LLMs and benchmarks\nshow that key-favored allocations (e.g., 4-bit keys, 2-bit values) retain up to\n98.3\\% accuracy compared to uniform allocations (e.g., 4-bit for both), while\nconserving memory. These results transform bit allocation from ad hoc tuning\ninto a theoretically grounded, geometry-driven design principle for efficient\nLLM inference. Source code is available at\nhttps://github.com/mohsenhariri/spectral-kv.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) suffer inference-time memory bottlenecks\ndominated by the attention Key-Value (KV) cache, which scales with model size\nand context length. While KV-cache quantization alleviates this cost, bit\nallocation between keys and values is often tuned heuristically, lacking\ntheoretical grounding and generalizability. This paper proposes two theorems\nthat anchor mixed-precision KV quantization in the intrinsic geometry of\nTransformer models. First, key projections systematically have larger spectral\nand Frobenius norms than value matrices, implying higher information density\nalong the key path. Second, for any given memory budget, prioritizing precision\nfor keys over values strictly reduces quantization error and better preserves\naccuracy. Empirical evaluations across various prominent LLMs and benchmarks\nshow that key-favored allocations (e.g., 4-bit keys, 2-bit values) retain up to\n98.3\\% accuracy compared to uniform allocations (e.g., 4-bit for both), while\nconserving memory. These results transform bit allocation from ad hoc tuning\ninto a theoretically grounded, geometry-driven design principle for efficient\nLLM inference. Source code is available at\nhttps://github.com/mohsenhariri/spectral-kv."
                },
                "authors": [
                    {
                        "name": "Mohsen Hariri"
                    },
                    {
                        "name": "Alan Luo"
                    },
                    {
                        "name": "Weicong Chen"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Tianyi Zhang"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Xia Hu"
                    },
                    {
                        "name": "Xiaotian Han"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    }
                ],
                "author_detail": {
                    "name": "Vipin Chaudhary"
                },
                "author": "Vipin Chaudhary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15075v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15075v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13602v1",
                "updated": "2025-10-15T14:33:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    33,
                    16,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T14:33:16Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    33,
                    16,
                    2,
                    288,
                    0
                ],
                "title": "NOSA: Native and Offloadable Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NOSA: Native and Offloadable Sparse Attention"
                },
                "summary": "Trainable sparse attention has emerged as a promising solution to address the\ndecoding efficiency bottleneck of LLMs in long-context processing,\nsignificantly saving memory accesses while minimally impacting task\nperformance. However, existing sparse attention methods leave a crucial\nlimitation unresolved: the size of the key-value (KV) cache remains unreduced,\nwhich constrains on-GPU batch sizes and throttles decoding throughput,\nespecially in large-scale batched inference. In this paper, we show that\ntrainable sparse attention naturally exhibits strong locality in token\nselection across adjacent decoding steps, thereby enabling KV cache offloading\nwithout altering the underlying attention computation. However, the inherent\nlocality remains insufficient to achieve efficient offloading, as the transfer\nof selected KV pairs between the CPU and GPU continues to dominate the overall\ndecoding cost. Building on this insight, we present NOSA, a trainable sparse\nattention framework designed to natively support KV cache offloading. NOSA\nintroduces explicit locality constraints by decomposing token selection into\nquery-aware and query-agnostic components, thereby reducing KV transfers while\npreserving the same attention computation as used during training. We pretrain\na 1B-parameter model with NOSA and conduct extensive benchmarks, showing that\nit preserves near-lossless performance while achieving up to a 2.3x improvement\nin decoding throughput compared with the vanilla trainable sparse attention\nbaseline (InfLLM-V2).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trainable sparse attention has emerged as a promising solution to address the\ndecoding efficiency bottleneck of LLMs in long-context processing,\nsignificantly saving memory accesses while minimally impacting task\nperformance. However, existing sparse attention methods leave a crucial\nlimitation unresolved: the size of the key-value (KV) cache remains unreduced,\nwhich constrains on-GPU batch sizes and throttles decoding throughput,\nespecially in large-scale batched inference. In this paper, we show that\ntrainable sparse attention naturally exhibits strong locality in token\nselection across adjacent decoding steps, thereby enabling KV cache offloading\nwithout altering the underlying attention computation. However, the inherent\nlocality remains insufficient to achieve efficient offloading, as the transfer\nof selected KV pairs between the CPU and GPU continues to dominate the overall\ndecoding cost. Building on this insight, we present NOSA, a trainable sparse\nattention framework designed to natively support KV cache offloading. NOSA\nintroduces explicit locality constraints by decomposing token selection into\nquery-aware and query-agnostic components, thereby reducing KV transfers while\npreserving the same attention computation as used during training. We pretrain\na 1B-parameter model with NOSA and conduct extensive benchmarks, showing that\nit preserves near-lossless performance while achieving up to a 2.3x improvement\nin decoding throughput compared with the vanilla trainable sparse attention\nbaseline (InfLLM-V2)."
                },
                "authors": [
                    {
                        "name": "Yuxiang Huang"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyuan Liu"
                },
                "author": "Zhiyuan Liu",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13401v1",
                "updated": "2025-10-15T10:56:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    10,
                    56,
                    37,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T10:56:37Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    10,
                    56,
                    37,
                    2,
                    288,
                    0
                ],
                "title": "F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs"
                },
                "summary": "Large Language Models (LLMs) have become increasingly prominent for daily\ntasks, from improving sound-totext translation to generating additional frames\nfor the latest video games. With the help of LLM inference frameworks, such as\nllama.cpp, which support optimizations such as KV-caching and quantization, it\nis now easier than ever to deploy LLMs on edge devices. Quantization is\nfundamental to enable LLMs on resource-constrained edge devices, and llama.cpp\nutilizes block floating point (BFP) quantization to drastically reduce the bit\nwidth of weights and input tensors, the memory footprint, and the computational\npower required to run LLMs. LLMs are typically quantized with mixed BFP\nquantization across the model layers to reduce the loss of model accuracy due\nto quantization. Therefore, to efficiently accelerate across the layers of\nBFP-quantized LLMs, specialized accelerators need to support different BFP\nvariants without reconfiguration. To address this issue, we propose a Flexible\nBlock FloatingPoint Quantization (F-BFQ) accelerator, which can dynamically\nswitch between two BFP quantization variants and perform matrix multiplication\n(MatMul) operations. Our initial F-BFQ accelerator design, deployed on the AMD\nKria board, reduces inference time by 1.4x on average over the Arm NEON-based\nCPU execution across three BFP quantized LLMs while achieving 5.2 tokens per\nsecond (~3.9 words per second).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become increasingly prominent for daily\ntasks, from improving sound-totext translation to generating additional frames\nfor the latest video games. With the help of LLM inference frameworks, such as\nllama.cpp, which support optimizations such as KV-caching and quantization, it\nis now easier than ever to deploy LLMs on edge devices. Quantization is\nfundamental to enable LLMs on resource-constrained edge devices, and llama.cpp\nutilizes block floating point (BFP) quantization to drastically reduce the bit\nwidth of weights and input tensors, the memory footprint, and the computational\npower required to run LLMs. LLMs are typically quantized with mixed BFP\nquantization across the model layers to reduce the loss of model accuracy due\nto quantization. Therefore, to efficiently accelerate across the layers of\nBFP-quantized LLMs, specialized accelerators need to support different BFP\nvariants without reconfiguration. To address this issue, we propose a Flexible\nBlock FloatingPoint Quantization (F-BFQ) accelerator, which can dynamically\nswitch between two BFP quantization variants and perform matrix multiplication\n(MatMul) operations. Our initial F-BFQ accelerator design, deployed on the AMD\nKria board, reduces inference time by 1.4x on average over the Arm NEON-based\nCPU execution across three BFP quantized LLMs while achieving 5.2 tokens per\nsecond (~3.9 words per second)."
                },
                "authors": [
                    {
                        "name": "Jude Haris"
                    },
                    {
                        "name": "José Cano"
                    }
                ],
                "author_detail": {
                    "name": "José Cano"
                },
                "author": "José Cano",
                "arxiv_comment": "Accepted to Workshop on New Approaches for Addressing the Computing\n  Requirements of LLMs and GNNs (LG-ARC) @ ISCA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13334v1",
                "updated": "2025-10-15T09:18:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    9,
                    18,
                    58,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T09:18:58Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    9,
                    18,
                    58,
                    2,
                    288,
                    0
                ],
                "title": "Taming the Fragility of KV Cache Eviction in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taming the Fragility of KV Cache Eviction in LLM Inference"
                },
                "summary": "Large language models have revolutionized natural language processing, yet\ntheir deployment remains hampered by the substantial memory and runtime\noverhead of the transformer's Key-Value cache. To mitigate this, recent methods\nemploy a scoring-aggregation framework to evict unimportant cache entries,\nbased on the stability assumption-that a fixed subset of entries remains\nconsistently important during generation. However, prior work has largely\nfocused on refining importance indicators for scoring, while defaulting to mean\naggregation due to a faithful trust in the stability assumption. In this work,\nwe argue that this underlying assumption is inherently fragile, making mean\naggregation highly vulnerable in extreme cases. To counter this, we propose a\nsimple yet elegant defensive aggregation strategy: a two-step, linear-time\napproach that controls worst-case risk, thereby defending against extreme cases\nwith negligible computational overhead. Embodying this strategy, we propose a\nnovel cache eviction method, DefensiveKV and its extension, Layer-DefensiveKV,\nwhich incorporates layer-wise budget allocation. Across seven task domains (18\ndatasets), our methods reduce generation quality loss by 2.3x and 4.3x\nrespectively, versus the strongest baseline under a 20% cache size. These\nresults set new performance benchmarks and pioneer a promising direction for\noptimizing cache eviction against underlying fragility through worst-case risk\nmanagement. Our code is available at https://github.com/FFY0/DefensiveKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have revolutionized natural language processing, yet\ntheir deployment remains hampered by the substantial memory and runtime\noverhead of the transformer's Key-Value cache. To mitigate this, recent methods\nemploy a scoring-aggregation framework to evict unimportant cache entries,\nbased on the stability assumption-that a fixed subset of entries remains\nconsistently important during generation. However, prior work has largely\nfocused on refining importance indicators for scoring, while defaulting to mean\naggregation due to a faithful trust in the stability assumption. In this work,\nwe argue that this underlying assumption is inherently fragile, making mean\naggregation highly vulnerable in extreme cases. To counter this, we propose a\nsimple yet elegant defensive aggregation strategy: a two-step, linear-time\napproach that controls worst-case risk, thereby defending against extreme cases\nwith negligible computational overhead. Embodying this strategy, we propose a\nnovel cache eviction method, DefensiveKV and its extension, Layer-DefensiveKV,\nwhich incorporates layer-wise budget allocation. Across seven task domains (18\ndatasets), our methods reduce generation quality loss by 2.3x and 4.3x\nrespectively, versus the strongest baseline under a 20% cache size. These\nresults set new performance benchmarks and pioneer a promising direction for\noptimizing cache eviction against underlying fragility through worst-case risk\nmanagement. Our code is available at https://github.com/FFY0/DefensiveKV."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Haoyu Guo"
                    },
                    {
                        "name": "JunLin Lv"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    },
                    {
                        "name": "Xike Xie"
                    }
                ],
                "author_detail": {
                    "name": "Xike Xie"
                },
                "author": "Xike Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13279v1",
                "updated": "2025-10-15T08:25:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    8,
                    25,
                    13,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T08:25:13Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    8,
                    25,
                    13,
                    2,
                    288,
                    0
                ],
                "title": "Partitioned Scheduling for DAG Tasks Considering Probabilistic Execution\n  Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Partitioned Scheduling for DAG Tasks Considering Probabilistic Execution\n  Time"
                },
                "summary": "Autonomous driving systems, critical for safety, require real-time guarantees\nand can be modeled as DAGs. Their acceleration features, such as caches and\npipelining, often result in execution times below the worst-case. Thus, a\nprobabilistic approach ensuring constraint satisfaction within a probability\nthreshold is more suitable than worst-case guarantees for these systems. This\npaper considers probabilistic guarantees for DAG tasks by utilizing the results\nof probabilistic guarantees for single processors, which have been relatively\nmore advanced than those for multi-core processors. This paper proposes a task\nset partitioning method that guarantees schedulability under the partitioned\nscheduling. The evaluation on randomly generated DAG task sets demonstrates\nthat the proposed method schedules more task sets with a smaller mean analysis\ntime compared to existing probabilistic schedulability analysis for DAGs. The\nevaluation also compares four bin-packing heuristics, revealing Item-Centric\nWorst-Fit-Decreasing schedules the most task sets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous driving systems, critical for safety, require real-time guarantees\nand can be modeled as DAGs. Their acceleration features, such as caches and\npipelining, often result in execution times below the worst-case. Thus, a\nprobabilistic approach ensuring constraint satisfaction within a probability\nthreshold is more suitable than worst-case guarantees for these systems. This\npaper considers probabilistic guarantees for DAG tasks by utilizing the results\nof probabilistic guarantees for single processors, which have been relatively\nmore advanced than those for multi-core processors. This paper proposes a task\nset partitioning method that guarantees schedulability under the partitioned\nscheduling. The evaluation on randomly generated DAG task sets demonstrates\nthat the proposed method schedules more task sets with a smaller mean analysis\ntime compared to existing probabilistic schedulability analysis for DAGs. The\nevaluation also compares four bin-packing heuristics, revealing Item-Centric\nWorst-Fit-Decreasing schedules the most task sets."
                },
                "authors": [
                    {
                        "name": "Fuma Omori"
                    },
                    {
                        "name": "Atsushi Yano"
                    },
                    {
                        "name": "Takuya Azumi"
                    }
                ],
                "author_detail": {
                    "name": "Takuya Azumi"
                },
                "author": "Takuya Azumi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13223v1",
                "updated": "2025-10-15T07:20:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    7,
                    20,
                    14,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T07:20:14Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    7,
                    20,
                    14,
                    2,
                    288,
                    0
                ],
                "title": "BanaServe: Unified KV Cache and Dynamic Module Migration for Balancing\n  Disaggregated LLM Serving in AI Infrastructure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BanaServe: Unified KV Cache and Dynamic Module Migration for Balancing\n  Disaggregated LLM Serving in AI Infrastructure"
                },
                "summary": "Large language models (LLMs) are increasingly deployed in AI infrastructure,\ndriving the need for high throughput, resource efficient serving systems.\nDisaggregated LLM serving, which separates prompt prefill from auto-regressive\ndecode, has emerged as a promising architecture by isolating their\nheterogeneous compute and memory demands. However, current disaggregated\nsystems face three key limitations: (i) static resource allocation cannot adapt\nto highly dynamic workloads, causing over-provisioning that wastes resources or\nunder-provisioning that violates service level objectives (SLOs); (ii) inherent\nload imbalance between prefill and decode stages, where prefill is\ncompute-bound and decode is memory-bound, causes under-utilization in one tier\nwhile the other becomes a bottleneck; and (iii) prefix cache aware routing\nskews load distribution, as high cache hit rate prefill nodes attract\ndisproportionately more requests, further degrading balance and efficiency. To\naddress these issues, we present BanaServe, a dynamic orchestration framework\nthat continuously rebalances computational and memory resources across prefill\nand decode instances while eliminating hotspots induced by cache. BanaServe\nintroduces layer level weight migration, attention level Key Value Cache (KV\nCache) migration, and Global KV Cache Store sharing with layer wise overlapped\ntransmission, enabling both coarse grained (layer level) and fine grained\n(attention level) load redistribution with minimal latency overhead. These\nmechanisms allow routers to perform purely load aware scheduling, unconstrained\nby cache placement. Compared to vLLM, BanaServe achieves 1.2x-3.9x higher\nthroughput with 3.9%-78.4% lower total processing time, and outperforms\nDistServe by 1.1x-2.8x in throughput with 1.4%-70.1% latency reduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly deployed in AI infrastructure,\ndriving the need for high throughput, resource efficient serving systems.\nDisaggregated LLM serving, which separates prompt prefill from auto-regressive\ndecode, has emerged as a promising architecture by isolating their\nheterogeneous compute and memory demands. However, current disaggregated\nsystems face three key limitations: (i) static resource allocation cannot adapt\nto highly dynamic workloads, causing over-provisioning that wastes resources or\nunder-provisioning that violates service level objectives (SLOs); (ii) inherent\nload imbalance between prefill and decode stages, where prefill is\ncompute-bound and decode is memory-bound, causes under-utilization in one tier\nwhile the other becomes a bottleneck; and (iii) prefix cache aware routing\nskews load distribution, as high cache hit rate prefill nodes attract\ndisproportionately more requests, further degrading balance and efficiency. To\naddress these issues, we present BanaServe, a dynamic orchestration framework\nthat continuously rebalances computational and memory resources across prefill\nand decode instances while eliminating hotspots induced by cache. BanaServe\nintroduces layer level weight migration, attention level Key Value Cache (KV\nCache) migration, and Global KV Cache Store sharing with layer wise overlapped\ntransmission, enabling both coarse grained (layer level) and fine grained\n(attention level) load redistribution with minimal latency overhead. These\nmechanisms allow routers to perform purely load aware scheduling, unconstrained\nby cache placement. Compared to vLLM, BanaServe achieves 1.2x-3.9x higher\nthroughput with 3.9%-78.4% lower total processing time, and outperforms\nDistServe by 1.1x-2.8x in throughput with 1.4%-70.1% latency reduction."
                },
                "authors": [
                    {
                        "name": "Yiyuan He"
                    },
                    {
                        "name": "Minxian Xu"
                    },
                    {
                        "name": "Jingfeng Wu"
                    },
                    {
                        "name": "Jianmin Hu"
                    },
                    {
                        "name": "Chong Ma"
                    },
                    {
                        "name": "Min Shen"
                    },
                    {
                        "name": "Le Chen"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Lin Qu"
                    },
                    {
                        "name": "Kejiang Ye"
                    }
                ],
                "author_detail": {
                    "name": "Kejiang Ye"
                },
                "author": "Kejiang Ye",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13084v1",
                "updated": "2025-10-15T01:55:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    1,
                    55,
                    32,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T01:55:32Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    1,
                    55,
                    32,
                    2,
                    288,
                    0
                ],
                "title": "Edit-Your-Interest: Efficient Video Editing via Feature Most-Similar\n  Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edit-Your-Interest: Efficient Video Editing via Feature Most-Similar\n  Propagation"
                },
                "summary": "Text-to-image (T2I) diffusion models have recently demonstrated significant\nprogress in video editing.\n  However, existing video editing methods are severely limited by their high\ncomputational overhead and memory consumption.\n  Furthermore, these approaches often sacrifice visual fidelity, leading to\nundesirable temporal inconsistencies and artifacts such as blurring and\npronounced mosaic-like patterns.\n  We propose Edit-Your-Interest, a lightweight, text-driven, zero-shot video\nediting method.\n  Edit-Your-Interest introduces a spatio-temporal feature memory to cache\nfeatures from previous frames, significantly reducing computational overhead\ncompared to full-sequence spatio-temporal modeling approaches.\n  Specifically, we first introduce a Spatio-Temporal Feature Memory bank (SFM),\nwhich is designed to efficiently cache and retain the crucial image tokens\nprocessed by spatial attention.\n  Second, we propose the Feature Most-Similar Propagation (FMP) method. FMP\npropagates the most relevant tokens from previous frames to subsequent ones,\npreserving temporal consistency.\n  Finally, we introduce an SFM update algorithm that continuously refreshes the\ncached features, ensuring their long-term relevance and effectiveness\nthroughout the video sequence.\n  Furthermore, we leverage cross-attention maps to automatically extract masks\nfor the instances of interest.\n  These masks are seamlessly integrated into the diffusion denoising process,\nenabling fine-grained control over target objects and allowing\nEdit-Your-Interest to perform highly accurate edits while robustly preserving\nthe background integrity.\n  Extensive experiments decisively demonstrate that the proposed\nEdit-Your-Interest outperforms state-of-the-art methods in both efficiency and\nvisual fidelity, validating its superior effectiveness and practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) diffusion models have recently demonstrated significant\nprogress in video editing.\n  However, existing video editing methods are severely limited by their high\ncomputational overhead and memory consumption.\n  Furthermore, these approaches often sacrifice visual fidelity, leading to\nundesirable temporal inconsistencies and artifacts such as blurring and\npronounced mosaic-like patterns.\n  We propose Edit-Your-Interest, a lightweight, text-driven, zero-shot video\nediting method.\n  Edit-Your-Interest introduces a spatio-temporal feature memory to cache\nfeatures from previous frames, significantly reducing computational overhead\ncompared to full-sequence spatio-temporal modeling approaches.\n  Specifically, we first introduce a Spatio-Temporal Feature Memory bank (SFM),\nwhich is designed to efficiently cache and retain the crucial image tokens\nprocessed by spatial attention.\n  Second, we propose the Feature Most-Similar Propagation (FMP) method. FMP\npropagates the most relevant tokens from previous frames to subsequent ones,\npreserving temporal consistency.\n  Finally, we introduce an SFM update algorithm that continuously refreshes the\ncached features, ensuring their long-term relevance and effectiveness\nthroughout the video sequence.\n  Furthermore, we leverage cross-attention maps to automatically extract masks\nfor the instances of interest.\n  These masks are seamlessly integrated into the diffusion denoising process,\nenabling fine-grained control over target objects and allowing\nEdit-Your-Interest to perform highly accurate edits while robustly preserving\nthe background integrity.\n  Extensive experiments decisively demonstrate that the proposed\nEdit-Your-Interest outperforms state-of-the-art methods in both efficiency and\nvisual fidelity, validating its superior effectiveness and practicality."
                },
                "authors": [
                    {
                        "name": "Yi Zuo"
                    },
                    {
                        "name": "Zitao Wang"
                    },
                    {
                        "name": "Lingling Li"
                    },
                    {
                        "name": "Xu Liu"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Licheng Jiao"
                    }
                ],
                "author_detail": {
                    "name": "Licheng Jiao"
                },
                "author": "Licheng Jiao",
                "arxiv_comment": "32 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15969v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15969v3",
                "updated": "2025-10-15T01:55:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    1,
                    55,
                    31,
                    2,
                    288,
                    0
                ],
                "published": "2025-06-19T02:25:04Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    25,
                    4,
                    3,
                    170,
                    0
                ],
                "title": "LazyEviction: Lagged KV Eviction with Attention Pattern Observation for\n  Efficient Long Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyEviction: Lagged KV Eviction with Attention Pattern Observation for\n  Efficient Long Reasoning"
                },
                "summary": "Large Language Models (LLMs) exhibit enhanced capabilities by\nChain-of-Thought reasoning. However, the extended reasoning sequences introduce\nsignificant GPU memory overhead due to increased key-value (KV) cache. Existing\nKV cache compression methods mitigate memory bottlenecks but struggle in long\nreasoning tasks. In this paper, we analyze attention patterns in reasoning\ntasks and reveal a Token Importance Recurrence phenomenon: a large proportion\nof tokens regain high attention after multiple decoding steps, which is failed\nto capture by existing works and may lead to unpredictable eviction on such\nperiodically critical tokens. To address this, we propose LazyEviction, an\nobservation window-based lagged eviction framework retaining latent recurring\ntokens by prioritized eviction based on tokens' recurrence patterns. Extensive\nexperiments demonstrate that LazyEviction reduces KV cache by 50%~70% while\nmaintaining comparable accuracy, outperforming existing KV cache compression\nbaselines. Our implementation code can be found at\nhttps://github.com/Halo-949/LazyEviction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit enhanced capabilities by\nChain-of-Thought reasoning. However, the extended reasoning sequences introduce\nsignificant GPU memory overhead due to increased key-value (KV) cache. Existing\nKV cache compression methods mitigate memory bottlenecks but struggle in long\nreasoning tasks. In this paper, we analyze attention patterns in reasoning\ntasks and reveal a Token Importance Recurrence phenomenon: a large proportion\nof tokens regain high attention after multiple decoding steps, which is failed\nto capture by existing works and may lead to unpredictable eviction on such\nperiodically critical tokens. To address this, we propose LazyEviction, an\nobservation window-based lagged eviction framework retaining latent recurring\ntokens by prioritized eviction based on tokens' recurrence patterns. Extensive\nexperiments demonstrate that LazyEviction reduces KV cache by 50%~70% while\nmaintaining comparable accuracy, outperforming existing KV cache compression\nbaselines. Our implementation code can be found at\nhttps://github.com/Halo-949/LazyEviction."
                },
                "authors": [
                    {
                        "name": "Haoyue Zhang"
                    },
                    {
                        "name": "Hualei Zhang"
                    },
                    {
                        "name": "Xiaosong Ma"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Song Guo"
                    }
                ],
                "author_detail": {
                    "name": "Song Guo"
                },
                "author": "Song Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15969v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15969v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12889v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12889v1",
                "updated": "2025-10-14T18:04:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    18,
                    4,
                    0,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T18:04:00Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    18,
                    4,
                    0,
                    1,
                    287,
                    0
                ],
                "title": "Dodoor: Efficient Randomized Decentralized Scheduling with Load Caching\n  for Heterogeneous Tasks and Clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dodoor: Efficient Randomized Decentralized Scheduling with Load Caching\n  for Heterogeneous Tasks and Clusters"
                },
                "summary": "This paper introduces Dodoor, an efficient randomized decentralized scheduler\ndesigned for task scheduling in modern data centers. Dodoor leverages advanced\nresearch on the weighted balls-into-bins model with b-batched setting. Unlike\nother decentralized schedulers that rely on real-time probing of remote\nservers, Dodoor makes scheduling decisions based on cached server information,\nwhich is updated in batches, to reduce communication overheads. To schedule\ntasks with dynamic, multidimensional resource requirements in heterogeneous\ncluster, Dodoor uses a novel load score to measure servers' loads for each\nscheduled task. This score captures the anti-affinity between servers and tasks\nin contrast to the commonly used heuristic of counting pending tasks to balance\nload. On a 101-node heterogeneous cluster, Dodoor is evaluated using two\nworkloads: (i) simulated Azure virtual machines placements and (ii) real\nserverless Python functions executions in Docker. The evaluation shows that\nDodoor reduces scheduling messages by 55--66% on both workloads. Dodoor can\nalso increase throughput by up to 33.2% and 21.5%, reduce mean makespan latency\nby 12.1% and 7.2%, and improve tail latency by 21.9% and 24.6% across the two\nworkloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces Dodoor, an efficient randomized decentralized scheduler\ndesigned for task scheduling in modern data centers. Dodoor leverages advanced\nresearch on the weighted balls-into-bins model with b-batched setting. Unlike\nother decentralized schedulers that rely on real-time probing of remote\nservers, Dodoor makes scheduling decisions based on cached server information,\nwhich is updated in batches, to reduce communication overheads. To schedule\ntasks with dynamic, multidimensional resource requirements in heterogeneous\ncluster, Dodoor uses a novel load score to measure servers' loads for each\nscheduled task. This score captures the anti-affinity between servers and tasks\nin contrast to the commonly used heuristic of counting pending tasks to balance\nload. On a 101-node heterogeneous cluster, Dodoor is evaluated using two\nworkloads: (i) simulated Azure virtual machines placements and (ii) real\nserverless Python functions executions in Docker. The evaluation shows that\nDodoor reduces scheduling messages by 55--66% on both workloads. Dodoor can\nalso increase throughput by up to 33.2% and 21.5%, reduce mean makespan latency\nby 12.1% and 7.2%, and improve tail latency by 21.9% and 24.6% across the two\nworkloads."
                },
                "authors": [
                    {
                        "name": "Wei Da"
                    },
                    {
                        "name": "Evangelia Kalyvianaki"
                    }
                ],
                "author_detail": {
                    "name": "Evangelia Kalyvianaki"
                },
                "author": "Evangelia Kalyvianaki",
                "arxiv_comment": "single column,20 pages and 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12889v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12889v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12872v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12872v1",
                "updated": "2025-10-14T18:00:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    18,
                    0,
                    1,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T18:00:01Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    18,
                    0,
                    1,
                    1,
                    287,
                    0
                ],
                "title": "KVCOMM: Online Cross-context KV-cache Communication for Efficient\n  LLM-based Multi-agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCOMM: Online Cross-context KV-cache Communication for Efficient\n  LLM-based Multi-agent Systems"
                },
                "summary": "Multi-agent large language model (LLM) systems are increasingly adopted for\ncomplex language processing tasks that require communication and coordination\namong agents. However, these systems often suffer substantial overhead from\nrepeated reprocessing of overlapping contexts across agents. In typical\npipelines, once an agent receives a message from its predecessor, the full\ncontext-including prior turns-must be reprocessed from scratch, leading to\ninefficient processing. While key-value (KV) caching is an effective solution\nfor avoiding redundant computation in single-agent settings where prefixes\nremain unchanged, it cannot be directly reused in multi-agent scenarios due to\ndiverging prefixes introduced by agent-specific context extensions. We identify\nthat the core challenge lies in the offset variance of KV-caches across agents.\nTo address this, we propose KVCOMM, a training-free framework that enables\nefficient prefilling in multi-agent inference by reusing KV-caches and aligning\ncache offsets of overlapping contexts under diverse prefix contexts. KVCOMM\nestimates and adjusts KV-caches for shared content by referencing a pool of\ncached examples-termed anchors-that store observed cache deviations under\nvarying prefixes. The anchor pool is maintained and updated online, allowing\ndynamic adaptation to distinct user requests and context structures. KVCOMM\nachieves over 70% reuse rate across diverse multi-agent workloads, including\nretrieval-augmented generation, math reasoning, and collaborative coding tasks,\nall without quality degradation. Particularly, when each fully-connected agent\nreceives 1K input tokens with 512 prefix tokens and 512 output tokens under a\nfive-agent setting, KVCOMM achieves up to 7.8x speedup compared to the standard\nprefill pipeline, reducing TTFT from ~430 ms to ~55 ms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent large language model (LLM) systems are increasingly adopted for\ncomplex language processing tasks that require communication and coordination\namong agents. However, these systems often suffer substantial overhead from\nrepeated reprocessing of overlapping contexts across agents. In typical\npipelines, once an agent receives a message from its predecessor, the full\ncontext-including prior turns-must be reprocessed from scratch, leading to\ninefficient processing. While key-value (KV) caching is an effective solution\nfor avoiding redundant computation in single-agent settings where prefixes\nremain unchanged, it cannot be directly reused in multi-agent scenarios due to\ndiverging prefixes introduced by agent-specific context extensions. We identify\nthat the core challenge lies in the offset variance of KV-caches across agents.\nTo address this, we propose KVCOMM, a training-free framework that enables\nefficient prefilling in multi-agent inference by reusing KV-caches and aligning\ncache offsets of overlapping contexts under diverse prefix contexts. KVCOMM\nestimates and adjusts KV-caches for shared content by referencing a pool of\ncached examples-termed anchors-that store observed cache deviations under\nvarying prefixes. The anchor pool is maintained and updated online, allowing\ndynamic adaptation to distinct user requests and context structures. KVCOMM\nachieves over 70% reuse rate across diverse multi-agent workloads, including\nretrieval-augmented generation, math reasoning, and collaborative coding tasks,\nall without quality degradation. Particularly, when each fully-connected agent\nreceives 1K input tokens with 512 prefix tokens and 512 output tokens under a\nfive-agent setting, KVCOMM achieves up to 7.8x speedup compared to the standard\nprefill pipeline, reducing TTFT from ~430 ms to ~55 ms."
                },
                "authors": [
                    {
                        "name": "Hancheng Ye"
                    },
                    {
                        "name": "Zhengqi Gao"
                    },
                    {
                        "name": "Mingyuan Ma"
                    },
                    {
                        "name": "Qinsi Wang"
                    },
                    {
                        "name": "Yuzhe Fu"
                    },
                    {
                        "name": "Ming-Yu Chung"
                    },
                    {
                        "name": "Yueqian Lin"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Jianyi Zhang"
                    },
                    {
                        "name": "Danyang Zhuo"
                    },
                    {
                        "name": "Yiran Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Chen"
                },
                "author": "Yiran Chen",
                "arxiv_comment": "Accepted for publication in NeurIPS2025. Code is available at\n  \\url{https://github.com/HankYe/KVCOMM}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12872v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12872v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12705v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12705v1",
                "updated": "2025-10-14T16:39:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    39,
                    29,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T16:39:29Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    39,
                    29,
                    1,
                    287,
                    0
                ],
                "title": "A GPU-resident Memory-Aware Algorithm for Accelerating Bidiagonalization\n  of Banded Matrices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A GPU-resident Memory-Aware Algorithm for Accelerating Bidiagonalization\n  of Banded Matrices"
                },
                "summary": "The reduction of a banded matrix to a bidiagonal form is a crucial step in\nthe Singular Value Decomposition (SVD), a cornerstone of scientific computing\nand AI. Despite being a highly parallel algorithm, it was previously believed\nto be unsuitable for GPU computation because it is memory bandwidth-bound.\nRecent developments in GPU hardware, including larger L1 memory per Streaming\nMultiprocessor/Compute Unit, have changed that. We present the first GPU\nalgorithm for reducing a banded matrix to bidiagonal form as part of the\nNextLA$.$jl open-source software package. Our algorithm is based on previous\nCPU-based multicore parallel cache-efficient bulge chasing algorithms and\nadapted to optimize for GPU throughput. We leverage Julia Language's Array\nabstractions and KernelAbstractions to implement a single hardware- and data\nprecision-agnostic function on NVIDIA, AMD, Intel, and Apple Metal GPUs for\nhalf, single, and double precision, and examine performance optimization across\nhardware architectures and data precision. We also develop a hardware-aware\nperformance model and identify key hyperparameters, such as inner tilewidth and\nblock concurrency, that govern optimal GPU execution for bandwidth-bound\nworkloads. We demonstrate highly parallel bandwidth-bound algorithm on the GPU\ncan outperform CPU-based implementations: the GPU algorithm outperforms\nmultithreaded CPU High-Performance libraries PLASMA and SLATE as of matrix size\n1024 x 1024 and by a factor over 100 for matrices of 32k x 32k. In addition,\nthe performance of the algorithm increases linearly with matrix bandwidth size,\nmaking faster reduction of larger matrix bandwidths now also possible. With\nthis work, we break memory bandwidth barriers, as well as matrix bandwidth\nbarriers, resulting in orders-of-magnitude faster algorithms for the reduction\nof banded matrices to bidiagonal form on the GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reduction of a banded matrix to a bidiagonal form is a crucial step in\nthe Singular Value Decomposition (SVD), a cornerstone of scientific computing\nand AI. Despite being a highly parallel algorithm, it was previously believed\nto be unsuitable for GPU computation because it is memory bandwidth-bound.\nRecent developments in GPU hardware, including larger L1 memory per Streaming\nMultiprocessor/Compute Unit, have changed that. We present the first GPU\nalgorithm for reducing a banded matrix to bidiagonal form as part of the\nNextLA$.$jl open-source software package. Our algorithm is based on previous\nCPU-based multicore parallel cache-efficient bulge chasing algorithms and\nadapted to optimize for GPU throughput. We leverage Julia Language's Array\nabstractions and KernelAbstractions to implement a single hardware- and data\nprecision-agnostic function on NVIDIA, AMD, Intel, and Apple Metal GPUs for\nhalf, single, and double precision, and examine performance optimization across\nhardware architectures and data precision. We also develop a hardware-aware\nperformance model and identify key hyperparameters, such as inner tilewidth and\nblock concurrency, that govern optimal GPU execution for bandwidth-bound\nworkloads. We demonstrate highly parallel bandwidth-bound algorithm on the GPU\ncan outperform CPU-based implementations: the GPU algorithm outperforms\nmultithreaded CPU High-Performance libraries PLASMA and SLATE as of matrix size\n1024 x 1024 and by a factor over 100 for matrices of 32k x 32k. In addition,\nthe performance of the algorithm increases linearly with matrix bandwidth size,\nmaking faster reduction of larger matrix bandwidths now also possible. With\nthis work, we break memory bandwidth barriers, as well as matrix bandwidth\nbarriers, resulting in orders-of-magnitude faster algorithms for the reduction\nof banded matrices to bidiagonal form on the GPU."
                },
                "authors": [
                    {
                        "name": "Evelyne Ringoot"
                    },
                    {
                        "name": "Rabab Alomairy"
                    },
                    {
                        "name": "Alan Edelman"
                    }
                ],
                "author_detail": {
                    "name": "Alan Edelman"
                },
                "author": "Alan Edelman",
                "arxiv_comment": "13 pages, 7 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12705v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12705v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02659v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02659v3",
                "updated": "2025-10-14T16:05:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    5,
                    11,
                    1,
                    287,
                    0
                ],
                "published": "2025-07-03T14:20:41Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding"
                },
                "summary": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup."
                },
                "authors": [
                    {
                        "name": "Ramchalam Kinattinkara Ramakrishnan"
                    },
                    {
                        "name": "Zhaocong Yuan"
                    },
                    {
                        "name": "Shaojie Zhuo"
                    },
                    {
                        "name": "Chen Feng"
                    },
                    {
                        "name": "Yicheng Lin"
                    },
                    {
                        "name": "Chenzheng Su"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaopeng Zhang"
                },
                "author": "Xiaopeng Zhang",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02659v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02659v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00299v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00299v5",
                "updated": "2025-10-14T15:42:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    42,
                    41,
                    1,
                    287,
                    0
                ],
                "published": "2025-02-01T03:49:47Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem. The code is available at\n\\href{https://github.com/NVIDIA/kvpress}{link}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem. The code is available at\n\\href{https://github.com/NVIDIA/kvpress}{link}."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00299v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00299v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12642v1",
                "updated": "2025-10-14T15:34:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    34,
                    35,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T15:34:35Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    34,
                    35,
                    1,
                    287,
                    0
                ],
                "title": "Aixel: A Unified, Adaptive and Extensible System for AI-powered Data\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aixel: A Unified, Adaptive and Extensible System for AI-powered Data\n  Analysis"
                },
                "summary": "A growing trend in modern data analysis is the integration of data management\nwith learning, guided by accuracy, latency, and cost requirements. In practice,\napplications draw data of different formats from many sources. In the\nmeanwhile, the objectives and budgets change over time. Existing systems handle\nthese applications across databases, analysis libraries, and tuning services.\nSuch fragmentation leads to complex user interaction, limited adaptability,\nsuboptimal performance, and poor extensibility across components. To address\nthese challenges, we present Aixel, a unified, adaptive, and extensible system\nfor AI-powered data analysis. The system organizes work across four layers:\napplication, task, model, and data. The task layer provides a declarative\ninterface to capture user intent, which is parsed into an executable operator\nplan. An optimizer compiles and schedules this plan to meet specified goals in\naccuracy, latency, and cost. The task layer coordinates the execution of data\nand model operators, with built-in support for reuse and caching to improve\nefficiency. The model layer offers versioned storage for index, metadata,\ntensors, and model artifacts. It supports adaptive construction, task-aligned\ndrift detection, and safe updates that reuse shared components. The data layer\nprovides unified data management capabilities, including indexing,\nconstraint-aware discovery, task-aligned selection, and comprehensive feature\nmanagement. With the above designed layers, Aixel delivers a user friendly,\nadaptive, efficient, and extensible system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A growing trend in modern data analysis is the integration of data management\nwith learning, guided by accuracy, latency, and cost requirements. In practice,\napplications draw data of different formats from many sources. In the\nmeanwhile, the objectives and budgets change over time. Existing systems handle\nthese applications across databases, analysis libraries, and tuning services.\nSuch fragmentation leads to complex user interaction, limited adaptability,\nsuboptimal performance, and poor extensibility across components. To address\nthese challenges, we present Aixel, a unified, adaptive, and extensible system\nfor AI-powered data analysis. The system organizes work across four layers:\napplication, task, model, and data. The task layer provides a declarative\ninterface to capture user intent, which is parsed into an executable operator\nplan. An optimizer compiles and schedules this plan to meet specified goals in\naccuracy, latency, and cost. The task layer coordinates the execution of data\nand model operators, with built-in support for reuse and caching to improve\nefficiency. The model layer offers versioned storage for index, metadata,\ntensors, and model artifacts. It supports adaptive construction, task-aligned\ndrift detection, and safe updates that reuse shared components. The data layer\nprovides unified data management capabilities, including indexing,\nconstraint-aware discovery, task-aligned selection, and comprehensive feature\nmanagement. With the above designed layers, Aixel delivers a user friendly,\nadaptive, efficient, and extensible system."
                },
                "authors": [
                    {
                        "name": "Meihui Zhang"
                    },
                    {
                        "name": "Liming Wang"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Zhaojing Luo"
                    }
                ],
                "author_detail": {
                    "name": "Zhaojing Luo"
                },
                "author": "Zhaojing Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12629v1",
                "updated": "2025-10-14T15:26:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    26,
                    9,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T15:26:09Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    26,
                    9,
                    1,
                    287,
                    0
                ],
                "title": "Noisy Neighbor: Exploiting RDMA for Resource Exhaustion Attacks in\n  Containerized Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noisy Neighbor: Exploiting RDMA for Resource Exhaustion Attacks in\n  Containerized Clouds"
                },
                "summary": "In modern containerized cloud environments, the adoption of RDMA (Remote\nDirect Memory Access) has expanded to reduce CPU overhead and enable\nhigh-performance data exchange. Achieving this requires strong performance\nisolation to ensure that one container's RDMA workload does not degrade the\nperformance of others, thereby maintaining critical security assurances.\nHowever, existing isolation techniques are difficult to apply effectively due\nto the complexity of microarchitectural resource management within RDMA NICs\n(RNICs). This paper experimentally analyzes two types of resource exhaustion\nattacks on NVIDIA BlueField-3: (i) state saturation attacks and (ii) pipeline\nsaturation attacks. Our results show that state saturation attacks can cause up\nto a 93.9% loss in bandwidth, a 1,117x increase in latency, and a 115% rise in\ncache misses for victim containers, while pipeline saturation attacks lead to\nsevere link-level congestion and significant amplification, where small verb\nrequests result in disproportionately high resource consumption. To mitigate\nthese threats and restore predictable security assurances, we propose HT-Verbs,\na threshold-driven framework based on real-time per-container RDMA verb\ntelemetry and adaptive resource classification that partitions RNIC resources\ninto hot, warm, and cold tiers and throttles abusive workloads without\nrequiring hardware modifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern containerized cloud environments, the adoption of RDMA (Remote\nDirect Memory Access) has expanded to reduce CPU overhead and enable\nhigh-performance data exchange. Achieving this requires strong performance\nisolation to ensure that one container's RDMA workload does not degrade the\nperformance of others, thereby maintaining critical security assurances.\nHowever, existing isolation techniques are difficult to apply effectively due\nto the complexity of microarchitectural resource management within RDMA NICs\n(RNICs). This paper experimentally analyzes two types of resource exhaustion\nattacks on NVIDIA BlueField-3: (i) state saturation attacks and (ii) pipeline\nsaturation attacks. Our results show that state saturation attacks can cause up\nto a 93.9% loss in bandwidth, a 1,117x increase in latency, and a 115% rise in\ncache misses for victim containers, while pipeline saturation attacks lead to\nsevere link-level congestion and significant amplification, where small verb\nrequests result in disproportionately high resource consumption. To mitigate\nthese threats and restore predictable security assurances, we propose HT-Verbs,\na threshold-driven framework based on real-time per-container RDMA verb\ntelemetry and adaptive resource classification that partitions RNIC resources\ninto hot, warm, and cold tiers and throttles abusive workloads without\nrequiring hardware modifications."
                },
                "authors": [
                    {
                        "name": "Gunwoo Kim"
                    },
                    {
                        "name": "Taejune Park"
                    },
                    {
                        "name": "Jinwoo Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jinwoo Kim"
                },
                "author": "Jinwoo Kim",
                "arxiv_comment": "20 pages, 14 figures, presented at the 4th International Workshop on\n  System Security Assurance (SecAssure 2025), co-located with ESORICS 2025, to\n  appear in Springer LNCS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12280v1",
                "updated": "2025-10-14T08:34:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    8,
                    34,
                    9,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T08:34:09Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    8,
                    34,
                    9,
                    1,
                    287,
                    0
                ],
                "title": "Analysis and Evaluation of Using Microsecond-Latency Memory for\n  In-Memory Indices and Caches in SSD-Based Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis and Evaluation of Using Microsecond-Latency Memory for\n  In-Memory Indices and Caches in SSD-Based Key-Value Stores"
                },
                "summary": "When key-value (KV) stores use SSDs for storing a large number of items,\noftentimes they also require large in-memory data structures including indices\nand caches to be traversed to reduce IOs. This paper considers offloading most\nof such data structures from the costly host DRAM to secondary memory whose\nlatency is in the microsecond range, an order of magnitude longer than those of\ncurrently available DIMM-mounted or CXL memory devices. While emerging\nmicrosecond-latency memory is likely to cost much less than DRAM, it can\nsignificantly slow down SSD-based KV stores if naively employed. This paper\nanalyzes and evaluates the impact of microsecond-level memory latency on the KV\noperation throughput. Our analysis finds that a well-known latency-hiding\ntechnique of software prefetching for long-latency memory from user-level\nthreads is effective. The novelty of our analysis lies in modeling how the\ninterplay between prefetching and IO affects performance, from which we derive\nan equation that well explains the throughput degradation due to long memory\nlatency. The model tells us that the presence of IO significantly enhances the\ntolerance to memory latency, leading to a finding that SSD-based KV stores can\nbe made latency-tolerant without devising new techniques for\nmicrosecond-latency memory. To confirm this, we design a microbenchmark as well\nas modify existing SSD-based KV stores so that they issue prefetches from\nuser-level threads, and run them while placing most of in-memory data\nstructures on FPGA-based memory with adjustable microsecond latency. The\nresults demonstrate that their KV operation throughputs can be well explained\nby our model, and the modified KV stores achieve near-DRAM throughputs for up\nto a memory latency of 5 microseconds. This suggests the possibility that\nSSD-based KV stores can use microsecond-latency memory as a cost-effective\nalternative to the host DRAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When key-value (KV) stores use SSDs for storing a large number of items,\noftentimes they also require large in-memory data structures including indices\nand caches to be traversed to reduce IOs. This paper considers offloading most\nof such data structures from the costly host DRAM to secondary memory whose\nlatency is in the microsecond range, an order of magnitude longer than those of\ncurrently available DIMM-mounted or CXL memory devices. While emerging\nmicrosecond-latency memory is likely to cost much less than DRAM, it can\nsignificantly slow down SSD-based KV stores if naively employed. This paper\nanalyzes and evaluates the impact of microsecond-level memory latency on the KV\noperation throughput. Our analysis finds that a well-known latency-hiding\ntechnique of software prefetching for long-latency memory from user-level\nthreads is effective. The novelty of our analysis lies in modeling how the\ninterplay between prefetching and IO affects performance, from which we derive\nan equation that well explains the throughput degradation due to long memory\nlatency. The model tells us that the presence of IO significantly enhances the\ntolerance to memory latency, leading to a finding that SSD-based KV stores can\nbe made latency-tolerant without devising new techniques for\nmicrosecond-latency memory. To confirm this, we design a microbenchmark as well\nas modify existing SSD-based KV stores so that they issue prefetches from\nuser-level threads, and run them while placing most of in-memory data\nstructures on FPGA-based memory with adjustable microsecond latency. The\nresults demonstrate that their KV operation throughputs can be well explained\nby our model, and the modified KV stores achieve near-DRAM throughputs for up\nto a memory latency of 5 microseconds. This suggests the possibility that\nSSD-based KV stores can use microsecond-latency memory as a cost-effective\nalternative to the host DRAM."
                },
                "authors": [
                    {
                        "name": "Yosuke Bando"
                    },
                    {
                        "name": "Akinobu Mita"
                    },
                    {
                        "name": "Kazuhiro Hiwada"
                    },
                    {
                        "name": "Shintaro Sano"
                    },
                    {
                        "name": "Tomoya Suzuki"
                    },
                    {
                        "name": "Yu Nakanishi"
                    },
                    {
                        "name": "Kazutaka Tomida"
                    },
                    {
                        "name": "Hirotsugu Kajihara"
                    },
                    {
                        "name": "Akiyuki Kaneko"
                    },
                    {
                        "name": "Daisuke Taki"
                    },
                    {
                        "name": "Yukimasa Miyamoto"
                    },
                    {
                        "name": "Tomokazu Yoshida"
                    },
                    {
                        "name": "Tatsuo Shiozawa"
                    }
                ],
                "author_detail": {
                    "name": "Tatsuo Shiozawa"
                },
                "author": "Tatsuo Shiozawa",
                "arxiv_doi": "10.1145/3769759",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3769759",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.12280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proc. ACM Manag. Data 3, 6 (SIGMOD), Article 294 (December 2025),\n  28 pages",
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10975v2",
                "updated": "2025-10-14T07:41:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    7,
                    41,
                    47,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-13T03:26:14Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    3,
                    26,
                    14,
                    0,
                    286,
                    0
                ],
                "title": "RoVer: Robot Reward Model as Test-Time Verifier for\n  Vision-Language-Action Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoVer: Robot Reward Model as Test-Time Verifier for\n  Vision-Language-Action Model"
                },
                "summary": "Vision-Language-Action (VLA) models have become a prominent paradigm for\nembodied intelligence, yet further performance improvements typically rely on\nscaling up training data and model size -- an approach that is prohibitively\nexpensive for robotics and fundamentally limited by data collection costs. We\naddress this limitation with $\\mathbf{RoVer}$, an embodied test-time scaling\nframework that uses a $\\mathbf{Ro}$bot Process Reward Model (PRM) as a\nTest-Time $\\mathbf{Ver}$ifier to enhance the capabilities of existing VLA\nmodels without modifying their architectures or weights. Specifically, RoVer\n(i) assigns scalar-based process rewards to evaluate the reliability of\ncandidate actions, and (ii) predicts an action-space direction for candidate\nexpansion/refinement. During inference, RoVer generates multiple candidate\nactions concurrently from the base policy, expands them along PRM-predicted\ndirections, and then scores all candidates with PRM to select the optimal\naction for execution. Notably, by caching shared perception features, it can\namortize perception cost and evaluate more candidates under the same test-time\ncomputational budget. Essentially, our approach effectively transforms\navailable computing resources into better action decision-making, realizing the\nbenefits of test-time scaling without extra training overhead. Our\ncontributions are threefold: (1) a general, plug-and-play test-time scaling\nframework for VLAs; (2) a PRM that jointly provides scalar process rewards and\nan action-space direction to guide exploration; and (3) an efficient\ndirection-guided sampling strategy that leverages a shared perception cache to\nenable scalable candidate generation and selection during inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have become a prominent paradigm for\nembodied intelligence, yet further performance improvements typically rely on\nscaling up training data and model size -- an approach that is prohibitively\nexpensive for robotics and fundamentally limited by data collection costs. We\naddress this limitation with $\\mathbf{RoVer}$, an embodied test-time scaling\nframework that uses a $\\mathbf{Ro}$bot Process Reward Model (PRM) as a\nTest-Time $\\mathbf{Ver}$ifier to enhance the capabilities of existing VLA\nmodels without modifying their architectures or weights. Specifically, RoVer\n(i) assigns scalar-based process rewards to evaluate the reliability of\ncandidate actions, and (ii) predicts an action-space direction for candidate\nexpansion/refinement. During inference, RoVer generates multiple candidate\nactions concurrently from the base policy, expands them along PRM-predicted\ndirections, and then scores all candidates with PRM to select the optimal\naction for execution. Notably, by caching shared perception features, it can\namortize perception cost and evaluate more candidates under the same test-time\ncomputational budget. Essentially, our approach effectively transforms\navailable computing resources into better action decision-making, realizing the\nbenefits of test-time scaling without extra training overhead. Our\ncontributions are threefold: (1) a general, plug-and-play test-time scaling\nframework for VLAs; (2) a PRM that jointly provides scalar process rewards and\nan action-space direction to guide exploration; and (3) an efficient\ndirection-guided sampling strategy that leverages a shared perception cache to\nenable scalable candidate generation and selection during inference."
                },
                "authors": [
                    {
                        "name": "Mingtong Dai"
                    },
                    {
                        "name": "Lingbo Liu"
                    },
                    {
                        "name": "Yongjie Bai"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zhouxia Wang"
                    },
                    {
                        "name": "Rui SU"
                    },
                    {
                        "name": "Chunjie Chen"
                    },
                    {
                        "name": "Liang Lin"
                    },
                    {
                        "name": "Xinyu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xinyu Wu"
                },
                "author": "Xinyu Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11496v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11496v2",
                "updated": "2025-10-14T05:05:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    5,
                    5,
                    14,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-13T15:04:38Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    4,
                    38,
                    0,
                    286,
                    0
                ],
                "title": "AndesVL Technical Report: An Efficient Mobile-side Multimodal Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AndesVL Technical Report: An Efficient Mobile-side Multimodal Large\n  Language Model"
                },
                "summary": "In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o,\nGemini, and Claude Sonnet have demonstrated outstanding performance with\nenormous model sizes reaching hundreds of billions of parameters, they\nsignificantly surpass the limitations in memory, power consumption, and\ncomputing capacity of edge devices such as mobile phones. This paper introduces\nAndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on\nQwen3's LLM and various visual encoders. We comprehensively outline the model\narchitectures, training pipeline, and training data of AndesVL, which achieves\nfirst-tier performance across a wide range of open-source benchmarks, including\nfields such as text-rich image understanding, reasoning and math, multi-image\ncomprehension, general VQA, hallucination mitigation, multilingual\nunderstanding, and GUI-related tasks when compared with state-of-the-art models\nof a similar scale. Furthermore, we introduce a 1+N LoRA architecture alongside\na Quantization-Aware LoRA Fine-Tuning (QALFT) framework to facilitate efficient\ntask adaptation and model compression during mobile-side deployment of AndesVL.\nMoreover, utilizing our cache eviction algorithm -- OKV -- along with\ncustomized speculative decoding and compression strategies, we achieve a 6.7x\npeak decoding speedup ratio, up to 30.9% memory reduction, and 1.8\nbits-per-weight when deploying AndesVL-4B on MediaTek Dimensity 9500 chips. We\nrelease all models on https://huggingface.co/OPPOer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o,\nGemini, and Claude Sonnet have demonstrated outstanding performance with\nenormous model sizes reaching hundreds of billions of parameters, they\nsignificantly surpass the limitations in memory, power consumption, and\ncomputing capacity of edge devices such as mobile phones. This paper introduces\nAndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on\nQwen3's LLM and various visual encoders. We comprehensively outline the model\narchitectures, training pipeline, and training data of AndesVL, which achieves\nfirst-tier performance across a wide range of open-source benchmarks, including\nfields such as text-rich image understanding, reasoning and math, multi-image\ncomprehension, general VQA, hallucination mitigation, multilingual\nunderstanding, and GUI-related tasks when compared with state-of-the-art models\nof a similar scale. Furthermore, we introduce a 1+N LoRA architecture alongside\na Quantization-Aware LoRA Fine-Tuning (QALFT) framework to facilitate efficient\ntask adaptation and model compression during mobile-side deployment of AndesVL.\nMoreover, utilizing our cache eviction algorithm -- OKV -- along with\ncustomized speculative decoding and compression strategies, we achieve a 6.7x\npeak decoding speedup ratio, up to 30.9% memory reduction, and 1.8\nbits-per-weight when deploying AndesVL-4B on MediaTek Dimensity 9500 chips. We\nrelease all models on https://huggingface.co/OPPOer."
                },
                "authors": [
                    {
                        "name": "Zhiwei Jin"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Yafei Liu"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Ruichen Wang"
                    },
                    {
                        "name": "Zhihao Li"
                    },
                    {
                        "name": "Qi Qi"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Dongze Hao"
                    },
                    {
                        "name": "Quanlong Zheng"
                    },
                    {
                        "name": "Yanhao Zhang"
                    },
                    {
                        "name": "Haobo Ji"
                    },
                    {
                        "name": "Jian Ma"
                    },
                    {
                        "name": "Zhitong Zheng"
                    },
                    {
                        "name": "Zhenyi Lin"
                    },
                    {
                        "name": "Haolin Deng"
                    },
                    {
                        "name": "Xin Zou"
                    },
                    {
                        "name": "Xiaojie Yin"
                    },
                    {
                        "name": "Ruilin Wang"
                    },
                    {
                        "name": "Liankai Cai"
                    },
                    {
                        "name": "Haijing Liu"
                    },
                    {
                        "name": "Yuqing Qiu"
                    },
                    {
                        "name": "Ke Chen"
                    },
                    {
                        "name": "Zixian Li"
                    },
                    {
                        "name": "Chi Xie"
                    },
                    {
                        "name": "Huafei Li"
                    },
                    {
                        "name": "Chenxing Li"
                    },
                    {
                        "name": "Chuangchuang Wang"
                    },
                    {
                        "name": "Kai Tang"
                    },
                    {
                        "name": "Zhiguang Zhu"
                    },
                    {
                        "name": "Kai Tang"
                    },
                    {
                        "name": "Wenmei Gao"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Jun Wu"
                    },
                    {
                        "name": "Chao Liu"
                    },
                    {
                        "name": "Qin Xie"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Haonan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Haonan Lu"
                },
                "author": "Haonan Lu",
                "arxiv_comment": "Tech report of OPPO AndesVL Team",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11496v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11496v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12051v1",
                "updated": "2025-10-14T01:26:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    1,
                    26,
                    36,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T01:26:36Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    1,
                    26,
                    36,
                    1,
                    287,
                    0
                ],
                "title": "APCE: Adaptive Progressive Context Expansion for Long Context Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APCE: Adaptive Progressive Context Expansion for Long Context Processing"
                },
                "summary": "Deploying useful Long-Context Transformer Models (LCTMs) requires addressing\ntwo key challenges: (1) A growing memory footprint due to quadratic\nself-attention and linear KV-cache scaling in memory as sequence length\nincreases; (2) the ContextRot phenomena where empirical evidence suggests that\ntransformer architecture's performance degrades with increasing context length.\nGiven the shared dependency on the input, a natural question arises: Can we\nsurgically select the most important input chunks for processing to\nsynergistically (a) reduce the memory footprint, and (b) mitigate the\nContextRot effects? In this paper, we answer this question in the affirmative\nfor long-context summarization tasks. We propose APCE as a context-aware\nsolution to select the most important input chunks through low-dimensional\nsemantic similarity matching with the current query. By directly operating on\nthe input, APCE decouples from strict dependency on underlying hardware or CUDA\nenvironments, promising a compatible solution scalable to different deployment\nsystems. Our empirical evaluations have demonstrated superior or on-par\nsummarization performance for APCE compared to the full dense baseline using a\nfraction (50%-70%) of the input sequence resulting in KV-cache and\nself-attention memory efficiency improvements. We hope our findings inspire\nfurther research on context-aware efficiency solutions for LCTMs geared towards\nother relevant long-context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying useful Long-Context Transformer Models (LCTMs) requires addressing\ntwo key challenges: (1) A growing memory footprint due to quadratic\nself-attention and linear KV-cache scaling in memory as sequence length\nincreases; (2) the ContextRot phenomena where empirical evidence suggests that\ntransformer architecture's performance degrades with increasing context length.\nGiven the shared dependency on the input, a natural question arises: Can we\nsurgically select the most important input chunks for processing to\nsynergistically (a) reduce the memory footprint, and (b) mitigate the\nContextRot effects? In this paper, we answer this question in the affirmative\nfor long-context summarization tasks. We propose APCE as a context-aware\nsolution to select the most important input chunks through low-dimensional\nsemantic similarity matching with the current query. By directly operating on\nthe input, APCE decouples from strict dependency on underlying hardware or CUDA\nenvironments, promising a compatible solution scalable to different deployment\nsystems. Our empirical evaluations have demonstrated superior or on-par\nsummarization performance for APCE compared to the full dense baseline using a\nfraction (50%-70%) of the input sequence resulting in KV-cache and\nself-attention memory efficiency improvements. We hope our findings inspire\nfurther research on context-aware efficiency solutions for LCTMs geared towards\nother relevant long-context tasks."
                },
                "authors": [
                    {
                        "name": "Baisub Lee"
                    },
                    {
                        "name": "Sanghyun Byun"
                    },
                    {
                        "name": "Mohanad Odema"
                    },
                    {
                        "name": "Jung Guack"
                    },
                    {
                        "name": "Jacob Song"
                    },
                    {
                        "name": "Woo Seong Chung"
                    }
                ],
                "author_detail": {
                    "name": "Woo Seong Chung"
                },
                "author": "Woo Seong Chung",
                "arxiv_comment": "NeurIPS 2025 Workshop: ML For Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22622v2",
                "updated": "2025-10-13T22:41:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    22,
                    41,
                    26,
                    0,
                    286,
                    0
                ],
                "published": "2025-09-26T17:48:24Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    48,
                    24,
                    4,
                    269,
                    0
                ],
                "title": "LongLive: Real-time Interactive Long Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongLive: Real-time Interactive Long Video Generation"
                },
                "summary": "We present LongLive, a frame-level autoregressive (AR) framework for\nreal-time and interactive long video generation. Long video generation presents\nchallenges in both efficiency and quality. Diffusion and Diffusion-Forcing\nmodels can produce high-quality videos but suffer from low efficiency due to\nbidirectional attention. Causal attention AR models support KV caching for\nfaster inference, but often degrade in quality on long videos due to memory\nchallenges during long-video training. In addition, beyond static prompt-based\ngeneration, interactive capabilities, such as streaming prompt inputs, are\ncritical for dynamic content creation, enabling users to guide narratives in\nreal time. This interactive requirement significantly increases complexity,\nespecially in ensuring visual consistency and semantic coherence during prompt\ntransitions. To address these challenges, LongLive adopts a causal, frame-level\nAR design that integrates a KV-recache mechanism that refreshes cached states\nwith new prompts for smooth, adherent switches; streaming long tuning to enable\nlong video training and to align training and inference (train-long-test-long);\nand short window attention paired with a frame-level attention sink, shorten as\nframe sink, preserving long-range consistency while enabling faster generation.\nWith these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model\nto minute-long generation in just 32 GPU-days. At inference, LongLive sustains\n20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both\nshort and long videos. LongLive supports up to 240-second videos on a single\nH100 GPU. LongLive further supports INT8-quantized inference with only marginal\nquality loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LongLive, a frame-level autoregressive (AR) framework for\nreal-time and interactive long video generation. Long video generation presents\nchallenges in both efficiency and quality. Diffusion and Diffusion-Forcing\nmodels can produce high-quality videos but suffer from low efficiency due to\nbidirectional attention. Causal attention AR models support KV caching for\nfaster inference, but often degrade in quality on long videos due to memory\nchallenges during long-video training. In addition, beyond static prompt-based\ngeneration, interactive capabilities, such as streaming prompt inputs, are\ncritical for dynamic content creation, enabling users to guide narratives in\nreal time. This interactive requirement significantly increases complexity,\nespecially in ensuring visual consistency and semantic coherence during prompt\ntransitions. To address these challenges, LongLive adopts a causal, frame-level\nAR design that integrates a KV-recache mechanism that refreshes cached states\nwith new prompts for smooth, adherent switches; streaming long tuning to enable\nlong video training and to align training and inference (train-long-test-long);\nand short window attention paired with a frame-level attention sink, shorten as\nframe sink, preserving long-range consistency while enabling faster generation.\nWith these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model\nto minute-long generation in just 32 GPU-days. At inference, LongLive sustains\n20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both\nshort and long videos. LongLive supports up to 240-second videos on a single\nH100 GPU. LongLive further supports INT8-quantized inference with only marginal\nquality loss."
                },
                "authors": [
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Ruihang Chu"
                    },
                    {
                        "name": "Yicheng Xiao"
                    },
                    {
                        "name": "Yuyang Zhao"
                    },
                    {
                        "name": "Xianbang Wang"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Enze Xie"
                    },
                    {
                        "name": "Yingcong Chen"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Yukang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yukang Chen"
                },
                "author": "Yukang Chen",
                "arxiv_comment": "Code, model, and demos are available at\n  https://github.com/NVlabs/LongLive",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11938v1",
                "updated": "2025-10-13T21:01:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    21,
                    1,
                    40,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T21:01:40Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    21,
                    1,
                    40,
                    0,
                    286,
                    0
                ],
                "title": "FlexPipe: Adapting Dynamic LLM Serving Through Inflight Pipeline\n  Refactoring in Fragmented Serverless Clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexPipe: Adapting Dynamic LLM Serving Through Inflight Pipeline\n  Refactoring in Fragmented Serverless Clusters"
                },
                "summary": "Serving Large Language Models (LLMs) in production faces significant\nchallenges from highly variable request patterns and severe resource\nfragmentation in serverless clusters. Current systems rely on static pipeline\nconfigurations that struggle to adapt to dynamic workload conditions, leading\nto substantial inefficiencies. We present FlexPipe, a novel system that\ndynamically reconfigures pipeline architectures during runtime to address these\nfundamental limitations. FlexPipe decomposes models into fine-grained stages\nand intelligently adjusts pipeline granularity based on real-time request\npattern analysis, implementing three key innovations: fine-grained model\npartitioning with preserved computational graph constraints, inflight pipeline\nrefactoring with consistent cache transitions, and topology-aware resource\nallocation that navigates GPU fragmentation. Comprehensive evaluation on an\n82-GPU cluster demonstrates that FlexPipe achieves up to 8.5x better resource\nefficiency while maintaining 38.3% lower latency compared to state-of-the-art\nsystems, reducing GPU reservation requirements from 75% to 30% of peak\ncapacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models (LLMs) in production faces significant\nchallenges from highly variable request patterns and severe resource\nfragmentation in serverless clusters. Current systems rely on static pipeline\nconfigurations that struggle to adapt to dynamic workload conditions, leading\nto substantial inefficiencies. We present FlexPipe, a novel system that\ndynamically reconfigures pipeline architectures during runtime to address these\nfundamental limitations. FlexPipe decomposes models into fine-grained stages\nand intelligently adjusts pipeline granularity based on real-time request\npattern analysis, implementing three key innovations: fine-grained model\npartitioning with preserved computational graph constraints, inflight pipeline\nrefactoring with consistent cache transitions, and topology-aware resource\nallocation that navigates GPU fragmentation. Comprehensive evaluation on an\n82-GPU cluster demonstrates that FlexPipe achieves up to 8.5x better resource\nefficiency while maintaining 38.3% lower latency compared to state-of-the-art\nsystems, reducing GPU reservation requirements from 75% to 30% of peak\ncapacity."
                },
                "authors": [
                    {
                        "name": "Yanying Lin"
                    },
                    {
                        "name": "Shijie Peng"
                    },
                    {
                        "name": "Chengzhi Lu"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Kejiang Ye"
                    }
                ],
                "author_detail": {
                    "name": "Kejiang Ye"
                },
                "author": "Kejiang Ye",
                "arxiv_doi": "10.1145/3767295.3769316",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3767295.3769316",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.11938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "EuroSys 26",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01875v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01875v3",
                "updated": "2025-10-13T17:15:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    15,
                    14,
                    0,
                    286,
                    0
                ],
                "published": "2025-08-03T18:15:42Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    18,
                    15,
                    42,
                    6,
                    215,
                    0
                ],
                "title": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding"
                },
                "summary": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios."
                },
                "authors": [
                    {
                        "name": "Haolin Yang"
                    },
                    {
                        "name": "Feilong Tang"
                    },
                    {
                        "name": "Lingxiao Zhao"
                    },
                    {
                        "name": "Xiang An"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Huifa Li"
                    },
                    {
                        "name": "Xinlin Zhuang"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Xiaofeng Zhang"
                    },
                    {
                        "name": "Abdalla Swikir"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Zongyuan Ge"
                    },
                    {
                        "name": "Imran Razzak"
                    }
                ],
                "author_detail": {
                    "name": "Imran Razzak"
                },
                "author": "Imran Razzak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01875v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01875v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17238v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17238v2",
                "updated": "2025-10-13T16:48:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    48,
                    37,
                    0,
                    286,
                    0
                ],
                "published": "2025-09-21T21:05:29Z",
                "published_parsed": [
                    2025,
                    9,
                    21,
                    21,
                    5,
                    29,
                    6,
                    264,
                    0
                ],
                "title": "MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with\n  RoE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with\n  RoE"
                },
                "summary": "The generation quality of large language models (LLMs) is often improved by\nutilizing inference-time sequence-level scaling methods (e.g.,\nChain-of-Thought). We introduce hyper-parallel scaling, a complementary\nframework that improves prediction quality at the token level. Hyper-parallel\nscaling computes and aggregates multiple output proposals for a single token\nfrom the model. We implement this concept in Mixture-of-Experts (MoE) models,\nwhich we refer to as Roster of Experts (RoE). RoE is a training-free inference\nalgorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects\ncontrolled stochasticity into the expert routing mechanism, enabling it to\nsample multiple diverse experts for each token and aggregate their outputs for\na more accurate final prediction. To overcome the computational cost, we\nintroduce an efficient batching strategy and a specialized KV-caching mechanism\nthat minimizes compute and memory overhead. For example, RoE enables a 7B MoE\nmodel to match the performance of a 10.5B MoE model while using 30% less\ncompute for inference. These gains are achieved without any fine-tuning of\nmodel parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generation quality of large language models (LLMs) is often improved by\nutilizing inference-time sequence-level scaling methods (e.g.,\nChain-of-Thought). We introduce hyper-parallel scaling, a complementary\nframework that improves prediction quality at the token level. Hyper-parallel\nscaling computes and aggregates multiple output proposals for a single token\nfrom the model. We implement this concept in Mixture-of-Experts (MoE) models,\nwhich we refer to as Roster of Experts (RoE). RoE is a training-free inference\nalgorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects\ncontrolled stochasticity into the expert routing mechanism, enabling it to\nsample multiple diverse experts for each token and aggregate their outputs for\na more accurate final prediction. To overcome the computational cost, we\nintroduce an efficient batching strategy and a specialized KV-caching mechanism\nthat minimizes compute and memory overhead. For example, RoE enables a 7B MoE\nmodel to match the performance of a 10.5B MoE model while using 30% less\ncompute for inference. These gains are achieved without any fine-tuning of\nmodel parameters."
                },
                "authors": [
                    {
                        "name": "Soheil Zibakhsh"
                    },
                    {
                        "name": "Mohammad Samragh"
                    },
                    {
                        "name": "Kumari Nishu"
                    },
                    {
                        "name": "Lauren Hannah"
                    },
                    {
                        "name": "Arnav Kundu"
                    },
                    {
                        "name": "Minsik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Minsik Cho"
                },
                "author": "Minsik Cho",
                "arxiv_comment": "Corrected typo in arxiv abstract",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17238v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17238v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11292v1",
                "updated": "2025-10-13T11:28:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    28,
                    30,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T11:28:30Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    28,
                    30,
                    0,
                    286,
                    0
                ],
                "title": "LouisKV: Efficient KV Cache Retrieval for Long Input-Output Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LouisKV: Efficient KV Cache Retrieval for Long Input-Output Sequences"
                },
                "summary": "While Key-Value (KV) cache succeeds in reducing redundant computations in\nauto-regressive models, it introduces significant memory overhead, limiting its\npractical deployment in long-sequence scenarios. Existing KV retrieval methods\nmitigate this by dynamically retaining only a subset of KV entries on the GPU.\nHowever, they still suffer from notable efficiency and accuracy bottlenecks due\nto per-token retrieval and coarse-grained page-level KV management, especially\nin long-output reasoning scenarios. With the emergence of large reasoning\nmodels, efficiently handling such scenarios has become increasingly important.\nTo address this issue, we present two key observations: (1) critical KVs\nexhibit strong temporal locality during decoding, and (2) these KVs exhibit\ndistinct distribution patterns across the input prompt and generated output.\nBuilding on these observations, we propose LouisKV, an efficient KV cache\nretrieval framework designed for various long-sequence scenarios. Specifically,\nLouisKV introduces a semantic-aware retrieval strategy leveraging temporal\nlocality to trigger retrieval only at semantic boundaries, drastically reducing\ncomputation and data transfer overhead. LouisKV also designs a decoupled,\nfine-grained management scheme that tailors differentiated strategies for input\nand output sequences to create retrieval units that better match the model's\nattention patterns, enabling precise identification of critical KVs.\nFurthermore, to boost efficiency, LouisKV incorporates several kernel-level\noptimizations, including custom Triton and CUDA kernels to accelerate the KV\nclustering and retrieval. Evaluations show that LouisKV achieves up to\n4.7$\\times$ speedup over state-of-the-art KV retrieval methods while\nmaintaining near-lossless accuracy across diverse long-sequence tasks,\nincluding long-input short-output, short-input long-output, and long-input\nlong-output scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Key-Value (KV) cache succeeds in reducing redundant computations in\nauto-regressive models, it introduces significant memory overhead, limiting its\npractical deployment in long-sequence scenarios. Existing KV retrieval methods\nmitigate this by dynamically retaining only a subset of KV entries on the GPU.\nHowever, they still suffer from notable efficiency and accuracy bottlenecks due\nto per-token retrieval and coarse-grained page-level KV management, especially\nin long-output reasoning scenarios. With the emergence of large reasoning\nmodels, efficiently handling such scenarios has become increasingly important.\nTo address this issue, we present two key observations: (1) critical KVs\nexhibit strong temporal locality during decoding, and (2) these KVs exhibit\ndistinct distribution patterns across the input prompt and generated output.\nBuilding on these observations, we propose LouisKV, an efficient KV cache\nretrieval framework designed for various long-sequence scenarios. Specifically,\nLouisKV introduces a semantic-aware retrieval strategy leveraging temporal\nlocality to trigger retrieval only at semantic boundaries, drastically reducing\ncomputation and data transfer overhead. LouisKV also designs a decoupled,\nfine-grained management scheme that tailors differentiated strategies for input\nand output sequences to create retrieval units that better match the model's\nattention patterns, enabling precise identification of critical KVs.\nFurthermore, to boost efficiency, LouisKV incorporates several kernel-level\noptimizations, including custom Triton and CUDA kernels to accelerate the KV\nclustering and retrieval. Evaluations show that LouisKV achieves up to\n4.7$\\times$ speedup over state-of-the-art KV retrieval methods while\nmaintaining near-lossless accuracy across diverse long-sequence tasks,\nincluding long-input short-output, short-input long-output, and long-input\nlong-output scenarios."
                },
                "authors": [
                    {
                        "name": "Wenbo Wu"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Xiurui Pan"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21725v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21725v2",
                "updated": "2025-10-13T11:21:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    21,
                    0,
                    0,
                    286,
                    0
                ],
                "published": "2025-03-27T17:37:12Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    37,
                    12,
                    3,
                    86,
                    0
                ],
                "title": "Low-noise environment for probing fundamental symmetries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-noise environment for probing fundamental symmetries"
                },
                "summary": "We present the design and characterization of a low-noise environment for\nmeasuring the electron's electric dipole moment (EDM) with a beam of molecules.\nTo minimize magnetic Johnson noise from metals, the design features ceramic\nelectric field plates housed in a glass vacuum chamber. To suppress external\nmagnetic noise the apparatus is enclosed within a cylindrical four-layer\nmu-metal shield with a shielding factor exceeding $10^6$ in one radial\ndirection and $10^5$ in the other. Finite element modelling shows that the\ndifference between these shielding factors is due to imperfect joints between\nsections of mu-metal. Using atomic magnetometers to monitor the magnetic field\ninside the shield, we measure noise below 40 fT/$\\sqrt{{\\rm Hz}}$ at 1 Hz and\nabove, rising to 500 fT/$\\sqrt{{\\rm Hz}}$ at 0.1 Hz. Analytical and numerical\nstudies show that residual magnetic Johnson noise contributes approximately 13\nfT/$\\sqrt{{\\rm Hz}}$. The background magnetic field averaged along the beamline\nis maintained below 3 pT, with typical gradients of a few nT/m. An electric\nfield of 20 kV/cm is applied without discharges and with leakage currents below\n1 nA. Each magnetometer measures the magnetic field correlated with the\ndirection of the applied electric field with a precision of 0.11 fT in 104\nhours of data. These results demonstrate that the apparatus is suitable for\nmeasuring the electron EDM with precision at the $10^{-31}$ e cm level. The\ndesign principles and characterization techniques presented here are broadly\napplicable to precision measurements probing fundamental symmetries in\nmolecules, atoms, and neutrons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the design and characterization of a low-noise environment for\nmeasuring the electron's electric dipole moment (EDM) with a beam of molecules.\nTo minimize magnetic Johnson noise from metals, the design features ceramic\nelectric field plates housed in a glass vacuum chamber. To suppress external\nmagnetic noise the apparatus is enclosed within a cylindrical four-layer\nmu-metal shield with a shielding factor exceeding $10^6$ in one radial\ndirection and $10^5$ in the other. Finite element modelling shows that the\ndifference between these shielding factors is due to imperfect joints between\nsections of mu-metal. Using atomic magnetometers to monitor the magnetic field\ninside the shield, we measure noise below 40 fT/$\\sqrt{{\\rm Hz}}$ at 1 Hz and\nabove, rising to 500 fT/$\\sqrt{{\\rm Hz}}$ at 0.1 Hz. Analytical and numerical\nstudies show that residual magnetic Johnson noise contributes approximately 13\nfT/$\\sqrt{{\\rm Hz}}$. The background magnetic field averaged along the beamline\nis maintained below 3 pT, with typical gradients of a few nT/m. An electric\nfield of 20 kV/cm is applied without discharges and with leakage currents below\n1 nA. Each magnetometer measures the magnetic field correlated with the\ndirection of the applied electric field with a precision of 0.11 fT in 104\nhours of data. These results demonstrate that the apparatus is suitable for\nmeasuring the electron EDM with precision at the $10^{-31}$ e cm level. The\ndesign principles and characterization techniques presented here are broadly\napplicable to precision measurements probing fundamental symmetries in\nmolecules, atoms, and neutrons."
                },
                "authors": [
                    {
                        "name": "F. J. Collings"
                    },
                    {
                        "name": "N. J. Fitch"
                    },
                    {
                        "name": "R. A. Jenkins"
                    },
                    {
                        "name": "J. M. Dyne"
                    },
                    {
                        "name": "E. Wursten"
                    },
                    {
                        "name": "M. T. Ziemba"
                    },
                    {
                        "name": "X. S. Zheng"
                    },
                    {
                        "name": "F. Castellini"
                    },
                    {
                        "name": "J. Lim"
                    },
                    {
                        "name": "B. E. Sauer"
                    },
                    {
                        "name": "M. R. Tarbutt"
                    }
                ],
                "author_detail": {
                    "name": "M. R. Tarbutt"
                },
                "author": "M. R. Tarbutt",
                "arxiv_doi": "10.1088/1367-2630/ae0ea7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1088/1367-2630/ae0ea7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.21725v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21725v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Added a figure, minor changes to text",
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19257v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19257v2",
                "updated": "2025-10-13T10:18:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    10,
                    18,
                    34,
                    0,
                    286,
                    0
                ],
                "published": "2025-08-15T12:03:34Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    12,
                    3,
                    34,
                    4,
                    227,
                    0
                ],
                "title": "TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for\n  Vision-Language-Action Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for\n  Vision-Language-Action Models"
                },
                "summary": "Vision-Language-Action (VLA) models process visual inputs independently at\neach timestep, discarding valuable temporal information inherent in robotic\nmanipulation tasks. This frame-by-frame processing makes models vulnerable to\nvisual noise while ignoring the substantial coherence between consecutive\nframes in manipulation sequences. We propose Temporal Token Fusion (TTF), a\ntraining-free approach that intelligently integrates historical and current\nvisual representations to enhance VLA inference quality. Our method employs\ndual-dimension detection combining efficient grayscale pixel difference\nanalysis with attention-based semantic relevance assessment, enabling selective\ntemporal token fusion through hard fusion strategies and keyframe anchoring to\nprevent error accumulation. Comprehensive experiments across LIBERO,\nSimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0\npercentage points average on LIBERO (72.4\\% vs 68.4\\% baseline),\ncross-environment validation on SimplerEnv (4.8\\% relative improvement), and\n8.7\\% relative improvement on real robot tasks. Our approach proves\nmodel-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,\nTTF reveals that selective Query matrix reuse in attention mechanisms enhances\nrather than compromises performance, suggesting promising directions for direct\nKQV matrix reuse strategies that achieve computational acceleration while\nimproving task success rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models process visual inputs independently at\neach timestep, discarding valuable temporal information inherent in robotic\nmanipulation tasks. This frame-by-frame processing makes models vulnerable to\nvisual noise while ignoring the substantial coherence between consecutive\nframes in manipulation sequences. We propose Temporal Token Fusion (TTF), a\ntraining-free approach that intelligently integrates historical and current\nvisual representations to enhance VLA inference quality. Our method employs\ndual-dimension detection combining efficient grayscale pixel difference\nanalysis with attention-based semantic relevance assessment, enabling selective\ntemporal token fusion through hard fusion strategies and keyframe anchoring to\nprevent error accumulation. Comprehensive experiments across LIBERO,\nSimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0\npercentage points average on LIBERO (72.4\\% vs 68.4\\% baseline),\ncross-environment validation on SimplerEnv (4.8\\% relative improvement), and\n8.7\\% relative improvement on real robot tasks. Our approach proves\nmodel-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,\nTTF reveals that selective Query matrix reuse in attention mechanisms enhances\nrather than compromises performance, suggesting promising directions for direct\nKQV matrix reuse strategies that achieve computational acceleration while\nimproving task success rates."
                },
                "authors": [
                    {
                        "name": "Chenghao Liu"
                    },
                    {
                        "name": "Jiachen Zhang"
                    },
                    {
                        "name": "Chengxuan Li"
                    },
                    {
                        "name": "Zhimu Zhou"
                    },
                    {
                        "name": "Shixin Wu"
                    },
                    {
                        "name": "Songfang Huang"
                    },
                    {
                        "name": "Huiling Duan"
                    }
                ],
                "author_detail": {
                    "name": "Huiling Duan"
                },
                "author": "Huiling Duan",
                "arxiv_comment": "Manuscript submitted to AAAI 2026, currently under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19257v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19257v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11236v1",
                "updated": "2025-10-13T10:17:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    10,
                    17,
                    21,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T10:17:21Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    10,
                    17,
                    21,
                    0,
                    286,
                    0
                ],
                "title": "XQuant: Achieving Ultra-Low Bit KV Cache Quantization with Cross-Layer\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XQuant: Achieving Ultra-Low Bit KV Cache Quantization with Cross-Layer\n  Compression"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\ndiverse natural language processing tasks. However, their extensive memory\nrequirements, particularly due to KV cache growth during long-text\nunderstanding and generation, present significant challenges for deployment in\nresource-constrained environments. Quantization has emerged as a promising\nsolution to reduce memory consumption while preserving historical information.\nWe propose XQuant, a training-free and plug-and-play framework that achieves\nultra-low equivalent bit-width KV cache quantization. XQuant introduces two key\ninnovations: a computationally negligible data-free calibration method and\ncross-layer KV cache compression, enabling quantization to sub-1.4 bits.\nExtensive experiments on TruthfulQA and LongBench demonstrate that XQuant\noutperforms state-of-the-art methods (e.g., KIVI-2bit and AsymKV-1.5bit) by\nachieving lower bit-width while maintaining superior performance, establishing\na better trade-off between memory efficiency and model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\ndiverse natural language processing tasks. However, their extensive memory\nrequirements, particularly due to KV cache growth during long-text\nunderstanding and generation, present significant challenges for deployment in\nresource-constrained environments. Quantization has emerged as a promising\nsolution to reduce memory consumption while preserving historical information.\nWe propose XQuant, a training-free and plug-and-play framework that achieves\nultra-low equivalent bit-width KV cache quantization. XQuant introduces two key\ninnovations: a computationally negligible data-free calibration method and\ncross-layer KV cache compression, enabling quantization to sub-1.4 bits.\nExtensive experiments on TruthfulQA and LongBench demonstrate that XQuant\noutperforms state-of-the-art methods (e.g., KIVI-2bit and AsymKV-1.5bit) by\nachieving lower bit-width while maintaining superior performance, establishing\na better trade-off between memory efficiency and model accuracy."
                },
                "authors": [
                    {
                        "name": "Haoqi Yang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Baoyuan Qi"
                    },
                    {
                        "name": "Guoming Liu"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "To be published in The 2025 Conference on Empirical Methods in\n  Natural Language Processing (EMNLP 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24695v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24695v2",
                "updated": "2025-10-13T09:12:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    9,
                    12,
                    27,
                    0,
                    286,
                    0
                ],
                "published": "2025-09-29T12:28:09Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    12,
                    28,
                    9,
                    0,
                    272,
                    0
                ],
                "title": "SANA-Video: Efficient Video Generation with Block Linear Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SANA-Video: Efficient Video Generation with Block Linear Diffusion\n  Transformer"
                },
                "summary": "We introduce SANA-Video, a small diffusion model that can efficiently\ngenerate videos up to 720x1280 resolution and minute-length duration.\nSANA-Video synthesizes high-resolution, high-quality and long videos with\nstrong text-video alignment at a remarkably fast speed, deployable on RTX 5090\nGPU. Two core designs ensure our efficient, effective and long video\ngeneration: (1) Linear DiT: We leverage linear attention as the core operation,\nwhich is more efficient than vanilla attention given the large number of tokens\nprocessed in video generation. (2) Constant-Memory KV cache for Block Linear\nAttention: we design block-wise autoregressive approach for long video\ngeneration by employing a constant-memory state, derived from the cumulative\nproperties of linear attention. This KV cache provides the Linear DiT with\nglobal context at a fixed memory cost, eliminating the need for a traditional\nKV cache and enabling efficient, minute-long video generation. In addition, we\nexplore effective data filters and model training strategies, narrowing the\ntraining cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of\nMovieGen. Given its low cost, SANA-Video achieves competitive performance\ncompared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B\nand SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover,\nSANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating\nthe inference speed of generating a 5-second 720p video from 71s to 29s (2.4x\nspeedup). In summary, SANA-Video enables low-cost, high-quality video\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SANA-Video, a small diffusion model that can efficiently\ngenerate videos up to 720x1280 resolution and minute-length duration.\nSANA-Video synthesizes high-resolution, high-quality and long videos with\nstrong text-video alignment at a remarkably fast speed, deployable on RTX 5090\nGPU. Two core designs ensure our efficient, effective and long video\ngeneration: (1) Linear DiT: We leverage linear attention as the core operation,\nwhich is more efficient than vanilla attention given the large number of tokens\nprocessed in video generation. (2) Constant-Memory KV cache for Block Linear\nAttention: we design block-wise autoregressive approach for long video\ngeneration by employing a constant-memory state, derived from the cumulative\nproperties of linear attention. This KV cache provides the Linear DiT with\nglobal context at a fixed memory cost, eliminating the need for a traditional\nKV cache and enabling efficient, minute-long video generation. In addition, we\nexplore effective data filters and model training strategies, narrowing the\ntraining cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of\nMovieGen. Given its low cost, SANA-Video achieves competitive performance\ncompared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B\nand SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover,\nSANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating\nthe inference speed of generating a 5-second 720p video from 71s to 29s (2.4x\nspeedup). In summary, SANA-Video enables low-cost, high-quality video\ngeneration."
                },
                "authors": [
                    {
                        "name": "Junsong Chen"
                    },
                    {
                        "name": "Yuyang Zhao"
                    },
                    {
                        "name": "Jincheng Yu"
                    },
                    {
                        "name": "Ruihang Chu"
                    },
                    {
                        "name": "Junyu Chen"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Xianbang Wang"
                    },
                    {
                        "name": "Yicheng Pan"
                    },
                    {
                        "name": "Daquan Zhou"
                    },
                    {
                        "name": "Huan Ling"
                    },
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Hongwei Yi"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Han Cai"
                    },
                    {
                        "name": "Sanja Fidler"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "arxiv_comment": "21 pages, 15 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24695v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24695v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15980v1",
                "updated": "2025-10-13T09:04:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    9,
                    4,
                    19,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T09:04:19Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    9,
                    4,
                    19,
                    0,
                    286,
                    0
                ],
                "title": "Cognitive Load Traces as Symbolic and Visual Accounts of Deep Model\n  Cognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Load Traces as Symbolic and Visual Accounts of Deep Model\n  Cognition"
                },
                "summary": "We propose \\textbf{Cognitive Load Traces} (CLTs) as a mid-level\ninterpretability framework for deep models, inspired by Cognitive Load Theory\nin human cognition. CLTs are defined as symbolic, temporally varying functions\nthat quantify model-internal resource allocation. Formally, we represent CLTs\nas a three-component stochastic process $(\\mathrm{IL}_t, \\mathrm{EL}_t,\n\\mathrm{GL}_t)$, corresponding to \\emph{Intrinsic}, \\emph{Extraneous}, and\n\\emph{Germane} load. Each component is instantiated through measurable proxies\nsuch as attention entropy, KV-cache miss ratio, representation dispersion, and\ndecoding stability. We propose both symbolic formulations and visualization\nmethods (load curves, simplex diagrams) that enable interpretable analysis of\nreasoning dynamics. Experiments on reasoning and planning benchmarks show that\nCLTs predict error-onset, reveal cognitive strategies, and enable load-guided\ninterventions that improve reasoning efficiency by 15-30\\% while maintaining\naccuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose \\textbf{Cognitive Load Traces} (CLTs) as a mid-level\ninterpretability framework for deep models, inspired by Cognitive Load Theory\nin human cognition. CLTs are defined as symbolic, temporally varying functions\nthat quantify model-internal resource allocation. Formally, we represent CLTs\nas a three-component stochastic process $(\\mathrm{IL}_t, \\mathrm{EL}_t,\n\\mathrm{GL}_t)$, corresponding to \\emph{Intrinsic}, \\emph{Extraneous}, and\n\\emph{Germane} load. Each component is instantiated through measurable proxies\nsuch as attention entropy, KV-cache miss ratio, representation dispersion, and\ndecoding stability. We propose both symbolic formulations and visualization\nmethods (load curves, simplex diagrams) that enable interpretable analysis of\nreasoning dynamics. Experiments on reasoning and planning benchmarks show that\nCLTs predict error-onset, reveal cognitive strategies, and enable load-guided\ninterventions that improve reasoning efficiency by 15-30\\% while maintaining\naccuracy."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Yanxuan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yanxuan Yu"
                },
                "author": "Yanxuan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11121v1",
                "updated": "2025-10-13T08:08:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    8,
                    8,
                    58,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T08:08:58Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    8,
                    8,
                    58,
                    0,
                    286,
                    0
                ],
                "title": "Refining Hybrid Genetic Search for CVRP via Reinforcement\n  Learning-Finetuned LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refining Hybrid Genetic Search for CVRP via Reinforcement\n  Learning-Finetuned LLM"
                },
                "summary": "While large language models (LLMs) are increasingly used as automated\nheuristic designers for vehicle routing problems (VRPs), current\nstate-of-the-art methods predominantly rely on prompting massive,\ngeneral-purpose models like GPT-4. This work challenges that paradigm by\ndemonstrating that a smaller, specialized LLM, when meticulously fine-tuned,\ncan generate components that surpass expert-crafted heuristics within advanced\nsolvers. We propose RFTHGS, a novel Reinforcement learning (RL) framework for\nFine-Tuning a small LLM to generate high-performance crossover operators for\nthe Hybrid Genetic Search (HGS) solver, applied to the Capacitated VRP (CVRP).\nOur method employs a multi-tiered, curriculum-based reward function that\nprogressively guides the LLM to master generating first compilable, then\nexecutable, and finally, superior-performing operators that exceed human expert\ndesigns. This is coupled with an operator caching mechanism that discourages\nplagiarism and promotes diversity during training. Comprehensive experiments\nshow that our fine-tuned LLM produces crossover operators which significantly\noutperform the expert-designed ones in HGS. The performance advantage remains\nconsistent, generalizing from small-scale instances to large-scale problems\nwith up to 1000 nodes. Furthermore, RFTHGS exceeds the performance of leading\nneuro-combinatorial baselines, prompt-based methods, and commercial LLMs such\nas GPT-4o and GPT-4o-mini.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) are increasingly used as automated\nheuristic designers for vehicle routing problems (VRPs), current\nstate-of-the-art methods predominantly rely on prompting massive,\ngeneral-purpose models like GPT-4. This work challenges that paradigm by\ndemonstrating that a smaller, specialized LLM, when meticulously fine-tuned,\ncan generate components that surpass expert-crafted heuristics within advanced\nsolvers. We propose RFTHGS, a novel Reinforcement learning (RL) framework for\nFine-Tuning a small LLM to generate high-performance crossover operators for\nthe Hybrid Genetic Search (HGS) solver, applied to the Capacitated VRP (CVRP).\nOur method employs a multi-tiered, curriculum-based reward function that\nprogressively guides the LLM to master generating first compilable, then\nexecutable, and finally, superior-performing operators that exceed human expert\ndesigns. This is coupled with an operator caching mechanism that discourages\nplagiarism and promotes diversity during training. Comprehensive experiments\nshow that our fine-tuned LLM produces crossover operators which significantly\noutperform the expert-designed ones in HGS. The performance advantage remains\nconsistent, generalizing from small-scale instances to large-scale problems\nwith up to 1000 nodes. Furthermore, RFTHGS exceeds the performance of leading\nneuro-combinatorial baselines, prompt-based methods, and commercial LLMs such\nas GPT-4o and GPT-4o-mini."
                },
                "authors": [
                    {
                        "name": "Rongjie Zhu"
                    },
                    {
                        "name": "Cong Zhang"
                    },
                    {
                        "name": "Zhiguang Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zhiguang Cao"
                },
                "author": "Zhiguang Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11011v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11011v1",
                "updated": "2025-10-13T05:03:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    5,
                    3,
                    23,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T05:03:23Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    5,
                    3,
                    23,
                    0,
                    286,
                    0
                ],
                "title": "GrASP: A Generalizable Address-based Semantic Prefetcher for Scalable\n  Transactional and Analytical Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GrASP: A Generalizable Address-based Semantic Prefetcher for Scalable\n  Transactional and Analytical Workloads"
                },
                "summary": "Data prefetching--loading data into the cache before it is requested--is\nessential for reducing I/O overhead and improving database performance. While\ntraditional prefetchers focus on sequential patterns, recent learning-based\napproaches, especially those leveraging data semantics, achieve higher accuracy\nfor complex access patterns. However, these methods often struggle with today's\ndynamic, ever-growing datasets and require frequent, timely fine-tuning.\nPrivacy constraints may also restrict access to complete datasets,\nnecessitating prefetchers that can learn effectively from samples. To address\nthese challenges, we present GrASP, a learning-based prefetcher designed for\nboth analytical and transactional workloads. GrASP enhances prefetching\naccuracy and scalability by leveraging logical block address deltas and\ncombining query representations with result encodings. It frames prefetching as\na context-aware multi-label classification task, using multi-layer LSTMs to\npredict delta patterns from embedded context. This delta modeling approach\nenables GrASP to generalize predictions from small samples to larger, dynamic\ndatasets without requiring extensive retraining. Experiments on real-world\ndatasets and industrial benchmarks demonstrate that GrASP generalizes to\ndatasets 250 times larger than the training data, achieving up to 45% higher\nhit ratios, 60% lower I/O time, and 55% lower end-to-end query execution\nlatency than existing baselines. On average, GrASP attains a 91.4% hit ratio, a\n90.8% I/O time reduction, and a 57.1% execution latency reduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data prefetching--loading data into the cache before it is requested--is\nessential for reducing I/O overhead and improving database performance. While\ntraditional prefetchers focus on sequential patterns, recent learning-based\napproaches, especially those leveraging data semantics, achieve higher accuracy\nfor complex access patterns. However, these methods often struggle with today's\ndynamic, ever-growing datasets and require frequent, timely fine-tuning.\nPrivacy constraints may also restrict access to complete datasets,\nnecessitating prefetchers that can learn effectively from samples. To address\nthese challenges, we present GrASP, a learning-based prefetcher designed for\nboth analytical and transactional workloads. GrASP enhances prefetching\naccuracy and scalability by leveraging logical block address deltas and\ncombining query representations with result encodings. It frames prefetching as\na context-aware multi-label classification task, using multi-layer LSTMs to\npredict delta patterns from embedded context. This delta modeling approach\nenables GrASP to generalize predictions from small samples to larger, dynamic\ndatasets without requiring extensive retraining. Experiments on real-world\ndatasets and industrial benchmarks demonstrate that GrASP generalizes to\ndatasets 250 times larger than the training data, achieving up to 45% higher\nhit ratios, 60% lower I/O time, and 55% lower end-to-end query execution\nlatency than existing baselines. On average, GrASP attains a 91.4% hit ratio, a\n90.8% I/O time reduction, and a 57.1% execution latency reduction."
                },
                "authors": [
                    {
                        "name": "Farzaneh Zirak"
                    },
                    {
                        "name": "Farhana Choudhury"
                    },
                    {
                        "name": "Renata Borovica-Gajic"
                    }
                ],
                "author_detail": {
                    "name": "Renata Borovica-Gajic"
                },
                "author": "Renata Borovica-Gajic",
                "arxiv_comment": "This is a preprint version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11011v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11011v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13860v1",
                "updated": "2025-10-13T04:04:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    4,
                    4,
                    54,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T04:04:54Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    4,
                    4,
                    54,
                    0,
                    286,
                    0
                ],
                "title": "ShishuLM: Lightweight Language Model with Hybrid Decoder-MLP\n  Architecture and Paired Weight Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShishuLM: Lightweight Language Model with Hybrid Decoder-MLP\n  Architecture and Paired Weight Sharing"
                },
                "summary": "While the transformer architecture has achieved state-of-the-art performance\non natural language processing tasks, these models impose substantial memory\nand computational overhead. Recent research has identified significant\narchitectural redundancies within these models, presenting opportunities for\noptimization without compromising performance. Taking insights from research in\nAI interpretability and inference-time layer pruning, we introduce an efficient\nlanguage model architecture, referred to as ShishuLM, which reduces both the\nparameter count and Key-Value (KV) cache requirements. Given the increasing\nimportance of Small Language Models (SLMs) in agentic AI systems, we evaluate\nour approach on two SLMs of different scales. Our analysis reveals that for\nmoderate-context scenarios, normalization coupled with attention computation is\nroughly linear with the input, enabling entire transformer blocks to be\napproximated through Multi-Layer Perceptrons (MLPs). Our results show that\nShishuLM provides up to 25% reduction in memory requirements and up to 40%\nimprovement in latency during both training and inference, compared to parent\nmodels. Our experimental and analytical findings provide insights towards\nbuilding more efficient SLM architectures from a pre-training standpoint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the transformer architecture has achieved state-of-the-art performance\non natural language processing tasks, these models impose substantial memory\nand computational overhead. Recent research has identified significant\narchitectural redundancies within these models, presenting opportunities for\noptimization without compromising performance. Taking insights from research in\nAI interpretability and inference-time layer pruning, we introduce an efficient\nlanguage model architecture, referred to as ShishuLM, which reduces both the\nparameter count and Key-Value (KV) cache requirements. Given the increasing\nimportance of Small Language Models (SLMs) in agentic AI systems, we evaluate\nour approach on two SLMs of different scales. Our analysis reveals that for\nmoderate-context scenarios, normalization coupled with attention computation is\nroughly linear with the input, enabling entire transformer blocks to be\napproximated through Multi-Layer Perceptrons (MLPs). Our results show that\nShishuLM provides up to 25% reduction in memory requirements and up to 40%\nimprovement in latency during both training and inference, compared to parent\nmodels. Our experimental and analytical findings provide insights towards\nbuilding more efficient SLM architectures from a pre-training standpoint."
                },
                "authors": [
                    {
                        "name": "Shivanshu Kumar"
                    },
                    {
                        "name": "Gopalakrishnan Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Gopalakrishnan Srinivasan"
                },
                "author": "Gopalakrishnan Srinivasan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10964v1",
                "updated": "2025-10-13T03:14:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    3,
                    14,
                    28,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T03:14:28Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    3,
                    14,
                    28,
                    0,
                    286,
                    0
                ],
                "title": "Not All Bits Are Equal: Scale-Dependent Memory Optimization Strategies\n  for Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Bits Are Equal: Scale-Dependent Memory Optimization Strategies\n  for Reasoning Models"
                },
                "summary": "While 4-bit quantization has emerged as a memory-optimal choice for\nnon-reasoning models and zero-shot tasks across scales, we show that this\nuniversal prescription fails for reasoning models, where the KV cache rather\nthan model size can dominate memory. Through systematic experiments across\n1,700 inference scenarios on AIME25 and GPQA-Diamond, we find a scale-dependent\ntrade-off: models with an effective size below 8-bit 4B parameters achieve\nbetter accuracy by allocating memory to more weights rather than longer\ngeneration, while larger models achieve better accuracy by allocating memory to\nlonger generations. This scale threshold also determines when parallel scaling\nbecomes memory-efficient and whether KV cache eviction outperforms KV\nquantization. Our findings show that memory optimization for LLMs cannot be\nscale-agnostic, while providing principled guidelines: for small reasoning\nmodels, prioritize model capacity over test-time compute, while for larger\nones, maximize test-time compute. Our results suggest that optimizing reasoning\nmodels for deployment requires fundamentally different strategies from those\nestablished for non-reasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While 4-bit quantization has emerged as a memory-optimal choice for\nnon-reasoning models and zero-shot tasks across scales, we show that this\nuniversal prescription fails for reasoning models, where the KV cache rather\nthan model size can dominate memory. Through systematic experiments across\n1,700 inference scenarios on AIME25 and GPQA-Diamond, we find a scale-dependent\ntrade-off: models with an effective size below 8-bit 4B parameters achieve\nbetter accuracy by allocating memory to more weights rather than longer\ngeneration, while larger models achieve better accuracy by allocating memory to\nlonger generations. This scale threshold also determines when parallel scaling\nbecomes memory-efficient and whether KV cache eviction outperforms KV\nquantization. Our findings show that memory optimization for LLMs cannot be\nscale-agnostic, while providing principled guidelines: for small reasoning\nmodels, prioritize model capacity over test-time compute, while for larger\nones, maximize test-time compute. Our results suggest that optimizing reasoning\nmodels for deployment requires fundamentally different strategies from those\nestablished for non-reasoning models."
                },
                "authors": [
                    {
                        "name": "Junhyuck Kim"
                    },
                    {
                        "name": "Ethan Ewer"
                    },
                    {
                        "name": "Taehong Moon"
                    },
                    {
                        "name": "Jongho Park"
                    },
                    {
                        "name": "Dimitris Papailiopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris Papailiopoulos"
                },
                "author": "Dimitris Papailiopoulos",
                "arxiv_comment": "20 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10862v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10862v1",
                "updated": "2025-10-13T00:11:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    0,
                    11,
                    2,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T00:11:02Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    0,
                    11,
                    2,
                    0,
                    286,
                    0
                ],
                "title": "A Joint Learning Approach to Hardware Caching and Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Joint Learning Approach to Hardware Caching and Prefetching"
                },
                "summary": "Several learned policies have been proposed to replace heuristics for\nscheduling, caching, and other system components in modern systems. By\nleveraging diverse features, learning from historical trends, and predicting\nfuture behaviors, such models promise to keep pace with ever-increasing\nworkload dynamism and continuous hardware evolution. However, policies trained\nin isolation may still achieve suboptimal performance when placed together. In\nthis paper, we inspect one such instance in the domain of hardware caching --\nfor the policies of cache replacement and prefetching. We argue that these two\npolicies are bidirectionally interdependent and make the case for training the\ntwo jointly. We propose a joint learning approach based on developing shared\nrepresentations for the features used by the two policies. We present two\napproaches to develop these shared representations, one based on a joint\nencoder and another based on contrastive learning of the embeddings, and\ndemonstrate promising preliminary results for both of these. Finally, we lay\ndown an agenda for future research in this direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several learned policies have been proposed to replace heuristics for\nscheduling, caching, and other system components in modern systems. By\nleveraging diverse features, learning from historical trends, and predicting\nfuture behaviors, such models promise to keep pace with ever-increasing\nworkload dynamism and continuous hardware evolution. However, policies trained\nin isolation may still achieve suboptimal performance when placed together. In\nthis paper, we inspect one such instance in the domain of hardware caching --\nfor the policies of cache replacement and prefetching. We argue that these two\npolicies are bidirectionally interdependent and make the case for training the\ntwo jointly. We propose a joint learning approach based on developing shared\nrepresentations for the features used by the two policies. We present two\napproaches to develop these shared representations, one based on a joint\nencoder and another based on contrastive learning of the embeddings, and\ndemonstrate promising preliminary results for both of these. Finally, we lay\ndown an agenda for future research in this direction."
                },
                "authors": [
                    {
                        "name": "Samuel Yuan"
                    },
                    {
                        "name": "Divyanshu Saxena"
                    },
                    {
                        "name": "Jiayi Chen"
                    },
                    {
                        "name": "Nihal Sharma"
                    },
                    {
                        "name": "Aditya Akella"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Akella"
                },
                "author": "Aditya Akella",
                "arxiv_comment": "Accepted at ML for Systems Workshop at the 39th Conference on Neural\n  Information Processing Systems (NeurIPS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10862v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10858v1",
                "updated": "2025-10-12T23:46:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    23,
                    46,
                    4,
                    6,
                    285,
                    0
                ],
                "published": "2025-10-12T23:46:04Z",
                "published_parsed": [
                    2025,
                    10,
                    12,
                    23,
                    46,
                    4,
                    6,
                    285,
                    0
                ],
                "title": "DriftBench: Defining and Generating Data and Query Workload Drift for\n  Benchmarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DriftBench: Defining and Generating Data and Query Workload Drift for\n  Benchmarking"
                },
                "summary": "Data and workload drift are key to evaluating database components such as\ncaching, cardinality estimation, indexing, and query optimization. Yet,\nexisting benchmarks are static, offering little to no support for modeling\ndrift. This limitation stems from the lack of clear definitions and tools for\ngenerating data and workload drift. Motivated by this gap, we propose a unified\ntaxonomy for data and workload drift, grounded in observations from both\nacademia and industry. Building on this foundation, we introduce DriftBench, a\nlightweight and extensible framework for generating data and workload drift in\nbenchmark inputs. Together, the taxonomy and DriftBench provide a standardized\nvocabulary and mechanism for modeling and generating drift in benchmarking. We\ndemonstrate their effectiveness through case studies involving data drift,\nworkload drift, and drift-aware cardinality estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data and workload drift are key to evaluating database components such as\ncaching, cardinality estimation, indexing, and query optimization. Yet,\nexisting benchmarks are static, offering little to no support for modeling\ndrift. This limitation stems from the lack of clear definitions and tools for\ngenerating data and workload drift. Motivated by this gap, we propose a unified\ntaxonomy for data and workload drift, grounded in observations from both\nacademia and industry. Building on this foundation, we introduce DriftBench, a\nlightweight and extensible framework for generating data and workload drift in\nbenchmark inputs. Together, the taxonomy and DriftBench provide a standardized\nvocabulary and mechanism for modeling and generating drift in benchmarking. We\ndemonstrate their effectiveness through case studies involving data drift,\nworkload drift, and drift-aware cardinality estimation."
                },
                "authors": [
                    {
                        "name": "Guanli Liu"
                    },
                    {
                        "name": "Renata Borovica-Gajic"
                    }
                ],
                "author_detail": {
                    "name": "Renata Borovica-Gajic"
                },
                "author": "Renata Borovica-Gajic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00313v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00313v2",
                "updated": "2025-10-12T23:17:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    23,
                    17,
                    39,
                    6,
                    285,
                    0
                ],
                "published": "2024-05-01T04:30:03Z",
                "published_parsed": [
                    2024,
                    5,
                    1,
                    4,
                    30,
                    3,
                    2,
                    122,
                    0
                ],
                "title": "Streamlining Image Editing with Layered Diffusion Brushes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streamlining Image Editing with Layered Diffusion Brushes"
                },
                "summary": "Denoising diffusion models have emerged as powerful tools for image\nmanipulation, yet interactive, localized editing workflows remain\nunderdeveloped. We introduce Layered Diffusion Brushes (LDB), a novel\ntraining-free framework that enables interactive, layer-based editing using\nstandard diffusion models. LDB defines each \"layer\" as a self-contained set of\nparameters guiding the generative process, enabling independent,\nnon-destructive, and fine-grained prompt-guided edits, even in overlapping\nregions. LDB leverages a unique intermediate latent caching approach to reduce\neach edit to only a few denoising steps, achieving 140~ms per edit on consumer\nGPUs. An editor implementing LDB, incorporating familiar layer concepts, was\nevaluated via user study and quantitative metrics. Results demonstrate LDB's\nsuperior speed alongside comparable or improved image quality, background\npreservation, and edit fidelity relative to state-of-the-art methods across\nvarious sequential image manipulation tasks. The findings highlight LDB's\nability to significantly enhance creative workflows by providing an intuitive\nand efficient approach to diffusion-based image editing and its potential for\nexpansion into related subdomains, such as video editing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Denoising diffusion models have emerged as powerful tools for image\nmanipulation, yet interactive, localized editing workflows remain\nunderdeveloped. We introduce Layered Diffusion Brushes (LDB), a novel\ntraining-free framework that enables interactive, layer-based editing using\nstandard diffusion models. LDB defines each \"layer\" as a self-contained set of\nparameters guiding the generative process, enabling independent,\nnon-destructive, and fine-grained prompt-guided edits, even in overlapping\nregions. LDB leverages a unique intermediate latent caching approach to reduce\neach edit to only a few denoising steps, achieving 140~ms per edit on consumer\nGPUs. An editor implementing LDB, incorporating familiar layer concepts, was\nevaluated via user study and quantitative metrics. Results demonstrate LDB's\nsuperior speed alongside comparable or improved image quality, background\npreservation, and edit fidelity relative to state-of-the-art methods across\nvarious sequential image manipulation tasks. The findings highlight LDB's\nability to significantly enhance creative workflows by providing an intuitive\nand efficient approach to diffusion-based image editing and its potential for\nexpansion into related subdomains, such as video editing."
                },
                "authors": [
                    {
                        "name": "Peyman Gholami"
                    },
                    {
                        "name": "Robert Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Robert Xiao"
                },
                "author": "Robert Xiao",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2306.00219",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00313v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00313v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10587v1",
                "updated": "2025-10-12T13:06:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    13,
                    6,
                    59,
                    6,
                    285,
                    0
                ],
                "published": "2025-10-12T13:06:59Z",
                "published_parsed": [
                    2025,
                    10,
                    12,
                    13,
                    6,
                    59,
                    6,
                    285,
                    0
                ],
                "title": "A Simple and Better Baseline for Visual Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Better Baseline for Visual Grounding"
                },
                "summary": "Visual grounding aims to predict the locations of target objects specified by\ntextual descriptions. For this task with linguistic and visual modalities,\nthere is a latest research line that focuses on only selecting the\nlinguistic-relevant visual regions for object localization to reduce the\ncomputational overhead. Albeit achieving impressive performance, it is\niteratively performed on different image scales, and at every iteration,\nlinguistic features and visual features need to be stored in a cache, incurring\nextra overhead. To facilitate the implementation, in this paper, we propose a\nfeature selection-based simple yet effective baseline for visual grounding,\ncalled FSVG. Specifically, we directly encapsulate the linguistic and visual\nmodalities into an overall network architecture without complicated iterative\nprocedures, and utilize the language in parallel as guidance to facilitate the\ninteraction between linguistic modal and visual modal for extracting effective\nvisual features. Furthermore, to reduce the computational cost, during the\nvisual feature learning, we introduce a similarity-based feature selection\nmechanism to only exploit language-related visual features for faster\nprediction. Extensive experiments conducted on several benchmark datasets\ncomprehensively substantiate that the proposed FSVG achieves a better balance\nbetween accuracy and efficiency beyond the current state-of-the-art methods.\nCode is available at https://github.com/jcwang0602/FSVG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual grounding aims to predict the locations of target objects specified by\ntextual descriptions. For this task with linguistic and visual modalities,\nthere is a latest research line that focuses on only selecting the\nlinguistic-relevant visual regions for object localization to reduce the\ncomputational overhead. Albeit achieving impressive performance, it is\niteratively performed on different image scales, and at every iteration,\nlinguistic features and visual features need to be stored in a cache, incurring\nextra overhead. To facilitate the implementation, in this paper, we propose a\nfeature selection-based simple yet effective baseline for visual grounding,\ncalled FSVG. Specifically, we directly encapsulate the linguistic and visual\nmodalities into an overall network architecture without complicated iterative\nprocedures, and utilize the language in parallel as guidance to facilitate the\ninteraction between linguistic modal and visual modal for extracting effective\nvisual features. Furthermore, to reduce the computational cost, during the\nvisual feature learning, we introduce a similarity-based feature selection\nmechanism to only exploit language-related visual features for faster\nprediction. Extensive experiments conducted on several benchmark datasets\ncomprehensively substantiate that the proposed FSVG achieves a better balance\nbetween accuracy and efficiency beyond the current state-of-the-art methods.\nCode is available at https://github.com/jcwang0602/FSVG."
                },
                "authors": [
                    {
                        "name": "Jingchao Wang"
                    },
                    {
                        "name": "Wenlong Zhang"
                    },
                    {
                        "name": "Dingjiang Huang"
                    },
                    {
                        "name": "Hong Wang"
                    },
                    {
                        "name": "Yefeng Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Yefeng Zheng"
                },
                "author": "Yefeng Zheng",
                "arxiv_comment": "ICME2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18809v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18809v2",
                "updated": "2025-10-12T10:09:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    10,
                    9,
                    53,
                    6,
                    285,
                    0
                ],
                "published": "2025-05-24T17:46:47Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    17,
                    46,
                    47,
                    5,
                    144,
                    0
                ],
                "title": "VORTA: Efficient Video Diffusion via Routing Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VORTA: Efficient Video Diffusion via Routing Sparse Attention"
                },
                "summary": "Video diffusion transformers have achieved remarkable progress in\nhigh-quality video generation, but remain computationally expensive due to the\nquadratic complexity of attention over high-dimensional video sequences. Recent\nacceleration methods enhance the efficiency by exploiting the local sparsity of\nattention scores; yet they often struggle with accelerating the long-range\ncomputation. To address this problem, we propose VORTA, an acceleration\nframework with two novel components: 1) a sparse attention mechanism that\nefficiently captures long-range dependencies, and 2) a routing strategy that\nadaptively replaces full 3D attention with specialized sparse attention\nvariants. VORTA achieves an end-to-end speedup $1.76\\times$ without loss of\nquality on VBench. Furthermore, it can seamlessly integrate with various other\nacceleration methods, such as model caching and step distillation, reaching up\nto speedup $14.41\\times$ with negligible performance degradation. VORTA\ndemonstrates its efficiency and enhances the practicality of video diffusion\ntransformers in real-world settings. Codes and weights are available at\nhttps://github.com/wenhao728/VORTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video diffusion transformers have achieved remarkable progress in\nhigh-quality video generation, but remain computationally expensive due to the\nquadratic complexity of attention over high-dimensional video sequences. Recent\nacceleration methods enhance the efficiency by exploiting the local sparsity of\nattention scores; yet they often struggle with accelerating the long-range\ncomputation. To address this problem, we propose VORTA, an acceleration\nframework with two novel components: 1) a sparse attention mechanism that\nefficiently captures long-range dependencies, and 2) a routing strategy that\nadaptively replaces full 3D attention with specialized sparse attention\nvariants. VORTA achieves an end-to-end speedup $1.76\\times$ without loss of\nquality on VBench. Furthermore, it can seamlessly integrate with various other\nacceleration methods, such as model caching and step distillation, reaching up\nto speedup $14.41\\times$ with negligible performance degradation. VORTA\ndemonstrates its efficiency and enhances the practicality of video diffusion\ntransformers in real-world settings. Codes and weights are available at\nhttps://github.com/wenhao728/VORTA."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Yifu Ding"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "Accepted by NeurIPS 2025. The code is available at\n  https://github.com/wenhao728/VORTA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18809v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18809v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01092v2",
                "updated": "2025-10-12T04:46:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    4,
                    46,
                    48,
                    6,
                    285,
                    0
                ],
                "published": "2025-09-01T03:31:44Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    31,
                    44,
                    0,
                    244,
                    0
                ],
                "title": "REFRAG: Rethinking RAG based Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REFRAG: Rethinking RAG based Decoding"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes."
                },
                "authors": [
                    {
                        "name": "Xiaoqiang Lin"
                    },
                    {
                        "name": "Aritra Ghosh"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    },
                    {
                        "name": "Anshumali Shrivastava"
                    },
                    {
                        "name": "Vijai Mohan"
                    }
                ],
                "author_detail": {
                    "name": "Vijai Mohan"
                },
                "author": "Vijai Mohan",
                "arxiv_comment": "fix typo perplexity->log perplexity; added recent papers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19274v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19274v3",
                "updated": "2025-10-12T04:04:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    4,
                    4,
                    34,
                    6,
                    285,
                    0
                ],
                "published": "2024-10-25T03:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    3,
                    1,
                    19,
                    4,
                    299,
                    0
                ],
                "title": "Neuralink: Fast LLM Inference on Smartphones with Neuron Co-Activation\n  Linking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuralink: Fast LLM Inference on Smartphones with Neuron Co-Activation\n  Linking"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Neuralink, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory.\nNeuralink leverages the concept of Neuron Co-Activation, where neurons\nfrequently activated together are linked to facilitate continuous read access\nand optimize I/O efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Neuralink achieves on\naverage $1.49\\times$ improvements in end-to-end latency compared to the\nstate-of-the-art. As the first solution to optimize storage placement under\nsparsity, Neuralink explores a new optimization space at the intersection of\nsparsity-driven algorithm and storage-level system co-design for LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Neuralink, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory.\nNeuralink leverages the concept of Neuron Co-Activation, where neurons\nfrequently activated together are linked to facilitate continuous read access\nand optimize I/O efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Neuralink achieves on\naverage $1.49\\times$ improvements in end-to-end latency compared to the\nstate-of-the-art. As the first solution to optimize storage placement under\nsparsity, Neuralink explores a new optimization space at the intersection of\nsparsity-driven algorithm and storage-level system co-design for LLM inference."
                },
                "authors": [
                    {
                        "name": "Tuowei Wang"
                    },
                    {
                        "name": "Ruwen Fan"
                    },
                    {
                        "name": "Minxing Huang"
                    },
                    {
                        "name": "Zixu Hao"
                    },
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Youyou Lu"
                    },
                    {
                        "name": "Yaoxue Zhang"
                    },
                    {
                        "name": "Ju Ren"
                    }
                ],
                "author_detail": {
                    "name": "Ju Ren"
                },
                "author": "Ju Ren",
                "arxiv_doi": "10.1145/3676642.3736114",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676642.3736114",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.19274v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19274v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, Vol.\n  3, Rotterdam, Netherlands, 2025, pp. 147-162",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10290v1",
                "updated": "2025-10-11T17:08:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    17,
                    8,
                    45,
                    5,
                    284,
                    0
                ],
                "published": "2025-10-11T17:08:45Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    17,
                    8,
                    45,
                    5,
                    284,
                    0
                ],
                "title": "Grounded AI for Code Review: Resource-Efficient Large-Model Serving in\n  Enterprise Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounded AI for Code Review: Resource-Efficient Large-Model Serving in\n  Enterprise Pipelines"
                },
                "summary": "Automated code review adoption lags in compliance-heavy settings, where\nstatic analyzers produce high-volume, low-rationale outputs, and naive LLM use\nrisks hallucination and incurring cost overhead. We present a production system\nfor grounded, PR-native review that pairs static-analysis findings with\nAST-guided context extraction and a single-GPU, on-demand serving stack\n(quantized open-weight model, multi-tier caching) to deliver concise\nexplanations and remediation guidance. Evaluated on safety-oriented C/C++\nstandards, the approach achieves sub-minute median first-feedback (offline p50\nbuild+LLM 59.8s) while maintaining competitive violation reduction and lower\nviolation rates versus larger proprietary models. The architecture is\ndecoupled: teams can adopt the grounding/prompting layer or the serving layer\nindependently. A small internal survey (n=8) provides directional signals of\nreduced triage effort and moderate perceived grounding, with participants\nreporting fewer human review iterations. We outline operational lessons and\nlimitations, emphasizing reproducibility, auditability, and pathways to broader\nstandards and assisted patching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated code review adoption lags in compliance-heavy settings, where\nstatic analyzers produce high-volume, low-rationale outputs, and naive LLM use\nrisks hallucination and incurring cost overhead. We present a production system\nfor grounded, PR-native review that pairs static-analysis findings with\nAST-guided context extraction and a single-GPU, on-demand serving stack\n(quantized open-weight model, multi-tier caching) to deliver concise\nexplanations and remediation guidance. Evaluated on safety-oriented C/C++\nstandards, the approach achieves sub-minute median first-feedback (offline p50\nbuild+LLM 59.8s) while maintaining competitive violation reduction and lower\nviolation rates versus larger proprietary models. The architecture is\ndecoupled: teams can adopt the grounding/prompting layer or the serving layer\nindependently. A small internal survey (n=8) provides directional signals of\nreduced triage effort and moderate perceived grounding, with participants\nreporting fewer human review iterations. We outline operational lessons and\nlimitations, emphasizing reproducibility, auditability, and pathways to broader\nstandards and assisted patching."
                },
                "authors": [
                    {
                        "name": "Sayan Mandal"
                    },
                    {
                        "name": "Hua Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hua Jiang"
                },
                "author": "Hua Jiang",
                "arxiv_comment": "Submitted to MLSys 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10219v1",
                "updated": "2025-10-11T13:52:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    13,
                    52,
                    48,
                    5,
                    284,
                    0
                ],
                "published": "2025-10-11T13:52:48Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    13,
                    52,
                    48,
                    5,
                    284,
                    0
                ],
                "title": "Old is Gold: Optimizing Single-threaded Applications with Exgen-Malloc",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Old is Gold: Optimizing Single-threaded Applications with Exgen-Malloc"
                },
                "summary": "Memory allocators hide beneath nearly every application stack, yet their\nperformance footprint extends far beyond their code size. Even small\ninefficiencies in the allocators ripple through caches and the rest of the\nmemory hierarchy, collectively imposing what operators often call a \"datacenter\ntax\". At hyperscale, even a 1% improvement in allocator efficiency can unlock\nmillions of dollars in savings and measurable reductions in datacenter energy\nconsumption. Modern memory allocators are designed to optimize allocation speed\nand memory fragmentation in multi-threaded environments, relying on complex\nmetadata and control logic to achieve high performance. However, the overhead\nintroduced by this complexity prompts a reevaluation of allocator design.\nNotably, such overhead can be avoided in single-threaded scenarios, which\ncontinue to be widely used across diverse application domains.\n  In this paper, we introduce Exgen-Malloc, a memory allocator purpose-built\nfor single-threaded applications. By specializing for single-threaded\nexecution, Exgen-Malloc eliminates unnecessary metadata, simplifies the control\nflow, thereby reducing overhead and improving allocation efficiency. Its core\ndesign features include a centralized heap, a single free-block list, and a\nbalanced strategy for memory commitment and relocation. Additionally,\nExgen-Malloc incorporates design principles in modern multi-threaded\nallocators, which do not exist in legacy single-threaded allocators such as\ndlmalloc. We evaluate Exgen-Malloc on two Intel Xeon platforms. Across both\nsystems, Exgen-Malloc achieves a speedup of 1.17x, 1.10x, and 1.93x over\ndlmalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench, respectively. In\naddition to performance, Exgen-Malloc achieves 6.2%, 0.1%, and 25.2% memory\nsavings over mimalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory allocators hide beneath nearly every application stack, yet their\nperformance footprint extends far beyond their code size. Even small\ninefficiencies in the allocators ripple through caches and the rest of the\nmemory hierarchy, collectively imposing what operators often call a \"datacenter\ntax\". At hyperscale, even a 1% improvement in allocator efficiency can unlock\nmillions of dollars in savings and measurable reductions in datacenter energy\nconsumption. Modern memory allocators are designed to optimize allocation speed\nand memory fragmentation in multi-threaded environments, relying on complex\nmetadata and control logic to achieve high performance. However, the overhead\nintroduced by this complexity prompts a reevaluation of allocator design.\nNotably, such overhead can be avoided in single-threaded scenarios, which\ncontinue to be widely used across diverse application domains.\n  In this paper, we introduce Exgen-Malloc, a memory allocator purpose-built\nfor single-threaded applications. By specializing for single-threaded\nexecution, Exgen-Malloc eliminates unnecessary metadata, simplifies the control\nflow, thereby reducing overhead and improving allocation efficiency. Its core\ndesign features include a centralized heap, a single free-block list, and a\nbalanced strategy for memory commitment and relocation. Additionally,\nExgen-Malloc incorporates design principles in modern multi-threaded\nallocators, which do not exist in legacy single-threaded allocators such as\ndlmalloc. We evaluate Exgen-Malloc on two Intel Xeon platforms. Across both\nsystems, Exgen-Malloc achieves a speedup of 1.17x, 1.10x, and 1.93x over\ndlmalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench, respectively. In\naddition to performance, Exgen-Malloc achieves 6.2%, 0.1%, and 25.2% memory\nsavings over mimalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Ruihao Li"
                    },
                    {
                        "name": "Lizy K. John"
                    },
                    {
                        "name": "Neeraja J. Yadwadkar"
                    }
                ],
                "author_detail": {
                    "name": "Neeraja J. Yadwadkar"
                },
                "author": "Neeraja J. Yadwadkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10129v1",
                "updated": "2025-10-11T09:28:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    9,
                    28,
                    26,
                    5,
                    284,
                    0
                ],
                "published": "2025-10-11T09:28:26Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    9,
                    28,
                    26,
                    5,
                    284,
                    0
                ],
                "title": "CacheClip: Accelerating RAG with Effective KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheClip: Accelerating RAG with Effective KV Cache Reuse"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems suffer from severe\ntime-to-first-token (TTFT) bottlenecks due to long input sequences. Existing KV\ncache reuse methods face a fundamental trade-off: prefix caching requires\nidentical prefixes that rarely occur in RAG scenarios, while direct\nprecomputation sacrifices quality due to missing inter-chunk attention and\nrepeated attention sinks. Recent methods like APE and CacheBlend partially\naddress these issues but remain inadequate for robust RAG applications. This\npaper presents CacheClip, a novel framework that achieves both fast TTFT and\nhigh generation quality. Our key insight is that small auxiliary LLMs exhibit\nsimilar last-layer attention distributions to primary LLMs (the target model\nfor generation), enabling efficient identification of tokens critical for\nrestoring inter-chunk attention, thereby significantly improving response\nquality on cross-chunk reasoning tasks. CacheClip integrates three techniques:\n(1) auxiliary-model-guided token selection for selective KV cache\nrecomputation, where the auxiliary model is finetuned to improve selection\naccuracy, (2) shared prefixes to eliminate redundant attention sinks, and (3)\ngrouping strategy to maintain local coherence during partial KV cache updates.\nExperiments show CacheClip retains up to 94.8% and 85.0% of full-attention\nperformance on NIAH and LongBench, outperforming APE and CacheBlend by 25.2%\nand 35.1% on NIAH (with reomp% = 20%). Meanwhile, CacheClip accelerates LLM\ninference by up to 1.92x in prefill time, providing a practical solution to the\nefficiency-quality trade-off in RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems suffer from severe\ntime-to-first-token (TTFT) bottlenecks due to long input sequences. Existing KV\ncache reuse methods face a fundamental trade-off: prefix caching requires\nidentical prefixes that rarely occur in RAG scenarios, while direct\nprecomputation sacrifices quality due to missing inter-chunk attention and\nrepeated attention sinks. Recent methods like APE and CacheBlend partially\naddress these issues but remain inadequate for robust RAG applications. This\npaper presents CacheClip, a novel framework that achieves both fast TTFT and\nhigh generation quality. Our key insight is that small auxiliary LLMs exhibit\nsimilar last-layer attention distributions to primary LLMs (the target model\nfor generation), enabling efficient identification of tokens critical for\nrestoring inter-chunk attention, thereby significantly improving response\nquality on cross-chunk reasoning tasks. CacheClip integrates three techniques:\n(1) auxiliary-model-guided token selection for selective KV cache\nrecomputation, where the auxiliary model is finetuned to improve selection\naccuracy, (2) shared prefixes to eliminate redundant attention sinks, and (3)\ngrouping strategy to maintain local coherence during partial KV cache updates.\nExperiments show CacheClip retains up to 94.8% and 85.0% of full-attention\nperformance on NIAH and LongBench, outperforming APE and CacheBlend by 25.2%\nand 35.1% on NIAH (with reomp% = 20%). Meanwhile, CacheClip accelerates LLM\ninference by up to 1.92x in prefill time, providing a practical solution to the\nefficiency-quality trade-off in RAG systems."
                },
                "authors": [
                    {
                        "name": "Bin Yang"
                    },
                    {
                        "name": "Qiuyu Leng"
                    },
                    {
                        "name": "Jun Zeng"
                    },
                    {
                        "name": "Zhenhua Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhenhua Wu"
                },
                "author": "Zhenhua Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17396v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17396v3",
                "updated": "2025-10-11T09:04:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    9,
                    4,
                    23,
                    5,
                    284,
                    0
                ],
                "published": "2025-09-22T06:56:35Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    56,
                    35,
                    0,
                    265,
                    0
                ],
                "title": "EpiCache: Episodic KV Cache Management for Long Conversational Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EpiCache: Episodic KV Cache Management for Long Conversational Question\n  Answering"
                },
                "summary": "Modern large language models (LLMs) extend context lengths to millions of\ntokens, enabling coherent, personalized responses grounded in long\nconversational histories. This ability, however, hinges on Key-Value (KV)\ncaching, whose memory grows linearly with dialogue length and quickly becomes\nthe bottleneck in resource-constrained environments. An active line of research\nfor reducing memory bottleneck is KV cache compression, which seeks to limit\ncache size while preserving accuracy. Yet existing methods face two major\nlimitations: (i) evicting the KV cache after full-context prefill causes\nunbounded peak memory, and (ii) query-dependent eviction narrows the cache to a\nsingle query, leading to failure cases in multi-turn conversations. We\nintroduce EpiCache, a training-free KV cache management framework for long\nconversational question answering (LongConvQA) under fixed memory budgets.\nEpiCache bounds cache growth through block-wise prefill and preserves\ntopic-relevant context via episodic KV compression, which clusters conversation\nhistory into coherent episodes and applies episode-specific KV cache eviction.\nWe further design an adaptive layer-wise budget allocation strategy that\nmeasures each layer's sensitivity to eviction and distributes the memory budget\nacross layers accordingly. Across three LongConvQA benchmarks, EpiCache\nimproves accuracy by up to 40%, maintains near-full KV accuracy under 4-6x\ncompression, and reduces latency/memory by up to 2.4x/3.5x, enabling efficient\nmulti-turn interaction under strict resource limits. Our code is available at\nhttps://github.com/apple/ml-epicache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) extend context lengths to millions of\ntokens, enabling coherent, personalized responses grounded in long\nconversational histories. This ability, however, hinges on Key-Value (KV)\ncaching, whose memory grows linearly with dialogue length and quickly becomes\nthe bottleneck in resource-constrained environments. An active line of research\nfor reducing memory bottleneck is KV cache compression, which seeks to limit\ncache size while preserving accuracy. Yet existing methods face two major\nlimitations: (i) evicting the KV cache after full-context prefill causes\nunbounded peak memory, and (ii) query-dependent eviction narrows the cache to a\nsingle query, leading to failure cases in multi-turn conversations. We\nintroduce EpiCache, a training-free KV cache management framework for long\nconversational question answering (LongConvQA) under fixed memory budgets.\nEpiCache bounds cache growth through block-wise prefill and preserves\ntopic-relevant context via episodic KV compression, which clusters conversation\nhistory into coherent episodes and applies episode-specific KV cache eviction.\nWe further design an adaptive layer-wise budget allocation strategy that\nmeasures each layer's sensitivity to eviction and distributes the memory budget\nacross layers accordingly. Across three LongConvQA benchmarks, EpiCache\nimproves accuracy by up to 40%, maintains near-full KV accuracy under 4-6x\ncompression, and reduces latency/memory by up to 2.4x/3.5x, enabling efficient\nmulti-turn interaction under strict resource limits. Our code is available at\nhttps://github.com/apple/ml-epicache."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Arnav Kundu"
                    },
                    {
                        "name": "Han-Byul Kim"
                    },
                    {
                        "name": "Richa Dixit"
                    },
                    {
                        "name": "Minsik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Minsik Cho"
                },
                "author": "Minsik Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17396v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17396v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10102v1",
                "updated": "2025-10-11T08:24:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    8,
                    24,
                    19,
                    5,
                    284,
                    0
                ],
                "published": "2025-10-11T08:24:19Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    8,
                    24,
                    19,
                    5,
                    284,
                    0
                ],
                "title": "PANTHER: Generative Pretraining Beyond Language for Sequential User\n  Behavior Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PANTHER: Generative Pretraining Beyond Language for Sequential User\n  Behavior Modeling"
                },
                "summary": "Large language models (LLMs) have shown that generative pretraining can\ndistill vast world knowledge into compact token representations. While LLMs\nencapsulate extensive world knowledge, they remain limited in modeling the\nbehavioral knowledge contained within user interaction histories. User behavior\nforms a distinct modality, where each action, defined by multi-dimensional\nattributes such as time, context, and transaction type, constitutes a\nbehavioral token. Modeling these high-cardinality sequences is challenging, and\ndiscriminative models often falter under limited supervision. To bridge this\ngap, we extend generative pretraining to user behavior, learning transferable\nrepresentations from unlabeled behavioral data analogous to how LLMs learn from\ntext. We present PANTHER, a hybrid generative-discriminative framework that\nunifies user behavior pretraining and downstream adaptation, enabling\nlarge-scale sequential user representation learning and real-time inference.\nPANTHER introduces: (1) Structured Tokenization to compress multi-dimensional\ntransaction attributes into an interpretable vocabulary; (2) Sequence Pattern\nRecognition Module (SPRM) for modeling periodic transaction motifs; (3) a\nUnified User-Profile Embedding that fuses static demographics with dynamic\ntransaction histories; and (4) Real-time scalability enabled by offline caching\nof pretrained embeddings for millisecond-level inference. Fully deployed and\noperational online at WeChat Pay, PANTHER delivers a 25.6 percent boost in\nnext-transaction prediction HitRate@1 and a 38.6 percent relative improvement\nin fraud detection recall over baselines. Cross-domain evaluations on public\nbenchmarks show strong generalization, achieving up to 21 percent HitRate@1\ngains over transformer baselines, establishing PANTHER as a scalable,\nhigh-performance framework for industrial sequential user behavior modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown that generative pretraining can\ndistill vast world knowledge into compact token representations. While LLMs\nencapsulate extensive world knowledge, they remain limited in modeling the\nbehavioral knowledge contained within user interaction histories. User behavior\nforms a distinct modality, where each action, defined by multi-dimensional\nattributes such as time, context, and transaction type, constitutes a\nbehavioral token. Modeling these high-cardinality sequences is challenging, and\ndiscriminative models often falter under limited supervision. To bridge this\ngap, we extend generative pretraining to user behavior, learning transferable\nrepresentations from unlabeled behavioral data analogous to how LLMs learn from\ntext. We present PANTHER, a hybrid generative-discriminative framework that\nunifies user behavior pretraining and downstream adaptation, enabling\nlarge-scale sequential user representation learning and real-time inference.\nPANTHER introduces: (1) Structured Tokenization to compress multi-dimensional\ntransaction attributes into an interpretable vocabulary; (2) Sequence Pattern\nRecognition Module (SPRM) for modeling periodic transaction motifs; (3) a\nUnified User-Profile Embedding that fuses static demographics with dynamic\ntransaction histories; and (4) Real-time scalability enabled by offline caching\nof pretrained embeddings for millisecond-level inference. Fully deployed and\noperational online at WeChat Pay, PANTHER delivers a 25.6 percent boost in\nnext-transaction prediction HitRate@1 and a 38.6 percent relative improvement\nin fraud detection recall over baselines. Cross-domain evaluations on public\nbenchmarks show strong generalization, achieving up to 21 percent HitRate@1\ngains over transformer baselines, establishing PANTHER as a scalable,\nhigh-performance framework for industrial sequential user behavior modeling."
                },
                "authors": [
                    {
                        "name": "Guilin Li"
                    },
                    {
                        "name": "Yun Zhang"
                    },
                    {
                        "name": "Xiuyuan Chen"
                    },
                    {
                        "name": "Chengqi Li"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Wenjia Wang"
                    },
                    {
                        "name": "Weiran Huang"
                    },
                    {
                        "name": "Matthias Hwai Yong Tan"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Hwai Yong Tan"
                },
                "author": "Matthias Hwai Yong Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09952v1",
                "updated": "2025-10-11T01:42:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    1,
                    42,
                    38,
                    5,
                    284,
                    0
                ],
                "published": "2025-10-11T01:42:38Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    1,
                    42,
                    38,
                    5,
                    284,
                    0
                ],
                "title": "HTTP Request Synchronization Defeats Discrepancy Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HTTP Request Synchronization Defeats Discrepancy Attacks"
                },
                "summary": "Contemporary web application architectures involve many layers of proxy\nservices that process traffic. Due to the complexity of HTTP and vendor design\ndecisions, these proxies sometimes process a given request in different ways.\nAttackers can exploit these processing discrepancies to launch damaging attacks\nincluding web cache poisoning and request smuggling. Discrepancy attacks are\nsurging, yet, there exists no systemic defense.\n  In this work, we propose the first comprehensive defense to address this\nproblem, called HTTP Request Synchronization. Our scheme uses standard HTTP\nextension mechanisms to augment each request with a complete processing\nhistory. It propagates this context through the traffic path detailing how each\nserver hop has processed said request. Using this history, every proxy server\ncan validate that their processing is consistent with all previous hops,\neliminating discrepancy attacks. We implement our scheme for 5 popular proxy\ntechnologies, Apache, NGINX, HAProxy, Varnish, and Cloudflare, demonstrating\nits practical impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary web application architectures involve many layers of proxy\nservices that process traffic. Due to the complexity of HTTP and vendor design\ndecisions, these proxies sometimes process a given request in different ways.\nAttackers can exploit these processing discrepancies to launch damaging attacks\nincluding web cache poisoning and request smuggling. Discrepancy attacks are\nsurging, yet, there exists no systemic defense.\n  In this work, we propose the first comprehensive defense to address this\nproblem, called HTTP Request Synchronization. Our scheme uses standard HTTP\nextension mechanisms to augment each request with a complete processing\nhistory. It propagates this context through the traffic path detailing how each\nserver hop has processed said request. Using this history, every proxy server\ncan validate that their processing is consistent with all previous hops,\neliminating discrepancy attacks. We implement our scheme for 5 popular proxy\ntechnologies, Apache, NGINX, HAProxy, Varnish, and Cloudflare, demonstrating\nits practical impact."
                },
                "authors": [
                    {
                        "name": "Cem Topcuoglu"
                    },
                    {
                        "name": "Kaan Onarlioglu"
                    },
                    {
                        "name": "Steven Sprecher"
                    },
                    {
                        "name": "Engin Kirda"
                    }
                ],
                "author_detail": {
                    "name": "Engin Kirda"
                },
                "author": "Engin Kirda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09907v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09907v1",
                "updated": "2025-10-10T22:43:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    22,
                    43,
                    54,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T22:43:54Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    22,
                    43,
                    54,
                    4,
                    283,
                    0
                ],
                "title": "Agentic Property-Based Testing: Finding Bugs Across the Python Ecosystem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Property-Based Testing: Finding Bugs Across the Python Ecosystem"
                },
                "summary": "Property-based testing (PBT) is a lightweight formal method, typically\nimplemented as a randomized testing framework. Users specify the input domain\nfor their test using combinators supplied by the PBT framework, and the\nexpected properties or invariants as a unit-test function. The framework then\nsearches for a counterexample, e.g. by generating inputs and calling the test\nfunction. In this work, we demonstrate an LLM-based agent which analyzes Python\nmodules, infers function-specific and cross-function properties from code and\ndocumentation, synthesizes and executes PBTs, reflects on outputs of these\ntests to confirm true bugs, and finally outputs actionable bug reports for the\ndeveloper. We perform an extensive evaluation of our agent across 100 popular\nPython packages. Of the bug reports generated by the agent, we found after\nmanual review that 56\\% were valid bugs and 32\\% were valid bugs that we would\nreport to maintainers. We then developed a ranking rubric to surface\nhigh-priority valid bugs to developers, and found that of the 21 top-scoring\nbugs, 86\\% were valid and 81\\% we would report. The bugs span diverse failure\nmodes from serialization failures to numerical precision errors to flawed cache\nimplementations. We reported 5 bugs, 4 with patches, including to NumPy and\ncloud computing SDKs, with 3 patches merged successfully. Our results suggest\nthat LLMs with PBT provides a rigorous and scalable method for autonomously\ntesting software. Our code and artifacts are available at:\nhttps://github.com/mmaaz-git/agentic-pbt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Property-based testing (PBT) is a lightweight formal method, typically\nimplemented as a randomized testing framework. Users specify the input domain\nfor their test using combinators supplied by the PBT framework, and the\nexpected properties or invariants as a unit-test function. The framework then\nsearches for a counterexample, e.g. by generating inputs and calling the test\nfunction. In this work, we demonstrate an LLM-based agent which analyzes Python\nmodules, infers function-specific and cross-function properties from code and\ndocumentation, synthesizes and executes PBTs, reflects on outputs of these\ntests to confirm true bugs, and finally outputs actionable bug reports for the\ndeveloper. We perform an extensive evaluation of our agent across 100 popular\nPython packages. Of the bug reports generated by the agent, we found after\nmanual review that 56\\% were valid bugs and 32\\% were valid bugs that we would\nreport to maintainers. We then developed a ranking rubric to surface\nhigh-priority valid bugs to developers, and found that of the 21 top-scoring\nbugs, 86\\% were valid and 81\\% we would report. The bugs span diverse failure\nmodes from serialization failures to numerical precision errors to flawed cache\nimplementations. We reported 5 bugs, 4 with patches, including to NumPy and\ncloud computing SDKs, with 3 patches merged successfully. Our results suggest\nthat LLMs with PBT provides a rigorous and scalable method for autonomously\ntesting software. Our code and artifacts are available at:\nhttps://github.com/mmaaz-git/agentic-pbt."
                },
                "authors": [
                    {
                        "name": "Muhammad Maaz"
                    },
                    {
                        "name": "Liam DeVoe"
                    },
                    {
                        "name": "Zac Hatfield-Dodds"
                    },
                    {
                        "name": "Nicholas Carlini"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Carlini"
                },
                "author": "Nicholas Carlini",
                "arxiv_comment": "4 pages (main), NeurIPS 2025, The 4th Deep Learning for Code Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09907v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09907v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09883v1",
                "updated": "2025-10-10T21:37:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    21,
                    37,
                    49,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T21:37:49Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    21,
                    37,
                    49,
                    4,
                    283,
                    0
                ],
                "title": "DELTA: Dynamic Layer-Aware Token Attention for Efficient Long-Context\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DELTA: Dynamic Layer-Aware Token Attention for Efficient Long-Context\n  Reasoning"
                },
                "summary": "Large reasoning models (LRMs) achieve state-of-the-art performance on\nchallenging benchmarks by generating long chains of intermediate steps, but\ntheir inference cost is dominated by decoding, where each new token must attend\nto the entire growing sequence. Existing sparse attention methods reduce\ncomputation by pruning the key-value (KV) cache, yet they suffer from severe\naccuracy degradation on reasoning tasks due to cumulative selection errors and\nthe dynamic importance of tokens over long derivations. We present\n\\textbf{DELTA}, a training-free sparse attention mechanism that achieves\ncomputational efficiency without sacrificing model accuracy. DELTA partitions\ntransformer layers into three groups: initial layers that use full attention, a\nsmall set of \\emph{selection layers} that identify salient tokens via\naggregated head-level attention scores, and subsequent \\emph{sparse-attention\nlayers} that attend only to the selected subset. This design preserves the full\nKV cache in GPU memory for accuracy, while avoiding expensive full-attention\ncomputation over many layers. On reasoning benchmarks such as AIME and\nGPQA-Diamond, DELTA matches or surpasses full attention in accuracy, while\nreducing the number of attended tokens by up to $5\\times$ and delivering\n$1.5\\times$ end-to-end speedup. Our results show that selective reuse of\nintermediate attention maps offers a robust path toward efficient long-context\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large reasoning models (LRMs) achieve state-of-the-art performance on\nchallenging benchmarks by generating long chains of intermediate steps, but\ntheir inference cost is dominated by decoding, where each new token must attend\nto the entire growing sequence. Existing sparse attention methods reduce\ncomputation by pruning the key-value (KV) cache, yet they suffer from severe\naccuracy degradation on reasoning tasks due to cumulative selection errors and\nthe dynamic importance of tokens over long derivations. We present\n\\textbf{DELTA}, a training-free sparse attention mechanism that achieves\ncomputational efficiency without sacrificing model accuracy. DELTA partitions\ntransformer layers into three groups: initial layers that use full attention, a\nsmall set of \\emph{selection layers} that identify salient tokens via\naggregated head-level attention scores, and subsequent \\emph{sparse-attention\nlayers} that attend only to the selected subset. This design preserves the full\nKV cache in GPU memory for accuracy, while avoiding expensive full-attention\ncomputation over many layers. On reasoning benchmarks such as AIME and\nGPQA-Diamond, DELTA matches or surpasses full attention in accuracy, while\nreducing the number of attended tokens by up to $5\\times$ and delivering\n$1.5\\times$ end-to-end speedup. Our results show that selective reuse of\nintermediate attention maps offers a robust path toward efficient long-context\nreasoning."
                },
                "authors": [
                    {
                        "name": "Hossein Entezari Zarch"
                    },
                    {
                        "name": "Lei Gao"
                    },
                    {
                        "name": "Chaoyi Jiang"
                    },
                    {
                        "name": "Murali Annavarm"
                    }
                ],
                "author_detail": {
                    "name": "Murali Annavarm"
                },
                "author": "Murali Annavarm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09847v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09847v1",
                "updated": "2025-10-10T20:19:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    20,
                    19,
                    44,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T20:19:44Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    20,
                    19,
                    44,
                    4,
                    283,
                    0
                ],
                "title": "THEAS: Efficient Power Management in Multi-Core CPUs via Cache-Aware\n  Resource Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "THEAS: Efficient Power Management in Multi-Core CPUs via Cache-Aware\n  Resource Scheduling"
                },
                "summary": "The dynamic adaptation of resource levels enables the system to enhance\nenergy efficiency while maintaining the necessary computational resources,\nparticularly in scenarios where workloads fluctuate significantly over time.\nThe proposed approach can play a crucial role in heterogeneous systems where\nworkload characteristics are not uniformly distributed, such as non-pinning\ntasks. The deployed THEAS algorithm in this research work ensures a balance\nbetween performance and power consumption, making it suitable for a wide range\nof real-time applications. A comparative analysis of the proposed THEAS\nalgorithm with well-known scheduling techniques such as Completely Fair\nScheduler (CFS), Energy-Aware Scheduling (EAS), Heterogeneous Scheduling\n(HeteroSched), and Utility-Based Scheduling is presented in Table III. Each\nscheme is compared based on adaptability, core selection criteria, performance\nscaling, cache awareness, overhead, and real-time suitability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dynamic adaptation of resource levels enables the system to enhance\nenergy efficiency while maintaining the necessary computational resources,\nparticularly in scenarios where workloads fluctuate significantly over time.\nThe proposed approach can play a crucial role in heterogeneous systems where\nworkload characteristics are not uniformly distributed, such as non-pinning\ntasks. The deployed THEAS algorithm in this research work ensures a balance\nbetween performance and power consumption, making it suitable for a wide range\nof real-time applications. A comparative analysis of the proposed THEAS\nalgorithm with well-known scheduling techniques such as Completely Fair\nScheduler (CFS), Energy-Aware Scheduling (EAS), Heterogeneous Scheduling\n(HeteroSched), and Utility-Based Scheduling is presented in Table III. Each\nscheme is compared based on adaptability, core selection criteria, performance\nscaling, cache awareness, overhead, and real-time suitability."
                },
                "authors": [
                    {
                        "name": "Said Muhammad"
                    },
                    {
                        "name": "Lahlou Laaziz"
                    },
                    {
                        "name": "Nadjia Kara"
                    },
                    {
                        "name": "Phat Tan Nguyen"
                    },
                    {
                        "name": "Timothy Murphy"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Murphy"
                },
                "author": "Timothy Murphy",
                "arxiv_comment": "Accepted and presented at the 13th IEEE International Conference on\n  Intelligent Mobile Computing 2025 (IMC), CISOSE 2025 in Tucson, Arizona, USA.\n  This is the author's accepted manuscript (AAM). The final published version\n  will appear in the IEEE conference proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09847v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09847v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09608v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09608v1",
                "updated": "2025-10-10T17:59:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    17,
                    59,
                    58,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T17:59:58Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    17,
                    59,
                    58,
                    4,
                    283,
                    0
                ],
                "title": "StreamingVLM: Real-Time Understanding for Infinite Video Streams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamingVLM: Real-Time Understanding for Infinite Video Streams"
                },
                "summary": "Vision-language models (VLMs) could power real-time assistants and autonomous\nagents, but they face a critical challenge: understanding near-infinite video\nstreams without escalating latency and memory usage. Processing entire videos\nwith full attention leads to quadratic computational costs and poor performance\non long videos. Meanwhile, simple sliding window methods are also flawed, as\nthey either break coherence or suffer from high latency due to redundant\nrecomputation. In this paper, we introduce StreamingVLM, a model designed for\nreal-time, stable understanding of infinite visual input. Our approach is a\nunified framework that aligns training with streaming inference. During\ninference, we maintain a compact KV cache by reusing states of attention sinks,\na short window of recent vision tokens, and a long window of recent text\ntokens. This streaming ability is instilled via a simple supervised fine-tuning\n(SFT) strategy that applies full attention on short, overlapped video chunks,\nwhich effectively mimics the inference-time attention pattern without training\non prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a\nnew benchmark with videos averaging over two hours that requires dense,\nper-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM\nachieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time\nperformance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy\nalso enhances general VQA abilities without any VQA-specific fine-tuning,\nimproving performance on LongVideoBench by +4.30 and OVOBench Realtime by\n+5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) could power real-time assistants and autonomous\nagents, but they face a critical challenge: understanding near-infinite video\nstreams without escalating latency and memory usage. Processing entire videos\nwith full attention leads to quadratic computational costs and poor performance\non long videos. Meanwhile, simple sliding window methods are also flawed, as\nthey either break coherence or suffer from high latency due to redundant\nrecomputation. In this paper, we introduce StreamingVLM, a model designed for\nreal-time, stable understanding of infinite visual input. Our approach is a\nunified framework that aligns training with streaming inference. During\ninference, we maintain a compact KV cache by reusing states of attention sinks,\na short window of recent vision tokens, and a long window of recent text\ntokens. This streaming ability is instilled via a simple supervised fine-tuning\n(SFT) strategy that applies full attention on short, overlapped video chunks,\nwhich effectively mimics the inference-time attention pattern without training\non prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a\nnew benchmark with videos averaging over two hours that requires dense,\nper-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM\nachieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time\nperformance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy\nalso enhances general VQA abilities without any VQA-specific fine-tuning,\nimproving performance on LongVideoBench by +4.30 and OVOBench Realtime by\n+5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm."
                },
                "authors": [
                    {
                        "name": "Ruyi Xu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Liuning He"
                    },
                    {
                        "name": "Kelly Peng"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "The first two authors contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09608v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.21228v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21228v3",
                "updated": "2025-10-22T17:58:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    58,
                    0,
                    2,
                    295,
                    0
                ],
                "published": "2024-10-28T17:14:01Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    14,
                    1,
                    0,
                    302,
                    0
                ],
                "title": "LoRA vs Full Fine-tuning: An Illusion of Equivalence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA vs Full Fine-tuning: An Illusion of Equivalence"
                },
                "summary": "Fine-tuning is a crucial paradigm for adapting pre-trained large language\nmodels to downstream tasks. Recently, methods like Low-Rank Adaptation (LoRA)\nhave been shown to effectively fine-tune LLMs with an extreme reduction in\ntrainable parameters. But, \\emph{are their learned solutions really\nequivalent?} We study how LoRA and full-finetuning change pre-trained models by\nanalyzing the model's weight matrices through the lens of their spectral\nproperties. We find that LoRA and full fine-tuning yield weight matrices whose\nsingular value decompositions exhibit very different structure: weight matrices\ntrained with LoRA have new, high-ranking singular vectors, which we call\n\\emph{intruder dimensions}, while those trained with full fine-tuning do not.\nFurther, we extend the finding that LoRA forgets less than full fine-tuning and\nfind its forgetting is vastly localized to the intruder dimension -- by\ncausally intervening on the intruder dimensions by changing their associated\nsingular values post-fine-tuning, we show that they cause forgetting. Moreover,\nscaling them down significantly improves modeling of the pre-training\ndistribution with a minimal drop in downstream task performance. Given this, we\nshould expect accumulating intruder dimensions to be harmful and lead to more\nforgetting. This will be amplified during continual learning because of\nsequentially fine-tuning, and we show that LoRA models do accumulate intruder\ndimensions here tend to perform worse in this setting, emphasizing the\npracticality of our findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning is a crucial paradigm for adapting pre-trained large language\nmodels to downstream tasks. Recently, methods like Low-Rank Adaptation (LoRA)\nhave been shown to effectively fine-tune LLMs with an extreme reduction in\ntrainable parameters. But, \\emph{are their learned solutions really\nequivalent?} We study how LoRA and full-finetuning change pre-trained models by\nanalyzing the model's weight matrices through the lens of their spectral\nproperties. We find that LoRA and full fine-tuning yield weight matrices whose\nsingular value decompositions exhibit very different structure: weight matrices\ntrained with LoRA have new, high-ranking singular vectors, which we call\n\\emph{intruder dimensions}, while those trained with full fine-tuning do not.\nFurther, we extend the finding that LoRA forgets less than full fine-tuning and\nfind its forgetting is vastly localized to the intruder dimension -- by\ncausally intervening on the intruder dimensions by changing their associated\nsingular values post-fine-tuning, we show that they cause forgetting. Moreover,\nscaling them down significantly improves modeling of the pre-training\ndistribution with a minimal drop in downstream task performance. Given this, we\nshould expect accumulating intruder dimensions to be harmful and lead to more\nforgetting. This will be amplified during continual learning because of\nsequentially fine-tuning, and we show that LoRA models do accumulate intruder\ndimensions here tend to perform worse in this setting, emphasizing the\npracticality of our findings."
                },
                "authors": [
                    {
                        "name": "Reece Shuttleworth"
                    },
                    {
                        "name": "Jacob Andreas"
                    },
                    {
                        "name": "Antonio Torralba"
                    },
                    {
                        "name": "Pratyusha Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Pratyusha Sharma"
                },
                "author": "Pratyusha Sharma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21228v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21228v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03762v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03762v3",
                "updated": "2025-10-22T17:55:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    55,
                    33,
                    2,
                    295,
                    0
                ],
                "published": "2025-02-06T03:48:25Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    3,
                    48,
                    25,
                    3,
                    37,
                    0
                ],
                "title": "Learning Reward Machines from Partially Observed Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Reward Machines from Partially Observed Policies"
                },
                "summary": "Inverse reinforcement learning is the problem of inferring a reward function\nfrom an optimal policy or demonstrations by an expert. In this work, it is\nassumed that the reward is expressed as a reward machine whose transitions\ndepend on atomic propositions associated with the state of a Markov Decision\nProcess (MDP). Our goal is to identify the true reward machine using finite\ninformation. To this end, we first introduce the notion of a prefix tree policy\nwhich associates a distribution of actions to each state of the MDP and each\nattainable finite sequence of atomic propositions. Then, we characterize an\nequivalence class of reward machines that can be identified given the prefix\ntree policy. Finally, we propose a SAT-based algorithm that uses information\nextracted from the prefix tree policy to solve for a reward machine. It is\nproved that if the prefix tree policy is known up to a sufficient (but finite)\ndepth, our algorithm recovers the exact reward machine up to the equivalence\nclass. This sufficient depth is derived as a function of the number of MDP\nstates and (an upper bound on) the number of states of the reward machine.\nThese results are further extended to the case where we only have access to\ndemonstrations from an optimal policy. Several examples, including discrete\ngrid and block worlds, a continuous state-space robotic arm, and real data from\nexperiments with mice, are used to demonstrate the effectiveness and generality\nof the approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inverse reinforcement learning is the problem of inferring a reward function\nfrom an optimal policy or demonstrations by an expert. In this work, it is\nassumed that the reward is expressed as a reward machine whose transitions\ndepend on atomic propositions associated with the state of a Markov Decision\nProcess (MDP). Our goal is to identify the true reward machine using finite\ninformation. To this end, we first introduce the notion of a prefix tree policy\nwhich associates a distribution of actions to each state of the MDP and each\nattainable finite sequence of atomic propositions. Then, we characterize an\nequivalence class of reward machines that can be identified given the prefix\ntree policy. Finally, we propose a SAT-based algorithm that uses information\nextracted from the prefix tree policy to solve for a reward machine. It is\nproved that if the prefix tree policy is known up to a sufficient (but finite)\ndepth, our algorithm recovers the exact reward machine up to the equivalence\nclass. This sufficient depth is derived as a function of the number of MDP\nstates and (an upper bound on) the number of states of the reward machine.\nThese results are further extended to the case where we only have access to\ndemonstrations from an optimal policy. Several examples, including discrete\ngrid and block worlds, a continuous state-space robotic arm, and real data from\nexperiments with mice, are used to demonstrate the effectiveness and generality\nof the approach."
                },
                "authors": [
                    {
                        "name": "Mohamad Louai Shehab"
                    },
                    {
                        "name": "Antoine Aspeel"
                    },
                    {
                        "name": "Necmiye Ozay"
                    }
                ],
                "author_detail": {
                    "name": "Necmiye Ozay"
                },
                "author": "Necmiye Ozay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03762v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03762v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17501v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17501v3",
                "updated": "2025-10-22T17:54:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    54,
                    43,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-20T12:54:32Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    54,
                    32,
                    0,
                    293,
                    0
                ],
                "title": "Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization"
                },
                "summary": "We propose a rubric-guided, pseudo-labeled, and prompt-driven zero-shot video\nsummarization framework that bridges large language models with structured\nsemantic reasoning. A small subset of human annotations is converted into\nhigh-confidence pseudo labels and organized into dataset-adaptive rubrics\ndefining clear evaluation dimensions such as thematic relevance, action detail,\nand narrative progression. During inference, boundary scenes, including the\nopening and closing segments, are scored independently based on their own\ndescriptions, while intermediate scenes incorporate concise summaries of\nadjacent segments to assess narrative continuity and redundancy. This design\nenables the language model to balance local salience with global coherence\nwithout any parameter tuning. Across three benchmarks, the proposed method\nachieves stable and competitive results, with F1 scores of 57.58 on SumMe,\n63.05 on TVSum, and 53.79 on QFVS, surpassing zero-shot baselines by +0.85,\n+0.84, and +0.37, respectively. These outcomes demonstrate that rubric-guided\npseudo labeling combined with contextual prompting effectively stabilizes\nLLM-based scoring and establishes a general, interpretable, and training-free\nparadigm for both generic and query-focused video summarization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a rubric-guided, pseudo-labeled, and prompt-driven zero-shot video\nsummarization framework that bridges large language models with structured\nsemantic reasoning. A small subset of human annotations is converted into\nhigh-confidence pseudo labels and organized into dataset-adaptive rubrics\ndefining clear evaluation dimensions such as thematic relevance, action detail,\nand narrative progression. During inference, boundary scenes, including the\nopening and closing segments, are scored independently based on their own\ndescriptions, while intermediate scenes incorporate concise summaries of\nadjacent segments to assess narrative continuity and redundancy. This design\nenables the language model to balance local salience with global coherence\nwithout any parameter tuning. Across three benchmarks, the proposed method\nachieves stable and competitive results, with F1 scores of 57.58 on SumMe,\n63.05 on TVSum, and 53.79 on QFVS, surpassing zero-shot baselines by +0.85,\n+0.84, and +0.37, respectively. These outcomes demonstrate that rubric-guided\npseudo labeling combined with contextual prompting effectively stabilizes\nLLM-based scoring and establishes a general, interpretable, and training-free\nparadigm for both generic and query-focused video summarization."
                },
                "authors": [
                    {
                        "name": "Yuanli Wu"
                    },
                    {
                        "name": "Long Zhang"
                    },
                    {
                        "name": "Yue Du"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17501v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17501v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24379v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24379v3",
                "updated": "2025-10-22T17:51:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    51,
                    21,
                    2,
                    295,
                    0
                ],
                "published": "2025-05-30T09:09:33Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    9,
                    9,
                    33,
                    4,
                    150,
                    0
                ],
                "title": "Unlearned but Not Forgotten: Data Extraction after Exact Unlearning in\n  LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlearned but Not Forgotten: Data Extraction after Exact Unlearning in\n  LLM"
                },
                "summary": "Large Language Models are typically trained on datasets collected from the\nweb, which may inadvertently contain harmful or sensitive personal information.\nTo address growing privacy concerns, unlearning methods have been proposed to\nremove the influence of specific data from trained models. Of these, exact\nunlearning -- which retrains the model from scratch without the target data --\nis widely regarded the gold standard for mitigating privacy risks in\ndeployment. In this paper, we revisit this assumption in a practical deployment\nsetting where both the pre- and post-unlearning logits API are exposed, such as\nin open-weight scenarios. Targeting this setting, we introduce a novel data\nextraction attack that leverages signals from the pre-unlearning model to guide\nthe post-unlearning model, uncovering patterns that reflect the removed data\ndistribution. Combining model guidance with a token filtering strategy, our\nattack significantly improves extraction success rates -- doubling performance\nin some cases -- across common benchmarks such as MUSE, TOFU, and WMDP.\nFurthermore, we demonstrate our attack's effectiveness on a simulated medical\ndiagnosis dataset to highlight real-world privacy risks associated with exact\nunlearning. In light of our findings, which suggest that unlearning may, in a\ncontradictory way, increase the risk of privacy leakage during real-world\ndeployments, we advocate for evaluation of unlearning methods to consider\nbroader threat models that account not only for post-unlearning models but also\nfor adversarial access to prior checkpoints. Code is publicly available at:\nhttps://github.com/Nicholas0228/unlearned_data_extraction_llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are typically trained on datasets collected from the\nweb, which may inadvertently contain harmful or sensitive personal information.\nTo address growing privacy concerns, unlearning methods have been proposed to\nremove the influence of specific data from trained models. Of these, exact\nunlearning -- which retrains the model from scratch without the target data --\nis widely regarded the gold standard for mitigating privacy risks in\ndeployment. In this paper, we revisit this assumption in a practical deployment\nsetting where both the pre- and post-unlearning logits API are exposed, such as\nin open-weight scenarios. Targeting this setting, we introduce a novel data\nextraction attack that leverages signals from the pre-unlearning model to guide\nthe post-unlearning model, uncovering patterns that reflect the removed data\ndistribution. Combining model guidance with a token filtering strategy, our\nattack significantly improves extraction success rates -- doubling performance\nin some cases -- across common benchmarks such as MUSE, TOFU, and WMDP.\nFurthermore, we demonstrate our attack's effectiveness on a simulated medical\ndiagnosis dataset to highlight real-world privacy risks associated with exact\nunlearning. In light of our findings, which suggest that unlearning may, in a\ncontradictory way, increase the risk of privacy leakage during real-world\ndeployments, we advocate for evaluation of unlearning methods to consider\nbroader threat models that account not only for post-unlearning models but also\nfor adversarial access to prior checkpoints. Code is publicly available at:\nhttps://github.com/Nicholas0228/unlearned_data_extraction_llm."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Wu"
                    },
                    {
                        "name": "Yifei Pang"
                    },
                    {
                        "name": "Terrance Liu"
                    },
                    {
                        "name": "Zhiwei Steven Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiwei Steven Wu"
                },
                "author": "Zhiwei Steven Wu",
                "arxiv_comment": "Accepted by Neurips 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24379v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24379v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19811v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19811v1",
                "updated": "2025-10-22T17:48:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    48,
                    23,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T17:48:23Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    48,
                    23,
                    2,
                    295,
                    0
                ],
                "title": "Hubble: a Model Suite to Advance the Study of LLM Memorization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hubble: a Model Suite to Advance the Study of LLM Memorization"
                },
                "summary": "We present Hubble, a suite of fully open-source large language models (LLMs)\nfor the scientific study of LLM memorization. Hubble models come in standard\nand perturbed variants: standard models are pretrained on a large English\ncorpus, and perturbed models are trained in the same way but with controlled\ninsertion of text (e.g., book passages, biographies, and test sets) designed to\nemulate key memorization risks. Our core release includes 8 models -- standard\nand perturbed models with 1B or 8B parameters, pretrained on 100B or 500B\ntokens -- establishing that memorization risks are determined by the frequency\nof sensitive data relative to size of the training corpus (i.e., a password\nappearing once in a smaller corpus is memorized better than the same password\nin a larger corpus). Our release also includes 6 perturbed models with text\ninserted at different pretraining phases, showing that sensitive data without\ncontinued exposure can be forgotten. These findings suggest two best practices\nfor addressing memorization risks: to dilute sensitive data by increasing the\nsize of the training corpus, and to order sensitive data to appear earlier in\ntraining. Beyond these general empirical findings, Hubble enables a broad range\nof memorization research; for example, analyzing the biographies reveals how\nreadily different types of private information are memorized. We also\ndemonstrate that the randomized insertions in Hubble make it an ideal testbed\nfor membership inference and machine unlearning, and invite the community to\nfurther explore, benchmark, and build upon our work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Hubble, a suite of fully open-source large language models (LLMs)\nfor the scientific study of LLM memorization. Hubble models come in standard\nand perturbed variants: standard models are pretrained on a large English\ncorpus, and perturbed models are trained in the same way but with controlled\ninsertion of text (e.g., book passages, biographies, and test sets) designed to\nemulate key memorization risks. Our core release includes 8 models -- standard\nand perturbed models with 1B or 8B parameters, pretrained on 100B or 500B\ntokens -- establishing that memorization risks are determined by the frequency\nof sensitive data relative to size of the training corpus (i.e., a password\nappearing once in a smaller corpus is memorized better than the same password\nin a larger corpus). Our release also includes 6 perturbed models with text\ninserted at different pretraining phases, showing that sensitive data without\ncontinued exposure can be forgotten. These findings suggest two best practices\nfor addressing memorization risks: to dilute sensitive data by increasing the\nsize of the training corpus, and to order sensitive data to appear earlier in\ntraining. Beyond these general empirical findings, Hubble enables a broad range\nof memorization research; for example, analyzing the biographies reveals how\nreadily different types of private information are memorized. We also\ndemonstrate that the randomized insertions in Hubble make it an ideal testbed\nfor membership inference and machine unlearning, and invite the community to\nfurther explore, benchmark, and build upon our work."
                },
                "authors": [
                    {
                        "name": "Johnny Tian-Zheng Wei"
                    },
                    {
                        "name": "Ameya Godbole"
                    },
                    {
                        "name": "Mohammad Aflah Khan"
                    },
                    {
                        "name": "Ryan Wang"
                    },
                    {
                        "name": "Xiaoyuan Zhu"
                    },
                    {
                        "name": "James Flemings"
                    },
                    {
                        "name": "Nitya Kashyap"
                    },
                    {
                        "name": "Krishna P. Gummadi"
                    },
                    {
                        "name": "Willie Neiswanger"
                    },
                    {
                        "name": "Robin Jia"
                    }
                ],
                "author_detail": {
                    "name": "Robin Jia"
                },
                "author": "Robin Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19811v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19807v1",
                "updated": "2025-10-22T17:41:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    41,
                    30,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T17:41:30Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    41,
                    30,
                    2,
                    295,
                    0
                ],
                "title": "Scaf-GRPO: Scaffolded Group Relative Policy Optimization for Enhancing\n  LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaf-GRPO: Scaffolded Group Relative Policy Optimization for Enhancing\n  LLM Reasoning"
                },
                "summary": "Reinforcement learning from verifiable rewards has emerged as a powerful\ntechnique for enhancing the complex reasoning abilities of Large Language\nModels (LLMs). However, these methods are fundamentally constrained by the\n''learning cliff'' phenomenon: when faced with problems far beyond their\ncurrent capabilities, models consistently fail, yielding a persistent\nzero-reward signal. In policy optimization algorithms like GRPO, this collapses\nthe advantage calculation to zero, rendering these difficult problems invisible\nto the learning gradient and stalling progress. To overcome this, we introduce\nScaf-GRPO (Scaffolded Group Relative Policy Optimization), a progressive\ntraining framework that strategically provides minimal guidance only when a\nmodel's independent learning has plateaued. The framework first diagnoses\nlearning stagnation and then intervenes by injecting tiered in-prompt hints,\nranging from abstract concepts to concrete steps, enabling the model to\nconstruct a valid solution by itself. Extensive experiments on challenging\nmathematics benchmarks demonstrate Scaf-GRPO's effectiveness, boosting the\npass@1 score of the Qwen2.5-Math-7B model on the AIME24 benchmark by a relative\n44.3% over a vanilla GRPO baseline. This result demonstrates our framework\nprovides a robust and effective methodology for unlocking a model's ability to\nsolve problems previously beyond its reach, a critical step towards extending\nthe frontier of autonomous reasoning in LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning from verifiable rewards has emerged as a powerful\ntechnique for enhancing the complex reasoning abilities of Large Language\nModels (LLMs). However, these methods are fundamentally constrained by the\n''learning cliff'' phenomenon: when faced with problems far beyond their\ncurrent capabilities, models consistently fail, yielding a persistent\nzero-reward signal. In policy optimization algorithms like GRPO, this collapses\nthe advantage calculation to zero, rendering these difficult problems invisible\nto the learning gradient and stalling progress. To overcome this, we introduce\nScaf-GRPO (Scaffolded Group Relative Policy Optimization), a progressive\ntraining framework that strategically provides minimal guidance only when a\nmodel's independent learning has plateaued. The framework first diagnoses\nlearning stagnation and then intervenes by injecting tiered in-prompt hints,\nranging from abstract concepts to concrete steps, enabling the model to\nconstruct a valid solution by itself. Extensive experiments on challenging\nmathematics benchmarks demonstrate Scaf-GRPO's effectiveness, boosting the\npass@1 score of the Qwen2.5-Math-7B model on the AIME24 benchmark by a relative\n44.3% over a vanilla GRPO baseline. This result demonstrates our framework\nprovides a robust and effective methodology for unlocking a model's ability to\nsolve problems previously beyond its reach, a critical step towards extending\nthe frontier of autonomous reasoning in LLM."
                },
                "authors": [
                    {
                        "name": "Xichen Zhang"
                    },
                    {
                        "name": "Sitong Wu"
                    },
                    {
                        "name": "Yinghao Zhu"
                    },
                    {
                        "name": "Haoru Tan"
                    },
                    {
                        "name": "Shaozuo Yu"
                    },
                    {
                        "name": "Ziyi He"
                    },
                    {
                        "name": "Jiaya Jia"
                    }
                ],
                "author_detail": {
                    "name": "Jiaya Jia"
                },
                "author": "Jiaya Jia",
                "arxiv_comment": "Code: https://github.com/dvlab-research/Scaf-GRPO",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19806v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19806v1",
                "updated": "2025-10-22T17:41:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    41,
                    20,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T17:41:20Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    41,
                    20,
                    2,
                    295,
                    0
                ],
                "title": "The Art of Asking: Multilingual Prompt Optimization for Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Art of Asking: Multilingual Prompt Optimization for Synthetic Data"
                },
                "summary": "Synthetic data has become a cornerstone for scaling large language models,\nyet its multilingual use remains bottlenecked by translation-based prompts.\nThis strategy inherits English-centric framing and style and neglects cultural\ndimensions, ultimately constraining model generalization. We argue that the\noverlooked prompt space-the very inputs that define training\ndistributions-offers a more powerful lever for improving multilingual\nperformance. We introduce a lightweight framework for prompt-space\noptimization, where translated prompts are systematically transformed for\nNaturalness, Cultural Adaptation, and Difficulty Enhancement. Using an\noff-the-shelf multilingual LLM, we apply these transformations to prompts for\n12 languages spanning 7 families. Under identical data conditions, our\napproaches achieve substantial and consistent downstream improvements over the\ntranslation-only baseline: +4.7% on Global-MMLU accuracy, +2.4% on Flores\nXCometXL and +35.3% wins in preferences on mArenaHard. We establish\nprompt-space optimization as a simple yet powerful paradigm for building\nmultilingual LLMs that are more robust, culturally grounded, and globally\ncapable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic data has become a cornerstone for scaling large language models,\nyet its multilingual use remains bottlenecked by translation-based prompts.\nThis strategy inherits English-centric framing and style and neglects cultural\ndimensions, ultimately constraining model generalization. We argue that the\noverlooked prompt space-the very inputs that define training\ndistributions-offers a more powerful lever for improving multilingual\nperformance. We introduce a lightweight framework for prompt-space\noptimization, where translated prompts are systematically transformed for\nNaturalness, Cultural Adaptation, and Difficulty Enhancement. Using an\noff-the-shelf multilingual LLM, we apply these transformations to prompts for\n12 languages spanning 7 families. Under identical data conditions, our\napproaches achieve substantial and consistent downstream improvements over the\ntranslation-only baseline: +4.7% on Global-MMLU accuracy, +2.4% on Flores\nXCometXL and +35.3% wins in preferences on mArenaHard. We establish\nprompt-space optimization as a simple yet powerful paradigm for building\nmultilingual LLMs that are more robust, culturally grounded, and globally\ncapable."
                },
                "authors": [
                    {
                        "name": "David Mora"
                    },
                    {
                        "name": "Viraat Aryabumi"
                    },
                    {
                        "name": "Wei-Yin Ko"
                    },
                    {
                        "name": "Sara Hooker"
                    },
                    {
                        "name": "Julia Kreutzer"
                    },
                    {
                        "name": "Marzieh Fadaee"
                    }
                ],
                "author_detail": {
                    "name": "Marzieh Fadaee"
                },
                "author": "Marzieh Fadaee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19806v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19806v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19799v1",
                "updated": "2025-10-22T17:35:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    35,
                    13,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T17:35:13Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    35,
                    13,
                    2,
                    295,
                    0
                ],
                "title": "Integrating Transparent Models, LLMs, and Practitioner-in-the-Loop: A\n  Case of Nonprofit Program Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Transparent Models, LLMs, and Practitioner-in-the-Loop: A\n  Case of Nonprofit Program Evaluation"
                },
                "summary": "Public and nonprofit organizations often hesitate to adopt AI tools because\nmost models are opaque even though standard approaches typically analyze\naggregate patterns rather than offering actionable, case-level guidance. This\nstudy tests a practitioner-in-the-loop workflow that pairs transparent\ndecision-tree models with large language models (LLMs) to improve predictive\naccuracy, interpretability, and the generation of practical insights. Using\ndata from an ongoing college-success program, we build interpretable decision\ntrees to surface key predictors. We then provide each tree's structure to an\nLLM, enabling it to reproduce case-level predictions grounded in the\ntransparent models. Practitioners participate throughout feature engineering,\nmodel design, explanation review, and usability assessment, ensuring that field\nexpertise informs the analysis at every stage. Results show that integrating\ntransparent models, LLMs, and practitioner input yields accurate, trustworthy,\nand actionable case-level evaluations, offering a viable pathway for\nresponsible AI adoption in the public and nonprofit sectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Public and nonprofit organizations often hesitate to adopt AI tools because\nmost models are opaque even though standard approaches typically analyze\naggregate patterns rather than offering actionable, case-level guidance. This\nstudy tests a practitioner-in-the-loop workflow that pairs transparent\ndecision-tree models with large language models (LLMs) to improve predictive\naccuracy, interpretability, and the generation of practical insights. Using\ndata from an ongoing college-success program, we build interpretable decision\ntrees to surface key predictors. We then provide each tree's structure to an\nLLM, enabling it to reproduce case-level predictions grounded in the\ntransparent models. Practitioners participate throughout feature engineering,\nmodel design, explanation review, and usability assessment, ensuring that field\nexpertise informs the analysis at every stage. Results show that integrating\ntransparent models, LLMs, and practitioner input yields accurate, trustworthy,\nand actionable case-level evaluations, offering a viable pathway for\nresponsible AI adoption in the public and nonprofit sectors."
                },
                "authors": [
                    {
                        "name": "Ji Ma"
                    },
                    {
                        "name": "Albert Casella"
                    }
                ],
                "author_detail": {
                    "name": "Albert Casella"
                },
                "author": "Albert Casella",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19798v1",
                "updated": "2025-10-22T17:34:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    34,
                    26,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T17:34:26Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    34,
                    26,
                    2,
                    295,
                    0
                ],
                "title": "Detecting gravitational lensing by matter currents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting gravitational lensing by matter currents"
                },
                "summary": "We explore the observational prospects for detecting gravitational lensing\ninduced by cosmological matter currents, a relativistic correction to the\nstandard scalar lensing effect arising from the motion of matter. We propose to\nisolate this contribution by cross-correlating the weak-lensing convergence\nfield with a reconstructed cosmic momentum field inferred from galaxy surveys.\nUsing numerical simulations, we demonstrate that this reconstructed momentum\nfield is uncorrelated with the scalar lensing signal, enabling a clean\nseparation of the gravitomagnetic component. We then forecast the detectability\nof this signal for upcoming wide-field galaxy and weak-lensing surveys, showing\nthat a statistically significant detection may be achievable under realistic\nobservational conditions. Such a measurement would provide the first direct\nprobe of the large-scale cosmic momentum field, offering a novel test of\ngeneral relativity and Lorentz invariance on cosmological scales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the observational prospects for detecting gravitational lensing\ninduced by cosmological matter currents, a relativistic correction to the\nstandard scalar lensing effect arising from the motion of matter. We propose to\nisolate this contribution by cross-correlating the weak-lensing convergence\nfield with a reconstructed cosmic momentum field inferred from galaxy surveys.\nUsing numerical simulations, we demonstrate that this reconstructed momentum\nfield is uncorrelated with the scalar lensing signal, enabling a clean\nseparation of the gravitomagnetic component. We then forecast the detectability\nof this signal for upcoming wide-field galaxy and weak-lensing surveys, showing\nthat a statistically significant detection may be achievable under realistic\nobservational conditions. Such a measurement would provide the first direct\nprobe of the large-scale cosmic momentum field, offering a novel test of\ngeneral relativity and Lorentz invariance on cosmological scales."
                },
                "authors": [
                    {
                        "name": "C. Murray"
                    },
                    {
                        "name": "R. Kou"
                    },
                    {
                        "name": "J. G. Bartlett"
                    }
                ],
                "author_detail": {
                    "name": "J. G. Bartlett"
                },
                "author": "J. G. Bartlett",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19796v1",
                "updated": "2025-10-22T17:30:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    30,
                    39,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T17:30:39Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    30,
                    39,
                    2,
                    295,
                    0
                ],
                "title": "Blackbox Model Provenance via Palimpsestic Membership Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blackbox Model Provenance via Palimpsestic Membership Inference"
                },
                "summary": "Suppose Alice trains an open-weight language model and Bob uses a blackbox\nderivative of Alice's model to produce text. Can Alice prove that Bob is using\nher model, either by querying Bob's derivative model (query setting) or from\nthe text alone (observational setting)? We formulate this question as an\nindependence testing problem--in which the null hypothesis is that Bob's model\nor text is independent of Alice's randomized training run--and investigate it\nthrough the lens of palimpsestic memorization in language models: models are\nmore likely to memorize data seen later in training, so we can test whether Bob\nis using Alice's model using test statistics that capture correlation between\nBob's model or text and the ordering of training examples in Alice's training\nrun. If Alice has randomly shuffled her training data, then any significant\ncorrelation amounts to exactly quantifiable statistical evidence against the\nnull hypothesis, regardless of the composition of Alice's training data. In the\nquery setting, we directly estimate (via prompting) the likelihood Bob's model\ngives to Alice's training examples and order; we correlate the likelihoods of\nover 40 fine-tunes of various Pythia and OLMo base models ranging from 1B to\n12B parameters with the base model's training data order, achieving a p-value\non the order of at most 1e-8 in all but six cases. In the observational\nsetting, we try two approaches based on estimating 1) the likelihood of Bob's\ntext overlapping with spans of Alice's training examples and 2) the likelihood\nof Bob's text with respect to different versions of Alice's model we obtain by\nrepeating the last phase (e.g., 1%) of her training run on reshuffled data. The\nsecond approach can reliably distinguish Bob's text from as little as a few\nhundred tokens; the first does not involve any retraining but requires many\nmore tokens (several hundred thousand) to achieve high power.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Suppose Alice trains an open-weight language model and Bob uses a blackbox\nderivative of Alice's model to produce text. Can Alice prove that Bob is using\nher model, either by querying Bob's derivative model (query setting) or from\nthe text alone (observational setting)? We formulate this question as an\nindependence testing problem--in which the null hypothesis is that Bob's model\nor text is independent of Alice's randomized training run--and investigate it\nthrough the lens of palimpsestic memorization in language models: models are\nmore likely to memorize data seen later in training, so we can test whether Bob\nis using Alice's model using test statistics that capture correlation between\nBob's model or text and the ordering of training examples in Alice's training\nrun. If Alice has randomly shuffled her training data, then any significant\ncorrelation amounts to exactly quantifiable statistical evidence against the\nnull hypothesis, regardless of the composition of Alice's training data. In the\nquery setting, we directly estimate (via prompting) the likelihood Bob's model\ngives to Alice's training examples and order; we correlate the likelihoods of\nover 40 fine-tunes of various Pythia and OLMo base models ranging from 1B to\n12B parameters with the base model's training data order, achieving a p-value\non the order of at most 1e-8 in all but six cases. In the observational\nsetting, we try two approaches based on estimating 1) the likelihood of Bob's\ntext overlapping with spans of Alice's training examples and 2) the likelihood\nof Bob's text with respect to different versions of Alice's model we obtain by\nrepeating the last phase (e.g., 1%) of her training run on reshuffled data. The\nsecond approach can reliably distinguish Bob's text from as little as a few\nhundred tokens; the first does not involve any retraining but requires many\nmore tokens (several hundred thousand) to achieve high power."
                },
                "authors": [
                    {
                        "name": "Rohith Kuditipudi"
                    },
                    {
                        "name": "Jing Huang"
                    },
                    {
                        "name": "Sally Zhu"
                    },
                    {
                        "name": "Diyi Yang"
                    },
                    {
                        "name": "Christopher Potts"
                    },
                    {
                        "name": "Percy Liang"
                    }
                ],
                "author_detail": {
                    "name": "Percy Liang"
                },
                "author": "Percy Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19791v1",
                "updated": "2025-10-22T17:26:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    26,
                    5,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T17:26:05Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    26,
                    5,
                    2,
                    295,
                    0
                ],
                "title": "ToolDreamer: Instilling LLM Reasoning Into Tool Retrievers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolDreamer: Instilling LLM Reasoning Into Tool Retrievers"
                },
                "summary": "Tool calling has become increasingly popular for Large Language Models\n(LLMs). However, for large tool sets, the resulting tokens would exceed the\nLLM's context window limit, making it impossible to include every tool. Hence,\nan external retriever is used to provide LLMs with the most relevant tools for\na query. Existing retrieval models rank tools based on the similarity between a\nuser query and a tool description (TD). This leads to suboptimal retrieval as\nuser requests are often poorly aligned with the language of TD. To remedy the\nissue, we propose ToolDreamer, a framework to condition retriever models to\nfetch tools based on hypothetical (synthetic) TD generated using an LLM, i.e.,\ndescription of tools that the LLM feels will be potentially useful for the\nquery. The framework enables a more natural alignment between queries and tools\nwithin the language space of TD's. We apply ToolDreamer on the ToolRet dataset\nand show that our method improves the performance of sparse and dense\nretrievers with and without training, thus showcasing its flexibility. Through\nour proposed framework, our aim is to offload a portion of the reasoning burden\nto the retriever so that the LLM may effectively handle a large collection of\ntools without inundating its context window.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool calling has become increasingly popular for Large Language Models\n(LLMs). However, for large tool sets, the resulting tokens would exceed the\nLLM's context window limit, making it impossible to include every tool. Hence,\nan external retriever is used to provide LLMs with the most relevant tools for\na query. Existing retrieval models rank tools based on the similarity between a\nuser query and a tool description (TD). This leads to suboptimal retrieval as\nuser requests are often poorly aligned with the language of TD. To remedy the\nissue, we propose ToolDreamer, a framework to condition retriever models to\nfetch tools based on hypothetical (synthetic) TD generated using an LLM, i.e.,\ndescription of tools that the LLM feels will be potentially useful for the\nquery. The framework enables a more natural alignment between queries and tools\nwithin the language space of TD's. We apply ToolDreamer on the ToolRet dataset\nand show that our method improves the performance of sparse and dense\nretrievers with and without training, thus showcasing its flexibility. Through\nour proposed framework, our aim is to offload a portion of the reasoning burden\nto the retriever so that the LLM may effectively handle a large collection of\ntools without inundating its context window."
                },
                "authors": [
                    {
                        "name": "Saptarshi Sengupta"
                    },
                    {
                        "name": "Zhengyu Zhou"
                    },
                    {
                        "name": "Jun Araki"
                    },
                    {
                        "name": "Xingbo Wang"
                    },
                    {
                        "name": "Bingqing Wang"
                    },
                    {
                        "name": "Suhang Wang"
                    },
                    {
                        "name": "Zhe Feng"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Feng"
                },
                "author": "Zhe Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19788v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19788v1",
                "updated": "2025-10-22T17:23:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    23,
                    18,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T17:23:18Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    23,
                    18,
                    2,
                    295,
                    0
                ],
                "title": "Benchmarking World-Model Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking World-Model Learning"
                },
                "summary": "Model-learning agents should gather information to learn world models that\nsupport many downstream tasks and inferences, such as predicting unobserved\nstates, estimating near- and far-term consequences of actions, planning action\nsequences, and detecting changes in dynamics. Current methods for learning and\nevaluating world models diverge from this goal: training and evaluation are\nanchored to next-frame prediction, and success is scored by reward maximization\nin the same environment. We propose WorldTest, a protocol to evaluate\nmodel-learning agents that separates reward-free interaction from a scored test\nphase in a different but related environment. WorldTest is\nopen-ended$\\unicode{x2014}$models should support many different tasks unknown\nahead of time$\\unicode{x2014}$and agnostic to model representation, allowing\ncomparison across approaches. We instantiated WorldTest with AutumnBench, a\nsuite of 43 interactive grid-world environments and 129 tasks across three\nfamilies: masked-frame prediction, planning, and predicting changes to the\ncausal dynamics. We compared 517 human participants and three frontier models\non AutumnBench. We found that humans outperform the models, and scaling compute\nimproves performance only in some environments but not others. WorldTest\nprovides a novel template$\\unicode{x2014}$reward-free exploration, derived\ntests, and behavior-based scoring$\\unicode{x2014}$to evaluate what agents learn\nabout environment dynamics, and AutumnBench exposes significant headroom in\nworld-model learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-learning agents should gather information to learn world models that\nsupport many downstream tasks and inferences, such as predicting unobserved\nstates, estimating near- and far-term consequences of actions, planning action\nsequences, and detecting changes in dynamics. Current methods for learning and\nevaluating world models diverge from this goal: training and evaluation are\nanchored to next-frame prediction, and success is scored by reward maximization\nin the same environment. We propose WorldTest, a protocol to evaluate\nmodel-learning agents that separates reward-free interaction from a scored test\nphase in a different but related environment. WorldTest is\nopen-ended$\\unicode{x2014}$models should support many different tasks unknown\nahead of time$\\unicode{x2014}$and agnostic to model representation, allowing\ncomparison across approaches. We instantiated WorldTest with AutumnBench, a\nsuite of 43 interactive grid-world environments and 129 tasks across three\nfamilies: masked-frame prediction, planning, and predicting changes to the\ncausal dynamics. We compared 517 human participants and three frontier models\non AutumnBench. We found that humans outperform the models, and scaling compute\nimproves performance only in some environments but not others. WorldTest\nprovides a novel template$\\unicode{x2014}$reward-free exploration, derived\ntests, and behavior-based scoring$\\unicode{x2014}$to evaluate what agents learn\nabout environment dynamics, and AutumnBench exposes significant headroom in\nworld-model learning."
                },
                "authors": [
                    {
                        "name": "Archana Warrier"
                    },
                    {
                        "name": "Dat Nyugen"
                    },
                    {
                        "name": "Michelangelo Naim"
                    },
                    {
                        "name": "Moksh Jain"
                    },
                    {
                        "name": "Yichao Liang"
                    },
                    {
                        "name": "Karen Schroeder"
                    },
                    {
                        "name": "Cambridge Yang"
                    },
                    {
                        "name": "Joshua B. Tenenbaum"
                    },
                    {
                        "name": "Sebastian Vollmer"
                    },
                    {
                        "name": "Kevin Ellis"
                    },
                    {
                        "name": "Zenna Tavares"
                    }
                ],
                "author_detail": {
                    "name": "Zenna Tavares"
                },
                "author": "Zenna Tavares",
                "arxiv_comment": "30 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19788v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19788v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19784v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19784v1",
                "updated": "2025-10-22T17:20:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    20,
                    12,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T17:20:12Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    20,
                    12,
                    2,
                    295,
                    0
                ],
                "title": "Environment Inference for Learning Generalizable Dynamical System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Environment Inference for Learning Generalizable Dynamical System"
                },
                "summary": "Data-driven methods offer efficient and robust solutions for analyzing\ncomplex dynamical systems but rely on the assumption of I.I.D. data, driving\nthe development of generalization techniques for handling environmental\ndifferences. These techniques, however, are limited by their dependence on\nenvironment labels, which are often unavailable during training due to data\nacquisition challenges, privacy concerns, and environmental variability,\nparticularly in large public datasets and privacy-sensitive domains. In\nresponse, we propose DynaInfer, a novel method that infers environment\nspecifications by analyzing prediction errors from fixed neural networks within\neach training round, enabling environment assignments directly from data. We\nprove our algorithm effectively solves the alternating optimization problem in\nunlabeled scenarios and validate it through extensive experiments across\ndiverse dynamical systems. Results show that DynaInfer outperforms existing\nenvironment assignment techniques, converges rapidly to true labels, and even\nachieves superior performance when environment labels are available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven methods offer efficient and robust solutions for analyzing\ncomplex dynamical systems but rely on the assumption of I.I.D. data, driving\nthe development of generalization techniques for handling environmental\ndifferences. These techniques, however, are limited by their dependence on\nenvironment labels, which are often unavailable during training due to data\nacquisition challenges, privacy concerns, and environmental variability,\nparticularly in large public datasets and privacy-sensitive domains. In\nresponse, we propose DynaInfer, a novel method that infers environment\nspecifications by analyzing prediction errors from fixed neural networks within\neach training round, enabling environment assignments directly from data. We\nprove our algorithm effectively solves the alternating optimization problem in\nunlabeled scenarios and validate it through extensive experiments across\ndiverse dynamical systems. Results show that DynaInfer outperforms existing\nenvironment assignment techniques, converges rapidly to true labels, and even\nachieves superior performance when environment labels are available."
                },
                "authors": [
                    {
                        "name": "Shixuan Liu"
                    },
                    {
                        "name": "Yue He"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Wenjing Yang"
                    },
                    {
                        "name": "Yunfei Wang"
                    },
                    {
                        "name": "Peng Cui"
                    },
                    {
                        "name": "Zhong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhong Liu"
                },
                "author": "Zhong Liu",
                "arxiv_comment": "NeurIPS 2025 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19784v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19784v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19782v1",
                "updated": "2025-10-22T17:16:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    16,
                    23,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T17:16:23Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    16,
                    23,
                    2,
                    295,
                    0
                ],
                "title": "Adapting Multilingual Models to Code-Mixed Tasks via Model Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting Multilingual Models to Code-Mixed Tasks via Model Merging"
                },
                "summary": "We study model merging as a practical alternative to conventional adaptation\nstrategies for code-mixed NLP. Starting from a multilingual base model, we: (i)\nperform continued pre-training (CPT) on unlabeled code-mixed text to obtain an\nadapted checkpoint, (ii) merge checkpoint with the base model, and (iii)\nfine-tune (FT) on the downstream task data. We evaluate our approach for\nsentence classification (sentiment and hate speech) task in English-Hindi\n(En-Hi) and English-Spanish (En-Es) using XLM-R and Llama-3.2-1B models. Our\nresults show that merged models consistently outperform full fine-tuning and\nCPT->FT. We observe gains of 2--5 points in F1 over full fine-tuning and ~1-2\npoints over CPT->FT, indicating that unlabeled data is leveraged more\neffectively via merging than via CPT alone. Zero-/few-shot prompting with\nlarger LLMs (e.g., Llama-3.3-70B) lags behind fine-tuned and merged\ncheckpoints, underscoring limits of in-context learning for code-mixed inputs.\nWe further test cross-pair transfer by training on En-Hi and evaluating on\nEn-Ta and En-Ml: merged checkpoints transfer more strongly than\nmonolingual-English baselines (e.g., TV/TIES variants reaching 0.65-0.68 F1 vs\n0.61-0.63 for full fine-tuning), suggesting that code-mixed knowledge is a more\nreliable substrate for low-resource pairs. We conclude with adaptation recipes\nmatched to common data regimes (labeled only; labeled+unlabeled; transfer-only)\nand discuss limitations and scaling considerations for broader tasks and larger\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study model merging as a practical alternative to conventional adaptation\nstrategies for code-mixed NLP. Starting from a multilingual base model, we: (i)\nperform continued pre-training (CPT) on unlabeled code-mixed text to obtain an\nadapted checkpoint, (ii) merge checkpoint with the base model, and (iii)\nfine-tune (FT) on the downstream task data. We evaluate our approach for\nsentence classification (sentiment and hate speech) task in English-Hindi\n(En-Hi) and English-Spanish (En-Es) using XLM-R and Llama-3.2-1B models. Our\nresults show that merged models consistently outperform full fine-tuning and\nCPT->FT. We observe gains of 2--5 points in F1 over full fine-tuning and ~1-2\npoints over CPT->FT, indicating that unlabeled data is leveraged more\neffectively via merging than via CPT alone. Zero-/few-shot prompting with\nlarger LLMs (e.g., Llama-3.3-70B) lags behind fine-tuned and merged\ncheckpoints, underscoring limits of in-context learning for code-mixed inputs.\nWe further test cross-pair transfer by training on En-Hi and evaluating on\nEn-Ta and En-Ml: merged checkpoints transfer more strongly than\nmonolingual-English baselines (e.g., TV/TIES variants reaching 0.65-0.68 F1 vs\n0.61-0.63 for full fine-tuning), suggesting that code-mixed knowledge is a more\nreliable substrate for low-resource pairs. We conclude with adaptation recipes\nmatched to common data regimes (labeled only; labeled+unlabeled; transfer-only)\nand discuss limitations and scaling considerations for broader tasks and larger\nmodels."
                },
                "authors": [
                    {
                        "name": "Prashant Kodali"
                    },
                    {
                        "name": "Vaishnavi Shivkumar"
                    },
                    {
                        "name": "Swarang Joshi"
                    },
                    {
                        "name": "Monojit Choudhary"
                    },
                    {
                        "name": "Ponnurangam Kumaraguru"
                    },
                    {
                        "name": "Manish Shrivastava"
                    }
                ],
                "author_detail": {
                    "name": "Manish Shrivastava"
                },
                "author": "Manish Shrivastava",
                "arxiv_comment": "9 pages, 5 tables, CODS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08800v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08800v2",
                "updated": "2025-10-22T17:14:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    14,
                    8,
                    2,
                    295,
                    0
                ],
                "published": "2025-06-10T13:47:22Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    13,
                    47,
                    22,
                    1,
                    161,
                    0
                ],
                "title": "Measuring Data Science Automation: A Survey of Evaluation Tools for AI\n  Assistants and Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Data Science Automation: A Survey of Evaluation Tools for AI\n  Assistants and Agents"
                },
                "summary": "Data science aims to extract insights from data to support decision-making\nprocesses. Recently, Large Language Models (LLMs) have been increasingly used\nas assistants for data science, by suggesting ideas, techniques and small code\nsnippets, or for the interpretation of results and reporting. Proper automation\nof some data-science activities is now promised by the rise of LLM agents,\ni.e., AI systems powered by an LLM equipped with additional affordances--such\nas code execution and knowledge bases--that can perform self-directed actions\nand interact with digital environments. In this paper, we survey the evaluation\nof LLM assistants and agents for data science. We find (1) a dominant focus on\na small subset of goal-oriented activities, largely ignoring data management\nand exploratory activities; (2) a concentration on pure assistance or fully\nautonomous agents, without considering intermediate levels of human-AI\ncollaboration; and (3) an emphasis on human substitution, therefore neglecting\nthe possibility of higher levels of automation thanks to task transformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data science aims to extract insights from data to support decision-making\nprocesses. Recently, Large Language Models (LLMs) have been increasingly used\nas assistants for data science, by suggesting ideas, techniques and small code\nsnippets, or for the interpretation of results and reporting. Proper automation\nof some data-science activities is now promised by the rise of LLM agents,\ni.e., AI systems powered by an LLM equipped with additional affordances--such\nas code execution and knowledge bases--that can perform self-directed actions\nand interact with digital environments. In this paper, we survey the evaluation\nof LLM assistants and agents for data science. We find (1) a dominant focus on\na small subset of goal-oriented activities, largely ignoring data management\nand exploratory activities; (2) a concentration on pure assistance or fully\nautonomous agents, without considering intermediate levels of human-AI\ncollaboration; and (3) an emphasis on human substitution, therefore neglecting\nthe possibility of higher levels of automation thanks to task transformation."
                },
                "authors": [
                    {
                        "name": "Irene Testini"
                    },
                    {
                        "name": "José Hernández-Orallo"
                    },
                    {
                        "name": "Lorenzo Pacchiardi"
                    }
                ],
                "author_detail": {
                    "name": "Lorenzo Pacchiardi"
                },
                "author": "Lorenzo Pacchiardi",
                "arxiv_comment": "Published in Transactions of Machine Learning Research (TMLR),\n  10/2025 https://openreview.net/forum?id=MB0TCLfLn1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08800v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08800v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19779v1",
                "updated": "2025-10-22T17:13:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    13,
                    0,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T17:13:00Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    13,
                    0,
                    2,
                    295,
                    0
                ],
                "title": "AdaSPEC: Selective Knowledge Distillation for Efficient Speculative\n  Decoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaSPEC: Selective Knowledge Distillation for Efficient Speculative\n  Decoders"
                },
                "summary": "Speculative Decoding (SD) accelerates large language model inference by\nemploying a small draft model to generate predictions, which are then verified\nby a larger target model. The effectiveness of SD hinges on the alignment\nbetween these models, which is typically enhanced by Knowledge Distillation\n(KD). However, conventional KD methods aim to minimize the KL divergence\nbetween the draft and target models across all tokens, a goal that is\nmisaligned with the true objective of SD, which is to maximize token acceptance\nrate. Therefore, draft models often struggle to fully assimilate the target\nmodel's knowledge due to capacity constraints, leading to suboptimal\nperformance. To address this challenge, we propose AdaSPEC, a novel method that\nincorporates selective token filtering into the KD process. AdaSPEC utilizes a\nreference model to identify and filter out difficult-to-fit tokens, enabling\nthe distillation of a draft model that better aligns with the target model on\nsimpler tokens. This approach improves the overall token acceptance rate\nwithout compromising generation quality. We evaluate AdaSPEC across diverse\ntasks, including arithmetic reasoning, instruction-following, coding, and\nsummarization, using model configurations of 31M/1.4B and 350M/2.7B parameters.\nOur results demonstrate that AdaSPEC consistently outperforms the\nstate-of-the-art DistillSpec method, achieving higher acceptance rates across\nall tasks (up to 15\\%). The code is publicly available at\nhttps://github.com/yuezhouhu/adaspec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative Decoding (SD) accelerates large language model inference by\nemploying a small draft model to generate predictions, which are then verified\nby a larger target model. The effectiveness of SD hinges on the alignment\nbetween these models, which is typically enhanced by Knowledge Distillation\n(KD). However, conventional KD methods aim to minimize the KL divergence\nbetween the draft and target models across all tokens, a goal that is\nmisaligned with the true objective of SD, which is to maximize token acceptance\nrate. Therefore, draft models often struggle to fully assimilate the target\nmodel's knowledge due to capacity constraints, leading to suboptimal\nperformance. To address this challenge, we propose AdaSPEC, a novel method that\nincorporates selective token filtering into the KD process. AdaSPEC utilizes a\nreference model to identify and filter out difficult-to-fit tokens, enabling\nthe distillation of a draft model that better aligns with the target model on\nsimpler tokens. This approach improves the overall token acceptance rate\nwithout compromising generation quality. We evaluate AdaSPEC across diverse\ntasks, including arithmetic reasoning, instruction-following, coding, and\nsummarization, using model configurations of 31M/1.4B and 350M/2.7B parameters.\nOur results demonstrate that AdaSPEC consistently outperforms the\nstate-of-the-art DistillSpec method, achieving higher acceptance rates across\nall tasks (up to 15\\%). The code is publicly available at\nhttps://github.com/yuezhouhu/adaspec."
                },
                "authors": [
                    {
                        "name": "Yuezhou Hu"
                    },
                    {
                        "name": "Jiaxin Guo"
                    },
                    {
                        "name": "Xinyu Feng"
                    },
                    {
                        "name": "Tuo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Zhao"
                },
                "author": "Tuo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19778v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19778v1",
                "updated": "2025-10-22T17:11:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    11,
                    49,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T17:11:49Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    11,
                    49,
                    2,
                    295,
                    0
                ],
                "title": "GaLLoP: Gradient-based Sparse Learning on Low-Magnitude Parameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GaLLoP: Gradient-based Sparse Learning on Low-Magnitude Parameters"
                },
                "summary": "Sparse fine-tuning techniques adapt LLMs to downstream tasks by only tuning a\nsparse subset of model parameters. However, the effectiveness of sparse\nadaptation depends on optimally selecting the model parameters to be\nfine-tuned. In this work, we introduce a novel sparse fine-tuning technique\nnamed GaLLoP: Gradient-based Sparse Learning on Low-Magnitude Parameters, which\nfine-tunes only those model parameters which have the largest gradient\nmagnitudes on downstream tasks and the smallest pre-trained magnitudes,\nintuitively prioritizing parameters that are highly task-relevant, but\nminimally disruptive to pre-trained knowledge. Our experimentation with LLaMA3\n8B and Gemma 2B as base models shows that GaLLoP consistently improves or\nmatches the in-distribution as well as out-of-distribution performance obtained\nvia the usage of other leading parameter-efficient fine-tuning techniques,\nincluding LoRA, DoRA, and SAFT. Our analysis demonstrates that GaLLoP mitigates\ncatastrophic forgetting and memorization of task data, as important pre-trained\nparameters remain unchanged, and stabilizes performance relative to other\nfine-tuning techniques, robustly generalizing across most random seeds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse fine-tuning techniques adapt LLMs to downstream tasks by only tuning a\nsparse subset of model parameters. However, the effectiveness of sparse\nadaptation depends on optimally selecting the model parameters to be\nfine-tuned. In this work, we introduce a novel sparse fine-tuning technique\nnamed GaLLoP: Gradient-based Sparse Learning on Low-Magnitude Parameters, which\nfine-tunes only those model parameters which have the largest gradient\nmagnitudes on downstream tasks and the smallest pre-trained magnitudes,\nintuitively prioritizing parameters that are highly task-relevant, but\nminimally disruptive to pre-trained knowledge. Our experimentation with LLaMA3\n8B and Gemma 2B as base models shows that GaLLoP consistently improves or\nmatches the in-distribution as well as out-of-distribution performance obtained\nvia the usage of other leading parameter-efficient fine-tuning techniques,\nincluding LoRA, DoRA, and SAFT. Our analysis demonstrates that GaLLoP mitigates\ncatastrophic forgetting and memorization of task data, as important pre-trained\nparameters remain unchanged, and stabilizes performance relative to other\nfine-tuning techniques, robustly generalizing across most random seeds."
                },
                "authors": [
                    {
                        "name": "Anand Choudhary"
                    },
                    {
                        "name": "Yasser Sulaıman"
                    },
                    {
                        "name": "Lukas Mauch"
                    },
                    {
                        "name": "Ghouthi Boukli Hacene"
                    },
                    {
                        "name": "Fabien Cardinaux"
                    },
                    {
                        "name": "Antoine Bosselut"
                    }
                ],
                "author_detail": {
                    "name": "Antoine Bosselut"
                },
                "author": "Antoine Bosselut",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19778v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19778v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19777v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19777v1",
                "updated": "2025-10-22T17:11:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    11,
                    30,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T17:11:30Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    11,
                    30,
                    2,
                    295,
                    0
                ],
                "title": "BOSQTGEN: Breaking the Sound Barrier in Test Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BOSQTGEN: Breaking the Sound Barrier in Test Generation"
                },
                "summary": "Modern software is increasingly built by composing APIs, elevating the API\ncontract to a critical role. Inadequate contracts, however, lead to mismatched\nexpectations and failures, creating a pressing need for robust conformance\ntesting. Current test generation techniques are hindered by key challenges:\npolyglot systems, source code inaccessibility, a cost-reliability trade-off,\nand, most critically, the difficulty of generating structured inputs.\n  We introduce BOSQTGEN, a novel black-box methodology and tool for API test\ngeneration. BOSQTGEN utilizes a novel approach for decomposing API\nspecifications into primitives, using LLMs to suggest coherent strata for them,\nand employing combinatorial testing to efficiently sample over these values.\nThis approach ensures coverage of critical interactions while avoiding the\nredundancy of random sampling.\n  The resulting BOSQTGEN system achieves an average of 82% code coverage on\nRESTful benchmarks, often a 20% or more increase over prior state-of-the-art\nsystems and nearing parity with hand-written test suites. Providing a fully\nAPI-driven approach to test generation, enables developers to automatically\ncreate high-quality test cases for validation or test-driven development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software is increasingly built by composing APIs, elevating the API\ncontract to a critical role. Inadequate contracts, however, lead to mismatched\nexpectations and failures, creating a pressing need for robust conformance\ntesting. Current test generation techniques are hindered by key challenges:\npolyglot systems, source code inaccessibility, a cost-reliability trade-off,\nand, most critically, the difficulty of generating structured inputs.\n  We introduce BOSQTGEN, a novel black-box methodology and tool for API test\ngeneration. BOSQTGEN utilizes a novel approach for decomposing API\nspecifications into primitives, using LLMs to suggest coherent strata for them,\nand employing combinatorial testing to efficiently sample over these values.\nThis approach ensures coverage of critical interactions while avoiding the\nredundancy of random sampling.\n  The resulting BOSQTGEN system achieves an average of 82% code coverage on\nRESTful benchmarks, often a 20% or more increase over prior state-of-the-art\nsystems and nearing parity with hand-written test suites. Providing a fully\nAPI-driven approach to test generation, enables developers to automatically\ncreate high-quality test cases for validation or test-driven development."
                },
                "authors": [
                    {
                        "name": "S M Sadrul Islam Asif"
                    },
                    {
                        "name": "James Chen"
                    },
                    {
                        "name": "Earl T. Barr"
                    },
                    {
                        "name": "Mark Marron"
                    }
                ],
                "author_detail": {
                    "name": "Mark Marron"
                },
                "author": "Mark Marron",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19777v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19777v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06045v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06045v3",
                "updated": "2025-10-23T06:14:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    6,
                    14,
                    11,
                    3,
                    296,
                    0
                ],
                "published": "2025-06-06T12:46:36Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    12,
                    46,
                    36,
                    4,
                    157,
                    0
                ],
                "title": "Diffusion-Based Hierarchical Graph Neural Networks for Simulating\n  Nonlinear Solid Mechanics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-Based Hierarchical Graph Neural Networks for Simulating\n  Nonlinear Solid Mechanics"
                },
                "summary": "Graph-based learned simulators have emerged as a promising approach for\nsimulating physical systems on unstructured meshes, offering speed and\ngeneralization across diverse geometries. However, they often struggle with\ncapturing global phenomena, such as bending or long-range correlations usually\noccurring in solid mechanics, and suffer from error accumulation over long\nrollouts due to their reliance on local message passing and direct next-step\nprediction. We address these limitations by introducing the Rolling\nDiffusion-Batched Inference Network (ROBIN), a novel learned simulator that\nintegrates two key innovations: (i) Rolling Diffusion-Batched Inference (ROBI),\na parallelized inference scheme that amortizes the cost of diffusion-based\nrefinement across physical time steps by overlapping denoising steps across a\ntemporal window. (ii) A Hierarchical Graph Neural Network built on algebraic\nmultigrid coarsening, enabling multiscale message passing across different mesh\nresolutions. This architecture, implemented via Algebraic-hierarchical Message\nPassing Networks, captures both fine-scale local dynamics and global structural\neffects critical for phenomena like beam bending or multi-body contact. We\nvalidate ROBIN on challenging 2D and 3D solid mechanics benchmarks involving\ngeometric, material, and contact nonlinearities. ROBIN achieves\nstate-of-the-art accuracy on all tasks, substantially outperforming existing\nnext-step learned simulators while reducing inference time by up to an order of\nmagnitude compared to standard diffusion simulators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-based learned simulators have emerged as a promising approach for\nsimulating physical systems on unstructured meshes, offering speed and\ngeneralization across diverse geometries. However, they often struggle with\ncapturing global phenomena, such as bending or long-range correlations usually\noccurring in solid mechanics, and suffer from error accumulation over long\nrollouts due to their reliance on local message passing and direct next-step\nprediction. We address these limitations by introducing the Rolling\nDiffusion-Batched Inference Network (ROBIN), a novel learned simulator that\nintegrates two key innovations: (i) Rolling Diffusion-Batched Inference (ROBI),\na parallelized inference scheme that amortizes the cost of diffusion-based\nrefinement across physical time steps by overlapping denoising steps across a\ntemporal window. (ii) A Hierarchical Graph Neural Network built on algebraic\nmultigrid coarsening, enabling multiscale message passing across different mesh\nresolutions. This architecture, implemented via Algebraic-hierarchical Message\nPassing Networks, captures both fine-scale local dynamics and global structural\neffects critical for phenomena like beam bending or multi-body contact. We\nvalidate ROBIN on challenging 2D and 3D solid mechanics benchmarks involving\ngeometric, material, and contact nonlinearities. ROBIN achieves\nstate-of-the-art accuracy on all tasks, substantially outperforming existing\nnext-step learned simulators while reducing inference time by up to an order of\nmagnitude compared to standard diffusion simulators."
                },
                "authors": [
                    {
                        "name": "Tobias Würth"
                    },
                    {
                        "name": "Niklas Freymuth"
                    },
                    {
                        "name": "Gerhard Neumann"
                    },
                    {
                        "name": "Luise Kärger"
                    }
                ],
                "author_detail": {
                    "name": "Luise Kärger"
                },
                "author": "Luise Kärger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06045v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06045v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19773v1",
                "updated": "2025-10-22T17:03:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    3,
                    55,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T17:03:55Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    3,
                    55,
                    2,
                    295,
                    0
                ],
                "title": "The Tail Tells All: Estimating Model-Level Membership Inference\n  Vulnerability Without Reference Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Tail Tells All: Estimating Model-Level Membership Inference\n  Vulnerability Without Reference Models"
                },
                "summary": "Membership inference attacks (MIAs) have emerged as the standard tool for\nevaluating the privacy risks of AI models. However, state-of-the-art attacks\nrequire training numerous, often computationally expensive, reference models,\nlimiting their practicality. We present a novel approach for estimating\nmodel-level vulnerability, the TPR at low FPR, to membership inference attacks\nwithout requiring reference models. Empirical analysis shows loss distributions\nto be asymmetric and heavy-tailed and suggests that most points at risk from\nMIAs have moved from the tail (high-loss region) to the head (low-loss region)\nof the distribution after training. We leverage this insight to propose a\nmethod to estimate model-level vulnerability from the training and testing\ndistribution alone: using the absence of outliers from the high-loss region as\na predictor of the risk. We evaluate our method, the TNR of a simple loss\nattack, across a wide range of architectures and datasets and show it to\naccurately estimate model-level vulnerability to the SOTA MIA attack (LiRA). We\nalso show our method to outperform both low-cost (few reference models) attacks\nsuch as RMIA and other measures of distribution difference. We finally evaluate\nthe use of non-linear functions to evaluate risk and show the approach to be\npromising to evaluate the risk in large-language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Membership inference attacks (MIAs) have emerged as the standard tool for\nevaluating the privacy risks of AI models. However, state-of-the-art attacks\nrequire training numerous, often computationally expensive, reference models,\nlimiting their practicality. We present a novel approach for estimating\nmodel-level vulnerability, the TPR at low FPR, to membership inference attacks\nwithout requiring reference models. Empirical analysis shows loss distributions\nto be asymmetric and heavy-tailed and suggests that most points at risk from\nMIAs have moved from the tail (high-loss region) to the head (low-loss region)\nof the distribution after training. We leverage this insight to propose a\nmethod to estimate model-level vulnerability from the training and testing\ndistribution alone: using the absence of outliers from the high-loss region as\na predictor of the risk. We evaluate our method, the TNR of a simple loss\nattack, across a wide range of architectures and datasets and show it to\naccurately estimate model-level vulnerability to the SOTA MIA attack (LiRA). We\nalso show our method to outperform both low-cost (few reference models) attacks\nsuch as RMIA and other measures of distribution difference. We finally evaluate\nthe use of non-linear functions to evaluate risk and show the approach to be\npromising to evaluate the risk in large-language models."
                },
                "authors": [
                    {
                        "name": "Euodia Dodd"
                    },
                    {
                        "name": "Nataša Krčo"
                    },
                    {
                        "name": "Igor Shilov"
                    },
                    {
                        "name": "Yves-Alexandre de Montjoye"
                    }
                ],
                "author_detail": {
                    "name": "Yves-Alexandre de Montjoye"
                },
                "author": "Yves-Alexandre de Montjoye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19771v1",
                "updated": "2025-10-22T17:00:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    0,
                    45,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T17:00:45Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    0,
                    45,
                    2,
                    295,
                    0
                ],
                "title": "Beyond Reactivity: Measuring Proactive Problem Solving in LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Reactivity: Measuring Proactive Problem Solving in LLM Agents"
                },
                "summary": "LLM-based agents are increasingly moving towards proactivity: rather than\nawaiting instruction, they exercise agency to anticipate user needs and solve\nthem autonomously. However, evaluating proactivity is challenging; current\nbenchmarks are constrained to localized context, limiting their ability to test\nreasoning across sources and longer time horizons. To address this gap, we\npresent PROBE (Proactive Resolution Of BottlEnecks). PROBE decomposes\nproactivity as a pipeline of three core capabilities: (1) searching for\nunspecified issues, (2) identifying specific bottlenecks, and (3) executing\nappropriate resolutions. We apply PROBE to evaluate leading LLMs and popular\nagentic frameworks, showing that even state-of-the-art models struggle to solve\nthis benchmark. Computing our consistent measurements across frontier LLMs and\nagents, we find that the best end-to-end performance of 40% is achieved by both\nGPT-5 and Claude Opus-4.1. Additionally, we demonstrate the relative\ncapabilities of each model and analyze mutual failure modes. Our results\nhighlight the current limitations of autonomous action in agentic systems, and\nexpose promising future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agents are increasingly moving towards proactivity: rather than\nawaiting instruction, they exercise agency to anticipate user needs and solve\nthem autonomously. However, evaluating proactivity is challenging; current\nbenchmarks are constrained to localized context, limiting their ability to test\nreasoning across sources and longer time horizons. To address this gap, we\npresent PROBE (Proactive Resolution Of BottlEnecks). PROBE decomposes\nproactivity as a pipeline of three core capabilities: (1) searching for\nunspecified issues, (2) identifying specific bottlenecks, and (3) executing\nappropriate resolutions. We apply PROBE to evaluate leading LLMs and popular\nagentic frameworks, showing that even state-of-the-art models struggle to solve\nthis benchmark. Computing our consistent measurements across frontier LLMs and\nagents, we find that the best end-to-end performance of 40% is achieved by both\nGPT-5 and Claude Opus-4.1. Additionally, we demonstrate the relative\ncapabilities of each model and analyze mutual failure modes. Our results\nhighlight the current limitations of autonomous action in agentic systems, and\nexpose promising future research directions."
                },
                "authors": [
                    {
                        "name": "Gil Pasternak"
                    },
                    {
                        "name": "Dheeraj Rajagopal"
                    },
                    {
                        "name": "Julia White"
                    },
                    {
                        "name": "Dhruv Atreja"
                    },
                    {
                        "name": "Matthew Thomas"
                    },
                    {
                        "name": "George Hurn-Maloney"
                    },
                    {
                        "name": "Ash Lewis"
                    }
                ],
                "author_detail": {
                    "name": "Ash Lewis"
                },
                "author": "Ash Lewis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19767v1",
                "updated": "2025-10-22T16:56:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    56,
                    1,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T16:56:01Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    56,
                    1,
                    2,
                    295,
                    0
                ],
                "title": "SmartSwitch: Advancing LLM Reasoning by Overcoming Underthinking via\n  Promoting Deeper Thought Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmartSwitch: Advancing LLM Reasoning by Overcoming Underthinking via\n  Promoting Deeper Thought Exploration"
                },
                "summary": "The long chain-of-thought (LongCoT) capability is central to the recent\nbreakthroughs achieved by large language models in complex reasoning tasks.\nHowever, the accompanying issue of ''underthinking'', where models exhibit\nshallow reasoning by frequently switching thoughts without sufficient\nexploration, limits both performance and token efficiency. To address this\nproblem, we propose a simple yet effective reasoning strategy: the SmartSwitch\ninference framework. This framework can be easily integrated into any large\nlanguage model as a plug-and-play solution, continuously monitoring the model's\nreasoning process to detect underthinking and guide it toward deeper\nexploration of promising but overlooked thoughts. Specifically, the perception\nmodule identifies points where thoughts switch and evaluates the potential of\nthe preceding thought using an off-the-shelf process reward model (PRM). If a\nhigh-potential thought is found to be prematurely abandoned, the intervention\nmodule interrupts the ongoing inference, backtracks to the point before the\nswitch, and inserts a \"deepening prompt\" to encourage further exploration along\nthat promising path. Extensive experiments on challenging mathematical\nreasoning benchmarks demonstrate that our method significantly enhances the\nperformance of various large language models of different sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The long chain-of-thought (LongCoT) capability is central to the recent\nbreakthroughs achieved by large language models in complex reasoning tasks.\nHowever, the accompanying issue of ''underthinking'', where models exhibit\nshallow reasoning by frequently switching thoughts without sufficient\nexploration, limits both performance and token efficiency. To address this\nproblem, we propose a simple yet effective reasoning strategy: the SmartSwitch\ninference framework. This framework can be easily integrated into any large\nlanguage model as a plug-and-play solution, continuously monitoring the model's\nreasoning process to detect underthinking and guide it toward deeper\nexploration of promising but overlooked thoughts. Specifically, the perception\nmodule identifies points where thoughts switch and evaluates the potential of\nthe preceding thought using an off-the-shelf process reward model (PRM). If a\nhigh-potential thought is found to be prematurely abandoned, the intervention\nmodule interrupts the ongoing inference, backtracks to the point before the\nswitch, and inserts a \"deepening prompt\" to encourage further exploration along\nthat promising path. Extensive experiments on challenging mathematical\nreasoning benchmarks demonstrate that our method significantly enhances the\nperformance of various large language models of different sizes."
                },
                "authors": [
                    {
                        "name": "Xichen Zhang"
                    },
                    {
                        "name": "Sitong Wu"
                    },
                    {
                        "name": "Haoru Tan"
                    },
                    {
                        "name": "Shaozuo Yu"
                    },
                    {
                        "name": "Yinghao Zhu"
                    },
                    {
                        "name": "Ziyi He"
                    },
                    {
                        "name": "Jiaya Jia"
                    }
                ],
                "author_detail": {
                    "name": "Jiaya Jia"
                },
                "author": "Jiaya Jia",
                "arxiv_comment": "Code: https://github.com/dvlab-research/SmartSwitch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19764v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19764v1",
                "updated": "2025-10-22T16:50:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    50,
                    0,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T16:50:00Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    50,
                    0,
                    2,
                    295,
                    0
                ],
                "title": "A flexible framework for structural plasticity in GPU-accelerated sparse\n  spiking neural networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A flexible framework for structural plasticity in GPU-accelerated sparse\n  spiking neural networks"
                },
                "summary": "The majority of research in both training Artificial Neural Networks (ANNs)\nand modeling learning in biological brains focuses on synaptic plasticity,\nwhere learning equates to changing the strength of existing connections.\nHowever, in biological brains, structural plasticity - where new connections\nare created and others removed - is also vital, not only for effective learning\nbut also for recovery from damage and optimal resource usage. Inspired by\nstructural plasticity, pruning is often used in machine learning to remove weak\nconnections from trained models to reduce the computational requirements of\ninference. However, the machine learning frameworks typically used for\nbackpropagation-based training of both ANNs and Spiking Neural Networks (SNNs)\nare optimized for dense connectivity, meaning that pruning does not help reduce\nthe training costs of ever-larger models. The GeNN simulator already supports\nefficient GPU-accelerated simulation of sparse SNNs for computational\nneuroscience and machine learning. Here, we present a new flexible framework\nfor implementing GPU-accelerated structural plasticity rules and demonstrate\nthis first using the e-prop supervised learning rule and DEEP R to train\nefficient, sparse SNN classifiers and then, in an unsupervised learning\ncontext, to learn topographic maps. Compared to baseline dense models, our\nsparse classifiers reduce training time by up to 10x while the DEEP R rewiring\nenables them to perform as well as the original models. We demonstrate\ntopographic map formation in faster-than-realtime simulations, provide insights\ninto the connectivity evolution, and measure simulation speed versus network\nsize. The proposed framework will enable further research into achieving and\nmaintaining sparsity in network structure and neural communication, as well as\nexploring the computational benefits of sparsity in a range of neuromorphic\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The majority of research in both training Artificial Neural Networks (ANNs)\nand modeling learning in biological brains focuses on synaptic plasticity,\nwhere learning equates to changing the strength of existing connections.\nHowever, in biological brains, structural plasticity - where new connections\nare created and others removed - is also vital, not only for effective learning\nbut also for recovery from damage and optimal resource usage. Inspired by\nstructural plasticity, pruning is often used in machine learning to remove weak\nconnections from trained models to reduce the computational requirements of\ninference. However, the machine learning frameworks typically used for\nbackpropagation-based training of both ANNs and Spiking Neural Networks (SNNs)\nare optimized for dense connectivity, meaning that pruning does not help reduce\nthe training costs of ever-larger models. The GeNN simulator already supports\nefficient GPU-accelerated simulation of sparse SNNs for computational\nneuroscience and machine learning. Here, we present a new flexible framework\nfor implementing GPU-accelerated structural plasticity rules and demonstrate\nthis first using the e-prop supervised learning rule and DEEP R to train\nefficient, sparse SNN classifiers and then, in an unsupervised learning\ncontext, to learn topographic maps. Compared to baseline dense models, our\nsparse classifiers reduce training time by up to 10x while the DEEP R rewiring\nenables them to perform as well as the original models. We demonstrate\ntopographic map formation in faster-than-realtime simulations, provide insights\ninto the connectivity evolution, and measure simulation speed versus network\nsize. The proposed framework will enable further research into achieving and\nmaintaining sparsity in network structure and neural communication, as well as\nexploring the computational benefits of sparsity in a range of neuromorphic\napplications."
                },
                "authors": [
                    {
                        "name": "James C. Knight"
                    },
                    {
                        "name": "Johanna Senk"
                    },
                    {
                        "name": "Thomas Nowotny"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Nowotny"
                },
                "author": "Thomas Nowotny",
                "arxiv_comment": "22 pages, 9 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19764v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19764v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19755v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19755v2",
                "updated": "2025-10-23T09:09:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    9,
                    9,
                    15,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-22T16:46:05Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    46,
                    5,
                    2,
                    295,
                    0
                ],
                "title": "A Survey on Cache Methods in Diffusion Models: Toward Efficient\n  Multi-Modal Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Cache Methods in Diffusion Models: Toward Efficient\n  Multi-Modal Generation"
                },
                "summary": "Diffusion Models have become a cornerstone of modern generative AI for their\nexceptional generation quality and controllability. However, their inherent\n\\textit{multi-step iterations} and \\textit{complex backbone networks} lead to\nprohibitive computational overhead and generation latency, forming a major\nbottleneck for real-time applications. Although existing acceleration\ntechniques have made progress, they still face challenges such as limited\napplicability, high training costs, or quality degradation.\n  Against this backdrop, \\textbf{Diffusion Caching} offers a promising\ntraining-free, architecture-agnostic, and efficient inference paradigm. Its\ncore mechanism identifies and reuses intrinsic computational redundancies in\nthe diffusion process. By enabling feature-level cross-step reuse and\ninter-layer scheduling, it reduces computation without modifying model\nparameters. This paper systematically reviews the theoretical foundations and\nevolution of Diffusion Caching and proposes a unified framework for its\nclassification and analysis.\n  Through comparative analysis of representative methods, we show that\nDiffusion Caching evolves from \\textit{static reuse} to \\textit{dynamic\nprediction}. This trend enhances caching flexibility across diverse tasks and\nenables integration with other acceleration techniques such as sampling\noptimization and model distillation, paving the way for a unified, efficient\ninference framework for future multimodal and interactive applications. We\nargue that this paradigm will become a key enabler of real-time and efficient\ngenerative AI, injecting new vitality into both theory and practice of\n\\textit{Efficient Generative Intelligence}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Models have become a cornerstone of modern generative AI for their\nexceptional generation quality and controllability. However, their inherent\n\\textit{multi-step iterations} and \\textit{complex backbone networks} lead to\nprohibitive computational overhead and generation latency, forming a major\nbottleneck for real-time applications. Although existing acceleration\ntechniques have made progress, they still face challenges such as limited\napplicability, high training costs, or quality degradation.\n  Against this backdrop, \\textbf{Diffusion Caching} offers a promising\ntraining-free, architecture-agnostic, and efficient inference paradigm. Its\ncore mechanism identifies and reuses intrinsic computational redundancies in\nthe diffusion process. By enabling feature-level cross-step reuse and\ninter-layer scheduling, it reduces computation without modifying model\nparameters. This paper systematically reviews the theoretical foundations and\nevolution of Diffusion Caching and proposes a unified framework for its\nclassification and analysis.\n  Through comparative analysis of representative methods, we show that\nDiffusion Caching evolves from \\textit{static reuse} to \\textit{dynamic\nprediction}. This trend enhances caching flexibility across diverse tasks and\nenables integration with other acceleration techniques such as sampling\noptimization and model distillation, paving the way for a unified, efficient\ninference framework for future multimodal and interactive applications. We\nargue that this paradigm will become a key enabler of real-time and efficient\ngenerative AI, injecting new vitality into both theory and practice of\n\\textit{Efficient Generative Intelligence}."
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Yuqi Lin"
                    },
                    {
                        "name": "Zhikai Wang"
                    },
                    {
                        "name": "Peiru Wang"
                    },
                    {
                        "name": "Peiliang Cai"
                    },
                    {
                        "name": "Qinming Zhou"
                    },
                    {
                        "name": "Zhengan Yan"
                    },
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Zhengyi Shi"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "22 pages,2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19755v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19755v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19752v1",
                "updated": "2025-10-22T16:43:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    43,
                    29,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T16:43:29Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    43,
                    29,
                    2,
                    295,
                    0
                ],
                "title": "Learning Affordances at Inference-Time for Vision-Language-Action Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Affordances at Inference-Time for Vision-Language-Action Models"
                },
                "summary": "Solving complex real-world control tasks often takes multiple tries: if we\nfail at first, we reflect on what went wrong, and change our strategy\naccordingly to avoid making the same mistake. In robotics,\nVision-Language-Action models (VLAs) offer a promising path towards solving\ncomplex control tasks, but lack the ability to contextually and dynamically\nreadjust behavior when they fail to accomplish a task. In this work, we\nintroduce Learning from Inference-Time Execution (LITEN), which connects a VLA\nlow-level policy to a high-level VLM that conditions on past experiences by\nincluding them in-context, allowing it to learn the affordances and\ncapabilities of the low-level VLA. Our approach iterates between a reasoning\nphase that generates and executes plans for the low-level VLA, and an\nassessment phase that reflects on the resulting execution and draws useful\nconclusions to be included in future reasoning contexts. Unlike similar\napproaches to self-refinement in non-robotics domains, LITEN must reflect on\nunstructured real-world robot trajectories (e.g., raw videos), which requires\nstructured guiderails during assessment. Our experimental results demonstrate\nLITEN is able to effectively learn from past experience to generate plans that\nuse high-affordance instructions to accomplish long-horizon tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving complex real-world control tasks often takes multiple tries: if we\nfail at first, we reflect on what went wrong, and change our strategy\naccordingly to avoid making the same mistake. In robotics,\nVision-Language-Action models (VLAs) offer a promising path towards solving\ncomplex control tasks, but lack the ability to contextually and dynamically\nreadjust behavior when they fail to accomplish a task. In this work, we\nintroduce Learning from Inference-Time Execution (LITEN), which connects a VLA\nlow-level policy to a high-level VLM that conditions on past experiences by\nincluding them in-context, allowing it to learn the affordances and\ncapabilities of the low-level VLA. Our approach iterates between a reasoning\nphase that generates and executes plans for the low-level VLA, and an\nassessment phase that reflects on the resulting execution and draws useful\nconclusions to be included in future reasoning contexts. Unlike similar\napproaches to self-refinement in non-robotics domains, LITEN must reflect on\nunstructured real-world robot trajectories (e.g., raw videos), which requires\nstructured guiderails during assessment. Our experimental results demonstrate\nLITEN is able to effectively learn from past experience to generate plans that\nuse high-affordance instructions to accomplish long-horizon tasks."
                },
                "authors": [
                    {
                        "name": "Ameesh Shah"
                    },
                    {
                        "name": "William Chen"
                    },
                    {
                        "name": "Adwait Godbole"
                    },
                    {
                        "name": "Federico Mora"
                    },
                    {
                        "name": "Sanjit A. Seshia"
                    },
                    {
                        "name": "Sergey Levine"
                    }
                ],
                "author_detail": {
                    "name": "Sergey Levine"
                },
                "author": "Sergey Levine",
                "arxiv_comment": "7 pages and appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T40",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.9; I.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19747v1",
                "updated": "2025-10-22T16:41:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    41,
                    16,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T16:41:16Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    41,
                    16,
                    2,
                    295,
                    0
                ],
                "title": "Review of Tools for Zero-Code LLM Based Application Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Review of Tools for Zero-Code LLM Based Application Development"
                },
                "summary": "Large Language Models (LLMs) are transforming software creation by enabling\nzero code development platforms. Our survey reviews recent platforms that let\nusers build applications without writing code, by leveraging LLMs as the brains\nof the development process. We adopt a broad survey methodology, categorizing\nplatforms based on key dimensions such as interface style, backend integration,\noutput type, and extensibility. We analyze both dedicated LLM based app\nbuilders (OpenAI's custom GPTs, Bolt.new, Dust.tt, Flowise, Cognosys) and\ngeneral no code platforms (e.g., Bubble, Glide) that integrate LLM\ncapabilities. We present a taxonomy categorizing these platforms by their\ninterface (conversational, visual, etc.), supported LLM backends, output type\n(chatbot, full application, workflow), and degree of extensibility. Core\nfeatures such as autonomous agents, memory management, workflow orchestration,\nand API integrations are in scope of the survey. We provide a detailed\ncomparison, highlighting each platform's strengths and limitations. Trade offs\n(customizability, scalability, vendor lock-in) are discussed in comparison with\ntraditional and low code development approaches. Finally, we outline future\ndirections, including multimodal interfaces, on device LLMs, and improved\norchestration for democratizing app creation with AI. Our findings indicate\nthat while zero code LLM platforms greatly reduce the barrier to creating AI\npowered applications, they still face challenges in flexibility and\nreliability. Overall, the landscape is rapidly evolving, offering exciting\nopportunities to empower non programmers to create sophisticated software.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are transforming software creation by enabling\nzero code development platforms. Our survey reviews recent platforms that let\nusers build applications without writing code, by leveraging LLMs as the brains\nof the development process. We adopt a broad survey methodology, categorizing\nplatforms based on key dimensions such as interface style, backend integration,\noutput type, and extensibility. We analyze both dedicated LLM based app\nbuilders (OpenAI's custom GPTs, Bolt.new, Dust.tt, Flowise, Cognosys) and\ngeneral no code platforms (e.g., Bubble, Glide) that integrate LLM\ncapabilities. We present a taxonomy categorizing these platforms by their\ninterface (conversational, visual, etc.), supported LLM backends, output type\n(chatbot, full application, workflow), and degree of extensibility. Core\nfeatures such as autonomous agents, memory management, workflow orchestration,\nand API integrations are in scope of the survey. We provide a detailed\ncomparison, highlighting each platform's strengths and limitations. Trade offs\n(customizability, scalability, vendor lock-in) are discussed in comparison with\ntraditional and low code development approaches. Finally, we outline future\ndirections, including multimodal interfaces, on device LLMs, and improved\norchestration for democratizing app creation with AI. Our findings indicate\nthat while zero code LLM platforms greatly reduce the barrier to creating AI\npowered applications, they still face challenges in flexibility and\nreliability. Overall, the landscape is rapidly evolving, offering exciting\nopportunities to empower non programmers to create sophisticated software."
                },
                "authors": [
                    {
                        "name": "Priyaranjan Pattnayak"
                    },
                    {
                        "name": "Hussain Bohra"
                    }
                ],
                "author_detail": {
                    "name": "Hussain Bohra"
                },
                "author": "Hussain Bohra",
                "arxiv_comment": "Accepted in 6th World Conference on Artificial Intelligence: Advances\n  and Applications (WCAIAA 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16949v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16949v5",
                "updated": "2025-10-22T16:32:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    32,
                    56,
                    2,
                    295,
                    0
                ],
                "published": "2025-08-23T08:47:31Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    8,
                    47,
                    31,
                    5,
                    235,
                    0
                ],
                "title": "Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement\n  Learning for General LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement\n  Learning for General LLM Reasoning"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have underscored the\npotential of Reinforcement Learning (RL) to facilitate the emergence of\nreasoning capabilities. Despite the encouraging results, a fundamental dilemma\npersists as RL improvement relies on learning from high-quality samples, yet\nthe exploration for such samples remains bounded by the inherent limitations of\nLLMs. This, in effect, creates an undesirable cycle in which what cannot be\nexplored cannot be learned. In this work, we propose Rubric-Scaffolded\nReinforcement Learning (RuscaRL), a novel instructional scaffolding framework\ndesigned to break the exploration bottleneck for general LLM reasoning.\nSpecifically, RuscaRL introduces checklist-style rubrics as (1) explicit\nscaffolding for exploration during rollout generation, where different rubrics\nare provided as external guidance within task instructions to steer diverse\nhigh-quality responses. This guidance is gradually decayed over time,\nencouraging the model to internalize the underlying reasoning patterns; (2)\nverifiable rewards for exploitation during model training, where we can obtain\nrobust LLM-as-a-Judge scores using rubrics as references, enabling effective RL\non general reasoning tasks. Extensive experiments demonstrate the superiority\nof the proposed RuscaRL across various benchmarks, effectively expanding\nreasoning boundaries under the Best-of-N evaluation. Notably, RuscaRL\nsignificantly boosts Qwen2.5-7B-Instruct from 23.6 to 50.3 on HealthBench-500,\nsurpassing GPT-4.1. Furthermore, our fine-tuned variant on\nQwen3-30B-A3B-Instruct achieves 61.1 on HealthBench-500, outperforming leading\nLLMs including OpenAI-o3. Our code is available at\nhttps://github.com/IANNXANG/RuscaRL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have underscored the\npotential of Reinforcement Learning (RL) to facilitate the emergence of\nreasoning capabilities. Despite the encouraging results, a fundamental dilemma\npersists as RL improvement relies on learning from high-quality samples, yet\nthe exploration for such samples remains bounded by the inherent limitations of\nLLMs. This, in effect, creates an undesirable cycle in which what cannot be\nexplored cannot be learned. In this work, we propose Rubric-Scaffolded\nReinforcement Learning (RuscaRL), a novel instructional scaffolding framework\ndesigned to break the exploration bottleneck for general LLM reasoning.\nSpecifically, RuscaRL introduces checklist-style rubrics as (1) explicit\nscaffolding for exploration during rollout generation, where different rubrics\nare provided as external guidance within task instructions to steer diverse\nhigh-quality responses. This guidance is gradually decayed over time,\nencouraging the model to internalize the underlying reasoning patterns; (2)\nverifiable rewards for exploitation during model training, where we can obtain\nrobust LLM-as-a-Judge scores using rubrics as references, enabling effective RL\non general reasoning tasks. Extensive experiments demonstrate the superiority\nof the proposed RuscaRL across various benchmarks, effectively expanding\nreasoning boundaries under the Best-of-N evaluation. Notably, RuscaRL\nsignificantly boosts Qwen2.5-7B-Instruct from 23.6 to 50.3 on HealthBench-500,\nsurpassing GPT-4.1. Furthermore, our fine-tuned variant on\nQwen3-30B-A3B-Instruct achieves 61.1 on HealthBench-500, outperforming leading\nLLMs including OpenAI-o3. Our code is available at\nhttps://github.com/IANNXANG/RuscaRL."
                },
                "authors": [
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Sunzhu Li"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Wenkai Fang"
                    },
                    {
                        "name": "Kongcheng Zhang"
                    },
                    {
                        "name": "Jiale Zhao"
                    },
                    {
                        "name": "Jingwen Yang"
                    },
                    {
                        "name": "Yihe Zhou"
                    },
                    {
                        "name": "Jianwei Lv"
                    },
                    {
                        "name": "Tongya Zheng"
                    },
                    {
                        "name": "Hengtong Lu"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Yan Xie"
                    },
                    {
                        "name": "Mingli Song"
                    }
                ],
                "author_detail": {
                    "name": "Mingli Song"
                },
                "author": "Mingli Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16949v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16949v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11791v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11791v2",
                "updated": "2025-10-22T16:27:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    27,
                    29,
                    2,
                    295,
                    0
                ],
                "published": "2025-06-13T13:54:30Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    54,
                    30,
                    4,
                    164,
                    0
                ],
                "title": "SEC-bench: Automated Benchmarking of LLM Agents on Real-World Software\n  Security Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEC-bench: Automated Benchmarking of LLM Agents on Real-World Software\n  Security Tasks"
                },
                "summary": "Rigorous security-focused evaluation of large language model (LLM) agents is\nimperative for establishing trust in their safe deployment throughout the\nsoftware development lifecycle. However, existing benchmarks largely rely on\nsynthetic challenges or simplified vulnerability datasets that fail to capture\nthe complexity and ambiguity encountered by security engineers in practice. We\nintroduce SEC-bench, the first fully automated benchmarking framework for\nevaluating LLM agents on authentic security engineering tasks. SEC-bench\nemploys a novel multi-agent scaffold that automatically constructs code\nrepositories with harnesses, reproduces vulnerabilities in isolated\nenvironments, and generates gold patches for reliable evaluation. Our framework\nautomatically creates high-quality software vulnerability datasets with\nreproducible artifacts at a cost of only $0.87 per instance. Using SEC-bench,\nwe implement two critical software security tasks to rigorously evaluate LLM\nagents' capabilities: proof-of-concept (PoC) generation and vulnerability\npatching. A comprehensive evaluation of state-of-the-art LLM code agents\nreveals significant performance gaps, achieving at most 18.0% success in PoC\ngeneration and 34.0% in vulnerability patching on our complete dataset. These\nresults highlight the crucial steps needed toward developing LLM agents that\nare more practical, intelligent, and autonomous for security engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rigorous security-focused evaluation of large language model (LLM) agents is\nimperative for establishing trust in their safe deployment throughout the\nsoftware development lifecycle. However, existing benchmarks largely rely on\nsynthetic challenges or simplified vulnerability datasets that fail to capture\nthe complexity and ambiguity encountered by security engineers in practice. We\nintroduce SEC-bench, the first fully automated benchmarking framework for\nevaluating LLM agents on authentic security engineering tasks. SEC-bench\nemploys a novel multi-agent scaffold that automatically constructs code\nrepositories with harnesses, reproduces vulnerabilities in isolated\nenvironments, and generates gold patches for reliable evaluation. Our framework\nautomatically creates high-quality software vulnerability datasets with\nreproducible artifacts at a cost of only $0.87 per instance. Using SEC-bench,\nwe implement two critical software security tasks to rigorously evaluate LLM\nagents' capabilities: proof-of-concept (PoC) generation and vulnerability\npatching. A comprehensive evaluation of state-of-the-art LLM code agents\nreveals significant performance gaps, achieving at most 18.0% success in PoC\ngeneration and 34.0% in vulnerability patching on our complete dataset. These\nresults highlight the crucial steps needed toward developing LLM agents that\nare more practical, intelligent, and autonomous for security engineering."
                },
                "authors": [
                    {
                        "name": "Hwiwon Lee"
                    },
                    {
                        "name": "Ziqi Zhang"
                    },
                    {
                        "name": "Hanxiao Lu"
                    },
                    {
                        "name": "Lingming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lingming Zhang"
                },
                "author": "Lingming Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11791v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11791v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02505v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02505v3",
                "updated": "2025-10-22T16:26:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    26,
                    36,
                    2,
                    295,
                    0
                ],
                "published": "2025-01-05T11:04:30Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    11,
                    4,
                    30,
                    6,
                    5,
                    0
                ],
                "title": "Learning when to rank: Estimation of partial rankings from sparse, noisy\n  comparisons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning when to rank: Estimation of partial rankings from sparse, noisy\n  comparisons"
                },
                "summary": "Ranking items based on pairwise comparisons is common, from using match\noutcomes to rank sports teams to using purchase or survey data to rank consumer\nproducts. Statistical inference-based methods such as the Bradley-Terry model,\nwhich extract rankings based on an underlying generative model, have emerged as\nflexible and powerful tools to tackle ranking in empirical data. In situations\nwith limited and/or noisy comparisons, it is often challenging to confidently\ndistinguish the performance of different items based on the evidence available\nin the data. However, most inference-based ranking methods choose to assign\neach item to a unique rank or score, suggesting a meaningful distinction when\nthere is none. Here, we develop a principled nonparametric Bayesian method,\nadaptable to any statistical ranking method, for learning partial rankings\n(rankings with ties) that distinguishes among the ranks of different items only\nwhen there is sufficient evidence available in the data. We develop a fast\nagglomerative algorithm to perform Maximum A Posteriori (MAP) inference of\npartial rankings under our framework and examine the performance of our method\non a variety of real and synthetic network datasets, finding that it frequently\ngives a more parsimonious summary of the data than traditional ranking,\nparticularly when observations are sparse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ranking items based on pairwise comparisons is common, from using match\noutcomes to rank sports teams to using purchase or survey data to rank consumer\nproducts. Statistical inference-based methods such as the Bradley-Terry model,\nwhich extract rankings based on an underlying generative model, have emerged as\nflexible and powerful tools to tackle ranking in empirical data. In situations\nwith limited and/or noisy comparisons, it is often challenging to confidently\ndistinguish the performance of different items based on the evidence available\nin the data. However, most inference-based ranking methods choose to assign\neach item to a unique rank or score, suggesting a meaningful distinction when\nthere is none. Here, we develop a principled nonparametric Bayesian method,\nadaptable to any statistical ranking method, for learning partial rankings\n(rankings with ties) that distinguishes among the ranks of different items only\nwhen there is sufficient evidence available in the data. We develop a fast\nagglomerative algorithm to perform Maximum A Posteriori (MAP) inference of\npartial rankings under our framework and examine the performance of our method\non a variety of real and synthetic network datasets, finding that it frequently\ngives a more parsimonious summary of the data than traditional ranking,\nparticularly when observations are sparse."
                },
                "authors": [
                    {
                        "name": "Sebastian Morel-Balbi"
                    },
                    {
                        "name": "Alec Kirkley"
                    }
                ],
                "author_detail": {
                    "name": "Alec Kirkley"
                },
                "author": "Alec Kirkley",
                "arxiv_comment": "36 pages, 22 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02505v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02505v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19734v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19734v1",
                "updated": "2025-10-22T16:25:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    25,
                    49,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T16:25:49Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    25,
                    49,
                    2,
                    295,
                    0
                ],
                "title": "Statistical Inference for Linear Functionals of Online Least-squares SGD\n  when $t \\gtrsim d^{1+δ}$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Inference for Linear Functionals of Online Least-squares SGD\n  when $t \\gtrsim d^{1+δ}$"
                },
                "summary": "Stochastic Gradient Descent (SGD) has become a cornerstone method in modern\ndata science. However, deploying SGD in high-stakes applications necessitates\nrigorous quantification of its inherent uncertainty. In this work, we establish\n\\emph{non-asymptotic Berry--Esseen bounds} for linear functionals of online\nleast-squares SGD, thereby providing a Gaussian Central Limit Theorem (CLT) in\na \\emph{growing-dimensional regime}. Existing approaches to high-dimensional\ninference for projection parameters, such as~\\cite{chang2023inference}, rely on\ninverting empirical covariance matrices and require at least $t \\gtrsim\nd^{3/2}$ iterations to achieve finite-sample Berry--Esseen guarantees,\nrendering them computationally expensive and restrictive in the allowable\ndimensional scaling. In contrast, we show that a CLT holds for SGD iterates\nwhen the number of iterations grows as $t \\gtrsim d^{1+\\delta}$ for any $\\delta\n> 0$, significantly extending the dimensional regime permitted by prior works\nwhile improving computational efficiency. The proposed online SGD-based\nprocedure operates in $\\mathcal{O}(td)$ time and requires only $\\mathcal{O}(d)$\nmemory, in contrast to the $\\mathcal{O}(td^2 + d^3)$ runtime of\ncovariance-inversion methods. To render the theory practically applicable, we\nfurther develop an \\emph{online variance estimator} for the asymptotic variance\nappearing in the CLT and establish \\emph{high-probability deviation bounds} for\nthis estimator. Collectively, these results yield the first fully online and\ndata-driven framework for constructing confidence intervals for SGD iterates in\nthe near-optimal scaling regime $t \\gtrsim d^{1+\\delta}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic Gradient Descent (SGD) has become a cornerstone method in modern\ndata science. However, deploying SGD in high-stakes applications necessitates\nrigorous quantification of its inherent uncertainty. In this work, we establish\n\\emph{non-asymptotic Berry--Esseen bounds} for linear functionals of online\nleast-squares SGD, thereby providing a Gaussian Central Limit Theorem (CLT) in\na \\emph{growing-dimensional regime}. Existing approaches to high-dimensional\ninference for projection parameters, such as~\\cite{chang2023inference}, rely on\ninverting empirical covariance matrices and require at least $t \\gtrsim\nd^{3/2}$ iterations to achieve finite-sample Berry--Esseen guarantees,\nrendering them computationally expensive and restrictive in the allowable\ndimensional scaling. In contrast, we show that a CLT holds for SGD iterates\nwhen the number of iterations grows as $t \\gtrsim d^{1+\\delta}$ for any $\\delta\n> 0$, significantly extending the dimensional regime permitted by prior works\nwhile improving computational efficiency. The proposed online SGD-based\nprocedure operates in $\\mathcal{O}(td)$ time and requires only $\\mathcal{O}(d)$\nmemory, in contrast to the $\\mathcal{O}(td^2 + d^3)$ runtime of\ncovariance-inversion methods. To render the theory practically applicable, we\nfurther develop an \\emph{online variance estimator} for the asymptotic variance\nappearing in the CLT and establish \\emph{high-probability deviation bounds} for\nthis estimator. Collectively, these results yield the first fully online and\ndata-driven framework for constructing confidence intervals for SGD iterates in\nthe near-optimal scaling regime $t \\gtrsim d^{1+\\delta}$."
                },
                "authors": [
                    {
                        "name": "Bhavya Agrawalla"
                    },
                    {
                        "name": "Krishnakumar Balasubramanian"
                    },
                    {
                        "name": "Promit Ghosal"
                    }
                ],
                "author_detail": {
                    "name": "Promit Ghosal"
                },
                "author": "Promit Ghosal",
                "arxiv_comment": "Improved version of arXiv:2302.09727 with new results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19734v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19734v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19733v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19733v2",
                "updated": "2025-10-23T09:50:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    9,
                    50,
                    7,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-22T16:25:43Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    25,
                    43,
                    2,
                    295,
                    0
                ],
                "title": "Zhyper: Factorized Hypernetworks for Conditioned LLM Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zhyper: Factorized Hypernetworks for Conditioned LLM Fine-Tuning"
                },
                "summary": "Large Language Model (LLM) conditioning refers to instructing an LLM to\ngenerate content in accordance with the norms and values of a specific culture,\nbeliefs of a particular political orientation, or any desired text-specified\nsemantic conditioning. Unfortunately, prompt engineering does not ensure that\nLLMs behave in accordance with a desired conditioning due to the inductive bias\nof the pre-training and alignment datasets. Prior works have focused on\nfine-tuning LLMs by directly conditioning the LoRA weights; however, such\nmethods introduce a large number of parameters. As a remedy, we propose Zhyper,\na parameter-efficient factorized hypernetwork framework that generates\ncontext-aware LoRA adapters from textual descriptions. Experiments on multiple\nbenchmarks show that Zhyper achieves competitive performance with up to 26x\nfewer parameters than the state-of-the-art baselines. Furthermore, we extend\nZhyper to cultural alignment, demonstrating improved generalization to\nout-of-domain settings and a better capturing of fine-grained contextual\nvalues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) conditioning refers to instructing an LLM to\ngenerate content in accordance with the norms and values of a specific culture,\nbeliefs of a particular political orientation, or any desired text-specified\nsemantic conditioning. Unfortunately, prompt engineering does not ensure that\nLLMs behave in accordance with a desired conditioning due to the inductive bias\nof the pre-training and alignment datasets. Prior works have focused on\nfine-tuning LLMs by directly conditioning the LoRA weights; however, such\nmethods introduce a large number of parameters. As a remedy, we propose Zhyper,\na parameter-efficient factorized hypernetwork framework that generates\ncontext-aware LoRA adapters from textual descriptions. Experiments on multiple\nbenchmarks show that Zhyper achieves competitive performance with up to 26x\nfewer parameters than the state-of-the-art baselines. Furthermore, we extend\nZhyper to cultural alignment, demonstrating improved generalization to\nout-of-domain settings and a better capturing of fine-grained contextual\nvalues."
                },
                "authors": [
                    {
                        "name": "M. H. I. Abdalla"
                    },
                    {
                        "name": "Zhipin Wang"
                    },
                    {
                        "name": "Christian Frey"
                    },
                    {
                        "name": "Steffen Eger"
                    },
                    {
                        "name": "Josif Grabocka"
                    }
                ],
                "author_detail": {
                    "name": "Josif Grabocka"
                },
                "author": "Josif Grabocka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19733v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19733v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01069v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01069v2",
                "updated": "2025-10-22T16:24:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    24,
                    57,
                    2,
                    295,
                    0
                ],
                "published": "2024-12-02T03:08:18Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    3,
                    8,
                    18,
                    0,
                    337,
                    0
                ],
                "title": "The Promise and Peril of Generative AI: Evidence from GPT as Sell-Side\n  Analysts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Promise and Peril of Generative AI: Evidence from GPT as Sell-Side\n  Analysts"
                },
                "summary": "Large language models (LLMs) promise to democratize financial analysis by\nreducing information-processing costs. Yet equal access does not ensure equal\noutcomes, as the locus of friction may shift from processing information to\nevaluating model outputs. We study GPT's earnings forecasts following corporate\nearnings releases and document two patterns. First, GPT's narrative attention\nis consistent and human-like but not always associated with higher forecast\naccuracy. Second, its quantitative reasoning varies substantially across\ncontexts, challenging the view that LLMs are uniformly weak at numerical tasks.\nBuilding on these insights, we propose a diagnostic framework that links\nforecast accuracy to observable processing features (i.e., narrative focus,\nnumerical reasoning, and self-assessed confidence). These indicators serve as\nproxies for this new form of information friction and alert investors when to\nexercise caution. Our study has implications for information frictions,\nregulatory oversight, and the economics of AI-mediated financial markets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) promise to democratize financial analysis by\nreducing information-processing costs. Yet equal access does not ensure equal\noutcomes, as the locus of friction may shift from processing information to\nevaluating model outputs. We study GPT's earnings forecasts following corporate\nearnings releases and document two patterns. First, GPT's narrative attention\nis consistent and human-like but not always associated with higher forecast\naccuracy. Second, its quantitative reasoning varies substantially across\ncontexts, challenging the view that LLMs are uniformly weak at numerical tasks.\nBuilding on these insights, we propose a diagnostic framework that links\nforecast accuracy to observable processing features (i.e., narrative focus,\nnumerical reasoning, and self-assessed confidence). These indicators serve as\nproxies for this new form of information friction and alert investors when to\nexercise caution. Our study has implications for information frictions,\nregulatory oversight, and the economics of AI-mediated financial markets."
                },
                "authors": [
                    {
                        "name": "Edward Li"
                    },
                    {
                        "name": "Min Shen"
                    },
                    {
                        "name": "Zhiyuan Tu"
                    },
                    {
                        "name": "Dexin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Dexin Zhou"
                },
                "author": "Dexin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01069v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01069v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19732v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19732v1",
                "updated": "2025-10-22T16:24:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    24,
                    47,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T16:24:47Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    24,
                    47,
                    2,
                    295,
                    0
                ],
                "title": "Memo: Training Memory-Efficient Embodied Agents with Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memo: Training Memory-Efficient Embodied Agents with Reinforcement\n  Learning"
                },
                "summary": "To enable embodied agents to operate effectively over extended timeframes, it\nis crucial to develop models that form and access memories to stay\ncontextualized in their environment. In the current paradigm of training\ntransformer-based policies for embodied sequential decision-making tasks,\nvisual inputs often overwhelm the context limits of transformers, while humans\ncan maintain and utilize a lifetime of experience compressed as memories.\nSignificant compression is possible in principle, as much of the input is\nirrelevant and can be abstracted. However, existing approaches predominantly\nfocus on either recurrent models with fixed-size memory or transformers with\nfull-context reliance. In this work, we propose Memo, a transformer-based\narchitecture and training recipe for reinforcement learning (RL) on\nmemory-intensive, long-horizon tasks. Memo incorporates the creation and\nretrieval of memory by interleaving periodic summarization tokens with the\ninputs of a model during training. We demonstrate Memo's effectiveness on a\ngridworld meta-RL benchmark and a multi-object navigation task in\nphoto-realistic indoor settings. Memo outperforms naive long-context\ntransformer baselines while being more compute and storage efficient.\nAdditionally, Memo generalizes better to longer contexts at inference time and\nremains robust in streaming settings, where historical context must be\ntruncated to fit inference constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To enable embodied agents to operate effectively over extended timeframes, it\nis crucial to develop models that form and access memories to stay\ncontextualized in their environment. In the current paradigm of training\ntransformer-based policies for embodied sequential decision-making tasks,\nvisual inputs often overwhelm the context limits of transformers, while humans\ncan maintain and utilize a lifetime of experience compressed as memories.\nSignificant compression is possible in principle, as much of the input is\nirrelevant and can be abstracted. However, existing approaches predominantly\nfocus on either recurrent models with fixed-size memory or transformers with\nfull-context reliance. In this work, we propose Memo, a transformer-based\narchitecture and training recipe for reinforcement learning (RL) on\nmemory-intensive, long-horizon tasks. Memo incorporates the creation and\nretrieval of memory by interleaving periodic summarization tokens with the\ninputs of a model during training. We demonstrate Memo's effectiveness on a\ngridworld meta-RL benchmark and a multi-object navigation task in\nphoto-realistic indoor settings. Memo outperforms naive long-context\ntransformer baselines while being more compute and storage efficient.\nAdditionally, Memo generalizes better to longer contexts at inference time and\nremains robust in streaming settings, where historical context must be\ntruncated to fit inference constraints."
                },
                "authors": [
                    {
                        "name": "Gunshi Gupta"
                    },
                    {
                        "name": "Karmesh Yadav"
                    },
                    {
                        "name": "Zsolt Kira"
                    },
                    {
                        "name": "Yarin Gal"
                    },
                    {
                        "name": "Rahaf Aljundi"
                    }
                ],
                "author_detail": {
                    "name": "Rahaf Aljundi"
                },
                "author": "Rahaf Aljundi",
                "arxiv_comment": "Accepted for Spotlight Presentation at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19732v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19732v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25486v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25486v2",
                "updated": "2025-10-22T16:24:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    24,
                    4,
                    2,
                    295,
                    0
                ],
                "published": "2025-09-29T20:40:54Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    20,
                    40,
                    54,
                    0,
                    272,
                    0
                ],
                "title": "Scalable Boltzmann Generators for equilibrium sampling of large-scale\n  materials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Boltzmann Generators for equilibrium sampling of large-scale\n  materials"
                },
                "summary": "The use of generative models to sample equilibrium distributions of many-body\nsystems, as first demonstrated by Boltzmann Generators, has attracted\nsubstantial interest due to their ability to produce unbiased and uncorrelated\nsamples in `one shot'. Despite their promise and impressive results across the\nnatural sciences, scaling these models to large systems remains a major\nchallenge. In this work, we introduce a Boltzmann Generator architecture that\naddresses this scalability bottleneck with a focus on applications in materials\nscience. We leverage augmented coupling flows in combination with graph neural\nnetworks to base the generation process on local environmental information,\nwhile allowing for energy-based training and fast inference. Compared to\nprevious architectures, our model trains significantly faster, requires far\nless computational resources, and achieves superior sampling efficiencies.\nCrucially, the architecture is transferable to larger system sizes, which\nallows for the efficient sampling of materials with simulation cells of\nunprecedented size. We demonstrate the potential of our approach by applying it\nto several materials systems, including Lennard-Jones crystals, ice phases of\nmW water, and the phase diagram of silicon, for system sizes well above one\nthousand atoms. The trained Boltzmann Generators produce highly accurate\nequilibrium ensembles for various crystal structures, as well as Helmholtz and\nGibbs free energies across a range of system sizes, able to reach scales where\nfinite-size effects become negligible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of generative models to sample equilibrium distributions of many-body\nsystems, as first demonstrated by Boltzmann Generators, has attracted\nsubstantial interest due to their ability to produce unbiased and uncorrelated\nsamples in `one shot'. Despite their promise and impressive results across the\nnatural sciences, scaling these models to large systems remains a major\nchallenge. In this work, we introduce a Boltzmann Generator architecture that\naddresses this scalability bottleneck with a focus on applications in materials\nscience. We leverage augmented coupling flows in combination with graph neural\nnetworks to base the generation process on local environmental information,\nwhile allowing for energy-based training and fast inference. Compared to\nprevious architectures, our model trains significantly faster, requires far\nless computational resources, and achieves superior sampling efficiencies.\nCrucially, the architecture is transferable to larger system sizes, which\nallows for the efficient sampling of materials with simulation cells of\nunprecedented size. We demonstrate the potential of our approach by applying it\nto several materials systems, including Lennard-Jones crystals, ice phases of\nmW water, and the phase diagram of silicon, for system sizes well above one\nthousand atoms. The trained Boltzmann Generators produce highly accurate\nequilibrium ensembles for various crystal structures, as well as Helmholtz and\nGibbs free energies across a range of system sizes, able to reach scales where\nfinite-size effects become negligible."
                },
                "authors": [
                    {
                        "name": "Maximilian Schebek"
                    },
                    {
                        "name": "Frank Noé"
                    },
                    {
                        "name": "Jutta Rogal"
                    }
                ],
                "author_detail": {
                    "name": "Jutta Rogal"
                },
                "author": "Jutta Rogal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25486v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25486v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00937v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00937v3",
                "updated": "2025-10-22T16:21:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    21,
                    57,
                    2,
                    295,
                    0
                ],
                "published": "2025-02-02T22:10:40Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    22,
                    10,
                    40,
                    6,
                    33,
                    0
                ],
                "title": "ModServe: Modality- and Stage-Aware Resource Disaggregation for Scalable\n  Multimodal Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ModServe: Modality- and Stage-Aware Resource Disaggregation for Scalable\n  Multimodal Model Serving"
                },
                "summary": "Large multimodal models (LMMs) demonstrate impressive capabilities in\nunderstanding images, videos, and audio beyond text. However, efficiently\nserving LMMs in production environments poses significant challenges due to\ntheir complex architectures and heterogeneous characteristics across their\nmulti-stage inference pipelines. We present the first comprehensive systems\nanalysis of two prominent LMM architectures, decoder-only and cross-attention,\nacross six representative open-source models, revealing key systems design\nimplications. We also present an in-depth analysis of production LMM inference\ntraces, uncovering unique workload characteristics, including variable,\nheavy-tailed request distributions and bursty traffic patterns. Based on these\ninsights, we propose ModServe, a modular LMM serving system that decouples\nstages for independent optimization and adaptive scaling. ModServe dynamically\nreconfigures stages and handles bursty traffic with modality-aware scheduling\nand autoscaling to meet tail latency SLOs while minimizing costs. ModServe\nachieves 3.3-5.5x higher throughput (leading to 25-41.3% cost saving) while\nmeeting SLOs on a 128-GPU cluster with production traces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large multimodal models (LMMs) demonstrate impressive capabilities in\nunderstanding images, videos, and audio beyond text. However, efficiently\nserving LMMs in production environments poses significant challenges due to\ntheir complex architectures and heterogeneous characteristics across their\nmulti-stage inference pipelines. We present the first comprehensive systems\nanalysis of two prominent LMM architectures, decoder-only and cross-attention,\nacross six representative open-source models, revealing key systems design\nimplications. We also present an in-depth analysis of production LMM inference\ntraces, uncovering unique workload characteristics, including variable,\nheavy-tailed request distributions and bursty traffic patterns. Based on these\ninsights, we propose ModServe, a modular LMM serving system that decouples\nstages for independent optimization and adaptive scaling. ModServe dynamically\nreconfigures stages and handles bursty traffic with modality-aware scheduling\nand autoscaling to meet tail latency SLOs while minimizing costs. ModServe\nachieves 3.3-5.5x higher throughput (leading to 25-41.3% cost saving) while\nmeeting SLOs on a 128-GPU cluster with production traces."
                },
                "authors": [
                    {
                        "name": "Haoran Qiu"
                    },
                    {
                        "name": "Anish Biswas"
                    },
                    {
                        "name": "Zihan Zhao"
                    },
                    {
                        "name": "Jayashree Mohan"
                    },
                    {
                        "name": "Alind Khare"
                    },
                    {
                        "name": "Esha Choukse"
                    },
                    {
                        "name": "Íñigo Goiri"
                    },
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Chetan Bansal"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Rodrigo Fonseca"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo Fonseca"
                },
                "author": "Rodrigo Fonseca",
                "arxiv_comment": "Published at ACM SoCC 2025; 14 pages, 20 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00937v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00937v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00939v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00939v2",
                "updated": "2025-10-22T16:17:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    17,
                    16,
                    2,
                    295,
                    0
                ],
                "published": "2025-04-01T16:22:15Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    16,
                    22,
                    15,
                    1,
                    91,
                    0
                ],
                "title": "WikiVideo: Article Generation from Multiple Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WikiVideo: Article Generation from Multiple Videos"
                },
                "summary": "We introduce the task of grounded article generation with the goal of\ncreating a Wikipedia-style article from multiple diverse videos about\nreal-world events -- from natural disasters to political elections -- where all\nthe information in the article is supported by video evidence. Videos are\nintuitive sources for retrieval-augmented generation (RAG), but most\ncontemporary RAG workflows focus heavily on text while existing methods for\nvideo-based summarization focus on low-level scene understanding rather than\nhigh-level event semantics. To close this gap, we introduce WikiVideo, a\nbenchmark consisting of expert-written articles and densely annotated videos\nthat provide evidence for articles' claims, facilitating the integration of\nvideo into RAG pipelines and enabling the creation of in-depth content that is\ngrounded in multimodal sources. We further propose Collaborative Article\nGeneration (CAG), a novel interactive method for article creation from multiple\nvideos. CAG leverages an iterative interaction between an r1-style reasoning\nmodel and a VideoLLM to draw higher-level inferences about the target event\nthan is possible with VideoLLMs alone, which fixate on low-level visual\nfeatures. We benchmark state-of-the-art VideoLLMs and CAG in both oracle\nretrieval and RAG settings and find that CAG consistently outperforms\nalternative methods, while suggesting intriguing avenues for future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the task of grounded article generation with the goal of\ncreating a Wikipedia-style article from multiple diverse videos about\nreal-world events -- from natural disasters to political elections -- where all\nthe information in the article is supported by video evidence. Videos are\nintuitive sources for retrieval-augmented generation (RAG), but most\ncontemporary RAG workflows focus heavily on text while existing methods for\nvideo-based summarization focus on low-level scene understanding rather than\nhigh-level event semantics. To close this gap, we introduce WikiVideo, a\nbenchmark consisting of expert-written articles and densely annotated videos\nthat provide evidence for articles' claims, facilitating the integration of\nvideo into RAG pipelines and enabling the creation of in-depth content that is\ngrounded in multimodal sources. We further propose Collaborative Article\nGeneration (CAG), a novel interactive method for article creation from multiple\nvideos. CAG leverages an iterative interaction between an r1-style reasoning\nmodel and a VideoLLM to draw higher-level inferences about the target event\nthan is possible with VideoLLMs alone, which fixate on low-level visual\nfeatures. We benchmark state-of-the-art VideoLLMs and CAG in both oracle\nretrieval and RAG settings and find that CAG consistently outperforms\nalternative methods, while suggesting intriguing avenues for future work."
                },
                "authors": [
                    {
                        "name": "Alexander Martin"
                    },
                    {
                        "name": "Reno Kriz"
                    },
                    {
                        "name": "William Gantt Walden"
                    },
                    {
                        "name": "Kate Sanders"
                    },
                    {
                        "name": "Hannah Recknor"
                    },
                    {
                        "name": "Eugene Yang"
                    },
                    {
                        "name": "Francis Ferraro"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme",
                "arxiv_comment": "Repo can be found here: https://github.com/alexmartin1722/wikivideo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00939v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00939v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18129v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18129v2",
                "updated": "2025-10-22T16:12:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    12,
                    30,
                    2,
                    295,
                    0
                ],
                "published": "2025-03-23T16:20:14Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    16,
                    20,
                    14,
                    6,
                    82,
                    0
                ],
                "title": "GeoBenchX: Benchmarking LLMs in Agent Solving Multistep Geospatial Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GeoBenchX: Benchmarking LLMs in Agent Solving Multistep Geospatial Tasks"
                },
                "summary": "This paper establishes a benchmark for evaluating tool-calling capabilities\nof large language models (LLMs) on multi-step geospatial tasks relevant to\ncommercial GIS practitioners. We assess eight commercial LLMs (Claude Sonnet\n3.5 and 4, Claude Haiku 3.5, Gemini 2.0 Flash, Gemini 2.5 Pro Preview, GPT-4o,\nGPT-4.1 and o4-mini) using a simple tool-calling agent equipped with 23\ngeospatial functions. Our benchmark comprises tasks in four categories of\nincreasing complexity, with both solvable and intentionally unsolvable tasks to\ntest rejection accuracy. We develop a LLM-as-Judge evaluation framework to\ncompare agent solutions against reference solutions. Results show o4-mini and\nClaude 3.5 Sonnet achieve the best overall performance, OpenAI's GPT-4.1,\nGPT-4o and Google's Gemini 2.5 Pro Preview do not fall far behind, but the last\ntwo are more efficient in identifying unsolvable tasks. Claude Sonnet 4, due\nits preference to provide any solution rather than reject a task, proved to be\nless accurate. We observe significant differences in token usage, with\nAnthropic models consuming more tokens than competitors. Common errors include\nmisunderstanding geometrical relationships, relying on outdated knowledge, and\ninefficient data manipulation. The resulting benchmark set, evaluation\nframework, and data generation pipeline are released as open-source resources\n(available at https://github.com/Solirinai/GeoBenchX), providing one more\nstandardized method for the ongoing evaluation of LLMs for GeoAI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper establishes a benchmark for evaluating tool-calling capabilities\nof large language models (LLMs) on multi-step geospatial tasks relevant to\ncommercial GIS practitioners. We assess eight commercial LLMs (Claude Sonnet\n3.5 and 4, Claude Haiku 3.5, Gemini 2.0 Flash, Gemini 2.5 Pro Preview, GPT-4o,\nGPT-4.1 and o4-mini) using a simple tool-calling agent equipped with 23\ngeospatial functions. Our benchmark comprises tasks in four categories of\nincreasing complexity, with both solvable and intentionally unsolvable tasks to\ntest rejection accuracy. We develop a LLM-as-Judge evaluation framework to\ncompare agent solutions against reference solutions. Results show o4-mini and\nClaude 3.5 Sonnet achieve the best overall performance, OpenAI's GPT-4.1,\nGPT-4o and Google's Gemini 2.5 Pro Preview do not fall far behind, but the last\ntwo are more efficient in identifying unsolvable tasks. Claude Sonnet 4, due\nits preference to provide any solution rather than reject a task, proved to be\nless accurate. We observe significant differences in token usage, with\nAnthropic models consuming more tokens than competitors. Common errors include\nmisunderstanding geometrical relationships, relying on outdated knowledge, and\ninefficient data manipulation. The resulting benchmark set, evaluation\nframework, and data generation pipeline are released as open-source resources\n(available at https://github.com/Solirinai/GeoBenchX), providing one more\nstandardized method for the ongoing evaluation of LLMs for GeoAI."
                },
                "authors": [
                    {
                        "name": "Varvara Krechetova"
                    },
                    {
                        "name": "Denis Kochedykov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Kochedykov"
                },
                "author": "Denis Kochedykov",
                "arxiv_comment": "Github with code and benchmark set:\n  https://github.com/Solirinai/GeoBenchX",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18129v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18129v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19722v1",
                "updated": "2025-10-22T16:07:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    7,
                    57,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T16:07:57Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    7,
                    57,
                    2,
                    295,
                    0
                ],
                "title": "Semi-Implicit Approaches for Large-Scale Bayesian Spatial Interpolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-Implicit Approaches for Large-Scale Bayesian Spatial Interpolation"
                },
                "summary": "Spatial statistics often rely on Gaussian processes (GPs) to capture\ndependencies across locations. However, their computational cost increases\nrapidly with the number of locations, potentially needing multiple hours even\nfor moderate sample sizes. To address this, we propose using Semi-Implicit\nVariational Inference (SIVI), a highly flexible Bayesian approximation method,\nfor scalable Bayesian spatial interpolation. We evaluated SIVI with a GP prior\nand a Nearest-Neighbour Gaussian Process (NNGP) prior compared to Automatic\nDifferentiation Variational Inference (ADVI), Pathfinder, and Hamiltonian Monte\nCarlo (HMC), the reference method in spatial statistics. Methods were compared\nbased on their predictive ability measured by the CRPS, the interval score, and\nthe negative log-predictive density across 50 replicates for both Gaussian and\nPoisson outcomes. SIVI-based methods achieved similar results to HMC, while\nbeing drastically faster. On average, for the Poisson scenario with 500\ntraining locations, SIVI reduced the computational time from roughly 6 hours\nfor HMC to 130 seconds. Furthermore, SIVI-NNGP analyzed a simulated land\nsurface temperature dataset of 150,000 locations while estimating all unknown\nmodel parameters in under two minutes. These results highlight the potential of\nSIVI as a flexible and scalable inference technique in spatial statistics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial statistics often rely on Gaussian processes (GPs) to capture\ndependencies across locations. However, their computational cost increases\nrapidly with the number of locations, potentially needing multiple hours even\nfor moderate sample sizes. To address this, we propose using Semi-Implicit\nVariational Inference (SIVI), a highly flexible Bayesian approximation method,\nfor scalable Bayesian spatial interpolation. We evaluated SIVI with a GP prior\nand a Nearest-Neighbour Gaussian Process (NNGP) prior compared to Automatic\nDifferentiation Variational Inference (ADVI), Pathfinder, and Hamiltonian Monte\nCarlo (HMC), the reference method in spatial statistics. Methods were compared\nbased on their predictive ability measured by the CRPS, the interval score, and\nthe negative log-predictive density across 50 replicates for both Gaussian and\nPoisson outcomes. SIVI-based methods achieved similar results to HMC, while\nbeing drastically faster. On average, for the Poisson scenario with 500\ntraining locations, SIVI reduced the computational time from roughly 6 hours\nfor HMC to 130 seconds. Furthermore, SIVI-NNGP analyzed a simulated land\nsurface temperature dataset of 150,000 locations while estimating all unknown\nmodel parameters in under two minutes. These results highlight the potential of\nSIVI as a flexible and scalable inference technique in spatial statistics."
                },
                "authors": [
                    {
                        "name": "Sébastien Garneau"
                    },
                    {
                        "name": "Carlos T. P. Zanini"
                    },
                    {
                        "name": "Alexandra M. Schmidt"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra M. Schmidt"
                },
                "arxiv_affiliation": "Department of Epidemiology, Biostatistics and Occupational Health, McGill University",
                "author": "Alexandra M. Schmidt",
                "arxiv_comment": "36 pages, 5 figures, 2 tables, 1 algorithm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18622v2",
                "updated": "2025-10-22T16:04:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    4,
                    35,
                    2,
                    295,
                    0
                ],
                "published": "2025-04-25T18:00:02Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    18,
                    0,
                    2,
                    4,
                    115,
                    0
                ],
                "title": "Gravitational waves from eccentric binary neutron star mergers:\n  Systematic biases and inadequacy of quasicircular templates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational waves from eccentric binary neutron star mergers:\n  Systematic biases and inadequacy of quasicircular templates"
                },
                "summary": "The use of quasicircular waveforms in matched-filter analyses of signals from\neccentric binary neutron star mergers can lead to biases in the source's\nparameter estimation. We demonstrate that significant biases can be present\nalready for moderate eccentricities $e_{0} \\gtrsim 0.05$ and signals detected\nby LIGO-Virgo-KAGRA with signal-to-noise ratio $\\gtrsim 12$. We perform\nsystematic Bayesian mock analyses of unequal-mass nonspinning binary neutron\nstar signals up to eccentricities $e_0 \\sim 0.1$ using quasicircular\neffective-one-body waveforms with spins. We find fractional signal-to-noise\nratio losses up to tens of percent and up to 16$\\sigma$ deviations in the\ninference of the chirp mass. The latter effect is sufficiently large to lead to\nan incorrect (and ambiguous) source identification. The inclusion of spin\nprecession in the quasicircular waveform does not capture eccentricity effects.\nWe conclude that high-precision observations with advanced (and next\ngeneration) detectors are likely to require standardized, accurate, and fast\neccentric waveforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of quasicircular waveforms in matched-filter analyses of signals from\neccentric binary neutron star mergers can lead to biases in the source's\nparameter estimation. We demonstrate that significant biases can be present\nalready for moderate eccentricities $e_{0} \\gtrsim 0.05$ and signals detected\nby LIGO-Virgo-KAGRA with signal-to-noise ratio $\\gtrsim 12$. We perform\nsystematic Bayesian mock analyses of unequal-mass nonspinning binary neutron\nstar signals up to eccentricities $e_0 \\sim 0.1$ using quasicircular\neffective-one-body waveforms with spins. We find fractional signal-to-noise\nratio losses up to tens of percent and up to 16$\\sigma$ deviations in the\ninference of the chirp mass. The latter effect is sufficiently large to lead to\nan incorrect (and ambiguous) source identification. The inclusion of spin\nprecession in the quasicircular waveform does not capture eccentricity effects.\nWe conclude that high-precision observations with advanced (and next\ngeneration) detectors are likely to require standardized, accurate, and fast\neccentric waveforms."
                },
                "authors": [
                    {
                        "name": "Giulia Huez"
                    },
                    {
                        "name": "Sebastiano Bernuzzi"
                    },
                    {
                        "name": "Matteo Breschi"
                    },
                    {
                        "name": "Rossella Gamba"
                    }
                ],
                "author_detail": {
                    "name": "Rossella Gamba"
                },
                "author": "Rossella Gamba",
                "arxiv_doi": "10.1103/mc64-1jtd",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/mc64-1jtd",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.18622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "13 pages, 12 figures",
                "arxiv_journal_ref": "Phys. Rev. D 112, 084054 (2025)",
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07364v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07364v3",
                "updated": "2025-10-22T16:02:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    2,
                    22,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-08T17:58:28Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    58,
                    28,
                    2,
                    281,
                    0
                ],
                "title": "Base Models Know How to Reason, Thinking Models Learn When",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Base Models Know How to Reason, Thinking Models Learn When"
                },
                "summary": "Why do thinking language models like DeepSeek R1 outperform their base\ncounterparts? Despite consistent performance gains, it remains unclear to what\nextent thinking models learn entirely new reasoning capabilities or repurpose\npre-existing base model ones. In this work, we propose a hybrid model where we\nactivate reasoning mechanisms in base models at the right time to elicit\nthinking-model-level reasoning chains, implying that thinking models exploit\nalready existing capabilities. To ground our analysis, we introduce an\nunsupervised, bottom-up approach for uncovering human-interpretable reasoning\nbehaviors in thinking models. This approach provides an unbiased method to\ndiscover reasoning behaviors without imposing manual or LLM-derived\nassumptions. Across three base and four thinking models, using GSM8K and\nMATH500, our hybrid model recovers up to 91% of the performance gap to thinking\nmodels without any weight updates while steering only 12% of tokens.\nConcretely, our empirical setup provides a simple, causal way to test the\neffectiveness of existing reasoning mechanisms in base models by invoking them\ndirectly and measuring the resulting task performance. More broadly, these\nresults reframe our understanding of how thinking models are trained:\npre-training is when models acquire most of their reasoning mechanisms, and\npost-training teaches efficient deployment of these mechanisms at the right\ntime, enabling efficient use of their inference-time compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why do thinking language models like DeepSeek R1 outperform their base\ncounterparts? Despite consistent performance gains, it remains unclear to what\nextent thinking models learn entirely new reasoning capabilities or repurpose\npre-existing base model ones. In this work, we propose a hybrid model where we\nactivate reasoning mechanisms in base models at the right time to elicit\nthinking-model-level reasoning chains, implying that thinking models exploit\nalready existing capabilities. To ground our analysis, we introduce an\nunsupervised, bottom-up approach for uncovering human-interpretable reasoning\nbehaviors in thinking models. This approach provides an unbiased method to\ndiscover reasoning behaviors without imposing manual or LLM-derived\nassumptions. Across three base and four thinking models, using GSM8K and\nMATH500, our hybrid model recovers up to 91% of the performance gap to thinking\nmodels without any weight updates while steering only 12% of tokens.\nConcretely, our empirical setup provides a simple, causal way to test the\neffectiveness of existing reasoning mechanisms in base models by invoking them\ndirectly and measuring the resulting task performance. More broadly, these\nresults reframe our understanding of how thinking models are trained:\npre-training is when models acquire most of their reasoning mechanisms, and\npost-training teaches efficient deployment of these mechanisms at the right\ntime, enabling efficient use of their inference-time compute."
                },
                "authors": [
                    {
                        "name": "Constantin Venhoff"
                    },
                    {
                        "name": "Iván Arcuschin"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Arthur Conmy"
                    },
                    {
                        "name": "Neel Nanda"
                    }
                ],
                "author_detail": {
                    "name": "Neel Nanda"
                },
                "author": "Neel Nanda",
                "arxiv_comment": "10 pages, Accepted to the Mechanistic Interpretability Workshop at\n  NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07364v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07364v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01735v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01735v3",
                "updated": "2025-10-22T16:01:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    1,
                    3,
                    2,
                    295,
                    0
                ],
                "published": "2024-10-02T16:46:38Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    46,
                    38,
                    2,
                    276,
                    0
                ],
                "title": "LASeR: Learning to Adaptively Select Reward Models with Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LASeR: Learning to Adaptively Select Reward Models with Multi-Armed\n  Bandits"
                },
                "summary": "Reward Models (RMs) are crucial to aligning large language models (LLMs), but\nthe degree to which an RM specialized to one task (e.g. writing) generalizes to\nnew tasks (e.g. math) is often not known a priori, often making using only one\nfixed RM to train LLMs suboptimal. However, optimizing LLMs with multiple RMs\nsimultaneously can incur a prohibitively high computational cost and lead to\nconflicting signals from different RMs that may degrade performance. To address\nthese challenges, we introduce LASeR (Learning to Adaptively Select Rewards),\nwhich frames reward model selection as a multi-armed bandit problem,\nefficiently and iteratively training LLMs using multiple RMs by selecting the\nmost well-suited RM for each instance. On commonsense and math reasoning tasks,\nwe show that LASeR boosts iterative LLM training, improving the absolute\naverage accuracy of Llama-3-8B over three datasets by 2.67% over an ensemble of\nRM scores while also showing superior efficiency (e.g., a 2x speedup).\nMoreover, on WildChat (open-ended instruction-following tasks), LASeR leads to\na 72.69% AlpacaEval win rate over the RM score ensemble baseline. Extending to\nlong-context generation, LASeR improves by 2.96 F1 points (avg.) on\nsingle-document QA tasks and 2.97 F1 points on few-shot learning over the RM\nscore ensemble baseline with best-of-n sampling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward Models (RMs) are crucial to aligning large language models (LLMs), but\nthe degree to which an RM specialized to one task (e.g. writing) generalizes to\nnew tasks (e.g. math) is often not known a priori, often making using only one\nfixed RM to train LLMs suboptimal. However, optimizing LLMs with multiple RMs\nsimultaneously can incur a prohibitively high computational cost and lead to\nconflicting signals from different RMs that may degrade performance. To address\nthese challenges, we introduce LASeR (Learning to Adaptively Select Rewards),\nwhich frames reward model selection as a multi-armed bandit problem,\nefficiently and iteratively training LLMs using multiple RMs by selecting the\nmost well-suited RM for each instance. On commonsense and math reasoning tasks,\nwe show that LASeR boosts iterative LLM training, improving the absolute\naverage accuracy of Llama-3-8B over three datasets by 2.67% over an ensemble of\nRM scores while also showing superior efficiency (e.g., a 2x speedup).\nMoreover, on WildChat (open-ended instruction-following tasks), LASeR leads to\na 72.69% AlpacaEval win rate over the RM score ensemble baseline. Extending to\nlong-context generation, LASeR improves by 2.96 F1 points (avg.) on\nsingle-document QA tasks and 2.97 F1 points on few-shot learning over the RM\nscore ensemble baseline with best-of-n sampling."
                },
                "authors": [
                    {
                        "name": "Duy Nguyen"
                    },
                    {
                        "name": "Archiki Prasad"
                    },
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "NeurIPS 2025 camera-ready. First two authors contributed equally.\n  Code: https://github.com/duykhuongnguyen/LASeR-MAB",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01735v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01735v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19705v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19705v1",
                "updated": "2025-10-22T15:56:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    56,
                    19,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T15:56:19Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    56,
                    19,
                    2,
                    295,
                    0
                ],
                "title": "Fast Inference via Hierarchical Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Inference via Hierarchical Speculative Decoding"
                },
                "summary": "Transformer language models generate text autoregressively, making inference\nlatency proportional to the number of tokens generated. Speculative decoding\nreduces this latency without sacrificing output quality, by leveraging a small\ndraft model to propose tokens that the larger target model verifies in\nparallel. In practice, however, there may exist a set of potential draft\nmodels- ranging from faster but less inaccurate, to slower yet more reliable.\nWe introduce Hierarchical Speculative Decoding (HSD), an algorithm that stacks\nthese draft models into a hierarchy, where each model proposes tokens, and the\nnext larger model verifies them in a single forward pass, until finally the\ntarget model verifies tokens. We derive an expression for the expected latency\nof any such hierarchy and show that selecting the latency-optimal hierarchy can\nbe done in polynomial time. Empirically, HSD gives up to 1.2x speed-up over the\nbest single-draft baseline, demonstrating the practicality of our algorithm in\nreducing generation latency beyond previous techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer language models generate text autoregressively, making inference\nlatency proportional to the number of tokens generated. Speculative decoding\nreduces this latency without sacrificing output quality, by leveraging a small\ndraft model to propose tokens that the larger target model verifies in\nparallel. In practice, however, there may exist a set of potential draft\nmodels- ranging from faster but less inaccurate, to slower yet more reliable.\nWe introduce Hierarchical Speculative Decoding (HSD), an algorithm that stacks\nthese draft models into a hierarchy, where each model proposes tokens, and the\nnext larger model verifies them in a single forward pass, until finally the\ntarget model verifies tokens. We derive an expression for the expected latency\nof any such hierarchy and show that selecting the latency-optimal hierarchy can\nbe done in polynomial time. Empirically, HSD gives up to 1.2x speed-up over the\nbest single-draft baseline, demonstrating the practicality of our algorithm in\nreducing generation latency beyond previous techniques."
                },
                "authors": [
                    {
                        "name": "Amir Globerson"
                    },
                    {
                        "name": "Haim Kaplan"
                    },
                    {
                        "name": "Yishay Mansour"
                    },
                    {
                        "name": "Clara Mohri"
                    },
                    {
                        "name": "Tal Schuster"
                    }
                ],
                "author_detail": {
                    "name": "Tal Schuster"
                },
                "author": "Tal Schuster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19705v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19705v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06760v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06760v5",
                "updated": "2025-10-23T08:23:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    8,
                    23,
                    7,
                    3,
                    296,
                    0
                ],
                "published": "2024-08-13T09:36:40Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    36,
                    40,
                    1,
                    226,
                    0
                ],
                "title": "Stratification in Randomised Clinical Trials for Rare Diseases and\n  Analysis of Covariance: Some Simple Theory and Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stratification in Randomised Clinical Trials for Rare Diseases and\n  Analysis of Covariance: Some Simple Theory and Recommendations"
                },
                "summary": "A simple device for balancing for a continuous covariate in clinical trials\nis to stratify by whether the covariate is above or below some target value,\ntypically the predicted median. This raises an issue as to which model should\nbe used for modelling the effect of treatment on the outcome variable, $Y$.\nShould one fit, the stratum indicator, $S$, the continuous covariate, $X$, both\nor neither? When a covariate is added to a linear model there are three\nconsequences for inference: 1) the mean square error effect, 2) the variance\ninflation factor and 3) second order precision. We consider that it is valuable\nto consider these three factors separately, even if, ultimately, it is their\njoint effect that matters. We present some simple theory, concentrating in\nparticular on the variance inflation factor, that may be used to guide\ntrialists in their choice of model. We also consider the case where the precise\nform of the relationship between the outcome and the covariate is not known. We\nconclude by recommending that the continuous covariate should always be in the\nmodel but that, depending on circumstances, there may be some justification in\nfitting the stratum indicator also.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A simple device for balancing for a continuous covariate in clinical trials\nis to stratify by whether the covariate is above or below some target value,\ntypically the predicted median. This raises an issue as to which model should\nbe used for modelling the effect of treatment on the outcome variable, $Y$.\nShould one fit, the stratum indicator, $S$, the continuous covariate, $X$, both\nor neither? When a covariate is added to a linear model there are three\nconsequences for inference: 1) the mean square error effect, 2) the variance\ninflation factor and 3) second order precision. We consider that it is valuable\nto consider these three factors separately, even if, ultimately, it is their\njoint effect that matters. We present some simple theory, concentrating in\nparticular on the variance inflation factor, that may be used to guide\ntrialists in their choice of model. We also consider the case where the precise\nform of the relationship between the outcome and the covariate is not known. We\nconclude by recommending that the continuous covariate should always be in the\nmodel but that, depending on circumstances, there may be some justification in\nfitting the stratum indicator also."
                },
                "authors": [
                    {
                        "name": "Stephen Senn"
                    },
                    {
                        "name": "Franz König"
                    },
                    {
                        "name": "Martin Posch"
                    }
                ],
                "author_detail": {
                    "name": "Martin Posch"
                },
                "author": "Martin Posch",
                "arxiv_comment": "24 pages, 7 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06760v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06760v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62J10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19698v1",
                "updated": "2025-10-22T15:50:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    50,
                    4,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T15:50:04Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    50,
                    4,
                    2,
                    295,
                    0
                ],
                "title": "RLIE: Rule Generation with Logistic Regression, Iterative Refinement,\n  and Evaluation for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RLIE: Rule Generation with Logistic Regression, Iterative Refinement,\n  and Evaluation for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) can propose rules in natural language,\nsidestepping the need for a predefined predicate space in traditional rule\nlearning. Yet many LLM-based approaches ignore interactions among rules, and\nthe opportunity to couple LLMs with probabilistic rule learning for robust\ninference remains underexplored. We present RLIE, a unified framework that\nintegrates LLMs with probabilistic modeling to learn a set of weighted rules.\nRLIE has four stages: (1) Rule generation, where an LLM proposes and filters\ncandidates; (2) Logistic regression, which learns probabilistic weights for\nglobal selection and calibration; (3) Iterative refinement, which updates the\nrule set using prediction errors; and (4) Evaluation, which compares the\nweighted rule set as a direct classifier with methods that inject rules into an\nLLM. We evaluate multiple inference strategies on real-world datasets. Applying\nrules directly with their learned weights yields superior performance, whereas\nprompting LLMs with the rules, weights, and logistic-model outputs surprisingly\ndegrades accuracy. This supports the view that LLMs excel at semantic\ngeneration and interpretation but are less reliable for precise probabilistic\nintegration. RLIE clarifies the potential and limitations of LLMs for inductive\nreasoning and couples them with classic probabilistic rule combination methods\nto enable more reliable neuro-symbolic reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can propose rules in natural language,\nsidestepping the need for a predefined predicate space in traditional rule\nlearning. Yet many LLM-based approaches ignore interactions among rules, and\nthe opportunity to couple LLMs with probabilistic rule learning for robust\ninference remains underexplored. We present RLIE, a unified framework that\nintegrates LLMs with probabilistic modeling to learn a set of weighted rules.\nRLIE has four stages: (1) Rule generation, where an LLM proposes and filters\ncandidates; (2) Logistic regression, which learns probabilistic weights for\nglobal selection and calibration; (3) Iterative refinement, which updates the\nrule set using prediction errors; and (4) Evaluation, which compares the\nweighted rule set as a direct classifier with methods that inject rules into an\nLLM. We evaluate multiple inference strategies on real-world datasets. Applying\nrules directly with their learned weights yields superior performance, whereas\nprompting LLMs with the rules, weights, and logistic-model outputs surprisingly\ndegrades accuracy. This supports the view that LLMs excel at semantic\ngeneration and interpretation but are less reliable for precise probabilistic\nintegration. RLIE clarifies the potential and limitations of LLMs for inductive\nreasoning and couples them with classic probabilistic rule combination methods\nto enable more reliable neuro-symbolic reasoning."
                },
                "authors": [
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Hua XU"
                    },
                    {
                        "name": "Zhangyi Hu"
                    },
                    {
                        "name": "Yutao Yue"
                    }
                ],
                "author_detail": {
                    "name": "Yutao Yue"
                },
                "author": "Yutao Yue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19689v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19689v1",
                "updated": "2025-10-22T15:37:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    37,
                    42,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T15:37:42Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    37,
                    42,
                    2,
                    295,
                    0
                ],
                "title": "Serverless GPU Architecture for Enterprise HR Analytics: A\n  Production-Scale BDaaS Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless GPU Architecture for Enterprise HR Analytics: A\n  Production-Scale BDaaS Implementation"
                },
                "summary": "Industrial and government organizations increasingly depend on data-driven\nanalytics for workforce, finance, and regulated decision processes, where\ntimeliness, cost efficiency, and compliance are critical. Distributed\nframeworks such as Spark and Flink remain effective for massive-scale batch or\nstreaming analytics but introduce coordination complexity and auditing\noverheads that misalign with moderate-scale, latency-sensitive inference.\nMeanwhile, cloud providers now offer serverless GPUs, and models such as TabNet\nenable interpretable tabular ML, motivating new deployment blueprints for\nregulated environments. In this paper, we present a production-oriented Big\nData as a Service (BDaaS) blueprint that integrates a single-node serverless\nGPU runtime with TabNet. The design leverages GPU acceleration for throughput,\nserverless elasticity for cost reduction, and feature-mask interpretability for\nIL4/FIPS compliance. We conduct benchmarks on the HR, Adult, and BLS datasets,\ncomparing our approach against Spark and CPU baselines. Our results show that\nGPU pipelines achieve up to 4.5x higher throughput, 98x lower latency, and 90%\nlower cost per 1K inferences compared to Spark baselines, while compliance\nmechanisms add only ~5.7 ms latency with p99 < 22 ms. Interpretability remains\nstable under peak load, ensuring reliable auditability. Taken together, these\nfindings provide a compliance-aware benchmark, a reproducible Helm-packaged\nblueprint, and a decision framework that demonstrate the practicality of\nsecure, interpretable, and cost-efficient serverless GPU analytics for\nregulated enterprise and government settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Industrial and government organizations increasingly depend on data-driven\nanalytics for workforce, finance, and regulated decision processes, where\ntimeliness, cost efficiency, and compliance are critical. Distributed\nframeworks such as Spark and Flink remain effective for massive-scale batch or\nstreaming analytics but introduce coordination complexity and auditing\noverheads that misalign with moderate-scale, latency-sensitive inference.\nMeanwhile, cloud providers now offer serverless GPUs, and models such as TabNet\nenable interpretable tabular ML, motivating new deployment blueprints for\nregulated environments. In this paper, we present a production-oriented Big\nData as a Service (BDaaS) blueprint that integrates a single-node serverless\nGPU runtime with TabNet. The design leverages GPU acceleration for throughput,\nserverless elasticity for cost reduction, and feature-mask interpretability for\nIL4/FIPS compliance. We conduct benchmarks on the HR, Adult, and BLS datasets,\ncomparing our approach against Spark and CPU baselines. Our results show that\nGPU pipelines achieve up to 4.5x higher throughput, 98x lower latency, and 90%\nlower cost per 1K inferences compared to Spark baselines, while compliance\nmechanisms add only ~5.7 ms latency with p99 < 22 ms. Interpretability remains\nstable under peak load, ensuring reliable auditability. Taken together, these\nfindings provide a compliance-aware benchmark, a reproducible Helm-packaged\nblueprint, and a decision framework that demonstrate the practicality of\nsecure, interpretable, and cost-efficient serverless GPU analytics for\nregulated enterprise and government settings."
                },
                "authors": [
                    {
                        "name": "Guilin Zhang"
                    },
                    {
                        "name": "Wulan Guo"
                    },
                    {
                        "name": "Ziqi Tan"
                    },
                    {
                        "name": "Srinivas Vippagunta"
                    },
                    {
                        "name": "Suchitra Raman"
                    },
                    {
                        "name": "Shreeshankar Chatterjee"
                    },
                    {
                        "name": "Ju Lin"
                    },
                    {
                        "name": "Shang Liu"
                    },
                    {
                        "name": "Mary Schladenhauffen"
                    },
                    {
                        "name": "Jeffrey Luo"
                    },
                    {
                        "name": "Hailong Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hailong Jiang"
                },
                "author": "Hailong Jiang",
                "arxiv_comment": "10 pages, 7 figures, 4 tables. Accepted to IEEE BigData 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19689v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19689v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.4; H.3.4; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19687v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19687v1",
                "updated": "2025-10-22T15:35:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    35,
                    0,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T15:35:00Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    35,
                    0,
                    2,
                    295,
                    0
                ],
                "title": "Are Large Language Models Sensitive to the Motives Behind Communication?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Large Language Models Sensitive to the Motives Behind Communication?"
                },
                "summary": "Human communication is motivated: people speak, write, and create content\nwith a particular communicative intent in mind. As a result, information that\nlarge language models (LLMs) and AI agents process is inherently framed by\nhumans' intentions and incentives. People are adept at navigating such nuanced\ninformation: we routinely identify benevolent or self-serving motives in order\nto decide what statements to trust. For LLMs to be effective in the real world,\nthey too must critically evaluate content by factoring in the motivations of\nthe source -- for instance, weighing the credibility of claims made in a sales\npitch. In this paper, we undertake a comprehensive study of whether LLMs have\nthis capacity for motivational vigilance. We first employ controlled\nexperiments from cognitive science to verify that LLMs' behavior is consistent\nwith rational models of learning from motivated testimony, and find they\nsuccessfully discount information from biased sources in a human-like manner.\nWe then extend our evaluation to sponsored online adverts, a more naturalistic\nreflection of LLM agents' information ecosystems. In these settings, we find\nthat LLMs' inferences do not track the rational models' predictions nearly as\nclosely -- partly due to additional information that distracts them from\nvigilance-relevant considerations. However, a simple steering intervention that\nboosts the salience of intentions and incentives substantially increases the\ncorrespondence between LLMs and the rational model. These results suggest that\nLLMs possess a basic sensitivity to the motivations of others, but generalizing\nto novel real-world settings will require further improvements to these models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human communication is motivated: people speak, write, and create content\nwith a particular communicative intent in mind. As a result, information that\nlarge language models (LLMs) and AI agents process is inherently framed by\nhumans' intentions and incentives. People are adept at navigating such nuanced\ninformation: we routinely identify benevolent or self-serving motives in order\nto decide what statements to trust. For LLMs to be effective in the real world,\nthey too must critically evaluate content by factoring in the motivations of\nthe source -- for instance, weighing the credibility of claims made in a sales\npitch. In this paper, we undertake a comprehensive study of whether LLMs have\nthis capacity for motivational vigilance. We first employ controlled\nexperiments from cognitive science to verify that LLMs' behavior is consistent\nwith rational models of learning from motivated testimony, and find they\nsuccessfully discount information from biased sources in a human-like manner.\nWe then extend our evaluation to sponsored online adverts, a more naturalistic\nreflection of LLM agents' information ecosystems. In these settings, we find\nthat LLMs' inferences do not track the rational models' predictions nearly as\nclosely -- partly due to additional information that distracts them from\nvigilance-relevant considerations. However, a simple steering intervention that\nboosts the salience of intentions and incentives substantially increases the\ncorrespondence between LLMs and the rational model. These results suggest that\nLLMs possess a basic sensitivity to the motivations of others, but generalizing\nto novel real-world settings will require further improvements to these models."
                },
                "authors": [
                    {
                        "name": "Addison J. Wu"
                    },
                    {
                        "name": "Ryan Liu"
                    },
                    {
                        "name": "Kerem Oktar"
                    },
                    {
                        "name": "Theodore R. Sumers"
                    },
                    {
                        "name": "Thomas L. Griffiths"
                    }
                ],
                "author_detail": {
                    "name": "Thomas L. Griffiths"
                },
                "author": "Thomas L. Griffiths",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19687v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19687v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18524v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18524v2",
                "updated": "2025-10-22T15:27:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    27,
                    6,
                    2,
                    295,
                    0
                ],
                "published": "2025-05-24T05:40:38Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    5,
                    40,
                    38,
                    5,
                    144,
                    0
                ],
                "title": "metaTextGrad: Automatically optimizing language model optimizers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "metaTextGrad: Automatically optimizing language model optimizers"
                },
                "summary": "Large language models (LLMs) are increasingly used in learning algorithms,\nevaluations, and optimization tasks. Recent studies have shown that using\nLLM-based optimizers to automatically optimize model prompts, demonstrations,\npredictions themselves, or other components can significantly enhance the\nperformance of AI systems, as demonstrated by frameworks such as DSPy and\nTextGrad. However, optimizers built on language models themselves are usually\ndesigned by humans with manual design choices; optimizers themselves are not\noptimized. Moreover, these optimizers are general purpose by design, to be\nuseful to a broad audience, and are not tailored for specific tasks. To address\nthese challenges, we propose metaTextGrad, which focuses on designing a\nmeta-optimizer to further enhance existing optimizers and align them to be good\noptimizers for a given task. Our approach consists of two key components: a\nmeta prompt optimizer and a meta structure optimizer. The combination of these\ntwo significantly improves performance across multiple benchmarks, achieving an\naverage absolute performance improvement of up to 6% compared to the best\nbaseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used in learning algorithms,\nevaluations, and optimization tasks. Recent studies have shown that using\nLLM-based optimizers to automatically optimize model prompts, demonstrations,\npredictions themselves, or other components can significantly enhance the\nperformance of AI systems, as demonstrated by frameworks such as DSPy and\nTextGrad. However, optimizers built on language models themselves are usually\ndesigned by humans with manual design choices; optimizers themselves are not\noptimized. Moreover, these optimizers are general purpose by design, to be\nuseful to a broad audience, and are not tailored for specific tasks. To address\nthese challenges, we propose metaTextGrad, which focuses on designing a\nmeta-optimizer to further enhance existing optimizers and align them to be good\noptimizers for a given task. Our approach consists of two key components: a\nmeta prompt optimizer and a meta structure optimizer. The combination of these\ntwo significantly improves performance across multiple benchmarks, achieving an\naverage absolute performance improvement of up to 6% compared to the best\nbaseline."
                },
                "authors": [
                    {
                        "name": "Guowei Xu"
                    },
                    {
                        "name": "Mert Yuksekgonul"
                    },
                    {
                        "name": "Carlos Guestrin"
                    },
                    {
                        "name": "James Zou"
                    }
                ],
                "author_detail": {
                    "name": "James Zou"
                },
                "author": "James Zou",
                "arxiv_comment": "21 pages, 2 figures",
                "arxiv_journal_ref": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18524v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18524v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02511v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02511v2",
                "updated": "2025-10-22T15:27:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    27,
                    3,
                    2,
                    295,
                    0
                ],
                "published": "2025-08-04T15:17:13Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    17,
                    13,
                    0,
                    216,
                    0
                ],
                "title": "Test-time Prompt Intervention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time Prompt Intervention"
                },
                "summary": "Test-time compute has led to remarkable success in the large language model\n(LLM) community, particularly for complex tasks, where longer chains of thought\n(CoTs) are generated to enhance reasoning capabilities. However, growing\nevidence reveals that such reasoning models often produce CoTs plagued by\nexcessive redundancy, including unnecessary verification steps and repetitive\nreasoning shifts. The root cause lies in post-training of them that overly rely\non outcome reward paradigms, as the data of process reward paradigms, which\nregulate intermediate reasoning steps, is difficult to construct at scale. To\naddress this, we propose PI, a novel framework for Test-time Prompt\nIntervention. PI provides an interface to dynamically guide and regulate\nreasoning paths during inference through timely (When module) and proper (How\nmodule) interventions and post-intervention sampling (Which module). This\nallows human problem-solving expertise and cognitive science principles to be\nseamlessly integrated into LLMs' reasoning processes, enhancing controllability\nand interpretability. Extensive experiments across multiple models and datasets\ndemonstrate that PI significantly shortens CoTs while reducing hallucination,\nyielding more concise and reliable reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time compute has led to remarkable success in the large language model\n(LLM) community, particularly for complex tasks, where longer chains of thought\n(CoTs) are generated to enhance reasoning capabilities. However, growing\nevidence reveals that such reasoning models often produce CoTs plagued by\nexcessive redundancy, including unnecessary verification steps and repetitive\nreasoning shifts. The root cause lies in post-training of them that overly rely\non outcome reward paradigms, as the data of process reward paradigms, which\nregulate intermediate reasoning steps, is difficult to construct at scale. To\naddress this, we propose PI, a novel framework for Test-time Prompt\nIntervention. PI provides an interface to dynamically guide and regulate\nreasoning paths during inference through timely (When module) and proper (How\nmodule) interventions and post-intervention sampling (Which module). This\nallows human problem-solving expertise and cognitive science principles to be\nseamlessly integrated into LLMs' reasoning processes, enhancing controllability\nand interpretability. Extensive experiments across multiple models and datasets\ndemonstrate that PI significantly shortens CoTs while reducing hallucination,\nyielding more concise and reliable reasoning."
                },
                "authors": [
                    {
                        "name": "Chenxu Yang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Mz Dai"
                    },
                    {
                        "name": "Dingyu Yao"
                    },
                    {
                        "name": "Mingyu Zheng"
                    },
                    {
                        "name": "Minghui Chen"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Weiping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Wang"
                },
                "author": "Weiping Wang",
                "arxiv_comment": "24 pages, 20 figures, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02511v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02511v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19676v1",
                "updated": "2025-10-22T15:22:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    22,
                    15,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T15:22:15Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    22,
                    15,
                    2,
                    295,
                    0
                ],
                "title": "CircuitGuard: Mitigating LLM Memorization in RTL Code Generation Against\n  IP Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CircuitGuard: Mitigating LLM Memorization in RTL Code Generation Against\n  IP Leakage"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success in generative\ntasks, including register-transfer level (RTL) hardware synthesis. However,\ntheir tendency to memorize training data poses critical risks when proprietary\nor security-sensitive designs are unintentionally exposed during inference.\nWhile prior work has examined memorization in natural language, RTL introduces\nunique challenges: In RTL, structurally different implementations (e.g.,\nbehavioral vs. gate-level descriptions) can realize the same hardware, leading\nto intellectual property (IP) leakage (full or partial) even without verbatim\noverlap. Conversely, even small syntactic variations (e.g., operator precedence\nor blocking vs. non-blocking assignments) can drastically alter circuit\nbehavior, making correctness preservation especially challenging. In this work,\nwe systematically study memorization in RTL code generation and propose\nCircuitGuard, a defense strategy that balances leakage reduction with\ncorrectness preservation. CircuitGuard (1) introduces a novel RTL-aware\nsimilarity metric that captures both structural and functional equivalence\nbeyond surface-level overlap, and (2) develops an activation-level steering\nmethod that identifies and attenuates transformer components most responsible\nfor memorization. Our empirical evaluation demonstrates that CircuitGuard\nidentifies (and isolates) 275 memorization-critical features across layers\n18-28 of Llama 3.1-8B model, achieving up to 80% reduction in semantic\nsimilarity to proprietary patterns while maintaining generation quality.\nCircuitGuard further shows 78-85% cross-domain transfer effectiveness, enabling\nrobust memorization mitigation across circuit categories without retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success in generative\ntasks, including register-transfer level (RTL) hardware synthesis. However,\ntheir tendency to memorize training data poses critical risks when proprietary\nor security-sensitive designs are unintentionally exposed during inference.\nWhile prior work has examined memorization in natural language, RTL introduces\nunique challenges: In RTL, structurally different implementations (e.g.,\nbehavioral vs. gate-level descriptions) can realize the same hardware, leading\nto intellectual property (IP) leakage (full or partial) even without verbatim\noverlap. Conversely, even small syntactic variations (e.g., operator precedence\nor blocking vs. non-blocking assignments) can drastically alter circuit\nbehavior, making correctness preservation especially challenging. In this work,\nwe systematically study memorization in RTL code generation and propose\nCircuitGuard, a defense strategy that balances leakage reduction with\ncorrectness preservation. CircuitGuard (1) introduces a novel RTL-aware\nsimilarity metric that captures both structural and functional equivalence\nbeyond surface-level overlap, and (2) develops an activation-level steering\nmethod that identifies and attenuates transformer components most responsible\nfor memorization. Our empirical evaluation demonstrates that CircuitGuard\nidentifies (and isolates) 275 memorization-critical features across layers\n18-28 of Llama 3.1-8B model, achieving up to 80% reduction in semantic\nsimilarity to proprietary patterns while maintaining generation quality.\nCircuitGuard further shows 78-85% cross-domain transfer effectiveness, enabling\nrobust memorization mitigation across circuit categories without retraining."
                },
                "authors": [
                    {
                        "name": "Nowfel Mashnoor"
                    },
                    {
                        "name": "Mohammad Akyash"
                    },
                    {
                        "name": "Hadi Kamali"
                    },
                    {
                        "name": "Kimia Azar"
                    }
                ],
                "author_detail": {
                    "name": "Kimia Azar"
                },
                "author": "Kimia Azar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19673v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19673v1",
                "updated": "2025-10-22T15:18:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    18,
                    59,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T15:18:59Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    18,
                    59,
                    2,
                    295,
                    0
                ],
                "title": "Beyond single tracers: CNN-based inference of galaxy mass profiles from\n  combined gas and stellar kinematics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond single tracers: CNN-based inference of galaxy mass profiles from\n  combined gas and stellar kinematics"
                },
                "summary": "We investigate whether combining gas and stellar kinematic maps provides\nmeasurable advantages in recovering galaxy mass profiles, compared to using\nsingle-component maps alone. While traditional methods struggle to integrate\nmulti-tracer data effectively, we test whether deep learning models can\nleverage this joint information. We develop a probabilistic convolutional\nneural network (CNN) framework trained and tested on mock galaxy kinematic maps\nfrom multiple cosmological simulation suites. Our model is trained on gas-only,\nstars-only, and combined gas+stellar velocity maps, allowing direct comparison\nof performance across tracers. To assess robustness, we include simulations\nwith differing feedback models and galaxy properties. Combining gas and stellar\nmaps reduces the dispersion in the inferred mass profiles by up to a factor of\n$\\sim$1.5 compared to models using either tracer independently. The CNN\narchitecture effectively captures complementary information from the two\ncomponents. However, we find limitations in generalizing between simulation\nsuites, with reduced performance when applying models trained on one suite to\ngalaxies from another.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate whether combining gas and stellar kinematic maps provides\nmeasurable advantages in recovering galaxy mass profiles, compared to using\nsingle-component maps alone. While traditional methods struggle to integrate\nmulti-tracer data effectively, we test whether deep learning models can\nleverage this joint information. We develop a probabilistic convolutional\nneural network (CNN) framework trained and tested on mock galaxy kinematic maps\nfrom multiple cosmological simulation suites. Our model is trained on gas-only,\nstars-only, and combined gas+stellar velocity maps, allowing direct comparison\nof performance across tracers. To assess robustness, we include simulations\nwith differing feedback models and galaxy properties. Combining gas and stellar\nmaps reduces the dispersion in the inferred mass profiles by up to a factor of\n$\\sim$1.5 compared to models using either tracer independently. The CNN\narchitecture effectively captures complementary information from the two\ncomponents. However, we find limitations in generalizing between simulation\nsuites, with reduced performance when applying models trained on one suite to\ngalaxies from another."
                },
                "authors": [
                    {
                        "name": "Julen Expósito-Márquez"
                    },
                    {
                        "name": "Arianna Di Cintio"
                    },
                    {
                        "name": "Chris Brook"
                    },
                    {
                        "name": "Jorge Sarrato-Alós"
                    },
                    {
                        "name": "Andrea V. Macciò"
                    }
                ],
                "author_detail": {
                    "name": "Andrea V. Macciò"
                },
                "author": "Andrea V. Macciò",
                "arxiv_comment": "11 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19673v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19673v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19670v1",
                "updated": "2025-10-22T15:16:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    16,
                    56,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T15:16:56Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    16,
                    56,
                    2,
                    295,
                    0
                ],
                "title": "CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware\n  Cloud-Edge Cooperation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware\n  Cloud-Edge Cooperation"
                },
                "summary": "We present CoSense-LLM, an edge-first framework that turns continuous\nmultimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and\nlightweight vision) into compact, verifiable semantic tokens and coordinates\nwith large language models under explicit latency, energy, bandwidth, and\nprivacy constraints. CoSense-LLM has four parts: (i) SenseFusion, a lightweight\nencoder that aligns sensor embeddings with language and compresses them into\nshort discrete code sequences; (ii) Edge-RAG, a local hybrid retrieval layer\nthat grounds generation in site specific policies and notes; (iii)\nPromptRouter, a cost and uncertainty aware policy that selects edge only\ngeneration, edge plus retrieval, or compact cloud escalation; and (iv) Secure\nExecution, an auditable redaction path that enforces data minimization so raw\nwaveforms never leave the device. The system works with modern serving\noptimizations, including paged or streaming KV caches, FlashAttention style\nkernels, speculative decoding, and quantized LoRA adapters, and supports on\ndevice personalization and federated updates under non IID drift. Across home,\noffice, and clinic deployments, CoSense-LLM delivers grounded explanations\nwhile meeting tight service level objectives: it sustains sub second (p95) end\nto end latency on edge dominant paths, reduces inter tier token and bandwidth\ncosts by preferring local retrieval grounded responses, and preserves privacy\nby transmitting only discrete codes and redacted metadata. Ablations show that\nEdge-RAG improves factual consistency and reduces contradictions, calibrated\nuncertainty enables selective abstention and controlled escalations, and KV\nplus decoding accelerators lower energy per decision. The results support an\nedge first design that treats semantics, privacy, and predictable latency as co\nequal goals for large model deployments in interference prone environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present CoSense-LLM, an edge-first framework that turns continuous\nmultimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and\nlightweight vision) into compact, verifiable semantic tokens and coordinates\nwith large language models under explicit latency, energy, bandwidth, and\nprivacy constraints. CoSense-LLM has four parts: (i) SenseFusion, a lightweight\nencoder that aligns sensor embeddings with language and compresses them into\nshort discrete code sequences; (ii) Edge-RAG, a local hybrid retrieval layer\nthat grounds generation in site specific policies and notes; (iii)\nPromptRouter, a cost and uncertainty aware policy that selects edge only\ngeneration, edge plus retrieval, or compact cloud escalation; and (iv) Secure\nExecution, an auditable redaction path that enforces data minimization so raw\nwaveforms never leave the device. The system works with modern serving\noptimizations, including paged or streaming KV caches, FlashAttention style\nkernels, speculative decoding, and quantized LoRA adapters, and supports on\ndevice personalization and federated updates under non IID drift. Across home,\noffice, and clinic deployments, CoSense-LLM delivers grounded explanations\nwhile meeting tight service level objectives: it sustains sub second (p95) end\nto end latency on edge dominant paths, reduces inter tier token and bandwidth\ncosts by preferring local retrieval grounded responses, and preserves privacy\nby transmitting only discrete codes and redacted metadata. Ablations show that\nEdge-RAG improves factual consistency and reduces contradictions, calibrated\nuncertainty enables selective abstention and controlled escalations, and KV\nplus decoding accelerators lower energy per decision. The results support an\nedge first design that treats semantics, privacy, and predictable latency as co\nequal goals for large model deployments in interference prone environments."
                },
                "authors": [
                    {
                        "name": "Hasan Akgul"
                    },
                    {
                        "name": "Mari Eplik"
                    },
                    {
                        "name": "Javier Rojas"
                    },
                    {
                        "name": "Aina Binti Abdullah"
                    },
                    {
                        "name": "Pieter van der Merwe"
                    }
                ],
                "author_detail": {
                    "name": "Pieter van der Merwe"
                },
                "author": "Pieter van der Merwe",
                "arxiv_comment": "19 pages,8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; C.2.4; C.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19669v1",
                "updated": "2025-10-22T15:16:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    16,
                    6,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T15:16:06Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    16,
                    6,
                    2,
                    295,
                    0
                ],
                "title": "DiffAdapt: Difficulty-Adaptive Reasoning for Token-Efficient LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffAdapt: Difficulty-Adaptive Reasoning for Token-Efficient LLM\n  Inference"
                },
                "summary": "Recent reasoning Large Language Models (LLMs) demonstrate remarkable\nproblem-solving abilities but often generate long thinking traces whose utility\nis unclear. Our work aims to improve their efficiency, enabling them to reach\nhigh performance without overthinking. First, we analyze the entropy of token\nprobabilities in reasoning traces. Across three models, we observe a consistent\nU-shaped entropy pattern: high entropy on easy problems despite high accuracy,\nlow entropy on problems with medium difficulty, and high entropy on hard\nproblems reflecting uncertainty. Specifically, we notice 22--25\\% entropy\nreduction from easy to medium difficulty regions, suggesting an {overthinking}\nphenomenon on easy instances. Building on these insights, we introduce\n\\textbf{DiffAdapt}, a lightweight framework that selects Easy/Normal/Hard\ninference strategies per question based on their difficulty and reasoning trace\nentropy. Each inference strategy consists of a fixed prompt, temperature and\nmaximum token length. In contrast to existing efficiency optimization methods,\nour approach does not fine-tune base LLM but a small probe that classifies\nLLM's final hidden state, allowing inexpensive adaptation. We comprehensively\nevaluate our method on five models and eight benchmarks. Our method achieves\ncomparable or improved accuracy while reducing token usage by up to 22.4\\%,\nestablishing a practical path toward compute-efficient reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent reasoning Large Language Models (LLMs) demonstrate remarkable\nproblem-solving abilities but often generate long thinking traces whose utility\nis unclear. Our work aims to improve their efficiency, enabling them to reach\nhigh performance without overthinking. First, we analyze the entropy of token\nprobabilities in reasoning traces. Across three models, we observe a consistent\nU-shaped entropy pattern: high entropy on easy problems despite high accuracy,\nlow entropy on problems with medium difficulty, and high entropy on hard\nproblems reflecting uncertainty. Specifically, we notice 22--25\\% entropy\nreduction from easy to medium difficulty regions, suggesting an {overthinking}\nphenomenon on easy instances. Building on these insights, we introduce\n\\textbf{DiffAdapt}, a lightweight framework that selects Easy/Normal/Hard\ninference strategies per question based on their difficulty and reasoning trace\nentropy. Each inference strategy consists of a fixed prompt, temperature and\nmaximum token length. In contrast to existing efficiency optimization methods,\nour approach does not fine-tune base LLM but a small probe that classifies\nLLM's final hidden state, allowing inexpensive adaptation. We comprehensively\nevaluate our method on five models and eight benchmarks. Our method achieves\ncomparable or improved accuracy while reducing token usage by up to 22.4\\%,\nestablishing a practical path toward compute-efficient reasoning."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Eunsol Choi"
                    }
                ],
                "author_detail": {
                    "name": "Eunsol Choi"
                },
                "author": "Eunsol Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06293v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06293v2",
                "updated": "2025-10-22T15:14:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    14,
                    5,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-07T11:52:32Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    11,
                    52,
                    32,
                    1,
                    280,
                    0
                ],
                "title": "BlockGPT: Spatio-Temporal Modelling of Rainfall via Frame-Level\n  Autoregression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlockGPT: Spatio-Temporal Modelling of Rainfall via Frame-Level\n  Autoregression"
                },
                "summary": "Predicting precipitation maps is a highly complex spatiotemporal modeling\ntask, critical for mitigating the impacts of extreme weather events. Short-term\nprecipitation forecasting, or nowcasting, requires models that are not only\naccurate but also computationally efficient for real-time applications. Current\nmethods, such as token-based autoregressive models, often suffer from flawed\ninductive biases and slow inference, while diffusion models can be\ncomputationally intensive. To address these limitations, we introduce BlockGPT,\na generative autoregressive transformer using batched tokenization (Block)\nmethod that predicts full two-dimensional fields (frames) at each time step.\nConceived as a model-agnostic paradigm for video prediction, BlockGPT\nfactorizes space-time by using self-attention within each frame and causal\nattention across frames; in this work, we instantiate it for precipitation\nnowcasting. We evaluate BlockGPT on two precipitation datasets, viz. KNMI\n(Netherlands) and SEVIR (U.S.), comparing it to state-of-the-art baselines\nincluding token-based (NowcastingGPT) and diffusion-based (DiffCast+Phydnet)\nmodels. The results show that BlockGPT achieves superior accuracy, event\nlocalization as measured by categorical metrics, and inference speeds up to 31x\nfaster than comparable baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting precipitation maps is a highly complex spatiotemporal modeling\ntask, critical for mitigating the impacts of extreme weather events. Short-term\nprecipitation forecasting, or nowcasting, requires models that are not only\naccurate but also computationally efficient for real-time applications. Current\nmethods, such as token-based autoregressive models, often suffer from flawed\ninductive biases and slow inference, while diffusion models can be\ncomputationally intensive. To address these limitations, we introduce BlockGPT,\na generative autoregressive transformer using batched tokenization (Block)\nmethod that predicts full two-dimensional fields (frames) at each time step.\nConceived as a model-agnostic paradigm for video prediction, BlockGPT\nfactorizes space-time by using self-attention within each frame and causal\nattention across frames; in this work, we instantiate it for precipitation\nnowcasting. We evaluate BlockGPT on two precipitation datasets, viz. KNMI\n(Netherlands) and SEVIR (U.S.), comparing it to state-of-the-art baselines\nincluding token-based (NowcastingGPT) and diffusion-based (DiffCast+Phydnet)\nmodels. The results show that BlockGPT achieves superior accuracy, event\nlocalization as measured by categorical metrics, and inference speeds up to 31x\nfaster than comparable baselines."
                },
                "authors": [
                    {
                        "name": "Cristian Meo"
                    },
                    {
                        "name": "Varun Sarathchandran"
                    },
                    {
                        "name": "Avijit Majhi"
                    },
                    {
                        "name": "Shao Hung"
                    },
                    {
                        "name": "Carlo Saccardi"
                    },
                    {
                        "name": "Ruben Imhoff"
                    },
                    {
                        "name": "Roberto Deidda"
                    },
                    {
                        "name": "Remko Uijlenhoet"
                    },
                    {
                        "name": "Justin Dauwels"
                    }
                ],
                "author_detail": {
                    "name": "Justin Dauwels"
                },
                "author": "Justin Dauwels",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06293v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06293v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19668v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19668v1",
                "updated": "2025-10-22T15:13:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    13,
                    52,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T15:13:52Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    13,
                    52,
                    2,
                    295,
                    0
                ],
                "title": "Unraveling Emotions with Pre-Trained Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unraveling Emotions with Pre-Trained Models"
                },
                "summary": "Transformer models have significantly advanced the field of emotion\nrecognition. However, there are still open challenges when exploring open-ended\nqueries for Large Language Models (LLMs). Although current models offer good\nresults, automatic emotion analysis in open texts presents significant\nchallenges, such as contextual ambiguity, linguistic variability, and\ndifficulty interpreting complex emotional expressions. These limitations make\nthe direct application of generalist models difficult. Accordingly, this work\ncompares the effectiveness of fine-tuning and prompt engineering in emotion\ndetection in three distinct scenarios: (i) performance of fine-tuned\npre-trained models and general-purpose LLMs using simple prompts; (ii)\neffectiveness of different emotion prompt designs with LLMs; and (iii) impact\nof emotion grouping techniques on these models. Experimental tests attain\nmetrics above 70% with a fine-tuned pre-trained model for emotion recognition.\nMoreover, the findings highlight that LLMs require structured prompt\nengineering and emotion grouping to enhance their performance. These\nadvancements improve sentiment analysis, human-computer interaction, and\nunderstanding of user behavior across various domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer models have significantly advanced the field of emotion\nrecognition. However, there are still open challenges when exploring open-ended\nqueries for Large Language Models (LLMs). Although current models offer good\nresults, automatic emotion analysis in open texts presents significant\nchallenges, such as contextual ambiguity, linguistic variability, and\ndifficulty interpreting complex emotional expressions. These limitations make\nthe direct application of generalist models difficult. Accordingly, this work\ncompares the effectiveness of fine-tuning and prompt engineering in emotion\ndetection in three distinct scenarios: (i) performance of fine-tuned\npre-trained models and general-purpose LLMs using simple prompts; (ii)\neffectiveness of different emotion prompt designs with LLMs; and (iii) impact\nof emotion grouping techniques on these models. Experimental tests attain\nmetrics above 70% with a fine-tuned pre-trained model for emotion recognition.\nMoreover, the findings highlight that LLMs require structured prompt\nengineering and emotion grouping to enhance their performance. These\nadvancements improve sentiment analysis, human-computer interaction, and\nunderstanding of user behavior across various domains."
                },
                "authors": [
                    {
                        "name": "Alejandro Pajón-Sanmartín"
                    },
                    {
                        "name": "Francisco De Arriba-Pérez"
                    },
                    {
                        "name": "Silvia García-Méndez"
                    },
                    {
                        "name": "Fátima Leal"
                    },
                    {
                        "name": "Benedita Malheiro"
                    },
                    {
                        "name": "Juan Carlos Burguillo-Rial"
                    }
                ],
                "author_detail": {
                    "name": "Juan Carlos Burguillo-Rial"
                },
                "author": "Juan Carlos Burguillo-Rial",
                "arxiv_doi": "10.1109/ACCESS.2025.3623877",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ACCESS.2025.3623877",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.19668v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19668v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19664v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19664v1",
                "updated": "2025-10-22T15:10:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    10,
                    13,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T15:10:13Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    10,
                    13,
                    2,
                    295,
                    0
                ],
                "title": "Parameter Estimation in River Transport Models With Immobile Phase\n  Exchange Using Dimensional Analysis and Reduced-Order Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter Estimation in River Transport Models With Immobile Phase\n  Exchange Using Dimensional Analysis and Reduced-Order Models"
                },
                "summary": "We propose a framework for parameter estimation in river transport models\nusing breakthrough curve data, which we refer to as Dimensionless Synthetic\nTransport Estimation (DSTE). We utilize this framework to parameterize the\none-dimensional advection-dispersion equation model, incorporating immobile\nphase exchange through a memory function. We solve the governing equation\nanalytically in the Laplace domain and numerically invert it to generate\nsynthetic breakthrough curves for different memory functions and boundary\nconditions. A dimensionless formulation enables decoupling the estimation of\nadvection velocity from other parameters, significantly reducing the number of\nrequired forward solutions. To improve computational efficiency, we apply a\nKarhunen-Loeve (KL) expansion to transform the synthetic dataset into a\nreduced-order space. Given a measured breakthrough curve, we estimate the\nadvection velocity by minimizing the distance from the measurement to the\nsynthetic data in KL space, and infer the remaining dimensionless parameters by\nProjected Barycentric Interpolation (PBI). We benchmark our method against\nseveral alternatives, including Laplace domain fitting, moment matching, global\nrandom optimization, and variations of the DSTE framework using\nnearest-neighbor interpolation and neural network-based estimation. Applied to\n295 breakthrough curves from 54 tracer tests in 25 rivers, DSTE delivers\naccurate parameter estimates. The resulting labeled dataset allows researchers\nto link transport parameters with hydraulic conditions, site characteristics,\nand measured concentrations. The synthetic dataset can be leveraged for the\nanalysis of new breakthrough curves, eliminating the need for additional\nforward simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a framework for parameter estimation in river transport models\nusing breakthrough curve data, which we refer to as Dimensionless Synthetic\nTransport Estimation (DSTE). We utilize this framework to parameterize the\none-dimensional advection-dispersion equation model, incorporating immobile\nphase exchange through a memory function. We solve the governing equation\nanalytically in the Laplace domain and numerically invert it to generate\nsynthetic breakthrough curves for different memory functions and boundary\nconditions. A dimensionless formulation enables decoupling the estimation of\nadvection velocity from other parameters, significantly reducing the number of\nrequired forward solutions. To improve computational efficiency, we apply a\nKarhunen-Loeve (KL) expansion to transform the synthetic dataset into a\nreduced-order space. Given a measured breakthrough curve, we estimate the\nadvection velocity by minimizing the distance from the measurement to the\nsynthetic data in KL space, and infer the remaining dimensionless parameters by\nProjected Barycentric Interpolation (PBI). We benchmark our method against\nseveral alternatives, including Laplace domain fitting, moment matching, global\nrandom optimization, and variations of the DSTE framework using\nnearest-neighbor interpolation and neural network-based estimation. Applied to\n295 breakthrough curves from 54 tracer tests in 25 rivers, DSTE delivers\naccurate parameter estimates. The resulting labeled dataset allows researchers\nto link transport parameters with hydraulic conditions, site characteristics,\nand measured concentrations. The synthetic dataset can be leveraged for the\nanalysis of new breakthrough curves, eliminating the need for additional\nforward simulations."
                },
                "authors": [
                    {
                        "name": "Manuel M. Reyna"
                    },
                    {
                        "name": "Alexandre M. Tartakovsky"
                    }
                ],
                "author_detail": {
                    "name": "Alexandre M. Tartakovsky"
                },
                "author": "Alexandre M. Tartakovsky",
                "arxiv_comment": "36 pages, 8 figures, submitted to Water Resources Research",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19664v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19664v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19661v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19661v1",
                "updated": "2025-10-22T15:06:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    6,
                    26,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T15:06:26Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    6,
                    26,
                    2,
                    295,
                    0
                ],
                "title": "AgentSense: LLMs Empower Generalizable and Explainable Web-Based\n  Participatory Urban Sensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentSense: LLMs Empower Generalizable and Explainable Web-Based\n  Participatory Urban Sensing"
                },
                "summary": "Web-based participatory urban sensing has emerged as a vital approach for\nmodern urban management by leveraging mobile individuals as distributed\nsensors. However, existing urban sensing systems struggle with limited\ngeneralization across diverse urban scenarios and poor interpretability in\ndecision-making. In this work, we introduce AgentSense, a hybrid, training-free\nframework that integrates large language models (LLMs) into participatory urban\nsensing through a multi-agent evolution system. AgentSense initially employs\nclassical planner to generate baseline solutions and then iteratively refines\nthem to adapt sensing task assignments to dynamic urban conditions and\nheterogeneous worker preferences, while producing natural language explanations\nthat enhance transparency and trust. Extensive experiments across two\nlarge-scale mobility datasets and seven types of dynamic disturbances\ndemonstrate that AgentSense offers distinct advantages in adaptivity and\nexplainability over traditional methods. Furthermore, compared to single-agent\nLLM baselines, our approach outperforms in both performance and robustness,\nwhile delivering more reasonable and transparent explanations. These results\nposition AgentSense as a significant advancement towards deploying adaptive and\nexplainable urban sensing systems on the web.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web-based participatory urban sensing has emerged as a vital approach for\nmodern urban management by leveraging mobile individuals as distributed\nsensors. However, existing urban sensing systems struggle with limited\ngeneralization across diverse urban scenarios and poor interpretability in\ndecision-making. In this work, we introduce AgentSense, a hybrid, training-free\nframework that integrates large language models (LLMs) into participatory urban\nsensing through a multi-agent evolution system. AgentSense initially employs\nclassical planner to generate baseline solutions and then iteratively refines\nthem to adapt sensing task assignments to dynamic urban conditions and\nheterogeneous worker preferences, while producing natural language explanations\nthat enhance transparency and trust. Extensive experiments across two\nlarge-scale mobility datasets and seven types of dynamic disturbances\ndemonstrate that AgentSense offers distinct advantages in adaptivity and\nexplainability over traditional methods. Furthermore, compared to single-agent\nLLM baselines, our approach outperforms in both performance and robustness,\nwhile delivering more reasonable and transparent explanations. These results\nposition AgentSense as a significant advancement towards deploying adaptive and\nexplainable urban sensing systems on the web."
                },
                "authors": [
                    {
                        "name": "Xusen Guo"
                    },
                    {
                        "name": "Mingxing Peng"
                    },
                    {
                        "name": "Xixuan Hao"
                    },
                    {
                        "name": "Xingchen Zou"
                    },
                    {
                        "name": "Qiongyan Wang"
                    },
                    {
                        "name": "Sijie Ruan"
                    },
                    {
                        "name": "Yuxuan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yuxuan Liang"
                },
                "author": "Yuxuan Liang",
                "arxiv_comment": "13 pages, 10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19661v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19648v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19648v1",
                "updated": "2025-10-22T14:53:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    53,
                    26,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T14:53:26Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    53,
                    26,
                    2,
                    295,
                    0
                ],
                "title": "Adaptive Ising machine based on phase-locking of an auto-oscillator to a\n  bi-harmonic external driving with noise",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Ising machine based on phase-locking of an auto-oscillator to a\n  bi-harmonic external driving with noise"
                },
                "summary": "We introduce a universal theory of phase auto-oscillators driven by a bi\nharmonic signal (having frequency components close to single and double of the\nfree-running oscillator frequency) with noise. With it, we show how\ndeterministic phase locking and stochastic phase slips can be continuously\ntuned by varying the relative amplitudes and frequencies of the driving\ncomponents. Using, as an example, a spin-torque nano-oscillator, we numerically\nvalidate this theory by implementing a deterministic Ising machine paradigm, a\nprobabilistic one, and dual-mode operation of the two. This demonstration\nintroduces the concept of adaptive Ising machines (AIM), a unified\noscillator-based architecture that dynamically combines both regimes within the\nsame hardware platform by properly tuning the amplitudes of the bi-harmonic\ndriving relative to the noise strength. Benchmarking on different classes of\ncombinatorial optimization problems, the AIM exhibits complementary performance\ncompared to oscillator based Ising machines and probabilistic Ising machines,\nwith adaptability to the specific problem class. This work introduces the first\nOIM capable of transitioning between deterministic and probabilistic\ncomputation taking advantage of a proper design of the trade-off between the\nstrength of phase-locking of an auto-oscillator to a bi harmonic external\ndriving and noise, opening a path toward scalable, CMOS compatible hardware for\nhybrid optimization and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a universal theory of phase auto-oscillators driven by a bi\nharmonic signal (having frequency components close to single and double of the\nfree-running oscillator frequency) with noise. With it, we show how\ndeterministic phase locking and stochastic phase slips can be continuously\ntuned by varying the relative amplitudes and frequencies of the driving\ncomponents. Using, as an example, a spin-torque nano-oscillator, we numerically\nvalidate this theory by implementing a deterministic Ising machine paradigm, a\nprobabilistic one, and dual-mode operation of the two. This demonstration\nintroduces the concept of adaptive Ising machines (AIM), a unified\noscillator-based architecture that dynamically combines both regimes within the\nsame hardware platform by properly tuning the amplitudes of the bi-harmonic\ndriving relative to the noise strength. Benchmarking on different classes of\ncombinatorial optimization problems, the AIM exhibits complementary performance\ncompared to oscillator based Ising machines and probabilistic Ising machines,\nwith adaptability to the specific problem class. This work introduces the first\nOIM capable of transitioning between deterministic and probabilistic\ncomputation taking advantage of a proper design of the trade-off between the\nstrength of phase-locking of an auto-oscillator to a bi harmonic external\ndriving and noise, opening a path toward scalable, CMOS compatible hardware for\nhybrid optimization and inference."
                },
                "authors": [
                    {
                        "name": "Eleonora Raimondo"
                    },
                    {
                        "name": "Andrea Grimaldi"
                    },
                    {
                        "name": "Vasyl Tyberkevych"
                    },
                    {
                        "name": "Riccardo Tomasello"
                    },
                    {
                        "name": "Anna Giordano"
                    },
                    {
                        "name": "Mario Carpentieri"
                    },
                    {
                        "name": "Andrei Slavin"
                    },
                    {
                        "name": "Massimo Chiappini"
                    },
                    {
                        "name": "Giovanni Finocchio"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Finocchio"
                },
                "author": "Giovanni Finocchio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19648v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19648v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19647v1",
                "updated": "2025-10-22T14:53:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    53,
                    10,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T14:53:10Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    53,
                    10,
                    2,
                    295,
                    0
                ],
                "title": "The Impact of Population III.1 Flash Reionization for CMB Polarization\n  and Thomson Scattering Optical Depth",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Population III.1 Flash Reionization for CMB Polarization\n  and Thomson Scattering Optical Depth"
                },
                "summary": "The Population III.1 theory for supermassive black hole (SMBH) formation\npredicts a very early ($z\\sim20-25$), transient phase, ``The Flash'', of cosmic\nreionization powered by supermassive stars that are SMBH progenitors. The\nuniverse then quickly recombined to become mostly neutral, with this state\npersisting until galaxies begin to reionize intergalactic gas again at $z\\sim\n10$. The overall Thomson scattering optical depth, $\\tau$, from The Flash has\nbeen shown to be $\\tau_{\\rm PopIII.1}\\sim0.03$, leading to a total\n$\\tau\\sim0.08-0.09$. Such a value, while significantly larger than that\npreviously inferred from {\\it Planck} observations of the low-$l$ $EE$\npolarization power spectrum of the CMB, can help relieve several ``tensions''\nfaced by the standard $\\Lambda$CDM cosmological model, especially the\npreference for negative neutrino masses and dynamic dark energy. Here we\ncompute $EE$ power spectra of example models of The Flash. We find that,\nbecause of its very high redshift, the contribution to $l\\lesssim8$ modes is\ndramatically reduced compared to usual low-$z$ reionization models for the same\nvalue of $\\tau$, while the power at $l\\gtrsim8$ is boosted. Thus the Pop III.1\nreionization scenario provides a natural way to increase $\\tau$, while\nremaining closer to the latest CMB low-$l$ polarization observations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Population III.1 theory for supermassive black hole (SMBH) formation\npredicts a very early ($z\\sim20-25$), transient phase, ``The Flash'', of cosmic\nreionization powered by supermassive stars that are SMBH progenitors. The\nuniverse then quickly recombined to become mostly neutral, with this state\npersisting until galaxies begin to reionize intergalactic gas again at $z\\sim\n10$. The overall Thomson scattering optical depth, $\\tau$, from The Flash has\nbeen shown to be $\\tau_{\\rm PopIII.1}\\sim0.03$, leading to a total\n$\\tau\\sim0.08-0.09$. Such a value, while significantly larger than that\npreviously inferred from {\\it Planck} observations of the low-$l$ $EE$\npolarization power spectrum of the CMB, can help relieve several ``tensions''\nfaced by the standard $\\Lambda$CDM cosmological model, especially the\npreference for negative neutrino masses and dynamic dark energy. Here we\ncompute $EE$ power spectra of example models of The Flash. We find that,\nbecause of its very high redshift, the contribution to $l\\lesssim8$ modes is\ndramatically reduced compared to usual low-$z$ reionization models for the same\nvalue of $\\tau$, while the power at $l\\gtrsim8$ is boosted. Thus the Pop III.1\nreionization scenario provides a natural way to increase $\\tau$, while\nremaining closer to the latest CMB low-$l$ polarization observations."
                },
                "authors": [
                    {
                        "name": "Jonathan C. Tan"
                    },
                    {
                        "name": "Eiichiro Komatsu"
                    }
                ],
                "author_detail": {
                    "name": "Eiichiro Komatsu"
                },
                "author": "Eiichiro Komatsu",
                "arxiv_comment": "Submitted to PRL, comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14847v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14847v2",
                "updated": "2025-10-22T14:52:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    52,
                    23,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-16T16:19:13Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    19,
                    13,
                    3,
                    289,
                    0
                ],
                "title": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond\n  Semantic Dependency Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond\n  Semantic Dependency Constraints"
                },
                "summary": "Video generation models have achieved remarkable progress, particularly\nexcelling in realistic scenarios; however, their performance degrades notably\nin imaginative scenarios. These prompts often involve rarely co-occurring\nconcepts with long-distance semantic relationships, falling outside training\ndistributions. Existing methods typically apply test-time scaling for improving\nvideo quality, but their fixed search spaces and static reward designs limit\nadaptability to imaginative scenarios. To fill this gap, we propose\nImagerySearch, a prompt-guided adaptive test-time search strategy that\ndynamically adjusts both the inference search space and reward function\naccording to semantic relationships in the prompt. This enables more coherent\nand visually plausible videos in challenging imaginative settings. To evaluate\nprogress in this direction, we introduce LDT-Bench, the first dedicated\nbenchmark for long-distance semantic prompts, consisting of 2,839 diverse\nconcept pairs and an automated protocol for assessing creative generation\ncapabilities. Extensive experiments show that ImagerySearch consistently\noutperforms strong video generation baselines and existing test-time scaling\napproaches on LDT-Bench, and achieves competitive improvements on VBench,\ndemonstrating its effectiveness across diverse prompt types. We will release\nLDT-Bench and code to facilitate future research on imaginative video\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video generation models have achieved remarkable progress, particularly\nexcelling in realistic scenarios; however, their performance degrades notably\nin imaginative scenarios. These prompts often involve rarely co-occurring\nconcepts with long-distance semantic relationships, falling outside training\ndistributions. Existing methods typically apply test-time scaling for improving\nvideo quality, but their fixed search spaces and static reward designs limit\nadaptability to imaginative scenarios. To fill this gap, we propose\nImagerySearch, a prompt-guided adaptive test-time search strategy that\ndynamically adjusts both the inference search space and reward function\naccording to semantic relationships in the prompt. This enables more coherent\nand visually plausible videos in challenging imaginative settings. To evaluate\nprogress in this direction, we introduce LDT-Bench, the first dedicated\nbenchmark for long-distance semantic prompts, consisting of 2,839 diverse\nconcept pairs and an automated protocol for assessing creative generation\ncapabilities. Extensive experiments show that ImagerySearch consistently\noutperforms strong video generation baselines and existing test-time scaling\napproaches on LDT-Bench, and achieves competitive improvements on VBench,\ndemonstrating its effectiveness across diverse prompt types. We will release\nLDT-Bench and code to facilitate future research on imaginative video\ngeneration."
                },
                "authors": [
                    {
                        "name": "Meiqi Wu"
                    },
                    {
                        "name": "Jiashu Zhu"
                    },
                    {
                        "name": "Xiaokun Feng"
                    },
                    {
                        "name": "Chubin Chen"
                    },
                    {
                        "name": "Chen Zhu"
                    },
                    {
                        "name": "Bingze Song"
                    },
                    {
                        "name": "Fangyuan Mao"
                    },
                    {
                        "name": "Jiahong Wu"
                    },
                    {
                        "name": "Xiangxiang Chu"
                    },
                    {
                        "name": "Kaiqi Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaiqi Huang"
                },
                "author": "Kaiqi Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14847v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14847v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19644v1",
                "updated": "2025-10-22T14:49:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    49,
                    21,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T14:49:21Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    49,
                    21,
                    2,
                    295,
                    0
                ],
                "title": "LLavaCode: Compressed Code Representations for Retrieval-Augmented Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLavaCode: Compressed Code Representations for Retrieval-Augmented Code\n  Generation"
                },
                "summary": "Retrieval-augmented generation has emerged as one of the most effective\napproaches for code completion, particularly when context from a surrounding\nrepository is essential. However, incorporating context significantly extends\nsequence length, leading to slower inference - a critical limitation for\ninteractive settings such as IDEs. In this work, we introduce LlavaCode, a\nframework that compresses code into compact, semantically rich representations\ninterpretable by code LLM, enhancing generation quality while reducing the\nretrieved context to only a few compressed single-token vectors. Using a small\nprojector module we can significantly increase the EM and ES metrics of coding\nmodel with negligible latency increase. Our experiments demonstrate that\ncompressed context enables 20-38% reduction in Time-to-First-Token (TTFT) on\nline completion tasks compared to full-RAG pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation has emerged as one of the most effective\napproaches for code completion, particularly when context from a surrounding\nrepository is essential. However, incorporating context significantly extends\nsequence length, leading to slower inference - a critical limitation for\ninteractive settings such as IDEs. In this work, we introduce LlavaCode, a\nframework that compresses code into compact, semantically rich representations\ninterpretable by code LLM, enhancing generation quality while reducing the\nretrieved context to only a few compressed single-token vectors. Using a small\nprojector module we can significantly increase the EM and ES metrics of coding\nmodel with negligible latency increase. Our experiments demonstrate that\ncompressed context enables 20-38% reduction in Time-to-First-Token (TTFT) on\nline completion tasks compared to full-RAG pipelines."
                },
                "authors": [
                    {
                        "name": "Daria Cherniuk"
                    },
                    {
                        "name": "Nikita Sukhorukov"
                    },
                    {
                        "name": "Nikita Sushko"
                    },
                    {
                        "name": "Daniil Gusak"
                    },
                    {
                        "name": "Danil Sivtsov"
                    },
                    {
                        "name": "Elena Tutubalina"
                    },
                    {
                        "name": "Evgeny Frolov"
                    }
                ],
                "author_detail": {
                    "name": "Evgeny Frolov"
                },
                "author": "Evgeny Frolov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19641v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19641v1",
                "updated": "2025-10-22T14:40:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    40,
                    24,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T14:40:24Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    40,
                    24,
                    2,
                    295,
                    0
                ],
                "title": "Style Attack Disguise: When Fonts Become a Camouflage for Adversarial\n  Intent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Style Attack Disguise: When Fonts Become a Camouflage for Adversarial\n  Intent"
                },
                "summary": "With social media growth, users employ stylistic fonts and font-like emoji to\nexpress individuality, creating visually appealing text that remains\nhuman-readable. However, these fonts introduce hidden vulnerabilities in NLP\nmodels: while humans easily read stylistic text, models process these\ncharacters as distinct tokens, causing interference. We identify this\nhuman-model perception gap and propose a style-based attack, Style Attack\nDisguise (SAD). We design two sizes: light for query efficiency and strong for\nsuperior attack performance. Experiments on sentiment classification and\nmachine translation across traditional models, LLMs, and commercial services\ndemonstrate SAD's strong attack performance. We also show SAD's potential\nthreats to multimodal tasks including text-to-image and text-to-speech\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With social media growth, users employ stylistic fonts and font-like emoji to\nexpress individuality, creating visually appealing text that remains\nhuman-readable. However, these fonts introduce hidden vulnerabilities in NLP\nmodels: while humans easily read stylistic text, models process these\ncharacters as distinct tokens, causing interference. We identify this\nhuman-model perception gap and propose a style-based attack, Style Attack\nDisguise (SAD). We design two sizes: light for query efficiency and strong for\nsuperior attack performance. Experiments on sentiment classification and\nmachine translation across traditional models, LLMs, and commercial services\ndemonstrate SAD's strong attack performance. We also show SAD's potential\nthreats to multimodal tasks including text-to-image and text-to-speech\ngeneration."
                },
                "authors": [
                    {
                        "name": "Yangshijie Zhang"
                    },
                    {
                        "name": "Xinda Wang"
                    },
                    {
                        "name": "Jialin Liu"
                    },
                    {
                        "name": "Wenqiang Wang"
                    },
                    {
                        "name": "Zhicong Ma"
                    },
                    {
                        "name": "Xingxing Jia"
                    }
                ],
                "author_detail": {
                    "name": "Xingxing Jia"
                },
                "author": "Xingxing Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19641v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10088v2",
                "updated": "2025-10-22T14:34:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    34,
                    49,
                    2,
                    295,
                    0
                ],
                "published": "2025-01-17T10:15:03Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    10,
                    15,
                    3,
                    4,
                    17,
                    0
                ],
                "title": "A recursive Bayesian neural network for constitutive modeling of sands\n  under monotonic and cyclic loading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A recursive Bayesian neural network for constitutive modeling of sands\n  under monotonic and cyclic loading"
                },
                "summary": "In geotechnical engineering, constitutive models are central to capturing\nsoil behavior across diverse drainage conditions, stress paths,and loading\nhistories. While data driven deep learning (DL) approaches have shown promise\nas alternatives to traditional constitutive formulations, their deployment\nrequires models that are both accurate and capable of quantifying predictive\nuncertainty. This study introduces a recursive Bayesian neural network (rBNN)\nframework that unifies temporal sequence learning with generalized Bayesian\ninference to achieve both predictive accuracy and rigorous uncertainty\nquantification. A key innovation is the incorporation of a sliding window\nrecursive structure that enables the model to effectively capture path\ndependent soil responses under monotonic and cyclic loading. By treating\nnetwork parameters as random variables and inferring their posterior\ndistributions via generalized variational inference, the rBNN produces well\ncalibrated confidence intervals alongside point predictions.The framework is\nvalidated against four datasets spanning both simulated and experimental\ntriaxial tests: monotonic loading using a Hardening Soil model simulation and\n28 CD tests on Baskarp sand, and cyclic loading using an exponential\nconstitutive simulation of CD CU tests and 37 experimental cyclic CU tests on\nOttawa F65 sand. This progression from monotonic to cyclic and from simulated\nto experimental data demonstrates the adaptability of the proposed approach\nacross varying levels of data fidelity and complexity. Comparative analyses\nwith LSTM, Encoder Decoder,and GRU architectures highlight that rBNN not only\nachieves competitive predictive accuracy but also provides reliable confidence\nintervals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In geotechnical engineering, constitutive models are central to capturing\nsoil behavior across diverse drainage conditions, stress paths,and loading\nhistories. While data driven deep learning (DL) approaches have shown promise\nas alternatives to traditional constitutive formulations, their deployment\nrequires models that are both accurate and capable of quantifying predictive\nuncertainty. This study introduces a recursive Bayesian neural network (rBNN)\nframework that unifies temporal sequence learning with generalized Bayesian\ninference to achieve both predictive accuracy and rigorous uncertainty\nquantification. A key innovation is the incorporation of a sliding window\nrecursive structure that enables the model to effectively capture path\ndependent soil responses under monotonic and cyclic loading. By treating\nnetwork parameters as random variables and inferring their posterior\ndistributions via generalized variational inference, the rBNN produces well\ncalibrated confidence intervals alongside point predictions.The framework is\nvalidated against four datasets spanning both simulated and experimental\ntriaxial tests: monotonic loading using a Hardening Soil model simulation and\n28 CD tests on Baskarp sand, and cyclic loading using an exponential\nconstitutive simulation of CD CU tests and 37 experimental cyclic CU tests on\nOttawa F65 sand. This progression from monotonic to cyclic and from simulated\nto experimental data demonstrates the adaptability of the proposed approach\nacross varying levels of data fidelity and complexity. Comparative analyses\nwith LSTM, Encoder Decoder,and GRU architectures highlight that rBNN not only\nachieves competitive predictive accuracy but also provides reliable confidence\nintervals."
                },
                "authors": [
                    {
                        "name": "Toiba Noor"
                    },
                    {
                        "name": "Soban Nasir Lone"
                    },
                    {
                        "name": "G. V. Ramana"
                    },
                    {
                        "name": "Rajdip Nayek"
                    }
                ],
                "author_detail": {
                    "name": "Rajdip Nayek"
                },
                "author": "Rajdip Nayek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08666v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08666v3",
                "updated": "2025-10-22T14:33:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    33,
                    49,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-09T16:19:42Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    19,
                    42,
                    3,
                    282,
                    0
                ],
                "title": "dInfer: An Efficient Inference Framework for Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "dInfer: An Efficient Inference Framework for Diffusion Language Models"
                },
                "summary": "Diffusion-based large language models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs, leveraging denoising-based generation\nto enable inherent parallelism. Even more and more open-sourced dLLM models\nemerge, yet their widespread adoption remains constrained by the lack of a\nstandardized and efficient inference framework. We present dInfer, an efficient\nand extensible framework for dLLM inference. dInfer decomposes the inference\npipeline into four modular components--model, diffusion iteration manager,\ndecoding strategy, and KV-cache manager--and integrates novel algorithms for\neach component alongside system-level optimizations. Through this combination\nof algorithmic innovations and system enhancements, dInfer achieves substantial\nefficiency gains without compromising output quality on LLaDA-MoE. At batch\nsize 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800\ntokens per second across six benchmarks on $8\\times$ H800 GPUs. Compared to\nprior systems, dInfer delivers a $10\\times$ speedup over Fast-dLLM while\nmaintaining similar model performance. Even compared to the AR model (with a\ncomparable number of activation parameters and performance) QWen2.5-3B, which\nis highly optimized with the latest vLLM inference engine, dInfer still\ndelivers a $2$-$3\\times$ speedup. The implementation of dInfer is open-sourced\nat https://github.com/inclusionAI/dInfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs, leveraging denoising-based generation\nto enable inherent parallelism. Even more and more open-sourced dLLM models\nemerge, yet their widespread adoption remains constrained by the lack of a\nstandardized and efficient inference framework. We present dInfer, an efficient\nand extensible framework for dLLM inference. dInfer decomposes the inference\npipeline into four modular components--model, diffusion iteration manager,\ndecoding strategy, and KV-cache manager--and integrates novel algorithms for\neach component alongside system-level optimizations. Through this combination\nof algorithmic innovations and system enhancements, dInfer achieves substantial\nefficiency gains without compromising output quality on LLaDA-MoE. At batch\nsize 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800\ntokens per second across six benchmarks on $8\\times$ H800 GPUs. Compared to\nprior systems, dInfer delivers a $10\\times$ speedup over Fast-dLLM while\nmaintaining similar model performance. Even compared to the AR model (with a\ncomparable number of activation parameters and performance) QWen2.5-3B, which\nis highly optimized with the latest vLLM inference engine, dInfer still\ndelivers a $2$-$3\\times$ speedup. The implementation of dInfer is open-sourced\nat https://github.com/inclusionAI/dInfer."
                },
                "authors": [
                    {
                        "name": "Yuxin Ma"
                    },
                    {
                        "name": "Lun Du"
                    },
                    {
                        "name": "Lanning Wei"
                    },
                    {
                        "name": "Kun Chen"
                    },
                    {
                        "name": "Qian Xu"
                    },
                    {
                        "name": "Kangyu Wang"
                    },
                    {
                        "name": "Guofeng Feng"
                    },
                    {
                        "name": "Guoshan Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Xiaojing Qi"
                    },
                    {
                        "name": "Xinyuan Zhang"
                    },
                    {
                        "name": "Zhen Tao"
                    },
                    {
                        "name": "Haibo Feng"
                    },
                    {
                        "name": "Ziyun Jiang"
                    },
                    {
                        "name": "Ying Xu"
                    },
                    {
                        "name": "Zenan Huang"
                    },
                    {
                        "name": "Yihong Zhuang"
                    },
                    {
                        "name": "Haokai Xu"
                    },
                    {
                        "name": "Jiaqi Hu"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    },
                    {
                        "name": "Junbo Zhao"
                    },
                    {
                        "name": "Jianguo Li"
                    },
                    {
                        "name": "Da Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Da Zheng"
                },
                "author": "Da Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08666v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08666v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19635v1",
                "updated": "2025-10-22T14:32:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    32,
                    13,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T14:32:13Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    32,
                    13,
                    2,
                    295,
                    0
                ],
                "title": "FAUST. XXVIII. High-Resolution ALMA Observations of Class 0/I Disks:\n  Structure, Optical Depths, and Temperatures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAUST. XXVIII. High-Resolution ALMA Observations of Class 0/I Disks:\n  Structure, Optical Depths, and Temperatures"
                },
                "summary": "We present high-resolution (~7.5 au) ALMA observations at 1.3 and 3 mm of 16\ndisks around Class 0/I protostars across multiple star-forming regions and a\nvariety of multiplicities, showing a range of disk sizes (~2-100 au) and\nincluding circumbinary disks (CBDs) in binaries with separations <100 au. The\ndisk properties show similarities to Class II disks, including (a) low spectral\nindex (SI) values (alpha=2.1) that increase with disk radius, (b) 3 mm disk\nsizes only marginally smaller than at 1.3 mm (<10%), and (c) radial intensity\nprofiles well described by modified self-similar profiles. We also find key\ndifferences: (i) SI values increasing with radius, but exceeding 2 only at the\ndisk edge (ii) higher brightness temperatures Tb, in some cases higher than the\npredicted temperatures due to irradiation, and (iii) ~10x higher luminosity at\na given size compared to the Class II disks. These results confirm significant\noptical depth in the observed Class 0/I disks, at both 1.3 and 3 mm, helping to\nexplain their higher luminosities, but higher temperatures are also required\nfor the most compact (< 40 au) disks, suggesting additional viscous heating.\nConsidering optical depth, most disk dust masses are estimated in the range\n30-900 Mearth (0.01-0.3 Msun in gas), resulting in some disks reaching marginal\ngravitational instability. The median location of the water iceline is ~3 au,\nbut it can extend beyond 10-20 au for the hottest disks. CBDs exhibit lower\noptical depths at both wavelengths and hence higher SI values (alpha=3.0), dust\nmasses of 100 Mearth, and beta~1.5 (2 Class 0 CBDs) and beta~1 (1 Class I CBD),\nsuggesting substantial grain growth only in the more evolved CBD. The inferred\nhigh optical depths provide a compelling explanation for the apparent scarcity\nof dust substructures in the younger disks at ~ 1 mm, despite mounting evidence\nfor early planet formation (ABRIDGED).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present high-resolution (~7.5 au) ALMA observations at 1.3 and 3 mm of 16\ndisks around Class 0/I protostars across multiple star-forming regions and a\nvariety of multiplicities, showing a range of disk sizes (~2-100 au) and\nincluding circumbinary disks (CBDs) in binaries with separations <100 au. The\ndisk properties show similarities to Class II disks, including (a) low spectral\nindex (SI) values (alpha=2.1) that increase with disk radius, (b) 3 mm disk\nsizes only marginally smaller than at 1.3 mm (<10%), and (c) radial intensity\nprofiles well described by modified self-similar profiles. We also find key\ndifferences: (i) SI values increasing with radius, but exceeding 2 only at the\ndisk edge (ii) higher brightness temperatures Tb, in some cases higher than the\npredicted temperatures due to irradiation, and (iii) ~10x higher luminosity at\na given size compared to the Class II disks. These results confirm significant\noptical depth in the observed Class 0/I disks, at both 1.3 and 3 mm, helping to\nexplain their higher luminosities, but higher temperatures are also required\nfor the most compact (< 40 au) disks, suggesting additional viscous heating.\nConsidering optical depth, most disk dust masses are estimated in the range\n30-900 Mearth (0.01-0.3 Msun in gas), resulting in some disks reaching marginal\ngravitational instability. The median location of the water iceline is ~3 au,\nbut it can extend beyond 10-20 au for the hottest disks. CBDs exhibit lower\noptical depths at both wavelengths and hence higher SI values (alpha=3.0), dust\nmasses of 100 Mearth, and beta~1.5 (2 Class 0 CBDs) and beta~1 (1 Class I CBD),\nsuggesting substantial grain growth only in the more evolved CBD. The inferred\nhigh optical depths provide a compelling explanation for the apparent scarcity\nof dust substructures in the younger disks at ~ 1 mm, despite mounting evidence\nfor early planet formation (ABRIDGED)."
                },
                "authors": [
                    {
                        "name": "M. J. Maureira"
                    },
                    {
                        "name": "J. E. Pineda"
                    },
                    {
                        "name": "H. B. Liu"
                    },
                    {
                        "name": "P. Caselli"
                    },
                    {
                        "name": "C. Chandler"
                    },
                    {
                        "name": "L. Testi"
                    },
                    {
                        "name": "D. Johnstone"
                    },
                    {
                        "name": "D. Segura-Cox"
                    },
                    {
                        "name": "L. Loinard"
                    },
                    {
                        "name": "E. Bianchi"
                    },
                    {
                        "name": "C. Codella"
                    },
                    {
                        "name": "A. Miotello"
                    },
                    {
                        "name": "L. Podio"
                    },
                    {
                        "name": "L. Cacciapuoti"
                    },
                    {
                        "name": "Y. Oya"
                    },
                    {
                        "name": "A. Lopez-Sepulcre"
                    },
                    {
                        "name": "N. Sakai"
                    },
                    {
                        "name": "Z. Zhang"
                    },
                    {
                        "name": "N. Cuello"
                    },
                    {
                        "name": "S. Ohashi"
                    },
                    {
                        "name": "Y. Aikawa"
                    },
                    {
                        "name": "G. Sabatini"
                    },
                    {
                        "name": "Y. Zhang"
                    },
                    {
                        "name": "C. Ceccarelli"
                    },
                    {
                        "name": "S. Yamamoto"
                    }
                ],
                "author_detail": {
                    "name": "S. Yamamoto"
                },
                "author": "S. Yamamoto",
                "arxiv_comment": "31 pages, 15 figures, Accepted for publication in A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19631v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19631v1",
                "updated": "2025-10-22T14:28:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    28,
                    33,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T14:28:33Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    28,
                    33,
                    2,
                    295,
                    0
                ],
                "title": "HSCodeComp: A Realistic and Expert-level Benchmark for Deep Search\n  Agents in Hierarchical Rule Application",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HSCodeComp: A Realistic and Expert-level Benchmark for Deep Search\n  Agents in Hierarchical Rule Application"
                },
                "summary": "Effective deep search agents must not only access open-domain and\ndomain-specific knowledge but also apply complex rules-such as legal clauses,\nmedical manuals and tariff rules. These rules often feature vague boundaries\nand implicit logic relationships, making precise application challenging for\nagents. However, this critical capability is largely overlooked by current\nagent benchmarks.\n  To fill this gap, we introduce HSCodeComp, the first realistic, expert-level\ne-commerce benchmark designed to evaluate deep search agents in hierarchical\nrule application. In this task, the deep reasoning process of agents is guided\nby these rules to predict 10-digit Harmonized System Code (HSCode) of products\nwith noisy but realistic descriptions. These codes, established by the World\nCustoms Organization, are vital for global supply chain efficiency. Built from\nreal-world data collected from large-scale e-commerce platforms, our proposed\nHSCodeComp comprises 632 product entries spanning diverse product categories,\nwith these HSCodes annotated by several human experts.\n  Extensive experimental results on several state-of-the-art LLMs, open-source,\nand closed-source agents reveal a huge performance gap: best agent achieves\nonly 46.8% 10-digit accuracy, far below human experts at 95.0%. Besides,\ndetailed analysis demonstrates the challenges of hierarchical rule application,\nand test-time scaling fails to improve performance further.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective deep search agents must not only access open-domain and\ndomain-specific knowledge but also apply complex rules-such as legal clauses,\nmedical manuals and tariff rules. These rules often feature vague boundaries\nand implicit logic relationships, making precise application challenging for\nagents. However, this critical capability is largely overlooked by current\nagent benchmarks.\n  To fill this gap, we introduce HSCodeComp, the first realistic, expert-level\ne-commerce benchmark designed to evaluate deep search agents in hierarchical\nrule application. In this task, the deep reasoning process of agents is guided\nby these rules to predict 10-digit Harmonized System Code (HSCode) of products\nwith noisy but realistic descriptions. These codes, established by the World\nCustoms Organization, are vital for global supply chain efficiency. Built from\nreal-world data collected from large-scale e-commerce platforms, our proposed\nHSCodeComp comprises 632 product entries spanning diverse product categories,\nwith these HSCodes annotated by several human experts.\n  Extensive experimental results on several state-of-the-art LLMs, open-source,\nand closed-source agents reveal a huge performance gap: best agent achieves\nonly 46.8% 10-digit accuracy, far below human experts at 95.0%. Besides,\ndetailed analysis demonstrates the challenges of hierarchical rule application,\nand test-time scaling fails to improve performance further."
                },
                "authors": [
                    {
                        "name": "Yiqian Yang"
                    },
                    {
                        "name": "Tian Lan"
                    },
                    {
                        "name": "Qianghuai Jia"
                    },
                    {
                        "name": "Li Zhu"
                    },
                    {
                        "name": "Hui Jiang"
                    },
                    {
                        "name": "Hang Zhu"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Weihua Luo"
                    },
                    {
                        "name": "Kaifu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kaifu Zhang"
                },
                "author": "Kaifu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19631v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19631v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.16519v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.16519v4",
                "updated": "2025-10-22T14:23:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    23,
                    58,
                    2,
                    295,
                    0
                ],
                "published": "2023-09-28T15:25:17Z",
                "published_parsed": [
                    2023,
                    9,
                    28,
                    15,
                    25,
                    17,
                    3,
                    271,
                    0
                ],
                "title": "AtomSurf : Surface Representation for Learning on Protein Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AtomSurf : Surface Representation for Learning on Protein Structures"
                },
                "summary": "While there has been significant progress in evaluating and comparing\ndifferent representations for learning on protein data, the role of\nsurface-based learning approaches remains not well-understood. In particular,\nthere is a lack of direct and fair benchmark comparison between the best\navailable surface-based learning methods against alternative representations\nsuch as graphs. Moreover, the few existing surface-based approaches either use\nsurface information in isolation or, at best, perform global pooling between\nsurface and graph-based architectures.\n  In this work, we fill this gap by first adapting a state-of-the-art surface\nencoder for protein learning tasks. We then perform a direct and fair\ncomparison of the resulting method against alternative approaches within the\nAtom3D benchmark, highlighting the limitations of pure surface-based learning.\nFinally, we propose an integrated approach, which allows learned feature\nsharing between graphs and surface representations on the level of nodes and\nvertices across all layers.\n  We demonstrate that the resulting architecture achieves state-of-the-art\nresults on all tasks in the Atom3D benchmark, while adhering to the strict\nbenchmark protocol, as well as more broadly on binding site identification and\nbinding pocket classification. Furthermore, we use coarsened surfaces and\noptimize our approach for efficiency, making our tool competitive in training\nand inference time with existing techniques. Code can be found online:\nhttps://github.com/Vincentx15/atomsurf",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While there has been significant progress in evaluating and comparing\ndifferent representations for learning on protein data, the role of\nsurface-based learning approaches remains not well-understood. In particular,\nthere is a lack of direct and fair benchmark comparison between the best\navailable surface-based learning methods against alternative representations\nsuch as graphs. Moreover, the few existing surface-based approaches either use\nsurface information in isolation or, at best, perform global pooling between\nsurface and graph-based architectures.\n  In this work, we fill this gap by first adapting a state-of-the-art surface\nencoder for protein learning tasks. We then perform a direct and fair\ncomparison of the resulting method against alternative approaches within the\nAtom3D benchmark, highlighting the limitations of pure surface-based learning.\nFinally, we propose an integrated approach, which allows learned feature\nsharing between graphs and surface representations on the level of nodes and\nvertices across all layers.\n  We demonstrate that the resulting architecture achieves state-of-the-art\nresults on all tasks in the Atom3D benchmark, while adhering to the strict\nbenchmark protocol, as well as more broadly on binding site identification and\nbinding pocket classification. Furthermore, we use coarsened surfaces and\noptimize our approach for efficiency, making our tool competitive in training\nand inference time with existing techniques. Code can be found online:\nhttps://github.com/Vincentx15/atomsurf"
                },
                "authors": [
                    {
                        "name": "Vincent Mallet"
                    },
                    {
                        "name": "Souhaib Attaiki"
                    },
                    {
                        "name": "Yangyang Miao"
                    },
                    {
                        "name": "Bruno Correia"
                    },
                    {
                        "name": "Maks Ovsjanikov"
                    }
                ],
                "author_detail": {
                    "name": "Maks Ovsjanikov"
                },
                "author": "Maks Ovsjanikov",
                "arxiv_comment": "Published as a conference paper at The Thirteenth International\n  Conference on Learning Representations (ICLR 2025). The official open-access\n  version is available at https://openreview.net/forum?id=ARQIJXFcTH",
                "arxiv_journal_ref": "The Thirteenth International Conference on Learning\n  Representations (ICLR), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.16519v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.16519v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19628v1",
                "updated": "2025-10-22T14:23:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    23,
                    50,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T14:23:50Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    23,
                    50,
                    2,
                    295,
                    0
                ],
                "title": "CrossNews-UA: A Cross-lingual News Semantic Similarity Benchmark for\n  Ukrainian, Polish, Russian, and English",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CrossNews-UA: A Cross-lingual News Semantic Similarity Benchmark for\n  Ukrainian, Polish, Russian, and English"
                },
                "summary": "In the era of social networks and rapid misinformation spread, news analysis\nremains a critical task. Detecting fake news across multiple languages,\nparticularly beyond English, poses significant challenges. Cross-lingual news\ncomparison offers a promising approach to verify information by leveraging\nexternal sources in different languages (Chen and Shu, 2024). However, existing\ndatasets for cross-lingual news analysis (Chen et al., 2022a) were manually\ncurated by journalists and experts, limiting their scalability and adaptability\nto new languages. In this work, we address this gap by introducing a scalable,\nexplainable crowdsourcing pipeline for cross-lingual news similarity\nassessment. Using this pipeline, we collected a novel dataset CrossNews-UA of\nnews pairs in Ukrainian as a central language with linguistically and\ncontextually relevant languages-Polish, Russian, and English. Each news pair is\nannotated for semantic similarity with detailed justifications based on the 4W\ncriteria (Who, What, Where, When). We further tested a range of models, from\ntraditional bag-of-words, Transformer-based architectures to large language\nmodels (LLMs). Our results highlight the challenges in multilingual news\nanalysis and offer insights into models performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of social networks and rapid misinformation spread, news analysis\nremains a critical task. Detecting fake news across multiple languages,\nparticularly beyond English, poses significant challenges. Cross-lingual news\ncomparison offers a promising approach to verify information by leveraging\nexternal sources in different languages (Chen and Shu, 2024). However, existing\ndatasets for cross-lingual news analysis (Chen et al., 2022a) were manually\ncurated by journalists and experts, limiting their scalability and adaptability\nto new languages. In this work, we address this gap by introducing a scalable,\nexplainable crowdsourcing pipeline for cross-lingual news similarity\nassessment. Using this pipeline, we collected a novel dataset CrossNews-UA of\nnews pairs in Ukrainian as a central language with linguistically and\ncontextually relevant languages-Polish, Russian, and English. Each news pair is\nannotated for semantic similarity with detailed justifications based on the 4W\ncriteria (Who, What, Where, When). We further tested a range of models, from\ntraditional bag-of-words, Transformer-based architectures to large language\nmodels (LLMs). Our results highlight the challenges in multilingual news\nanalysis and offer insights into models performance."
                },
                "authors": [
                    {
                        "name": "Daryna Dementieva"
                    },
                    {
                        "name": "Evgeniya Sukhodolskaya"
                    },
                    {
                        "name": "Alexander Fraser"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Fraser"
                },
                "author": "Alexander Fraser",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17138v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17138v2",
                "updated": "2025-10-22T14:21:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    21,
                    7,
                    2,
                    295,
                    0
                ],
                "published": "2025-09-21T16:02:42Z",
                "published_parsed": [
                    2025,
                    9,
                    21,
                    16,
                    2,
                    42,
                    6,
                    264,
                    0
                ],
                "title": "Analyzing Memory Effects in Large Language Models through the lens of\n  Cognitive Psychology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing Memory Effects in Large Language Models through the lens of\n  Cognitive Psychology"
                },
                "summary": "Memory, a fundamental component of human cognition, exhibits adaptive yet\nfallible characteristics as illustrated by Schacter's memory \"sins\".These\ncognitive phenomena have been studied extensively in psychology and\nneuroscience, but the extent to which artificial systems, specifically Large\nLanguage Models (LLMs), emulate these cognitive phenomena remains\nunderexplored. This study uses human memory research as a lens for\nunderstanding LLMs and systematically investigates human memory effects in\nstate-of-the-art LLMs using paradigms drawn from psychological research. We\nevaluate seven key memory phenomena, comparing human behavior to LLM\nperformance. Both people and models remember less when overloaded with\ninformation (list length effect) and remember better with repeated exposure\n(list strength effect). They also show similar difficulties when retrieving\noverlapping information, where storing too many similar facts leads to\nconfusion (fan effect). Like humans, LLMs are susceptible to falsely\n\"remembering\" words that were never shown but are related to others (false\nmemories), and they can apply prior learning to new, related situations\n(cross-domain generalization). However, LLMs differ in two key ways: they are\nless influenced by the order in which information is presented (positional\nbias) and more robust when processing random or meaningless material (nonsense\neffect). These results reveal both alignments and divergences in how LLMs and\nhumans reconstruct memory. The findings help clarify how memory-like behavior\nin LLMs echoes core features of human cognition, while also highlighting the\narchitectural differences that lead to distinct patterns of error and success.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory, a fundamental component of human cognition, exhibits adaptive yet\nfallible characteristics as illustrated by Schacter's memory \"sins\".These\ncognitive phenomena have been studied extensively in psychology and\nneuroscience, but the extent to which artificial systems, specifically Large\nLanguage Models (LLMs), emulate these cognitive phenomena remains\nunderexplored. This study uses human memory research as a lens for\nunderstanding LLMs and systematically investigates human memory effects in\nstate-of-the-art LLMs using paradigms drawn from psychological research. We\nevaluate seven key memory phenomena, comparing human behavior to LLM\nperformance. Both people and models remember less when overloaded with\ninformation (list length effect) and remember better with repeated exposure\n(list strength effect). They also show similar difficulties when retrieving\noverlapping information, where storing too many similar facts leads to\nconfusion (fan effect). Like humans, LLMs are susceptible to falsely\n\"remembering\" words that were never shown but are related to others (false\nmemories), and they can apply prior learning to new, related situations\n(cross-domain generalization). However, LLMs differ in two key ways: they are\nless influenced by the order in which information is presented (positional\nbias) and more robust when processing random or meaningless material (nonsense\neffect). These results reveal both alignments and divergences in how LLMs and\nhumans reconstruct memory. The findings help clarify how memory-like behavior\nin LLMs echoes core features of human cognition, while also highlighting the\narchitectural differences that lead to distinct patterns of error and success."
                },
                "authors": [
                    {
                        "name": "Zhaoyang Cao"
                    },
                    {
                        "name": "Lael Schooler"
                    },
                    {
                        "name": "Reza Zafarani"
                    }
                ],
                "author_detail": {
                    "name": "Reza Zafarani"
                },
                "author": "Reza Zafarani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17138v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17138v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19616v1",
                "updated": "2025-10-22T14:12:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    12,
                    0,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T14:12:00Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    12,
                    0,
                    2,
                    295,
                    0
                ],
                "title": "PBBQ: A Persian Bias Benchmark Dataset Curated with Human-AI\n  Collaboration for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PBBQ: A Persian Bias Benchmark Dataset Curated with Human-AI\n  Collaboration for Large Language Models"
                },
                "summary": "With the increasing adoption of large language models (LLMs), ensuring their\nalignment with social norms has become a critical concern. While prior research\nhas examined bias detection in various languages, there remains a significant\ngap in resources addressing social biases within Persian cultural contexts. In\nthis work, we introduce PBBQ, a comprehensive benchmark dataset designed to\nevaluate social biases in Persian LLMs. Our benchmark, which encompasses 16\ncultural categories, was developed through questionnaires completed by 250\ndiverse individuals across multiple demographics, in close collaboration with\nsocial science experts to ensure its validity. The resulting PBBQ dataset\ncontains over 37,000 carefully curated questions, providing a foundation for\nthe evaluation and mitigation of bias in Persian language models. We benchmark\nseveral open-source LLMs, a closed-source model, and Persian-specific\nfine-tuned models on PBBQ. Our findings reveal that current LLMs exhibit\nsignificant social biases across Persian culture. Additionally, by comparing\nmodel outputs to human responses, we observe that LLMs often replicate human\nbias patterns, highlighting the complex interplay between learned\nrepresentations and cultural stereotypes.Upon acceptance of the paper, our PBBQ\ndataset will be publicly available for use in future work. Content warning:\nThis paper contains unsafe content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing adoption of large language models (LLMs), ensuring their\nalignment with social norms has become a critical concern. While prior research\nhas examined bias detection in various languages, there remains a significant\ngap in resources addressing social biases within Persian cultural contexts. In\nthis work, we introduce PBBQ, a comprehensive benchmark dataset designed to\nevaluate social biases in Persian LLMs. Our benchmark, which encompasses 16\ncultural categories, was developed through questionnaires completed by 250\ndiverse individuals across multiple demographics, in close collaboration with\nsocial science experts to ensure its validity. The resulting PBBQ dataset\ncontains over 37,000 carefully curated questions, providing a foundation for\nthe evaluation and mitigation of bias in Persian language models. We benchmark\nseveral open-source LLMs, a closed-source model, and Persian-specific\nfine-tuned models on PBBQ. Our findings reveal that current LLMs exhibit\nsignificant social biases across Persian culture. Additionally, by comparing\nmodel outputs to human responses, we observe that LLMs often replicate human\nbias patterns, highlighting the complex interplay between learned\nrepresentations and cultural stereotypes.Upon acceptance of the paper, our PBBQ\ndataset will be publicly available for use in future work. Content warning:\nThis paper contains unsafe content."
                },
                "authors": [
                    {
                        "name": "Farhan Farsi"
                    },
                    {
                        "name": "Shayan Bali"
                    },
                    {
                        "name": "Fatemeh Valeh"
                    },
                    {
                        "name": "Parsa Ghofrani"
                    },
                    {
                        "name": "Alireza Pakniat"
                    },
                    {
                        "name": "Kian Kashfipour"
                    },
                    {
                        "name": "Amir H. Payberah"
                    }
                ],
                "author_detail": {
                    "name": "Amir H. Payberah"
                },
                "author": "Amir H. Payberah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19615v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19615v1",
                "updated": "2025-10-22T14:11:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    11,
                    44,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T14:11:44Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    11,
                    44,
                    2,
                    295,
                    0
                ],
                "title": "FidelityGPT: Correcting Decompilation Distortions with Retrieval\n  Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FidelityGPT: Correcting Decompilation Distortions with Retrieval\n  Augmented Generation"
                },
                "summary": "Decompilation converts machine code into human-readable form, enabling\nanalysis and debugging without source code. However, fidelity issues often\ndegrade the readability and semantic accuracy of decompiled output. Existing\nmethods, such as variable renaming or structural simplification, provide\npartial improvements but lack robust detection and correction, particularly for\ncomplex closed-source binaries. We present FidelityGPT, a framework that\nenhances decompiled code accuracy and readability by systematically detecting\nand correcting semantic distortions. FidelityGPT introduces distortion-aware\nprompt templates tailored to closed-source settings and integrates\nRetrieval-Augmented Generation (RAG) with a dynamic semantic intensity\nalgorithm to locate distorted lines and retrieve semantically similar code from\na database. A variable dependency algorithm further mitigates long-context\nlimitations by analyzing redundant variables and integrating their dependencies\ninto the prompt context. Evaluated on 620 function pairs from a binary\nsimilarity benchmark, FidelityGPT achieved an average detection accuracy of 89%\nand a precision of 83%. Compared to the state-of-the-art DeGPT (Fix Rate 83%,\nCorrected Fix Rate 37%), FidelityGPT attained 94% FR and 64% CFR, demonstrating\nsignificant gains in accuracy and readability. These results highlight its\npotential to advance LLM-based decompilation and reverse engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decompilation converts machine code into human-readable form, enabling\nanalysis and debugging without source code. However, fidelity issues often\ndegrade the readability and semantic accuracy of decompiled output. Existing\nmethods, such as variable renaming or structural simplification, provide\npartial improvements but lack robust detection and correction, particularly for\ncomplex closed-source binaries. We present FidelityGPT, a framework that\nenhances decompiled code accuracy and readability by systematically detecting\nand correcting semantic distortions. FidelityGPT introduces distortion-aware\nprompt templates tailored to closed-source settings and integrates\nRetrieval-Augmented Generation (RAG) with a dynamic semantic intensity\nalgorithm to locate distorted lines and retrieve semantically similar code from\na database. A variable dependency algorithm further mitigates long-context\nlimitations by analyzing redundant variables and integrating their dependencies\ninto the prompt context. Evaluated on 620 function pairs from a binary\nsimilarity benchmark, FidelityGPT achieved an average detection accuracy of 89%\nand a precision of 83%. Compared to the state-of-the-art DeGPT (Fix Rate 83%,\nCorrected Fix Rate 37%), FidelityGPT attained 94% FR and 64% CFR, demonstrating\nsignificant gains in accuracy and readability. These results highlight its\npotential to advance LLM-based decompilation and reverse engineering."
                },
                "authors": [
                    {
                        "name": "Zhiping Zhou"
                    },
                    {
                        "name": "Xiaohong Li"
                    },
                    {
                        "name": "Ruitao Feng"
                    },
                    {
                        "name": "Yao Zhang"
                    },
                    {
                        "name": "Yuekang Li"
                    },
                    {
                        "name": "Wenbu Feng"
                    },
                    {
                        "name": "Yunqian Wang"
                    },
                    {
                        "name": "Yuqing Li"
                    }
                ],
                "author_detail": {
                    "name": "Yuqing Li"
                },
                "author": "Yuqing Li",
                "arxiv_doi": "10.14722/ndss.2026.230989",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.14722/ndss.2026.230989",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.19615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19615v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15268v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15268v2",
                "updated": "2025-10-22T14:04:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    4,
                    44,
                    2,
                    295,
                    0
                ],
                "published": "2025-07-21T06:13:53Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    6,
                    13,
                    53,
                    0,
                    202,
                    0
                ],
                "title": "IM-Chat: A Multi-agent LLM Framework Integrating Tool-Calling and\n  Diffusion Modeling for Knowledge Transfer in Injection Molding Industry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IM-Chat: A Multi-agent LLM Framework Integrating Tool-Calling and\n  Diffusion Modeling for Knowledge Transfer in Injection Molding Industry"
                },
                "summary": "The injection molding industry faces critical challenges in preserving and\ntransferring field knowledge, particularly as experienced workers retire and\nmultilingual barriers hinder effective communication. This study introduces\nIM-Chat, a multi-agent framework based on large language models (LLMs),\ndesigned to facilitate knowledge transfer in injection molding. IM-Chat\nintegrates both limited documented knowledge (e.g., troubleshooting tables,\nmanuals) and extensive field data modeled through a data-driven process\ncondition generator that infers optimal manufacturing settings from\nenvironmental inputs such as temperature and humidity, enabling robust and\ncontext-aware task resolution. By adopting a retrieval-augmented generation\n(RAG) strategy and tool-calling agents within a modular architecture, IM-Chat\nensures adaptability without the need for fine-tuning. Performance was assessed\nacross 100 single-tool and 60 hybrid tasks for GPT-4o, GPT-4o-mini, and\nGPT-3.5-turbo by domain experts using a 10-point rubric focused on relevance\nand correctness, and was further supplemented by automated evaluation using\nGPT-4o guided by a domain-adapted instruction prompt. The evaluation results\nindicate that more capable models tend to achieve higher accuracy, particularly\nin complex, tool-integrated scenarios. In addition, compared with the\nfine-tuned single-agent LLM, IM-Chat demonstrated superior accuracy,\nparticularly in quantitative reasoning, and greater scalability in handling\nmultiple information sources. Overall, these findings demonstrate the viability\nof multi-agent LLM systems for industrial knowledge workflows and establish\nIM-Chat as a scalable and generalizable approach to AI-assisted decision\nsupport in manufacturing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The injection molding industry faces critical challenges in preserving and\ntransferring field knowledge, particularly as experienced workers retire and\nmultilingual barriers hinder effective communication. This study introduces\nIM-Chat, a multi-agent framework based on large language models (LLMs),\ndesigned to facilitate knowledge transfer in injection molding. IM-Chat\nintegrates both limited documented knowledge (e.g., troubleshooting tables,\nmanuals) and extensive field data modeled through a data-driven process\ncondition generator that infers optimal manufacturing settings from\nenvironmental inputs such as temperature and humidity, enabling robust and\ncontext-aware task resolution. By adopting a retrieval-augmented generation\n(RAG) strategy and tool-calling agents within a modular architecture, IM-Chat\nensures adaptability without the need for fine-tuning. Performance was assessed\nacross 100 single-tool and 60 hybrid tasks for GPT-4o, GPT-4o-mini, and\nGPT-3.5-turbo by domain experts using a 10-point rubric focused on relevance\nand correctness, and was further supplemented by automated evaluation using\nGPT-4o guided by a domain-adapted instruction prompt. The evaluation results\nindicate that more capable models tend to achieve higher accuracy, particularly\nin complex, tool-integrated scenarios. In addition, compared with the\nfine-tuned single-agent LLM, IM-Chat demonstrated superior accuracy,\nparticularly in quantitative reasoning, and greater scalability in handling\nmultiple information sources. Overall, these findings demonstrate the viability\nof multi-agent LLM systems for industrial knowledge workflows and establish\nIM-Chat as a scalable and generalizable approach to AI-assisted decision\nsupport in manufacturing."
                },
                "authors": [
                    {
                        "name": "Junhyeong Lee"
                    },
                    {
                        "name": "Joon-Young Kim"
                    },
                    {
                        "name": "Heekyu Kim"
                    },
                    {
                        "name": "Inhyo Lee"
                    },
                    {
                        "name": "Seunghwa Ryu"
                    }
                ],
                "author_detail": {
                    "name": "Seunghwa Ryu"
                },
                "author": "Seunghwa Ryu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15268v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15268v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19611v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19611v1",
                "updated": "2025-10-22T14:04:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    4,
                    42,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T14:04:42Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    4,
                    42,
                    2,
                    295,
                    0
                ],
                "title": "A Climate-Aware Deep Learning Framework for Generalizable Epidemic\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Climate-Aware Deep Learning Framework for Generalizable Epidemic\n  Forecasting"
                },
                "summary": "Precise outbreak forecasting of infectious diseases is essential for\neffective public health responses and epidemic control. The increased\navailability of machine learning (ML) methods for time-series forecasting\npresents an enticing avenue to enhance outbreak forecasting. Though the\nCOVID-19 outbreak demonstrated the value of applying ML models to predict\nepidemic profiles, using ML models to forecast endemic diseases remains\nunderexplored. In this work, we present ForecastNet-XCL (an ensemble model\nbased on XGBoost+CNN+BiLSTM), a deep learning hybrid framework designed to\naddresses this gap by creating accurate multi-week RSV forecasts up to 100\nweeks in advance based on climate and temporal data, without access to\nreal-time surveillance on RSV. The framework combines high-resolution feature\nlearning with long-range temporal dependency capturing mechanisms, bolstered by\nan autoregressive module trained on climate-controlled lagged relations.\nStochastic inference returns probabilistic intervals to inform decision-making.\nEvaluated across 34 U.S. states, ForecastNet-XCL reliably outperformed\nstatistical baselines, individual neural nets, and conventional ensemble\nmethods in both within- and cross-state scenarios, sustaining accuracy over\nextended forecast horizons. Training on climatologically diverse datasets\nenhanced generalization furthermore, particularly in locations having irregular\nor biennial RSV patterns. ForecastNet-XCL's efficiency, performance, and\nuncertainty-aware design make it a deployable early-warning tool amid\nescalating climate pressures and constrained surveillance resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precise outbreak forecasting of infectious diseases is essential for\neffective public health responses and epidemic control. The increased\navailability of machine learning (ML) methods for time-series forecasting\npresents an enticing avenue to enhance outbreak forecasting. Though the\nCOVID-19 outbreak demonstrated the value of applying ML models to predict\nepidemic profiles, using ML models to forecast endemic diseases remains\nunderexplored. In this work, we present ForecastNet-XCL (an ensemble model\nbased on XGBoost+CNN+BiLSTM), a deep learning hybrid framework designed to\naddresses this gap by creating accurate multi-week RSV forecasts up to 100\nweeks in advance based on climate and temporal data, without access to\nreal-time surveillance on RSV. The framework combines high-resolution feature\nlearning with long-range temporal dependency capturing mechanisms, bolstered by\nan autoregressive module trained on climate-controlled lagged relations.\nStochastic inference returns probabilistic intervals to inform decision-making.\nEvaluated across 34 U.S. states, ForecastNet-XCL reliably outperformed\nstatistical baselines, individual neural nets, and conventional ensemble\nmethods in both within- and cross-state scenarios, sustaining accuracy over\nextended forecast horizons. Training on climatologically diverse datasets\nenhanced generalization furthermore, particularly in locations having irregular\nor biennial RSV patterns. ForecastNet-XCL's efficiency, performance, and\nuncertainty-aware design make it a deployable early-warning tool amid\nescalating climate pressures and constrained surveillance resources."
                },
                "authors": [
                    {
                        "name": "Jinpyo Hong"
                    },
                    {
                        "name": "Rachel E. Baker"
                    }
                ],
                "author_detail": {
                    "name": "Rachel E. Baker"
                },
                "author": "Rachel E. Baker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19611v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19611v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24625v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24625v3",
                "updated": "2025-10-22T14:04:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    4,
                    1,
                    2,
                    295,
                    0
                ],
                "published": "2025-05-30T14:16:41Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    16,
                    41,
                    4,
                    150,
                    0
                ],
                "title": "Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision\n  Geometry Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision\n  Geometry Priors"
                },
                "summary": "Previous research has investigated the application of Multimodal Large\nLanguage Models (MLLMs) in understanding 3D scenes by interpreting them as\nvideos. These approaches generally depend on comprehensive 3D data inputs, such\nas point clouds or reconstructed Bird's-Eye View (BEV) maps. In our research,\nwe advance this field by enhancing the capability of MLLMs to understand and\nreason in 3D spaces directly from video data, without the need for additional\n3D input. We propose a novel and efficient method called the Video-3D Geometry\nLarge Language Model (VG LLM). Our approach utilizes a 3D visual geometry\nencoder to extract 3D prior information from video sequences. This information\nis then integrated with visual tokens and input into the MLLM. Extensive\nexperiments have shown that our method has achieved substantial improvements in\nvarious tasks related to 3D scene understanding and spatial reasoning, all\ndirectly learned from video sources. Impressively, our 4B model, which does not\nrely on explicit 3D data inputs, achieves competitive results compared to\nexisting state-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the\nVSI-Bench evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous research has investigated the application of Multimodal Large\nLanguage Models (MLLMs) in understanding 3D scenes by interpreting them as\nvideos. These approaches generally depend on comprehensive 3D data inputs, such\nas point clouds or reconstructed Bird's-Eye View (BEV) maps. In our research,\nwe advance this field by enhancing the capability of MLLMs to understand and\nreason in 3D spaces directly from video data, without the need for additional\n3D input. We propose a novel and efficient method called the Video-3D Geometry\nLarge Language Model (VG LLM). Our approach utilizes a 3D visual geometry\nencoder to extract 3D prior information from video sequences. This information\nis then integrated with visual tokens and input into the MLLM. Extensive\nexperiments have shown that our method has achieved substantial improvements in\nvarious tasks related to 3D scene understanding and spatial reasoning, all\ndirectly learned from video sources. Impressively, our 4B model, which does not\nrely on explicit 3D data inputs, achieves competitive results compared to\nexisting state-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the\nVSI-Bench evaluations."
                },
                "authors": [
                    {
                        "name": "Duo Zheng"
                    },
                    {
                        "name": "Shijia Huang"
                    },
                    {
                        "name": "Yanyang Li"
                    },
                    {
                        "name": "Liwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Liwei Wang"
                },
                "author": "Liwei Wang",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24625v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24625v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19606v1",
                "updated": "2025-10-22T14:00:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    0,
                    23,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T14:00:23Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    0,
                    23,
                    2,
                    295,
                    0
                ],
                "title": "Revisiting the Radio Lateral Distribution Function: An amplitude\n  dependence on $X_{\\rm max}$ and primary composition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting the Radio Lateral Distribution Function: An amplitude\n  dependence on $X_{\\rm max}$ and primary composition"
                },
                "summary": "We show that there is a strong dependence of the radio LDF electric field\namplitudes at ground level on the position of $X_{\\rm max}$ in the atmosphere,\neven accounting for differences in the EM energy of the showers. Since an\n$X_{\\rm max}$ dependence leads to a primary composition dependence, this\nimplies that information on the mass composition is encoded not only in the LDF\nshape but also in its amplitude. This $X_{\\rm max}$ dependence can be explained\nin terms of two competing scalings of the measured electric field: One goes\nwith $(1/\\rho)^J$, where $\\rho$ is the air density at $X_{\\rm max}$ and $J$ is\na zenith dependent non-linearity factor describing coherence loss. This density\nscaling tends to decrease the geomagnetic emission of deeper showers. The other\nscaling goes with $(1/R)$, where $R$ is the distance from $X_{\\rm max}$ to the\ncore at ground, and instead increases the measured electric field of deeper\nshowers. At low zenith angles, the $(1/R)$ scaling is stronger and leads to\nlarger measured electric fields as $X_{\\rm max}$ increases. The picture at\nhigher zeniths, i.e., lower densities, is more nuanced. In this region, the\ndeflections due to the Lorentz force are much larger and introduce extra time\ndelays between the particle tracks, decreasing the coherence of the emission.\nThis loss of coherence is highly dependent on the strength of the geomagnetic\nfield and can slow down, or even reverse the increase of the radio emission\nwith decreasing air density. This strong, yet historically overlooked LDF\namplitude dependence on $X_{\\rm max}$/composition could be used to directly\ninfer, even bypassing any $X_{\\rm max}$ reconstruction, the cosmic ray primary\ncomposition on an event-by-event basis. It could also have some repercussions\non other radio reconstruction methods, such as a possible $X_{\\rm\nmax}$/composition bias on shower electromagnetic energy reconstruction methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show that there is a strong dependence of the radio LDF electric field\namplitudes at ground level on the position of $X_{\\rm max}$ in the atmosphere,\neven accounting for differences in the EM energy of the showers. Since an\n$X_{\\rm max}$ dependence leads to a primary composition dependence, this\nimplies that information on the mass composition is encoded not only in the LDF\nshape but also in its amplitude. This $X_{\\rm max}$ dependence can be explained\nin terms of two competing scalings of the measured electric field: One goes\nwith $(1/\\rho)^J$, where $\\rho$ is the air density at $X_{\\rm max}$ and $J$ is\na zenith dependent non-linearity factor describing coherence loss. This density\nscaling tends to decrease the geomagnetic emission of deeper showers. The other\nscaling goes with $(1/R)$, where $R$ is the distance from $X_{\\rm max}$ to the\ncore at ground, and instead increases the measured electric field of deeper\nshowers. At low zenith angles, the $(1/R)$ scaling is stronger and leads to\nlarger measured electric fields as $X_{\\rm max}$ increases. The picture at\nhigher zeniths, i.e., lower densities, is more nuanced. In this region, the\ndeflections due to the Lorentz force are much larger and introduce extra time\ndelays between the particle tracks, decreasing the coherence of the emission.\nThis loss of coherence is highly dependent on the strength of the geomagnetic\nfield and can slow down, or even reverse the increase of the radio emission\nwith decreasing air density. This strong, yet historically overlooked LDF\namplitude dependence on $X_{\\rm max}$/composition could be used to directly\ninfer, even bypassing any $X_{\\rm max}$ reconstruction, the cosmic ray primary\ncomposition on an event-by-event basis. It could also have some repercussions\non other radio reconstruction methods, such as a possible $X_{\\rm\nmax}$/composition bias on shower electromagnetic energy reconstruction methods."
                },
                "authors": [
                    {
                        "name": "Washington R. Carvalho Jr."
                    },
                    {
                        "name": "Lech Piotrowski"
                    }
                ],
                "author_detail": {
                    "name": "Lech Piotrowski"
                },
                "author": "Lech Piotrowski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16206v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16206v2",
                "updated": "2025-10-22T13:56:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    13,
                    56,
                    17,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-17T20:38:12Z",
                "published_parsed": [
                    2025,
                    10,
                    17,
                    20,
                    38,
                    12,
                    4,
                    290,
                    0
                ],
                "title": "The Right to Be Remembered: Preserving Maximally Truthful Digital Memory\n  in the Age of AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Right to Be Remembered: Preserving Maximally Truthful Digital Memory\n  in the Age of AI"
                },
                "summary": "Since the rapid expansion of large language models (LLMs), people have begun\nto rely on them for information retrieval. While traditional search engines\ndisplay ranked lists of sources shaped by search engine optimization (SEO),\nadvertising, and personalization, LLMs typically provide a synthesized response\nthat feels singular and authoritative. While both approaches carry risks of\nbias and omission, LLMs may amplify the effect by collapsing multiple\nperspectives into one answer, reducing users ability or inclination to compare\nalternatives. This concentrates power over information in a few LLM vendors\nwhose systems effectively shape what is remembered and what is overlooked. As a\nresult, certain narratives, individuals or groups, may be disproportionately\nsuppressed, while others are disproportionately elevated. Over time, this\ncreates a new threat: the gradual erasure of those with limited digital\npresence, and the amplification of those already prominent, reshaping\ncollective memory. To address these concerns, this paper presents a concept of\nthe Right To Be Remembered (RTBR) which encompasses minimizing the risk of\nAI-driven information omission, embracing the right of fair treatment, while\nensuring that the generated content would be maximally truthful.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the rapid expansion of large language models (LLMs), people have begun\nto rely on them for information retrieval. While traditional search engines\ndisplay ranked lists of sources shaped by search engine optimization (SEO),\nadvertising, and personalization, LLMs typically provide a synthesized response\nthat feels singular and authoritative. While both approaches carry risks of\nbias and omission, LLMs may amplify the effect by collapsing multiple\nperspectives into one answer, reducing users ability or inclination to compare\nalternatives. This concentrates power over information in a few LLM vendors\nwhose systems effectively shape what is remembered and what is overlooked. As a\nresult, certain narratives, individuals or groups, may be disproportionately\nsuppressed, while others are disproportionately elevated. Over time, this\ncreates a new threat: the gradual erasure of those with limited digital\npresence, and the amplification of those already prominent, reshaping\ncollective memory. To address these concerns, this paper presents a concept of\nthe Right To Be Remembered (RTBR) which encompasses minimizing the risk of\nAI-driven information omission, embracing the right of fair treatment, while\nensuring that the generated content would be maximally truthful."
                },
                "authors": [
                    {
                        "name": "Alex Zhavoronkov"
                    },
                    {
                        "name": "Dominika Wilczok"
                    },
                    {
                        "name": "Roman Yampolskiy"
                    }
                ],
                "author_detail": {
                    "name": "Roman Yampolskiy"
                },
                "author": "Roman Yampolskiy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16206v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16206v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13878v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13878v2",
                "updated": "2025-10-22T13:55:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    13,
                    55,
                    29,
                    2,
                    295,
                    0
                ],
                "published": "2025-05-20T03:32:37Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    3,
                    32,
                    37,
                    1,
                    140,
                    0
                ],
                "title": "InfiFPO: Implicit Model Fusion via Preference Optimization in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiFPO: Implicit Model Fusion via Preference Optimization in Large\n  Language Models"
                },
                "summary": "Model fusion combines multiple Large Language Models (LLMs) with different\nstrengths into a more powerful, integrated model through lightweight training\nmethods. Existing works on model fusion focus primarily on supervised\nfine-tuning (SFT), leaving preference alignment (PA) --a critical phase for\nenhancing LLM performance--largely unexplored. The current few fusion methods\non PA phase, like WRPO, simplify the process by utilizing only response outputs\nfrom source models while discarding their probability information. To address\nthis limitation, we propose InfiFPO, a preference optimization method for\nimplicit model fusion. InfiFPO replaces the reference model in Direct\nPreference Optimization (DPO) with a fused source model that synthesizes\nmulti-source probabilities at the sequence level, circumventing complex\nvocabulary alignment challenges in previous works and meanwhile maintaining the\nprobability information. By introducing probability clipping and max-margin\nfusion strategies, InfiFPO enables the pivot model to align with human\npreferences while effectively distilling knowledge from source models.\nComprehensive experiments on 11 widely-used benchmarks demonstrate that InfiFPO\nconsistently outperforms existing model fusion and preference optimization\nmethods. When using Phi-4 as the pivot model, InfiFPO improve its average\nperformance from 79.95 to 83.33 on 11 benchmarks, significantly improving its\ncapabilities in mathematics, coding, and reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model fusion combines multiple Large Language Models (LLMs) with different\nstrengths into a more powerful, integrated model through lightweight training\nmethods. Existing works on model fusion focus primarily on supervised\nfine-tuning (SFT), leaving preference alignment (PA) --a critical phase for\nenhancing LLM performance--largely unexplored. The current few fusion methods\non PA phase, like WRPO, simplify the process by utilizing only response outputs\nfrom source models while discarding their probability information. To address\nthis limitation, we propose InfiFPO, a preference optimization method for\nimplicit model fusion. InfiFPO replaces the reference model in Direct\nPreference Optimization (DPO) with a fused source model that synthesizes\nmulti-source probabilities at the sequence level, circumventing complex\nvocabulary alignment challenges in previous works and meanwhile maintaining the\nprobability information. By introducing probability clipping and max-margin\nfusion strategies, InfiFPO enables the pivot model to align with human\npreferences while effectively distilling knowledge from source models.\nComprehensive experiments on 11 widely-used benchmarks demonstrate that InfiFPO\nconsistently outperforms existing model fusion and preference optimization\nmethods. When using Phi-4 as the pivot model, InfiFPO improve its average\nperformance from 79.95 to 83.33 on 11 benchmarks, significantly improving its\ncapabilities in mathematics, coding, and reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Yanggan Gu"
                    },
                    {
                        "name": "Yuanyi Wang"
                    },
                    {
                        "name": "Zhaoyi Yan"
                    },
                    {
                        "name": "Yiming Zhang"
                    },
                    {
                        "name": "Qi Zhou"
                    },
                    {
                        "name": "Fei Wu"
                    },
                    {
                        "name": "Hongxia Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hongxia Yang"
                },
                "author": "Hongxia Yang",
                "arxiv_journal_ref": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13878v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13878v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19955v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19955v3",
                "updated": "2025-10-22T13:33:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    13,
                    33,
                    52,
                    2,
                    295,
                    0
                ],
                "published": "2025-05-26T13:18:37Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    13,
                    18,
                    37,
                    0,
                    146,
                    0
                ],
                "title": "MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research"
                },
                "summary": "Recent advancements in AI agents have demonstrated their growing potential to\ndrive and support scientific discovery. In this work, we introduce MLR-Bench, a\ncomprehensive benchmark for evaluating AI agents on open-ended machine learning\nresearch. MLR-Bench includes three key components: (1) 201 research tasks\nsourced from NeurIPS, ICLR, and ICML workshops covering diverse ML topics; (2)\nMLR-Judge, an automated evaluation framework combining LLM-based reviewers with\ncarefully designed review rubrics to assess research quality; and (3)\nMLR-Agent, a modular agent scaffold capable of completing research tasks\nthrough four stages: idea generation, proposal formulation, experimentation,\nand paper writing. Our framework supports both stepwise assessment across these\ndistinct research stages, and end-to-end evaluation of the final research\npaper. We then use MLR-Bench to evaluate six frontier LLMs and an advanced\ncoding agent, finding that while LLMs are effective at generating coherent\nideas and well-structured papers, current coding agents frequently (e.g., in\n80% of the cases) produce fabricated or invalidated experimental\nresults--posing a major barrier to scientific reliability. We validate\nMLR-Judge through human evaluation, showing high agreement with expert\nreviewers, supporting its potential as a scalable tool for research evaluation.\nWe open-source MLR-Bench to help the community benchmark, diagnose, and improve\nAI research agents toward trustworthy and transparent scientific discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in AI agents have demonstrated their growing potential to\ndrive and support scientific discovery. In this work, we introduce MLR-Bench, a\ncomprehensive benchmark for evaluating AI agents on open-ended machine learning\nresearch. MLR-Bench includes three key components: (1) 201 research tasks\nsourced from NeurIPS, ICLR, and ICML workshops covering diverse ML topics; (2)\nMLR-Judge, an automated evaluation framework combining LLM-based reviewers with\ncarefully designed review rubrics to assess research quality; and (3)\nMLR-Agent, a modular agent scaffold capable of completing research tasks\nthrough four stages: idea generation, proposal formulation, experimentation,\nand paper writing. Our framework supports both stepwise assessment across these\ndistinct research stages, and end-to-end evaluation of the final research\npaper. We then use MLR-Bench to evaluate six frontier LLMs and an advanced\ncoding agent, finding that while LLMs are effective at generating coherent\nideas and well-structured papers, current coding agents frequently (e.g., in\n80% of the cases) produce fabricated or invalidated experimental\nresults--posing a major barrier to scientific reliability. We validate\nMLR-Judge through human evaluation, showing high agreement with expert\nreviewers, supporting its potential as a scalable tool for research evaluation.\nWe open-source MLR-Bench to help the community benchmark, diagnose, and improve\nAI research agents toward trustworthy and transparent scientific discovery."
                },
                "authors": [
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Miao Xiong"
                    },
                    {
                        "name": "Yujie Lu"
                    },
                    {
                        "name": "Wei Han"
                    },
                    {
                        "name": "Ailin Deng"
                    },
                    {
                        "name": "Yufei He"
                    },
                    {
                        "name": "Jiaying Wu"
                    },
                    {
                        "name": "Yibo Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "arxiv_comment": "49 pages, 9 figures. Accepted by NeurIPS 2025 D&B Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19955v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19955v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19579v1",
                "updated": "2025-10-22T13:29:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    13,
                    29,
                    32,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T13:29:32Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    13,
                    29,
                    32,
                    2,
                    295,
                    0
                ],
                "title": "Multi-modal Co-learning for Earth Observation: Enhancing single-modality\n  models via modality collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal Co-learning for Earth Observation: Enhancing single-modality\n  models via modality collaboration"
                },
                "summary": "Multi-modal co-learning is emerging as an effective paradigm in machine\nlearning, enabling models to collaboratively learn from different modalities to\nenhance single-modality predictions. Earth Observation (EO) represents a\nquintessential domain for multi-modal data analysis, wherein diverse remote\nsensors collect data to sense our planet. This unprecedented volume of data\nintroduces novel challenges. Specifically, the access to the same sensor\nmodalities at both training and inference stages becomes increasingly complex\nbased on real-world constraints affecting remote sensing platforms. In this\ncontext, multi-modal co-learning presents a promising strategy to leverage the\nvast amount of sensor-derived data available at the training stage to improve\nsingle-modality models for inference-time deployment. Most current research\nefforts focus on designing customized solutions for either particular\ndownstream tasks or specific modalities available at the inference stage. To\naddress this, we propose a novel multi-modal co-learning framework capable of\ngeneralizing across various tasks without targeting a specific modality for\ninference. Our approach combines contrastive and modality discriminative\nlearning together to guide single-modality models to structure the internal\nmodel manifold into modality-shared and modality-specific information. We\nevaluate our framework on four EO benchmarks spanning classification and\nregression tasks across different sensor modalities, where only one of the\nmodalities available during training is accessible at inference time. Our\nresults demonstrate consistent predictive improvements over state-of-the-art\napproaches from the recent machine learning and computer vision literature, as\nwell as EO-specific methods. The obtained findings validate our framework in\nthe single-modality inference scenarios across a diverse range of EO\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal co-learning is emerging as an effective paradigm in machine\nlearning, enabling models to collaboratively learn from different modalities to\nenhance single-modality predictions. Earth Observation (EO) represents a\nquintessential domain for multi-modal data analysis, wherein diverse remote\nsensors collect data to sense our planet. This unprecedented volume of data\nintroduces novel challenges. Specifically, the access to the same sensor\nmodalities at both training and inference stages becomes increasingly complex\nbased on real-world constraints affecting remote sensing platforms. In this\ncontext, multi-modal co-learning presents a promising strategy to leverage the\nvast amount of sensor-derived data available at the training stage to improve\nsingle-modality models for inference-time deployment. Most current research\nefforts focus on designing customized solutions for either particular\ndownstream tasks or specific modalities available at the inference stage. To\naddress this, we propose a novel multi-modal co-learning framework capable of\ngeneralizing across various tasks without targeting a specific modality for\ninference. Our approach combines contrastive and modality discriminative\nlearning together to guide single-modality models to structure the internal\nmodel manifold into modality-shared and modality-specific information. We\nevaluate our framework on four EO benchmarks spanning classification and\nregression tasks across different sensor modalities, where only one of the\nmodalities available during training is accessible at inference time. Our\nresults demonstrate consistent predictive improvements over state-of-the-art\napproaches from the recent machine learning and computer vision literature, as\nwell as EO-specific methods. The obtained findings validate our framework in\nthe single-modality inference scenarios across a diverse range of EO\napplications."
                },
                "authors": [
                    {
                        "name": "Francisco Mena"
                    },
                    {
                        "name": "Dino Ienco"
                    },
                    {
                        "name": "Cassio F. Dantas"
                    },
                    {
                        "name": "Roberto Interdonato"
                    },
                    {
                        "name": "Andreas Dengel"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Dengel"
                },
                "author": "Andreas Dengel",
                "arxiv_comment": "Accepted at the Machine Learning journal, CfP: Discovery Science 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19578v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19578v1",
                "updated": "2025-10-22T13:28:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    13,
                    28,
                    49,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T13:28:49Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    13,
                    28,
                    49,
                    2,
                    295,
                    0
                ],
                "title": "VGD: Visual Geometry Gaussian Splatting for Feed-Forward Surround-view\n  Driving Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VGD: Visual Geometry Gaussian Splatting for Feed-Forward Surround-view\n  Driving Reconstruction"
                },
                "summary": "Feed-forward surround-view autonomous driving scene reconstruction offers\nfast, generalizable inference ability, which faces the core challenge of\nensuring generalization while elevating novel view quality. Due to the\nsurround-view with minimal overlap regions, existing methods typically fail to\nensure geometric consistency and reconstruction quality for novel views. To\ntackle this tension, we claim that geometric information must be learned\nexplicitly, and the resulting features should be leveraged to guide the\nelevating of semantic quality in novel views. In this paper, we introduce\n\\textbf{Visual Gaussian Driving (VGD)}, a novel feed-forward end-to-end\nlearning framework designed to address this challenge. To achieve generalizable\ngeometric estimation, we design a lightweight variant of the VGGT architecture\nto efficiently distill its geometric priors from the pre-trained VGGT to the\ngeometry branch. Furthermore, we design a Gaussian Head that fuses multi-scale\ngeometry tokens to predict Gaussian parameters for novel view rendering, which\nshares the same patch backbone as the geometry branch. Finally, we integrate\nmulti-scale features from both geometry and Gaussian head branches to jointly\nsupervise a semantic refinement model, optimizing rendering quality through\nfeature-consistent learning. Experiments on nuScenes demonstrate that our\napproach significantly outperforms state-of-the-art methods in both objective\nmetrics and subjective quality under various settings, which validates VGD's\nscalability and high-fidelity surround-view reconstruction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feed-forward surround-view autonomous driving scene reconstruction offers\nfast, generalizable inference ability, which faces the core challenge of\nensuring generalization while elevating novel view quality. Due to the\nsurround-view with minimal overlap regions, existing methods typically fail to\nensure geometric consistency and reconstruction quality for novel views. To\ntackle this tension, we claim that geometric information must be learned\nexplicitly, and the resulting features should be leveraged to guide the\nelevating of semantic quality in novel views. In this paper, we introduce\n\\textbf{Visual Gaussian Driving (VGD)}, a novel feed-forward end-to-end\nlearning framework designed to address this challenge. To achieve generalizable\ngeometric estimation, we design a lightweight variant of the VGGT architecture\nto efficiently distill its geometric priors from the pre-trained VGGT to the\ngeometry branch. Furthermore, we design a Gaussian Head that fuses multi-scale\ngeometry tokens to predict Gaussian parameters for novel view rendering, which\nshares the same patch backbone as the geometry branch. Finally, we integrate\nmulti-scale features from both geometry and Gaussian head branches to jointly\nsupervise a semantic refinement model, optimizing rendering quality through\nfeature-consistent learning. Experiments on nuScenes demonstrate that our\napproach significantly outperforms state-of-the-art methods in both objective\nmetrics and subjective quality under various settings, which validates VGD's\nscalability and high-fidelity surround-view reconstruction."
                },
                "authors": [
                    {
                        "name": "Junhong Lin"
                    },
                    {
                        "name": "Kangli Wang"
                    },
                    {
                        "name": "Shunzhou Wang"
                    },
                    {
                        "name": "Songlin Fan"
                    },
                    {
                        "name": "Ge Li"
                    },
                    {
                        "name": "Wei Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wei Gao"
                },
                "author": "Wei Gao",
                "arxiv_comment": "10 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19578v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19578v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19577v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19577v1",
                "updated": "2025-10-22T13:28:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    13,
                    28,
                    36,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T13:28:36Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    13,
                    28,
                    36,
                    2,
                    295,
                    0
                ],
                "title": "gem5 Co-Pilot: AI Assistant Agent for Architectural Design Space\n  Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "gem5 Co-Pilot: AI Assistant Agent for Architectural Design Space\n  Exploration"
                },
                "summary": "Generative AI is increasing the productivity of software and hardware\ndevelopment across many application domains. In this work, we utilize the power\nof Large Language Models (LLMs) to develop a co-pilot agent for assisting gem5\nusers with automating design space exploration. Computer architecture design\nspace exploration is complex and time-consuming, given that numerous parameter\nsettings and simulation statistics must be analyzed before improving the\ncurrent design. The emergence of LLMs has significantly accelerated the\nanalysis of long-text data as well as smart decision making, two key functions\nin a successful design space exploration task. In this project, we first build\ngem5 Co-Pilot, an AI agent assistant for gem5, which comes with a webpage-GUI\nfor smooth user interaction, agent automation, and result summarization. We\nalso implemented a language for design space exploration, as well as a Design\nSpace Database (DSDB). With DSDB, gem5 Co-Pilot effectively implements a\nRetrieval Augmented Generation system for gem5 design space exploration. We\nexperiment on cost-constraint optimization with four cost ranges and compare\nour results with two baseline models. Results show that gem5 Co-Pilot can\nquickly identify optimal parameters for specific design constraints based on\nperformance and cost, with limited user interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI is increasing the productivity of software and hardware\ndevelopment across many application domains. In this work, we utilize the power\nof Large Language Models (LLMs) to develop a co-pilot agent for assisting gem5\nusers with automating design space exploration. Computer architecture design\nspace exploration is complex and time-consuming, given that numerous parameter\nsettings and simulation statistics must be analyzed before improving the\ncurrent design. The emergence of LLMs has significantly accelerated the\nanalysis of long-text data as well as smart decision making, two key functions\nin a successful design space exploration task. In this project, we first build\ngem5 Co-Pilot, an AI agent assistant for gem5, which comes with a webpage-GUI\nfor smooth user interaction, agent automation, and result summarization. We\nalso implemented a language for design space exploration, as well as a Design\nSpace Database (DSDB). With DSDB, gem5 Co-Pilot effectively implements a\nRetrieval Augmented Generation system for gem5 design space exploration. We\nexperiment on cost-constraint optimization with four cost ranges and compare\nour results with two baseline models. Results show that gem5 Co-Pilot can\nquickly identify optimal parameters for specific design constraints based on\nperformance and cost, with limited user interaction."
                },
                "authors": [
                    {
                        "name": "Zuoming Fu"
                    },
                    {
                        "name": "Alex Manley"
                    },
                    {
                        "name": "Mohammad Alian"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Alian"
                },
                "author": "Mohammad Alian",
                "arxiv_comment": "Accepted by CAMS25, October, 2025, Seoul, Republic of Korea",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19577v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19577v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19572v1",
                "updated": "2025-10-22T13:25:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    13,
                    25,
                    39,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T13:25:39Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    13,
                    25,
                    39,
                    2,
                    295,
                    0
                ],
                "title": "VBx for End-to-End Neural and Clustering-based Diarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VBx for End-to-End Neural and Clustering-based Diarization"
                },
                "summary": "We present improvements to speaker diarization in the two-stage end-to-end\nneural diarization with vector clustering (EEND-VC) framework. The first stage\nemploys a Conformer-based EEND model with WavLM features to infer frame-level\nspeaker activity within short windows. The identities and counts of global\nspeakers are then derived in the second stage by clustering speaker embeddings\nacross windows. The focus of this work is to improve the second stage; we\nfilter unreliable embeddings from short segments and reassign them after\nclustering. We also integrate the VBx clustering to improve robustness when the\nnumber of speakers is large and individual speaking durations are limited.\nEvaluation on a compound benchmark spanning multiple domains is conducted\nwithout fine-tuning the EEND model or tuning clustering parameters per dataset.\nDespite this, the system generalizes well and matches or exceeds recent\nstate-of-the-art performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present improvements to speaker diarization in the two-stage end-to-end\nneural diarization with vector clustering (EEND-VC) framework. The first stage\nemploys a Conformer-based EEND model with WavLM features to infer frame-level\nspeaker activity within short windows. The identities and counts of global\nspeakers are then derived in the second stage by clustering speaker embeddings\nacross windows. The focus of this work is to improve the second stage; we\nfilter unreliable embeddings from short segments and reassign them after\nclustering. We also integrate the VBx clustering to improve robustness when the\nnumber of speakers is large and individual speaking durations are limited.\nEvaluation on a compound benchmark spanning multiple domains is conducted\nwithout fine-tuning the EEND model or tuning clustering parameters per dataset.\nDespite this, the system generalizes well and matches or exceeds recent\nstate-of-the-art performance."
                },
                "authors": [
                    {
                        "name": "Petr Pálka"
                    },
                    {
                        "name": "Jiangyu Han"
                    },
                    {
                        "name": "Marc Delcroix"
                    },
                    {
                        "name": "Naohiro Tawara"
                    },
                    {
                        "name": "Lukáš Burget"
                    }
                ],
                "author_detail": {
                    "name": "Lukáš Burget"
                },
                "author": "Lukáš Burget",
                "arxiv_comment": "Submitted to ICASSP 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01675v2",
                "updated": "2025-10-22T13:17:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    13,
                    17,
                    36,
                    2,
                    295,
                    0
                ],
                "published": "2025-03-03T15:48:01Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    15,
                    48,
                    1,
                    0,
                    62,
                    0
                ],
                "title": "Using (Not-so) Large Language Models to Generate Simulation Models in a\n  Formal DSL: A Study on Reaction Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using (Not-so) Large Language Models to Generate Simulation Models in a\n  Formal DSL: A Study on Reaction Networks"
                },
                "summary": "Formal languages are an integral part of modeling and simulation. They allow\nthe distillation of knowledge into concise simulation models amenable to\nautomatic execution, interpretation, and analysis. However, the arguably most\nhumanly accessible means of expressing models is through natural language,\nwhich is not easily interpretable by computers. Here, we evaluate how a Large\nLanguage Model (LLM) might be used for formalizing natural language into\nsimulation models. Existing studies only explored using very large LLMs, like\nthe commercial GPT models, without fine-tuning model weights. To close this\ngap, we show how an open-weights, 7B-parameter Mistral model can be fine-tuned\nto translate natural language descriptions to reaction network models in a\ndomain-specific language, offering a self-hostable, compute-efficient, and\nmemory efficient alternative. To this end, we develop a synthetic data\ngenerator to serve as the basis for fine-tuning and evaluation. Our\nquantitative evaluation shows that our fine-tuned Mistral model can recover the\nground truth simulation model in up to 84.5% of cases. In addition, our\nsmall-scale user study demonstrates the model's practical potential for\none-time generation as well as interactive modeling in various domains. While\npromising, in its current form, the fine-tuned small LLM cannot catch up with\nlarge LLMs. We conclude that higher-quality training data are required, and\nexpect future small and open-source LLMs to offer new opportunities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal languages are an integral part of modeling and simulation. They allow\nthe distillation of knowledge into concise simulation models amenable to\nautomatic execution, interpretation, and analysis. However, the arguably most\nhumanly accessible means of expressing models is through natural language,\nwhich is not easily interpretable by computers. Here, we evaluate how a Large\nLanguage Model (LLM) might be used for formalizing natural language into\nsimulation models. Existing studies only explored using very large LLMs, like\nthe commercial GPT models, without fine-tuning model weights. To close this\ngap, we show how an open-weights, 7B-parameter Mistral model can be fine-tuned\nto translate natural language descriptions to reaction network models in a\ndomain-specific language, offering a self-hostable, compute-efficient, and\nmemory efficient alternative. To this end, we develop a synthetic data\ngenerator to serve as the basis for fine-tuning and evaluation. Our\nquantitative evaluation shows that our fine-tuned Mistral model can recover the\nground truth simulation model in up to 84.5% of cases. In addition, our\nsmall-scale user study demonstrates the model's practical potential for\none-time generation as well as interactive modeling in various domains. While\npromising, in its current form, the fine-tuned small LLM cannot catch up with\nlarge LLMs. We conclude that higher-quality training data are required, and\nexpect future small and open-source LLMs to offer new opportunities."
                },
                "authors": [
                    {
                        "name": "Justin N. Kreikemeyer"
                    },
                    {
                        "name": "Miłosz Jankowski"
                    },
                    {
                        "name": "Pia Wilsdorf"
                    },
                    {
                        "name": "Adelinde M. Uhrmacher"
                    }
                ],
                "author_detail": {
                    "name": "Adelinde M. Uhrmacher"
                },
                "author": "Adelinde M. Uhrmacher",
                "arxiv_doi": "10.1145/3733719",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3733719",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.01675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "27 pages, 5 figures; supplemental material available at\n  https://doi.org/10.1145/3733719",
                "arxiv_journal_ref": "ACM Trans. Model. Comput. Simul. 35, 4, Article 31 (October 2025),\n  27 pages",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19557v1",
                "updated": "2025-10-22T13:13:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    13,
                    13,
                    27,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T13:13:27Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    13,
                    13,
                    27,
                    2,
                    295,
                    0
                ],
                "title": "The Intricate Dance of Prompt Complexity, Quality, Diversity, and\n  Consistency in T2I Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Intricate Dance of Prompt Complexity, Quality, Diversity, and\n  Consistency in T2I Models"
                },
                "summary": "Text-to-image (T2I) models offer great potential for creating virtually\nlimitless synthetic data, a valuable resource compared to fixed and finite real\ndatasets. Previous works evaluate the utility of synthetic data from T2I models\non three key desiderata: quality, diversity, and consistency. While prompt\nengineering is the primary means of interacting with T2I models, the systematic\nimpact of prompt complexity on these critical utility axes remains\nunderexplored. In this paper, we first conduct synthetic experiments to\nmotivate the difficulty of generalization w.r.t. prompt complexity and explain\nthe observed difficulty with theoretical derivations. Then, we introduce a new\nevaluation framework that can compare the utility of real data and synthetic\ndata, and present a comprehensive analysis of how prompt complexity influences\nthe utility of synthetic data generated by commonly used T2I models. We conduct\nour study across diverse datasets, including CC12M, ImageNet-1k, and DCI, and\nevaluate different inference-time intervention methods. Our synthetic\nexperiments show that generalizing to more general conditions is harder than\nthe other way round, since the former needs an estimated likelihood that is not\nlearned by diffusion models. Our large-scale empirical experiments reveal that\nincreasing prompt complexity results in lower conditional diversity and prompt\nconsistency, while reducing the synthetic-to-real distribution shift, which\naligns with the synthetic experiments. Moreover, current inference-time\ninterventions can augment the diversity of the generations at the expense of\nmoving outside the support of real data. Among those interventions, prompt\nexpansion, by deliberately using a pre-trained language model as a likelihood\nestimator, consistently achieves the highest performance in both image\ndiversity and aesthetics, even higher than that of real data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) models offer great potential for creating virtually\nlimitless synthetic data, a valuable resource compared to fixed and finite real\ndatasets. Previous works evaluate the utility of synthetic data from T2I models\non three key desiderata: quality, diversity, and consistency. While prompt\nengineering is the primary means of interacting with T2I models, the systematic\nimpact of prompt complexity on these critical utility axes remains\nunderexplored. In this paper, we first conduct synthetic experiments to\nmotivate the difficulty of generalization w.r.t. prompt complexity and explain\nthe observed difficulty with theoretical derivations. Then, we introduce a new\nevaluation framework that can compare the utility of real data and synthetic\ndata, and present a comprehensive analysis of how prompt complexity influences\nthe utility of synthetic data generated by commonly used T2I models. We conduct\nour study across diverse datasets, including CC12M, ImageNet-1k, and DCI, and\nevaluate different inference-time intervention methods. Our synthetic\nexperiments show that generalizing to more general conditions is harder than\nthe other way round, since the former needs an estimated likelihood that is not\nlearned by diffusion models. Our large-scale empirical experiments reveal that\nincreasing prompt complexity results in lower conditional diversity and prompt\nconsistency, while reducing the synthetic-to-real distribution shift, which\naligns with the synthetic experiments. Moreover, current inference-time\ninterventions can augment the diversity of the generations at the expense of\nmoving outside the support of real data. Among those interventions, prompt\nexpansion, by deliberately using a pre-trained language model as a likelihood\nestimator, consistently achieves the highest performance in both image\ndiversity and aesthetics, even higher than that of real data."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Zhang"
                    },
                    {
                        "name": "Aaron Courville"
                    },
                    {
                        "name": "Michal Drozdzal"
                    },
                    {
                        "name": "Adriana Romero-Soriano"
                    }
                ],
                "author_detail": {
                    "name": "Adriana Romero-Soriano"
                },
                "author": "Adriana Romero-Soriano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05346v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05346v2",
                "updated": "2025-10-22T13:08:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    13,
                    8,
                    29,
                    2,
                    295,
                    0
                ],
                "published": "2025-09-02T14:21:59Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    14,
                    21,
                    59,
                    1,
                    245,
                    0
                ],
                "title": "Benchmarking Large Language Models for Personalized Guidance in\n  AI-Enhanced Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Large Language Models for Personalized Guidance in\n  AI-Enhanced Learning"
                },
                "summary": "While Large Language Models (LLMs) are increasingly envisioned as intelligent\nassistants for personalized learning, systematic head-to-head evaluations in\nauthentic learning scenarios remain scarce. This study presents an empirical\ncomparison of three state-of-the-art LLMs on a tutoring task simulating a\nrealistic learning setting. Using a dataset containing a student's responses to\nten mixed-format questions with correctness labels, each model was asked to (i)\nanalyze the quiz to identify underlying knowledge components, (ii) infer the\nstudent's mastery profile, and (iii) generate targeted guidance for\nimprovement. To mitigate subjectivity and evaluator bias, Gemini was employed\nas a virtual judge to perform pairwise comparisons across multiple dimensions:\naccuracy, clarity, actionability, and appropriateness. Results analyzed via the\nBradley-Terry model reveal that GPT-4o is generally preferred, producing\nfeedback that is more informative and better structured than its counterparts,\nwhereas DeepSeek-V3 and GLM-4.5 demonstrate intermittent strengths but lower\nconsistency. These findings highlight the feasibility of deploying LLMs as\nadvanced teaching assistants for individualized support and provide\nmethodological insights for subsequent empirical research on LLM-driven\npersonalized learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) are increasingly envisioned as intelligent\nassistants for personalized learning, systematic head-to-head evaluations in\nauthentic learning scenarios remain scarce. This study presents an empirical\ncomparison of three state-of-the-art LLMs on a tutoring task simulating a\nrealistic learning setting. Using a dataset containing a student's responses to\nten mixed-format questions with correctness labels, each model was asked to (i)\nanalyze the quiz to identify underlying knowledge components, (ii) infer the\nstudent's mastery profile, and (iii) generate targeted guidance for\nimprovement. To mitigate subjectivity and evaluator bias, Gemini was employed\nas a virtual judge to perform pairwise comparisons across multiple dimensions:\naccuracy, clarity, actionability, and appropriateness. Results analyzed via the\nBradley-Terry model reveal that GPT-4o is generally preferred, producing\nfeedback that is more informative and better structured than its counterparts,\nwhereas DeepSeek-V3 and GLM-4.5 demonstrate intermittent strengths but lower\nconsistency. These findings highlight the feasibility of deploying LLMs as\nadvanced teaching assistants for individualized support and provide\nmethodological insights for subsequent empirical research on LLM-driven\npersonalized learning."
                },
                "authors": [
                    {
                        "name": "Bo Yuan"
                    },
                    {
                        "name": "Jiazi Hu"
                    }
                ],
                "author_detail": {
                    "name": "Jiazi Hu"
                },
                "author": "Jiazi Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05346v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05346v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.18608v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.18608v3",
                "updated": "2025-10-22T12:54:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    12,
                    54,
                    22,
                    2,
                    295,
                    0
                ],
                "published": "2023-10-28T06:31:06Z",
                "published_parsed": [
                    2023,
                    10,
                    28,
                    6,
                    31,
                    6,
                    5,
                    301,
                    0
                ],
                "title": "Embedding in Recommender Systems: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedding in Recommender Systems: A Survey"
                },
                "summary": "Recommender systems have become an essential component of many online\nplatforms, providing personalized recommendations to users. A crucial aspect is\nembedding techniques that convert the high-dimensional discrete features, such\nas user and item IDs, into low-dimensional continuous vectors, which can\nenhance the recommendation performance. Embedding techniques have\nrevolutionized the capture of complex entity relationships, generating\nsignificant research interest. This survey presents a comprehensive analysis of\nrecent advances in recommender system embedding techniques. We examine\ncentralized embedding approaches across matrix, sequential, and graph\nstructures. In matrix-based scenarios, collaborative filtering generates\nembeddings that effectively model user-item preferences, particularly in sparse\ndata environments. For sequential data, we explore various approaches including\nrecurrent neural networks and self-supervised methods such as contrastive and\ngenerative learning. In graph-structured contexts, we analyze techniques like\nnode2vec that leverage network relationships, along with applicable\nself-supervised methods. Our survey addresses critical scalability challenges\nin embedding methods and explores innovative directions in recommender systems.\nWe introduce emerging approaches, including AutoML, hashing techniques, and\nquantization methods, to enhance performance while reducing computational\ncomplexity. Additionally, we examine the promising role of Large Language\nModels (LLMs) in embedding enhancement. Through detailed discussion of various\narchitectures and methodologies, this survey aims to provide a thorough\noverview of state-of-the-art embedding techniques in recommender systems, while\nhighlighting key challenges and future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems have become an essential component of many online\nplatforms, providing personalized recommendations to users. A crucial aspect is\nembedding techniques that convert the high-dimensional discrete features, such\nas user and item IDs, into low-dimensional continuous vectors, which can\nenhance the recommendation performance. Embedding techniques have\nrevolutionized the capture of complex entity relationships, generating\nsignificant research interest. This survey presents a comprehensive analysis of\nrecent advances in recommender system embedding techniques. We examine\ncentralized embedding approaches across matrix, sequential, and graph\nstructures. In matrix-based scenarios, collaborative filtering generates\nembeddings that effectively model user-item preferences, particularly in sparse\ndata environments. For sequential data, we explore various approaches including\nrecurrent neural networks and self-supervised methods such as contrastive and\ngenerative learning. In graph-structured contexts, we analyze techniques like\nnode2vec that leverage network relationships, along with applicable\nself-supervised methods. Our survey addresses critical scalability challenges\nin embedding methods and explores innovative directions in recommender systems.\nWe introduce emerging approaches, including AutoML, hashing techniques, and\nquantization methods, to enhance performance while reducing computational\ncomplexity. Additionally, we examine the promising role of Large Language\nModels (LLMs) in embedding enhancement. Through detailed discussion of various\narchitectures and methodologies, this survey aims to provide a thorough\noverview of state-of-the-art embedding techniques in recommender systems, while\nhighlighting key challenges and future research directions."
                },
                "authors": [
                    {
                        "name": "Maolin Wang"
                    },
                    {
                        "name": "Xinjian Zhao"
                    },
                    {
                        "name": "Wanyu Wang"
                    },
                    {
                        "name": "Sheng Zhang"
                    },
                    {
                        "name": "Jiansheng Li"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Binhao Wang"
                    },
                    {
                        "name": "Shucheng Zhou"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Ruocheng Guo"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhao"
                },
                "author": "Xiangyu Zhao",
                "arxiv_comment": "47 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.18608v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.18608v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19540v1",
                "updated": "2025-10-22T12:46:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    12,
                    46,
                    33,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T12:46:33Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    12,
                    46,
                    33,
                    2,
                    295,
                    0
                ],
                "title": "Heat-Transfer Enhancement by Parametric Sloshing in Horizontal\n  Cylinders: Experiments and EKF-Based Identification of Nusselt Numbers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heat-Transfer Enhancement by Parametric Sloshing in Horizontal\n  Cylinders: Experiments and EKF-Based Identification of Nusselt Numbers"
                },
                "summary": "Vertical forcing of partially filled horizontal cylindrical tanks can induce\nstrong sloshing motion, disrupting thermal stratification between the subcooled\nliquid and the superheated vapor, and causing significant pressure variations.\nWhile this configuration is critical for cryogenic fuel storage in future\naircraft and ground transport, its thermodynamic consequences remain poorly\ncharacterized. This work presents an experimental investigation of\nsloshing-induced heat and mass transfer in a horizontally oriented cylinder\nunder vertical harmonic excitation. Using a hydrofluoroether fluid (3M Novec\nHFE-7000), decoupled isothermal and non-isothermal campaigns are conducted to\ncharacterize the kinematic and thermodynamic responses across various fill\nlevels and forcing amplitudes, near resonance of the first longitudinal\nsymmetric mode $(2,0)$. To quantify the underlying heat and mass transfer\nprocesses, a lumped-parameter model is coupled with an Augmented-state Extended\nKalman Filter (AEKF) to infer time-resolved Nusselt numbers from the\nexperimental data. The results confirm the existence of a critical forcing\nthreshold, below which the fluid remains quiescent and thermally stratified.\nAbove this threshold, parametric resonance triggers strong liquid motion,\nleading to complete thermal destratification and a rapid pressure drop. At a\n50% fill level, the primary jet-like modal response intermittently alternates\nwith a planar $(1,0)$ mode, indicating subharmonic resonance from non-linear\nmode interactions. The AEKF-based inference reveals that the onset of thermal\ndestratification causes a step-change increase in interfacial and wall Nusselt\nnumbers by several orders of magnitude. Pressure-rate decomposition shows that\nthe pressure evolution is dominated by phase change.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vertical forcing of partially filled horizontal cylindrical tanks can induce\nstrong sloshing motion, disrupting thermal stratification between the subcooled\nliquid and the superheated vapor, and causing significant pressure variations.\nWhile this configuration is critical for cryogenic fuel storage in future\naircraft and ground transport, its thermodynamic consequences remain poorly\ncharacterized. This work presents an experimental investigation of\nsloshing-induced heat and mass transfer in a horizontally oriented cylinder\nunder vertical harmonic excitation. Using a hydrofluoroether fluid (3M Novec\nHFE-7000), decoupled isothermal and non-isothermal campaigns are conducted to\ncharacterize the kinematic and thermodynamic responses across various fill\nlevels and forcing amplitudes, near resonance of the first longitudinal\nsymmetric mode $(2,0)$. To quantify the underlying heat and mass transfer\nprocesses, a lumped-parameter model is coupled with an Augmented-state Extended\nKalman Filter (AEKF) to infer time-resolved Nusselt numbers from the\nexperimental data. The results confirm the existence of a critical forcing\nthreshold, below which the fluid remains quiescent and thermally stratified.\nAbove this threshold, parametric resonance triggers strong liquid motion,\nleading to complete thermal destratification and a rapid pressure drop. At a\n50% fill level, the primary jet-like modal response intermittently alternates\nwith a planar $(1,0)$ mode, indicating subharmonic resonance from non-linear\nmode interactions. The AEKF-based inference reveals that the onset of thermal\ndestratification causes a step-change increase in interfacial and wall Nusselt\nnumbers by several orders of magnitude. Pressure-rate decomposition shows that\nthe pressure evolution is dominated by phase change."
                },
                "authors": [
                    {
                        "name": "Samuel Akatchi Ahizi"
                    },
                    {
                        "name": "Francisco Monteiro"
                    },
                    {
                        "name": "Ramon Abarca"
                    },
                    {
                        "name": "Miguel Alfonso Mendez"
                    }
                ],
                "author_detail": {
                    "name": "Miguel Alfonso Mendez"
                },
                "author": "Miguel Alfonso Mendez",
                "arxiv_comment": "Submitted to Applied Thermal Engineering",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19539v1",
                "updated": "2025-10-22T12:45:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    12,
                    45,
                    30,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T12:45:30Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    12,
                    45,
                    30,
                    2,
                    295,
                    0
                ],
                "title": "Probing Accretion Disk Winds of Stratified Nature with Fe XXVI Doublet\n  in Black Hole X-ray Binaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing Accretion Disk Winds of Stratified Nature with Fe XXVI Doublet\n  in Black Hole X-ray Binaries"
                },
                "summary": "Powerful ionized accretion disk winds are often observed during episodic\noutbursts in Galactic black hole transients. Among those X-ray absorbers,\n\\fexxvi\\ doublet structure (Ly$\\alpha_1$+Ly$\\alpha_2$ with $\\sim 20$eV apart)\nhas a unique potential to better probe the underlying physical nature of the\nwind; i.e. density and kinematics. We demonstrate, based on a\nphysically-motivated magnetic disk wind scenario of a stratified structure in\ndensity and velocity, that the doublet line profile can be effectively utilized\nas a diagnostics to measure wind density and associated velocity dispersion\n(due to thermal turbulence and/or dynamical shear motion in winds). Our\nsimulated doublet spectra with post-process radiative transfer calculations\nindicate that the profile can be (1) broad with a single peak for higher\nvelocity dispersion ($\\gsim 5,000$ km~s$^{-1}$), (2) a standard shape with 1:2\ncanonical flux ratio for moderate dispersion ($\\sim 1,000-5,000$ km~s$^{-1}$)\nor (3) double-peaked with its flux ratio approaching 1:1 for lower velocity\ndispersion ($\\lsim 1,000$ km~s$^{-1}$) in optically-thin regime, allowing\nvarious line shape. Such a diversity in doublet profile is indeed unambiguously\nseen in recent observations with XRISM/Resolve at microcalorimeter resolution.\nWe show that some implications inferred from the model will help constrain the\nlocal wind physics where \\fexxvi\\ is predominantly produced in a large-scale,\nstratified wind.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Powerful ionized accretion disk winds are often observed during episodic\noutbursts in Galactic black hole transients. Among those X-ray absorbers,\n\\fexxvi\\ doublet structure (Ly$\\alpha_1$+Ly$\\alpha_2$ with $\\sim 20$eV apart)\nhas a unique potential to better probe the underlying physical nature of the\nwind; i.e. density and kinematics. We demonstrate, based on a\nphysically-motivated magnetic disk wind scenario of a stratified structure in\ndensity and velocity, that the doublet line profile can be effectively utilized\nas a diagnostics to measure wind density and associated velocity dispersion\n(due to thermal turbulence and/or dynamical shear motion in winds). Our\nsimulated doublet spectra with post-process radiative transfer calculations\nindicate that the profile can be (1) broad with a single peak for higher\nvelocity dispersion ($\\gsim 5,000$ km~s$^{-1}$), (2) a standard shape with 1:2\ncanonical flux ratio for moderate dispersion ($\\sim 1,000-5,000$ km~s$^{-1}$)\nor (3) double-peaked with its flux ratio approaching 1:1 for lower velocity\ndispersion ($\\lsim 1,000$ km~s$^{-1}$) in optically-thin regime, allowing\nvarious line shape. Such a diversity in doublet profile is indeed unambiguously\nseen in recent observations with XRISM/Resolve at microcalorimeter resolution.\nWe show that some implications inferred from the model will help constrain the\nlocal wind physics where \\fexxvi\\ is predominantly produced in a large-scale,\nstratified wind."
                },
                "authors": [
                    {
                        "name": "Keigo Fukumura"
                    },
                    {
                        "name": "Shoji Ogawa"
                    },
                    {
                        "name": "Atsushi Tanimoto"
                    },
                    {
                        "name": "Francesco Tombesi"
                    },
                    {
                        "name": "Alfredo Luminari"
                    },
                    {
                        "name": "Maxime Parra"
                    },
                    {
                        "name": "Megumi Shidatsu"
                    },
                    {
                        "name": "Liyi Gu"
                    },
                    {
                        "name": "Ehud Behar"
                    }
                ],
                "author_detail": {
                    "name": "Ehud Behar"
                },
                "author": "Ehud Behar",
                "arxiv_comment": "10 pages, 4 figures, accepted to ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16551v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16551v3",
                "updated": "2025-10-22T12:15:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    12,
                    15,
                    26,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-18T15:46:11Z",
                "published_parsed": [
                    2025,
                    10,
                    18,
                    15,
                    46,
                    11,
                    5,
                    291,
                    0
                ],
                "title": "From Reviews to Actionable Insights: An LLM-Based Approach for Attribute\n  and Feature Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Reviews to Actionable Insights: An LLM-Based Approach for Attribute\n  and Feature Extraction"
                },
                "summary": "This research proposes a systematic, large language model (LLM) approach for\nextracting product and service attributes, features, and associated sentiments\nfrom customer reviews. Grounded in marketing theory, the framework\ndistinguishes perceptual attributes from actionable features, producing\ninterpretable and managerially actionable insights. We apply the methodology to\n20,000 Yelp reviews of Starbucks stores and evaluate eight prompt variants on a\nrandom subset of reviews. Model performance is assessed through agreement with\nhuman annotations and predictive validity for customer ratings. Results show\nhigh consistency between LLMs and human coders and strong predictive validity,\nconfirming the reliability of the approach. Human coders required a median of\nsix minutes per review, whereas the LLM processed each in two seconds,\ndelivering comparable insights at a scale unattainable through manual coding.\nManagerially, the analysis identifies attributes and features that most\nstrongly influence customer satisfaction and their associated sentiments,\nenabling firms to pinpoint \"joy points,\" address \"pain points,\" and design\ntargeted interventions. We demonstrate how structured review data can power an\nactionable marketing dashboard that tracks sentiment over time and across\nstores, benchmarks performance, and highlights high-leverage features for\nimprovement. Simulations indicate that enhancing sentiment for key service\nfeatures could yield 1-2% average revenue gains per store.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research proposes a systematic, large language model (LLM) approach for\nextracting product and service attributes, features, and associated sentiments\nfrom customer reviews. Grounded in marketing theory, the framework\ndistinguishes perceptual attributes from actionable features, producing\ninterpretable and managerially actionable insights. We apply the methodology to\n20,000 Yelp reviews of Starbucks stores and evaluate eight prompt variants on a\nrandom subset of reviews. Model performance is assessed through agreement with\nhuman annotations and predictive validity for customer ratings. Results show\nhigh consistency between LLMs and human coders and strong predictive validity,\nconfirming the reliability of the approach. Human coders required a median of\nsix minutes per review, whereas the LLM processed each in two seconds,\ndelivering comparable insights at a scale unattainable through manual coding.\nManagerially, the analysis identifies attributes and features that most\nstrongly influence customer satisfaction and their associated sentiments,\nenabling firms to pinpoint \"joy points,\" address \"pain points,\" and design\ntargeted interventions. We demonstrate how structured review data can power an\nactionable marketing dashboard that tracks sentiment over time and across\nstores, benchmarks performance, and highlights high-leverage features for\nimprovement. Simulations indicate that enhancing sentiment for key service\nfeatures could yield 1-2% average revenue gains per store."
                },
                "authors": [
                    {
                        "name": "Khaled Boughanmi"
                    },
                    {
                        "name": "Kamel Jedidi"
                    },
                    {
                        "name": "Nour Jedidi"
                    }
                ],
                "author_detail": {
                    "name": "Nour Jedidi"
                },
                "author": "Nour Jedidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16551v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16551v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15568v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15568v5",
                "updated": "2025-10-22T12:05:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    12,
                    5,
                    11,
                    2,
                    295,
                    0
                ],
                "published": "2025-08-21T13:42:49Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    13,
                    42,
                    49,
                    3,
                    233,
                    0
                ],
                "title": "Backpropagation-Free Test-Time Adaptation via Probabilistic Gaussian\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backpropagation-Free Test-Time Adaptation via Probabilistic Gaussian\n  Alignment"
                },
                "summary": "Test-time adaptation (TTA) enhances the zero-shot robustness under\ndistribution shifts by leveraging unlabeled test data during inference. Despite\nnotable advances, several challenges still limit its broader applicability.\nFirst, most methods rely on backpropagation or iterative optimization, which\nlimits scalability and hinders real-time deployment. Second, they lack explicit\nmodeling of class-conditional feature distributions. This modeling is crucial\nfor producing reliable decision boundaries and calibrated predictions, but it\nremains underexplored due to the lack of both source data and supervision at\ntest time. In this paper, we propose ADAPT, an Advanced Distribution-Aware and\nbackPropagation-free Test-time adaptation method. We reframe TTA as a Gaussian\nprobabilistic inference task by modeling class-conditional likelihoods using\ngradually updated class means and a shared covariance matrix. This enables\nclosed-form, training-free inference. To correct potential likelihood bias, we\nintroduce lightweight regularization guided by CLIP priors and a historical\nknowledge bank. ADAPT requires no source data, no gradient updates, and no full\naccess to target data, supporting both online and transductive settings.\nExtensive experiments across diverse benchmarks demonstrate that our method\nachieves state-of-the-art performance under a wide range of distribution shifts\nwith superior scalability and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation (TTA) enhances the zero-shot robustness under\ndistribution shifts by leveraging unlabeled test data during inference. Despite\nnotable advances, several challenges still limit its broader applicability.\nFirst, most methods rely on backpropagation or iterative optimization, which\nlimits scalability and hinders real-time deployment. Second, they lack explicit\nmodeling of class-conditional feature distributions. This modeling is crucial\nfor producing reliable decision boundaries and calibrated predictions, but it\nremains underexplored due to the lack of both source data and supervision at\ntest time. In this paper, we propose ADAPT, an Advanced Distribution-Aware and\nbackPropagation-free Test-time adaptation method. We reframe TTA as a Gaussian\nprobabilistic inference task by modeling class-conditional likelihoods using\ngradually updated class means and a shared covariance matrix. This enables\nclosed-form, training-free inference. To correct potential likelihood bias, we\nintroduce lightweight regularization guided by CLIP priors and a historical\nknowledge bank. ADAPT requires no source data, no gradient updates, and no full\naccess to target data, supporting both online and transductive settings.\nExtensive experiments across diverse benchmarks demonstrate that our method\nachieves state-of-the-art performance under a wide range of distribution shifts\nwith superior scalability and robustness."
                },
                "authors": [
                    {
                        "name": "Youjia Zhang"
                    },
                    {
                        "name": "Youngeun Kim"
                    },
                    {
                        "name": "Young-Geun Choi"
                    },
                    {
                        "name": "Hongyeob Kim"
                    },
                    {
                        "name": "Huiling Liu"
                    },
                    {
                        "name": "Sungeun Hong"
                    }
                ],
                "author_detail": {
                    "name": "Sungeun Hong"
                },
                "author": "Sungeun Hong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15568v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15568v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19507v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19507v1",
                "updated": "2025-10-22T12:03:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    12,
                    3,
                    43,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T12:03:43Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    12,
                    3,
                    43,
                    2,
                    295,
                    0
                ],
                "title": "Teaming LLMs to Detect and Mitigate Hallucinations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaming LLMs to Detect and Mitigate Hallucinations"
                },
                "summary": "Recent work has demonstrated state-of-the-art results in large language model\n(LLM) hallucination detection and mitigation through consistency-based\napproaches which involve aggregating multiple responses sampled from a single\nLLM for a given prompt. These approaches help offset limitations stemming from\nthe imperfect data on which LLMs are trained, which includes biases and\nunder-representation of information required at deployment time among other\nlimitations which can lead to hallucinations. We show that extending these\nsingle-model consistency methods to combine responses from multiple LLMs with\ndifferent training data, training schemes and model architectures can result in\nsubstantial further improvements in hallucination detection and mitigation\ncapabilities beyond their single-model consistency counterparts. We evaluate\nthis \\emph{consortium consistency} approach across many model teams from a pool\nof 15 LLMs and explore under what conditions it is beneficial to team together\ndifferent LLMs in this manner. Further, we show that these performance\nimprovements often come with reduced inference costs, offsetting a significant\ndrawback with single-model consistency methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has demonstrated state-of-the-art results in large language model\n(LLM) hallucination detection and mitigation through consistency-based\napproaches which involve aggregating multiple responses sampled from a single\nLLM for a given prompt. These approaches help offset limitations stemming from\nthe imperfect data on which LLMs are trained, which includes biases and\nunder-representation of information required at deployment time among other\nlimitations which can lead to hallucinations. We show that extending these\nsingle-model consistency methods to combine responses from multiple LLMs with\ndifferent training data, training schemes and model architectures can result in\nsubstantial further improvements in hallucination detection and mitigation\ncapabilities beyond their single-model consistency counterparts. We evaluate\nthis \\emph{consortium consistency} approach across many model teams from a pool\nof 15 LLMs and explore under what conditions it is beneficial to team together\ndifferent LLMs in this manner. Further, we show that these performance\nimprovements often come with reduced inference costs, offsetting a significant\ndrawback with single-model consistency methods."
                },
                "authors": [
                    {
                        "name": "Demian Till"
                    },
                    {
                        "name": "John Smeaton"
                    },
                    {
                        "name": "Peter Haubrick"
                    },
                    {
                        "name": "Gouse Saheb"
                    },
                    {
                        "name": "Florian Graef"
                    },
                    {
                        "name": "David Berman"
                    }
                ],
                "author_detail": {
                    "name": "David Berman"
                },
                "author": "David Berman",
                "arxiv_comment": "Accepted to NeurIPS 2025 workshop on Reliable ML from Unreliable Data",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19507v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03743v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03743v3",
                "updated": "2025-10-22T12:02:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    12,
                    2,
                    1,
                    2,
                    295,
                    0
                ],
                "published": "2025-07-04T18:00:00Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    18,
                    0,
                    0,
                    4,
                    185,
                    0
                ],
                "title": "Bimetric gravity improves the fit to DESI BAO and eases the Hubble\n  tension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bimetric gravity improves the fit to DESI BAO and eases the Hubble\n  tension"
                },
                "summary": "We investigate whether the latest combination of DESI DR2 baryon acoustic\noscillation (BAO) measurements, cosmic microwave background (CMB) data (Planck\n2018 + ACT), and Type Ia supernovae (SNe Ia) compilations (Pantheon+, Union3,\nand DES Y5) favor a dynamical dark energy component, and explore if such a\nscenario can simultaneously help resolve the Hubble tension. We contrast two\nframeworks: the widely used phenomenological $w_0 w_a$CDM model, and bimetric\ngravity, a fundamental modification of general relativity that naturally gives\nrise to phantom dark energy. The $w_0 w_a$CDM model is moderately preferred\nover $\\Lambda$CDM, at the $2$-$4 \\, \\sigma$ level, when fitting DESI DR2 + CMB\n+ SNe Ia, but it exacerbates the Hubble tension. By comparison, bimetric\ngravity provides a modest improvement in fit quality, at the $1 \\, \\sigma$\nlevel, but, by inferring $H_0 = 69.0 \\pm 0.4 \\, \\mathrm{km/s/Mpc}$, it\npartially eases the Hubble tension, from a $5 \\,\\sigma$ discrepancy to a $3.7\n\\, \\sigma$ tension. Including locally calibrated SNe Ia brings the overall\npreference for the bimetric model over $\\Lambda$CDM to the $2 \\, \\sigma$ level,\ncomparable to that of the $w_0 w_a$CDM model when including the local SN Ia\ncalibration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate whether the latest combination of DESI DR2 baryon acoustic\noscillation (BAO) measurements, cosmic microwave background (CMB) data (Planck\n2018 + ACT), and Type Ia supernovae (SNe Ia) compilations (Pantheon+, Union3,\nand DES Y5) favor a dynamical dark energy component, and explore if such a\nscenario can simultaneously help resolve the Hubble tension. We contrast two\nframeworks: the widely used phenomenological $w_0 w_a$CDM model, and bimetric\ngravity, a fundamental modification of general relativity that naturally gives\nrise to phantom dark energy. The $w_0 w_a$CDM model is moderately preferred\nover $\\Lambda$CDM, at the $2$-$4 \\, \\sigma$ level, when fitting DESI DR2 + CMB\n+ SNe Ia, but it exacerbates the Hubble tension. By comparison, bimetric\ngravity provides a modest improvement in fit quality, at the $1 \\, \\sigma$\nlevel, but, by inferring $H_0 = 69.0 \\pm 0.4 \\, \\mathrm{km/s/Mpc}$, it\npartially eases the Hubble tension, from a $5 \\,\\sigma$ discrepancy to a $3.7\n\\, \\sigma$ tension. Including locally calibrated SNe Ia brings the overall\npreference for the bimetric model over $\\Lambda$CDM to the $2 \\, \\sigma$ level,\ncomparable to that of the $w_0 w_a$CDM model when including the local SN Ia\ncalibration."
                },
                "authors": [
                    {
                        "name": "Marcus Högås"
                    },
                    {
                        "name": "Edvard Mörtsell"
                    }
                ],
                "author_detail": {
                    "name": "Edvard Mörtsell"
                },
                "author": "Edvard Mörtsell",
                "arxiv_comment": "v3: added Appendix A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03743v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03743v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12718v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12718v2",
                "updated": "2025-10-22T12:00:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    12,
                    0,
                    52,
                    2,
                    295,
                    0
                ],
                "published": "2024-12-17T09:33:06Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    33,
                    6,
                    1,
                    352,
                    0
                ],
                "title": "ASAP: Advancing Semantic Alignment Promotes Multi-Modal Manipulation\n  Detecting and Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASAP: Advancing Semantic Alignment Promotes Multi-Modal Manipulation\n  Detecting and Grounding"
                },
                "summary": "We present ASAP, a new framework for detecting and grounding multi-modal\nmedia manipulation (DGM4).Upon thorough examination, we observe that accurate\nfine-grained cross-modal semantic alignment between the image and text is vital\nfor accurately manipulation detection and grounding. While existing DGM4\nmethods pay rare attention to the cross-modal alignment, hampering the accuracy\nof manipulation detecting to step further. To remedy this issue, this work\ntargets to advance the semantic alignment learning to promote this task.\nParticularly, we utilize the off-the-shelf Multimodal Large-Language Models\n(MLLMs) and Large Language Models (LLMs) to construct paired image-text pairs,\nespecially for the manipulated instances. Subsequently, a cross-modal alignment\nlearning is performed to enhance the semantic alignment. Besides the explicit\nauxiliary clues, we further design a Manipulation-Guided Cross Attention (MGCA)\nto provide implicit guidance for augmenting the manipulation perceiving. With\nthe grounding truth available during training, MGCA encourages the model to\nconcentrate more on manipulated components while downplaying normal ones,\nenhancing the model's ability to capture manipulations. Extensive experiments\nare conducted on the DGM4 dataset, the results demonstrate that our model can\nsurpass the comparison method with a clear margin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present ASAP, a new framework for detecting and grounding multi-modal\nmedia manipulation (DGM4).Upon thorough examination, we observe that accurate\nfine-grained cross-modal semantic alignment between the image and text is vital\nfor accurately manipulation detection and grounding. While existing DGM4\nmethods pay rare attention to the cross-modal alignment, hampering the accuracy\nof manipulation detecting to step further. To remedy this issue, this work\ntargets to advance the semantic alignment learning to promote this task.\nParticularly, we utilize the off-the-shelf Multimodal Large-Language Models\n(MLLMs) and Large Language Models (LLMs) to construct paired image-text pairs,\nespecially for the manipulated instances. Subsequently, a cross-modal alignment\nlearning is performed to enhance the semantic alignment. Besides the explicit\nauxiliary clues, we further design a Manipulation-Guided Cross Attention (MGCA)\nto provide implicit guidance for augmenting the manipulation perceiving. With\nthe grounding truth available during training, MGCA encourages the model to\nconcentrate more on manipulated components while downplaying normal ones,\nenhancing the model's ability to capture manipulations. Extensive experiments\nare conducted on the DGM4 dataset, the results demonstrate that our model can\nsurpass the comparison method with a clear margin."
                },
                "authors": [
                    {
                        "name": "Zhenxing Zhang"
                    },
                    {
                        "name": "Yaxiong Wang"
                    },
                    {
                        "name": "Lechao Cheng"
                    },
                    {
                        "name": "Zhun Zhong"
                    },
                    {
                        "name": "Dan Guo"
                    },
                    {
                        "name": "Meng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Meng Wang"
                },
                "author": "Meng Wang",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12718v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12718v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "Multimedia",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19506v1",
                "updated": "2025-10-22T12:00:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    12,
                    0,
                    21,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T12:00:21Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    12,
                    0,
                    21,
                    2,
                    295,
                    0
                ],
                "title": "Lookahead Routing for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lookahead Routing for Large Language Models"
                },
                "summary": "Large language model (LLM) routers improve the efficiency of multi-model\nsystems by directing each query to the most appropriate model while leveraging\nthe diverse strengths of heterogeneous LLMs. Most existing approaches frame\nrouting as a classification problem based solely on the input query. While this\nreduces overhead by avoiding inference across all models, it overlooks valuable\ninformation that could be gleaned from potential outputs and fails to capture\nimplicit intent or contextual nuances that often emerge only during response\ngeneration. These limitations can result in suboptimal routing decisions,\nparticularly for complex or ambiguous queries that require deeper semantic\nunderstanding. To address this challenge, we propose Lookahead, a routing\nframework that \"foresees\" potential model outputs by predicting their latent\nrepresentations and uses these predictions to guide model selection, thus\nenabling more informed routing without full inference. Within this framework,\nwe implement two approaches based on causal and masked language models.\nEmpirical evaluations across seven public benchmarks - spanning instruction\nfollowing, mathematical reasoning, and code generation - show that Lookahead\nconsistently outperforms existing routing baselines, achieving an average\nperformance gain of 7.7% over the state-of-the-art. Our code is available at\nhttps://github.com/huangcb01/lookahead-routing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) routers improve the efficiency of multi-model\nsystems by directing each query to the most appropriate model while leveraging\nthe diverse strengths of heterogeneous LLMs. Most existing approaches frame\nrouting as a classification problem based solely on the input query. While this\nreduces overhead by avoiding inference across all models, it overlooks valuable\ninformation that could be gleaned from potential outputs and fails to capture\nimplicit intent or contextual nuances that often emerge only during response\ngeneration. These limitations can result in suboptimal routing decisions,\nparticularly for complex or ambiguous queries that require deeper semantic\nunderstanding. To address this challenge, we propose Lookahead, a routing\nframework that \"foresees\" potential model outputs by predicting their latent\nrepresentations and uses these predictions to guide model selection, thus\nenabling more informed routing without full inference. Within this framework,\nwe implement two approaches based on causal and masked language models.\nEmpirical evaluations across seven public benchmarks - spanning instruction\nfollowing, mathematical reasoning, and code generation - show that Lookahead\nconsistently outperforms existing routing baselines, achieving an average\nperformance gain of 7.7% over the state-of-the-art. Our code is available at\nhttps://github.com/huangcb01/lookahead-routing."
                },
                "authors": [
                    {
                        "name": "Canbin Huang"
                    },
                    {
                        "name": "Tianyuan Shi"
                    },
                    {
                        "name": "Yuhua Zhu"
                    },
                    {
                        "name": "Ruijun Chen"
                    },
                    {
                        "name": "Xiaojun Quan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Quan"
                },
                "author": "Xiaojun Quan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08878v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08878v3",
                "updated": "2025-10-22T11:58:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    11,
                    58,
                    26,
                    2,
                    295,
                    0
                ],
                "published": "2024-08-02T17:42:03Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    17,
                    42,
                    3,
                    4,
                    215,
                    0
                ],
                "title": "Knowledge Prompting: How Knowledge Engineers Use Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Prompting: How Knowledge Engineers Use Large Language Models"
                },
                "summary": "Despite many advances in knowledge engineering (KE), challenges remain in\nareas such as engineering knowledge graphs (KGs) at scale, keeping up with\nevolving domain knowledge, multilingualism, and multimodality. Recently, KE has\nused LLMs to support semi-automatic tasks, but the most effective use of LLMs\nto support knowledge engineers across the KE activites is still in its infancy.\nTo explore the vision of LLM copilots for KE and change existing KE practices,\nwe conducted a multimethod study during a KE hackathon. We investigated\nparticipants' views on the use of LLMs, the challenges they face, the skills\nthey may need to integrate LLMs into their practices, and how they use LLMs\nresponsibly. We found participants felt LLMs could contribute to improving\nefficiency when engineering KGs, but presented increased challenges around the\nalready complex issues of evaluating the KE tasks. We discovered prompting to\nbe a useful but undervalued skill for knowledge engineers working with LLMs,\nand note that natural language processing skills may become more relevant\nacross more roles in KG construction. Integrating LLMs into KE tasks needs to\nbe mindful of potential risks and harms related to responsible AI. Given the\nlimited ethical training, most knowledge engineers receive solutions such as\nour suggested `KG cards' based on data cards could be a useful guide for KG\nconstruction. Our findings can support designers of KE AI copilots, KE\nresearchers, and practitioners using advanced AI to develop trustworthy\napplications, propose new methodologies for KE and operate new technologies\nresponsibly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite many advances in knowledge engineering (KE), challenges remain in\nareas such as engineering knowledge graphs (KGs) at scale, keeping up with\nevolving domain knowledge, multilingualism, and multimodality. Recently, KE has\nused LLMs to support semi-automatic tasks, but the most effective use of LLMs\nto support knowledge engineers across the KE activites is still in its infancy.\nTo explore the vision of LLM copilots for KE and change existing KE practices,\nwe conducted a multimethod study during a KE hackathon. We investigated\nparticipants' views on the use of LLMs, the challenges they face, the skills\nthey may need to integrate LLMs into their practices, and how they use LLMs\nresponsibly. We found participants felt LLMs could contribute to improving\nefficiency when engineering KGs, but presented increased challenges around the\nalready complex issues of evaluating the KE tasks. We discovered prompting to\nbe a useful but undervalued skill for knowledge engineers working with LLMs,\nand note that natural language processing skills may become more relevant\nacross more roles in KG construction. Integrating LLMs into KE tasks needs to\nbe mindful of potential risks and harms related to responsible AI. Given the\nlimited ethical training, most knowledge engineers receive solutions such as\nour suggested `KG cards' based on data cards could be a useful guide for KG\nconstruction. Our findings can support designers of KE AI copilots, KE\nresearchers, and practitioners using advanced AI to develop trustworthy\napplications, propose new methodologies for KE and operate new technologies\nresponsibly."
                },
                "authors": [
                    {
                        "name": "Elisavet Koutsiana"
                    },
                    {
                        "name": "Johanna Walker"
                    },
                    {
                        "name": "Michelle Nwachukwu"
                    },
                    {
                        "name": "Bohui Zhang"
                    },
                    {
                        "name": "Albert Meroño-Peñuela"
                    },
                    {
                        "name": "Elena Simperl"
                    }
                ],
                "author_detail": {
                    "name": "Elena Simperl"
                },
                "author": "Elena Simperl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08878v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08878v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21318v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21318v2",
                "updated": "2025-10-22T11:54:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    11,
                    54,
                    52,
                    2,
                    295,
                    0
                ],
                "published": "2025-03-27T09:57:26Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    9,
                    57,
                    26,
                    3,
                    86,
                    0
                ],
                "title": "Explicit error bounds and guaranteed convergence of the Koopman-Hill\n  projection stability method for linear time-periodic dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explicit error bounds and guaranteed convergence of the Koopman-Hill\n  projection stability method for linear time-periodic dynamics"
                },
                "summary": "The Koopman-Hill projection method offers an efficient approach for stability\nanalysis of linear time-periodic systems, and thereby also for the Floquet\nstability analysis of periodic solutions of nonlinear systems. However, its\naccuracy has previously been supported only by numerical evidence, lacking\nrigorous theoretical guarantees. This paper presents the first explicit error\nbound for the truncation error of the Koopman-Hill projection method,\nestablishing a solid theoretical foundation for its application. The bound\napplies to linear time-periodic systems whose Fourier coefficients decay\nexponentially with a sufficient rate, and is derived using constructive series\nexpansions. The bound quantifies the difference between the true and\napproximated fundamental solution matrices, clarifies conditions for guaranteed\nconvergence, and enables conservative but reliable inference of Floquet\nmultipliers and stability properties. Additionally, the same methodology\napplied to a subharmonic formulation demonstrates improved convergence rates of\nthe latter. Numerical examples, including the Mathieu equation and the Duffing\noscillator, illustrate the practical relevance of the bound and underscore its\nimportance as the first rigorous theoretical justification for the Koopman-Hill\nprojection method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Koopman-Hill projection method offers an efficient approach for stability\nanalysis of linear time-periodic systems, and thereby also for the Floquet\nstability analysis of periodic solutions of nonlinear systems. However, its\naccuracy has previously been supported only by numerical evidence, lacking\nrigorous theoretical guarantees. This paper presents the first explicit error\nbound for the truncation error of the Koopman-Hill projection method,\nestablishing a solid theoretical foundation for its application. The bound\napplies to linear time-periodic systems whose Fourier coefficients decay\nexponentially with a sufficient rate, and is derived using constructive series\nexpansions. The bound quantifies the difference between the true and\napproximated fundamental solution matrices, clarifies conditions for guaranteed\nconvergence, and enables conservative but reliable inference of Floquet\nmultipliers and stability properties. Additionally, the same methodology\napplied to a subharmonic formulation demonstrates improved convergence rates of\nthe latter. Numerical examples, including the Mathieu equation and the Duffing\noscillator, illustrate the practical relevance of the bound and underscore its\nimportance as the first rigorous theoretical justification for the Koopman-Hill\nprojection method."
                },
                "authors": [
                    {
                        "name": "Fabia Bayer"
                    },
                    {
                        "name": "Remco I. Leine"
                    }
                ],
                "author_detail": {
                    "name": "Remco I. Leine"
                },
                "author": "Remco I. Leine",
                "arxiv_comment": "preprint, 39 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21318v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21318v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "35L70 (Primary), 37M20 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19498v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19498v1",
                "updated": "2025-10-22T11:50:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    11,
                    50,
                    0,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T11:50:00Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    11,
                    50,
                    0,
                    2,
                    295,
                    0
                ],
                "title": "Energy-Efficient and Dequantization-Free Q-LLMs: A Spiking Neural\n  Network Approach to Salient Value Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy-Efficient and Dequantization-Free Q-LLMs: A Spiking Neural\n  Network Approach to Salient Value Mitigation"
                },
                "summary": "In the era of large language models (LLMs), weight-activation quantization\nhelps fit models on edge device by reducing memory and compute bit-widths.\nHowever, three challenges persist for energy constrained hardware: (1) even\nafter quantization, multiply-accumulate (MAC) operations remain unavoidable and\ncontinue to dominate energy consumption; (2) dequantization (or\nper-tensor/channel rescaling) introduces extra arithmetic and data movement,\nincreasing latency and energy; (3) uniform parameters bit widths clip salient\nvalues-while intra-channel mixed precision is generally impractical on current\nmatrix hardware and memory. In contrast, brain-inspired Spiking Neural Networks\n(SNNs), owing to their binary spike-based information representation and the\nIntegrate-and-Fire (IF) paradigm, naturally support mixed-precision storage and\nenergy-efficient computation by replacing complex MACs with temporal Accumulate\n(ACCs). Motivated by this property, we propose SpikeQuant, which selectively\napplies mixed-precision quantization to activations with salient values and\nre-encodes them into binary spike counts, thereby enabling dynamic mixed\nstorage of different bitwidths. Furthermore, by embedding the quantization\nscale into the threshold of the IF mechanism, our approach performs\nenergy-efficient linear transformations on weights and activations while\navoiding explicit dequantization. Experimental results demonstrate that\nSpikeQuant consistently achieves near-FP16 perplexity under W4A4 quantization\nwhile reducing energy cost by up to 4.6 times compared to existing methods,\nhighlighting its effectiveness for accurate and energy-efficient LLM\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of large language models (LLMs), weight-activation quantization\nhelps fit models on edge device by reducing memory and compute bit-widths.\nHowever, three challenges persist for energy constrained hardware: (1) even\nafter quantization, multiply-accumulate (MAC) operations remain unavoidable and\ncontinue to dominate energy consumption; (2) dequantization (or\nper-tensor/channel rescaling) introduces extra arithmetic and data movement,\nincreasing latency and energy; (3) uniform parameters bit widths clip salient\nvalues-while intra-channel mixed precision is generally impractical on current\nmatrix hardware and memory. In contrast, brain-inspired Spiking Neural Networks\n(SNNs), owing to their binary spike-based information representation and the\nIntegrate-and-Fire (IF) paradigm, naturally support mixed-precision storage and\nenergy-efficient computation by replacing complex MACs with temporal Accumulate\n(ACCs). Motivated by this property, we propose SpikeQuant, which selectively\napplies mixed-precision quantization to activations with salient values and\nre-encodes them into binary spike counts, thereby enabling dynamic mixed\nstorage of different bitwidths. Furthermore, by embedding the quantization\nscale into the threshold of the IF mechanism, our approach performs\nenergy-efficient linear transformations on weights and activations while\navoiding explicit dequantization. Experimental results demonstrate that\nSpikeQuant consistently achieves near-FP16 perplexity under W4A4 quantization\nwhile reducing energy cost by up to 4.6 times compared to existing methods,\nhighlighting its effectiveness for accurate and energy-efficient LLM\ndeployment."
                },
                "authors": [
                    {
                        "name": "Chenyu Wang"
                    },
                    {
                        "name": "Zhanglu Yan"
                    },
                    {
                        "name": "Zhi Zhou"
                    },
                    {
                        "name": "Xu Chen"
                    },
                    {
                        "name": "Weng-Fai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Weng-Fai Wong"
                },
                "author": "Weng-Fai Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19498v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09001v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09001v5",
                "updated": "2025-10-22T11:46:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    11,
                    46,
                    51,
                    2,
                    295,
                    0
                ],
                "published": "2025-05-13T22:36:07Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    22,
                    36,
                    7,
                    1,
                    133,
                    0
                ],
                "title": "Causal Feedback Discovery using Convergence Cross Mapping on Sea Ice\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Feedback Discovery using Convergence Cross Mapping on Sea Ice\n  Data"
                },
                "summary": "Identifying causal relationships in climate systems remains challenging due\nto nonlinear, coupled dynamics that limit the effectiveness of linear and\nstochastic causal discovery approaches. This study benchmarks Convergence Cross\nMapping (CCM) against Granger causality, PCMCI, and VarLiNGAM using both\nsynthetic datasets with ground truth causal links and 41 years of Arctic\nclimate data (1979--2021). Unlike stochastic models that rely on autoregressive\nresidual dependence, CCM leverages Takens' state-space reconstruction and\ndelay-embedding to reconstruct attractor manifolds from time series. Cross\nmapping between reconstructed manifolds exploits deterministic signatures of\ncausation, enabling the detection of weak and bidirectional causal links that\nlinear models fail to resolve. Results demonstrate that CCM achieves higher\nspecificity and fewer false positives on synthetic benchmarks, while\nmaintaining robustness under observational noise and limited sample lengths. On\nArctic data, CCM reveals significant causal interactions between sea ice extent\nand atmospheric variables like specific humidity, longwave radiation, and\nsurface temperature with a $p$-value of $0.009$, supporting ice-albedo\nfeedbacks and moisture-radiation couplings central to Arctic amplification. In\ncontrast, stochastic approaches miss these nonlinear dependencies or infer\nspurious causal relations. This work establishes CCM as a robust causal\ninference tool for nonlinear climate dynamics and provides the first systematic\nbenchmarking framework for method selection in climate research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying causal relationships in climate systems remains challenging due\nto nonlinear, coupled dynamics that limit the effectiveness of linear and\nstochastic causal discovery approaches. This study benchmarks Convergence Cross\nMapping (CCM) against Granger causality, PCMCI, and VarLiNGAM using both\nsynthetic datasets with ground truth causal links and 41 years of Arctic\nclimate data (1979--2021). Unlike stochastic models that rely on autoregressive\nresidual dependence, CCM leverages Takens' state-space reconstruction and\ndelay-embedding to reconstruct attractor manifolds from time series. Cross\nmapping between reconstructed manifolds exploits deterministic signatures of\ncausation, enabling the detection of weak and bidirectional causal links that\nlinear models fail to resolve. Results demonstrate that CCM achieves higher\nspecificity and fewer false positives on synthetic benchmarks, while\nmaintaining robustness under observational noise and limited sample lengths. On\nArctic data, CCM reveals significant causal interactions between sea ice extent\nand atmospheric variables like specific humidity, longwave radiation, and\nsurface temperature with a $p$-value of $0.009$, supporting ice-albedo\nfeedbacks and moisture-radiation couplings central to Arctic amplification. In\ncontrast, stochastic approaches miss these nonlinear dependencies or infer\nspurious causal relations. This work establishes CCM as a robust causal\ninference tool for nonlinear climate dynamics and provides the first systematic\nbenchmarking framework for method selection in climate research."
                },
                "authors": [
                    {
                        "name": "Francis Nji"
                    },
                    {
                        "name": "Seraj Al Mahmud Mostafa"
                    },
                    {
                        "name": "Jianwu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianwu Wang"
                },
                "author": "Jianwu Wang",
                "arxiv_comment": "Accepted in ACM Sigspatial Conference, PolDS Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09001v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09001v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19497v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19497v1",
                "updated": "2025-10-22T11:45:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    11,
                    45,
                    44,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T11:45:44Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    11,
                    45,
                    44,
                    2,
                    295,
                    0
                ],
                "title": "Modeling realistic human behavior using generative agents in a\n  multimodal transport system: Software architecture and Application to\n  Toulouse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling realistic human behavior using generative agents in a\n  multimodal transport system: Software architecture and Application to\n  Toulouse"
                },
                "summary": "Modeling realistic human behaviour to understand people's mode choices in\norder to propose personalised mobility solutions remains challenging. This\npaper presents an architecture for modeling realistic human mobility behavior\nin complex multimodal transport systems, demonstrated through a case study in\nToulouse, France. We apply Large Language Models (LLMs) within an agent-based\nsimulation to capture decision-making in a real urban setting. The framework\nintegrates the GAMA simulation platform with an LLM-based generative agent,\nalong with General Transit Feed Specification (GTFS) data for public transport,\nand OpenTripPlanner for multimodal routing. GAMA platform models the\ninteractive transport environment, providing visualization and dynamic agent\ninteractions while eliminating the need to construct the simulation environment\nfrom scratch. This design enables a stronger focus on developing generative\nagents and evaluating their performance in transport decision-making processes.\nOver a simulated month, results show that agents not only make context-aware\ntransport decisions but also form habits over time. We conclude that combining\nLLMs with agent-based simulation offers a promising direction for advancing\nintelligent transportation systems and personalised multimodal mobility\nsolutions. We also discuss some limitations of this approach and outline future\nwork on scaling to larger regions, integrating real-time data, and refining\nmemory models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling realistic human behaviour to understand people's mode choices in\norder to propose personalised mobility solutions remains challenging. This\npaper presents an architecture for modeling realistic human mobility behavior\nin complex multimodal transport systems, demonstrated through a case study in\nToulouse, France. We apply Large Language Models (LLMs) within an agent-based\nsimulation to capture decision-making in a real urban setting. The framework\nintegrates the GAMA simulation platform with an LLM-based generative agent,\nalong with General Transit Feed Specification (GTFS) data for public transport,\nand OpenTripPlanner for multimodal routing. GAMA platform models the\ninteractive transport environment, providing visualization and dynamic agent\ninteractions while eliminating the need to construct the simulation environment\nfrom scratch. This design enables a stronger focus on developing generative\nagents and evaluating their performance in transport decision-making processes.\nOver a simulated month, results show that agents not only make context-aware\ntransport decisions but also form habits over time. We conclude that combining\nLLMs with agent-based simulation offers a promising direction for advancing\nintelligent transportation systems and personalised multimodal mobility\nsolutions. We also discuss some limitations of this approach and outline future\nwork on scaling to larger regions, integrating real-time data, and refining\nmemory models."
                },
                "authors": [
                    {
                        "name": "Trung-Dung Vu"
                    },
                    {
                        "name": "Benoit Gaudou"
                    },
                    {
                        "name": "Kamaldeep Singh Oberoi"
                    }
                ],
                "author_detail": {
                    "name": "Kamaldeep Singh Oberoi"
                },
                "author": "Kamaldeep Singh Oberoi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19497v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19496v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19496v1",
                "updated": "2025-10-22T11:44:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    11,
                    44,
                    31,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T11:44:31Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    11,
                    44,
                    31,
                    2,
                    295,
                    0
                ],
                "title": "CARES: Context-Aware Resolution Selector for VLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CARES: Context-Aware Resolution Selector for VLMs"
                },
                "summary": "Large vision-language models (VLMs) commonly process images at native or high\nresolution to remain effective across tasks. This inflates visual tokens ofter\nto 97-99% of total tokens, resulting in high compute and latency, even when\nlow-resolution images would suffice. We introduce \\emph{CARES}-a\n\\textbf{C}ontext-\\textbf{A}ware \\textbf{R}esolution \\textbf{S}elector, a\nlightweight preprocessing module that, given an image-query pair, predicts the\n\\emph{minimal} sufficient input resolution. CARES uses a compact VLM (350M) to\nextract features and predict when a target pretrained VLM's response converges\nto its peak ability to answer correctly. Though trained as a discrete\nclassifier over a set of optional resolutions, CARES interpolates continuous\nresolutions at inference for fine-grained control. Across five multimodal\nbenchmarks spanning documents and natural images, as well as diverse target\nVLMs, CARES preserves task performance while reducing compute by up to 80%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large vision-language models (VLMs) commonly process images at native or high\nresolution to remain effective across tasks. This inflates visual tokens ofter\nto 97-99% of total tokens, resulting in high compute and latency, even when\nlow-resolution images would suffice. We introduce \\emph{CARES}-a\n\\textbf{C}ontext-\\textbf{A}ware \\textbf{R}esolution \\textbf{S}elector, a\nlightweight preprocessing module that, given an image-query pair, predicts the\n\\emph{minimal} sufficient input resolution. CARES uses a compact VLM (350M) to\nextract features and predict when a target pretrained VLM's response converges\nto its peak ability to answer correctly. Though trained as a discrete\nclassifier over a set of optional resolutions, CARES interpolates continuous\nresolutions at inference for fine-grained control. Across five multimodal\nbenchmarks spanning documents and natural images, as well as diverse target\nVLMs, CARES preserves task performance while reducing compute by up to 80%."
                },
                "authors": [
                    {
                        "name": "Moshe Kimhi"
                    },
                    {
                        "name": "Nimrod Shabtay"
                    },
                    {
                        "name": "Raja Giryes"
                    },
                    {
                        "name": "Chaim Baskin"
                    },
                    {
                        "name": "Eli Schwartz"
                    }
                ],
                "author_detail": {
                    "name": "Eli Schwartz"
                },
                "author": "Eli Schwartz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19496v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19496v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19492v1",
                "updated": "2025-10-22T11:39:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    11,
                    39,
                    1,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T11:39:01Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    11,
                    39,
                    1,
                    2,
                    295,
                    0
                ],
                "title": "Machine Text Detectors are Membership Inference Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Text Detectors are Membership Inference Attacks"
                },
                "summary": "Although membership inference attacks (MIAs) and machine-generated text\ndetection target different goals, identifying training samples and synthetic\ntexts, their methods often exploit similar signals based on a language model's\nprobability distribution. Despite this shared methodological foundation, the\ntwo tasks have been independently studied, which may lead to conclusions that\noverlook stronger methods and valuable insights developed in the other task. In\nthis work, we theoretically and empirically investigate the transferability,\ni.e., how well a method originally developed for one task performs on the\nother, between MIAs and machine text detection. For our theoretical\ncontribution, we prove that the metric that achieves the asymptotically highest\nperformance on both tasks is the same. We unify a large proportion of the\nexisting literature in the context of this optimal metric and hypothesize that\nthe accuracy with which a given method approximates this metric is directly\ncorrelated with its transferability. Our large-scale empirical experiments,\nincluding 7 state-of-the-art MIA methods and 5 state-of-the-art machine text\ndetectors across 13 domains and 10 generators, demonstrate very strong rank\ncorrelation (rho > 0.6) in cross-task performance. We notably find that\nBinoculars, originally designed for machine text detection, achieves\nstate-of-the-art performance on MIA benchmarks as well, demonstrating the\npractical impact of the transferability. Our findings highlight the need for\ngreater cross-task awareness and collaboration between the two research\ncommunities. To facilitate cross-task developments and fair evaluations, we\nintroduce MINT, a unified evaluation suite for MIAs and machine-generated text\ndetection, with implementation of 15 recent methods from both tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although membership inference attacks (MIAs) and machine-generated text\ndetection target different goals, identifying training samples and synthetic\ntexts, their methods often exploit similar signals based on a language model's\nprobability distribution. Despite this shared methodological foundation, the\ntwo tasks have been independently studied, which may lead to conclusions that\noverlook stronger methods and valuable insights developed in the other task. In\nthis work, we theoretically and empirically investigate the transferability,\ni.e., how well a method originally developed for one task performs on the\nother, between MIAs and machine text detection. For our theoretical\ncontribution, we prove that the metric that achieves the asymptotically highest\nperformance on both tasks is the same. We unify a large proportion of the\nexisting literature in the context of this optimal metric and hypothesize that\nthe accuracy with which a given method approximates this metric is directly\ncorrelated with its transferability. Our large-scale empirical experiments,\nincluding 7 state-of-the-art MIA methods and 5 state-of-the-art machine text\ndetectors across 13 domains and 10 generators, demonstrate very strong rank\ncorrelation (rho > 0.6) in cross-task performance. We notably find that\nBinoculars, originally designed for machine text detection, achieves\nstate-of-the-art performance on MIA benchmarks as well, demonstrating the\npractical impact of the transferability. Our findings highlight the need for\ngreater cross-task awareness and collaboration between the two research\ncommunities. To facilitate cross-task developments and fair evaluations, we\nintroduce MINT, a unified evaluation suite for MIAs and machine-generated text\ndetection, with implementation of 15 recent methods from both tasks."
                },
                "authors": [
                    {
                        "name": "Ryuto Koike"
                    },
                    {
                        "name": "Liam Dugan"
                    },
                    {
                        "name": "Masahiro Kaneko"
                    },
                    {
                        "name": "Chris Callison-Burch"
                    },
                    {
                        "name": "Naoaki Okazaki"
                    }
                ],
                "author_detail": {
                    "name": "Naoaki Okazaki"
                },
                "author": "Naoaki Okazaki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.21228v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21228v3",
                "updated": "2025-10-22T17:58:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    58,
                    0,
                    2,
                    295,
                    0
                ],
                "published": "2024-10-28T17:14:01Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    14,
                    1,
                    0,
                    302,
                    0
                ],
                "title": "LoRA vs Full Fine-tuning: An Illusion of Equivalence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA vs Full Fine-tuning: An Illusion of Equivalence"
                },
                "summary": "Fine-tuning is a crucial paradigm for adapting pre-trained large language\nmodels to downstream tasks. Recently, methods like Low-Rank Adaptation (LoRA)\nhave been shown to effectively fine-tune LLMs with an extreme reduction in\ntrainable parameters. But, \\emph{are their learned solutions really\nequivalent?} We study how LoRA and full-finetuning change pre-trained models by\nanalyzing the model's weight matrices through the lens of their spectral\nproperties. We find that LoRA and full fine-tuning yield weight matrices whose\nsingular value decompositions exhibit very different structure: weight matrices\ntrained with LoRA have new, high-ranking singular vectors, which we call\n\\emph{intruder dimensions}, while those trained with full fine-tuning do not.\nFurther, we extend the finding that LoRA forgets less than full fine-tuning and\nfind its forgetting is vastly localized to the intruder dimension -- by\ncausally intervening on the intruder dimensions by changing their associated\nsingular values post-fine-tuning, we show that they cause forgetting. Moreover,\nscaling them down significantly improves modeling of the pre-training\ndistribution with a minimal drop in downstream task performance. Given this, we\nshould expect accumulating intruder dimensions to be harmful and lead to more\nforgetting. This will be amplified during continual learning because of\nsequentially fine-tuning, and we show that LoRA models do accumulate intruder\ndimensions here tend to perform worse in this setting, emphasizing the\npracticality of our findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning is a crucial paradigm for adapting pre-trained large language\nmodels to downstream tasks. Recently, methods like Low-Rank Adaptation (LoRA)\nhave been shown to effectively fine-tune LLMs with an extreme reduction in\ntrainable parameters. But, \\emph{are their learned solutions really\nequivalent?} We study how LoRA and full-finetuning change pre-trained models by\nanalyzing the model's weight matrices through the lens of their spectral\nproperties. We find that LoRA and full fine-tuning yield weight matrices whose\nsingular value decompositions exhibit very different structure: weight matrices\ntrained with LoRA have new, high-ranking singular vectors, which we call\n\\emph{intruder dimensions}, while those trained with full fine-tuning do not.\nFurther, we extend the finding that LoRA forgets less than full fine-tuning and\nfind its forgetting is vastly localized to the intruder dimension -- by\ncausally intervening on the intruder dimensions by changing their associated\nsingular values post-fine-tuning, we show that they cause forgetting. Moreover,\nscaling them down significantly improves modeling of the pre-training\ndistribution with a minimal drop in downstream task performance. Given this, we\nshould expect accumulating intruder dimensions to be harmful and lead to more\nforgetting. This will be amplified during continual learning because of\nsequentially fine-tuning, and we show that LoRA models do accumulate intruder\ndimensions here tend to perform worse in this setting, emphasizing the\npracticality of our findings."
                },
                "authors": [
                    {
                        "name": "Reece Shuttleworth"
                    },
                    {
                        "name": "Jacob Andreas"
                    },
                    {
                        "name": "Antonio Torralba"
                    },
                    {
                        "name": "Pratyusha Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Pratyusha Sharma"
                },
                "author": "Pratyusha Sharma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21228v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21228v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17501v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17501v3",
                "updated": "2025-10-22T17:54:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    54,
                    43,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-20T12:54:32Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    54,
                    32,
                    0,
                    293,
                    0
                ],
                "title": "Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization"
                },
                "summary": "We propose a rubric-guided, pseudo-labeled, and prompt-driven zero-shot video\nsummarization framework that bridges large language models with structured\nsemantic reasoning. A small subset of human annotations is converted into\nhigh-confidence pseudo labels and organized into dataset-adaptive rubrics\ndefining clear evaluation dimensions such as thematic relevance, action detail,\nand narrative progression. During inference, boundary scenes, including the\nopening and closing segments, are scored independently based on their own\ndescriptions, while intermediate scenes incorporate concise summaries of\nadjacent segments to assess narrative continuity and redundancy. This design\nenables the language model to balance local salience with global coherence\nwithout any parameter tuning. Across three benchmarks, the proposed method\nachieves stable and competitive results, with F1 scores of 57.58 on SumMe,\n63.05 on TVSum, and 53.79 on QFVS, surpassing zero-shot baselines by +0.85,\n+0.84, and +0.37, respectively. These outcomes demonstrate that rubric-guided\npseudo labeling combined with contextual prompting effectively stabilizes\nLLM-based scoring and establishes a general, interpretable, and training-free\nparadigm for both generic and query-focused video summarization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a rubric-guided, pseudo-labeled, and prompt-driven zero-shot video\nsummarization framework that bridges large language models with structured\nsemantic reasoning. A small subset of human annotations is converted into\nhigh-confidence pseudo labels and organized into dataset-adaptive rubrics\ndefining clear evaluation dimensions such as thematic relevance, action detail,\nand narrative progression. During inference, boundary scenes, including the\nopening and closing segments, are scored independently based on their own\ndescriptions, while intermediate scenes incorporate concise summaries of\nadjacent segments to assess narrative continuity and redundancy. This design\nenables the language model to balance local salience with global coherence\nwithout any parameter tuning. Across three benchmarks, the proposed method\nachieves stable and competitive results, with F1 scores of 57.58 on SumMe,\n63.05 on TVSum, and 53.79 on QFVS, surpassing zero-shot baselines by +0.85,\n+0.84, and +0.37, respectively. These outcomes demonstrate that rubric-guided\npseudo labeling combined with contextual prompting effectively stabilizes\nLLM-based scoring and establishes a general, interpretable, and training-free\nparadigm for both generic and query-focused video summarization."
                },
                "authors": [
                    {
                        "name": "Yuanli Wu"
                    },
                    {
                        "name": "Long Zhang"
                    },
                    {
                        "name": "Yue Du"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17501v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17501v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24379v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24379v3",
                "updated": "2025-10-22T17:51:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    51,
                    21,
                    2,
                    295,
                    0
                ],
                "published": "2025-05-30T09:09:33Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    9,
                    9,
                    33,
                    4,
                    150,
                    0
                ],
                "title": "Unlearned but Not Forgotten: Data Extraction after Exact Unlearning in\n  LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlearned but Not Forgotten: Data Extraction after Exact Unlearning in\n  LLM"
                },
                "summary": "Large Language Models are typically trained on datasets collected from the\nweb, which may inadvertently contain harmful or sensitive personal information.\nTo address growing privacy concerns, unlearning methods have been proposed to\nremove the influence of specific data from trained models. Of these, exact\nunlearning -- which retrains the model from scratch without the target data --\nis widely regarded the gold standard for mitigating privacy risks in\ndeployment. In this paper, we revisit this assumption in a practical deployment\nsetting where both the pre- and post-unlearning logits API are exposed, such as\nin open-weight scenarios. Targeting this setting, we introduce a novel data\nextraction attack that leverages signals from the pre-unlearning model to guide\nthe post-unlearning model, uncovering patterns that reflect the removed data\ndistribution. Combining model guidance with a token filtering strategy, our\nattack significantly improves extraction success rates -- doubling performance\nin some cases -- across common benchmarks such as MUSE, TOFU, and WMDP.\nFurthermore, we demonstrate our attack's effectiveness on a simulated medical\ndiagnosis dataset to highlight real-world privacy risks associated with exact\nunlearning. In light of our findings, which suggest that unlearning may, in a\ncontradictory way, increase the risk of privacy leakage during real-world\ndeployments, we advocate for evaluation of unlearning methods to consider\nbroader threat models that account not only for post-unlearning models but also\nfor adversarial access to prior checkpoints. Code is publicly available at:\nhttps://github.com/Nicholas0228/unlearned_data_extraction_llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are typically trained on datasets collected from the\nweb, which may inadvertently contain harmful or sensitive personal information.\nTo address growing privacy concerns, unlearning methods have been proposed to\nremove the influence of specific data from trained models. Of these, exact\nunlearning -- which retrains the model from scratch without the target data --\nis widely regarded the gold standard for mitigating privacy risks in\ndeployment. In this paper, we revisit this assumption in a practical deployment\nsetting where both the pre- and post-unlearning logits API are exposed, such as\nin open-weight scenarios. Targeting this setting, we introduce a novel data\nextraction attack that leverages signals from the pre-unlearning model to guide\nthe post-unlearning model, uncovering patterns that reflect the removed data\ndistribution. Combining model guidance with a token filtering strategy, our\nattack significantly improves extraction success rates -- doubling performance\nin some cases -- across common benchmarks such as MUSE, TOFU, and WMDP.\nFurthermore, we demonstrate our attack's effectiveness on a simulated medical\ndiagnosis dataset to highlight real-world privacy risks associated with exact\nunlearning. In light of our findings, which suggest that unlearning may, in a\ncontradictory way, increase the risk of privacy leakage during real-world\ndeployments, we advocate for evaluation of unlearning methods to consider\nbroader threat models that account not only for post-unlearning models but also\nfor adversarial access to prior checkpoints. Code is publicly available at:\nhttps://github.com/Nicholas0228/unlearned_data_extraction_llm."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Wu"
                    },
                    {
                        "name": "Yifei Pang"
                    },
                    {
                        "name": "Terrance Liu"
                    },
                    {
                        "name": "Zhiwei Steven Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiwei Steven Wu"
                },
                "author": "Zhiwei Steven Wu",
                "arxiv_comment": "Accepted by Neurips 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24379v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24379v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19811v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19811v1",
                "updated": "2025-10-22T17:48:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    48,
                    23,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T17:48:23Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    48,
                    23,
                    2,
                    295,
                    0
                ],
                "title": "Hubble: a Model Suite to Advance the Study of LLM Memorization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hubble: a Model Suite to Advance the Study of LLM Memorization"
                },
                "summary": "We present Hubble, a suite of fully open-source large language models (LLMs)\nfor the scientific study of LLM memorization. Hubble models come in standard\nand perturbed variants: standard models are pretrained on a large English\ncorpus, and perturbed models are trained in the same way but with controlled\ninsertion of text (e.g., book passages, biographies, and test sets) designed to\nemulate key memorization risks. Our core release includes 8 models -- standard\nand perturbed models with 1B or 8B parameters, pretrained on 100B or 500B\ntokens -- establishing that memorization risks are determined by the frequency\nof sensitive data relative to size of the training corpus (i.e., a password\nappearing once in a smaller corpus is memorized better than the same password\nin a larger corpus). Our release also includes 6 perturbed models with text\ninserted at different pretraining phases, showing that sensitive data without\ncontinued exposure can be forgotten. These findings suggest two best practices\nfor addressing memorization risks: to dilute sensitive data by increasing the\nsize of the training corpus, and to order sensitive data to appear earlier in\ntraining. Beyond these general empirical findings, Hubble enables a broad range\nof memorization research; for example, analyzing the biographies reveals how\nreadily different types of private information are memorized. We also\ndemonstrate that the randomized insertions in Hubble make it an ideal testbed\nfor membership inference and machine unlearning, and invite the community to\nfurther explore, benchmark, and build upon our work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Hubble, a suite of fully open-source large language models (LLMs)\nfor the scientific study of LLM memorization. Hubble models come in standard\nand perturbed variants: standard models are pretrained on a large English\ncorpus, and perturbed models are trained in the same way but with controlled\ninsertion of text (e.g., book passages, biographies, and test sets) designed to\nemulate key memorization risks. Our core release includes 8 models -- standard\nand perturbed models with 1B or 8B parameters, pretrained on 100B or 500B\ntokens -- establishing that memorization risks are determined by the frequency\nof sensitive data relative to size of the training corpus (i.e., a password\nappearing once in a smaller corpus is memorized better than the same password\nin a larger corpus). Our release also includes 6 perturbed models with text\ninserted at different pretraining phases, showing that sensitive data without\ncontinued exposure can be forgotten. These findings suggest two best practices\nfor addressing memorization risks: to dilute sensitive data by increasing the\nsize of the training corpus, and to order sensitive data to appear earlier in\ntraining. Beyond these general empirical findings, Hubble enables a broad range\nof memorization research; for example, analyzing the biographies reveals how\nreadily different types of private information are memorized. We also\ndemonstrate that the randomized insertions in Hubble make it an ideal testbed\nfor membership inference and machine unlearning, and invite the community to\nfurther explore, benchmark, and build upon our work."
                },
                "authors": [
                    {
                        "name": "Johnny Tian-Zheng Wei"
                    },
                    {
                        "name": "Ameya Godbole"
                    },
                    {
                        "name": "Mohammad Aflah Khan"
                    },
                    {
                        "name": "Ryan Wang"
                    },
                    {
                        "name": "Xiaoyuan Zhu"
                    },
                    {
                        "name": "James Flemings"
                    },
                    {
                        "name": "Nitya Kashyap"
                    },
                    {
                        "name": "Krishna P. Gummadi"
                    },
                    {
                        "name": "Willie Neiswanger"
                    },
                    {
                        "name": "Robin Jia"
                    }
                ],
                "author_detail": {
                    "name": "Robin Jia"
                },
                "author": "Robin Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19811v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19807v1",
                "updated": "2025-10-22T17:41:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    41,
                    30,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T17:41:30Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    41,
                    30,
                    2,
                    295,
                    0
                ],
                "title": "Scaf-GRPO: Scaffolded Group Relative Policy Optimization for Enhancing\n  LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaf-GRPO: Scaffolded Group Relative Policy Optimization for Enhancing\n  LLM Reasoning"
                },
                "summary": "Reinforcement learning from verifiable rewards has emerged as a powerful\ntechnique for enhancing the complex reasoning abilities of Large Language\nModels (LLMs). However, these methods are fundamentally constrained by the\n''learning cliff'' phenomenon: when faced with problems far beyond their\ncurrent capabilities, models consistently fail, yielding a persistent\nzero-reward signal. In policy optimization algorithms like GRPO, this collapses\nthe advantage calculation to zero, rendering these difficult problems invisible\nto the learning gradient and stalling progress. To overcome this, we introduce\nScaf-GRPO (Scaffolded Group Relative Policy Optimization), a progressive\ntraining framework that strategically provides minimal guidance only when a\nmodel's independent learning has plateaued. The framework first diagnoses\nlearning stagnation and then intervenes by injecting tiered in-prompt hints,\nranging from abstract concepts to concrete steps, enabling the model to\nconstruct a valid solution by itself. Extensive experiments on challenging\nmathematics benchmarks demonstrate Scaf-GRPO's effectiveness, boosting the\npass@1 score of the Qwen2.5-Math-7B model on the AIME24 benchmark by a relative\n44.3% over a vanilla GRPO baseline. This result demonstrates our framework\nprovides a robust and effective methodology for unlocking a model's ability to\nsolve problems previously beyond its reach, a critical step towards extending\nthe frontier of autonomous reasoning in LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning from verifiable rewards has emerged as a powerful\ntechnique for enhancing the complex reasoning abilities of Large Language\nModels (LLMs). However, these methods are fundamentally constrained by the\n''learning cliff'' phenomenon: when faced with problems far beyond their\ncurrent capabilities, models consistently fail, yielding a persistent\nzero-reward signal. In policy optimization algorithms like GRPO, this collapses\nthe advantage calculation to zero, rendering these difficult problems invisible\nto the learning gradient and stalling progress. To overcome this, we introduce\nScaf-GRPO (Scaffolded Group Relative Policy Optimization), a progressive\ntraining framework that strategically provides minimal guidance only when a\nmodel's independent learning has plateaued. The framework first diagnoses\nlearning stagnation and then intervenes by injecting tiered in-prompt hints,\nranging from abstract concepts to concrete steps, enabling the model to\nconstruct a valid solution by itself. Extensive experiments on challenging\nmathematics benchmarks demonstrate Scaf-GRPO's effectiveness, boosting the\npass@1 score of the Qwen2.5-Math-7B model on the AIME24 benchmark by a relative\n44.3% over a vanilla GRPO baseline. This result demonstrates our framework\nprovides a robust and effective methodology for unlocking a model's ability to\nsolve problems previously beyond its reach, a critical step towards extending\nthe frontier of autonomous reasoning in LLM."
                },
                "authors": [
                    {
                        "name": "Xichen Zhang"
                    },
                    {
                        "name": "Sitong Wu"
                    },
                    {
                        "name": "Yinghao Zhu"
                    },
                    {
                        "name": "Haoru Tan"
                    },
                    {
                        "name": "Shaozuo Yu"
                    },
                    {
                        "name": "Ziyi He"
                    },
                    {
                        "name": "Jiaya Jia"
                    }
                ],
                "author_detail": {
                    "name": "Jiaya Jia"
                },
                "author": "Jiaya Jia",
                "arxiv_comment": "Code: https://github.com/dvlab-research/Scaf-GRPO",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19806v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19806v1",
                "updated": "2025-10-22T17:41:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    41,
                    20,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T17:41:20Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    41,
                    20,
                    2,
                    295,
                    0
                ],
                "title": "The Art of Asking: Multilingual Prompt Optimization for Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Art of Asking: Multilingual Prompt Optimization for Synthetic Data"
                },
                "summary": "Synthetic data has become a cornerstone for scaling large language models,\nyet its multilingual use remains bottlenecked by translation-based prompts.\nThis strategy inherits English-centric framing and style and neglects cultural\ndimensions, ultimately constraining model generalization. We argue that the\noverlooked prompt space-the very inputs that define training\ndistributions-offers a more powerful lever for improving multilingual\nperformance. We introduce a lightweight framework for prompt-space\noptimization, where translated prompts are systematically transformed for\nNaturalness, Cultural Adaptation, and Difficulty Enhancement. Using an\noff-the-shelf multilingual LLM, we apply these transformations to prompts for\n12 languages spanning 7 families. Under identical data conditions, our\napproaches achieve substantial and consistent downstream improvements over the\ntranslation-only baseline: +4.7% on Global-MMLU accuracy, +2.4% on Flores\nXCometXL and +35.3% wins in preferences on mArenaHard. We establish\nprompt-space optimization as a simple yet powerful paradigm for building\nmultilingual LLMs that are more robust, culturally grounded, and globally\ncapable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic data has become a cornerstone for scaling large language models,\nyet its multilingual use remains bottlenecked by translation-based prompts.\nThis strategy inherits English-centric framing and style and neglects cultural\ndimensions, ultimately constraining model generalization. We argue that the\noverlooked prompt space-the very inputs that define training\ndistributions-offers a more powerful lever for improving multilingual\nperformance. We introduce a lightweight framework for prompt-space\noptimization, where translated prompts are systematically transformed for\nNaturalness, Cultural Adaptation, and Difficulty Enhancement. Using an\noff-the-shelf multilingual LLM, we apply these transformations to prompts for\n12 languages spanning 7 families. Under identical data conditions, our\napproaches achieve substantial and consistent downstream improvements over the\ntranslation-only baseline: +4.7% on Global-MMLU accuracy, +2.4% on Flores\nXCometXL and +35.3% wins in preferences on mArenaHard. We establish\nprompt-space optimization as a simple yet powerful paradigm for building\nmultilingual LLMs that are more robust, culturally grounded, and globally\ncapable."
                },
                "authors": [
                    {
                        "name": "David Mora"
                    },
                    {
                        "name": "Viraat Aryabumi"
                    },
                    {
                        "name": "Wei-Yin Ko"
                    },
                    {
                        "name": "Sara Hooker"
                    },
                    {
                        "name": "Julia Kreutzer"
                    },
                    {
                        "name": "Marzieh Fadaee"
                    }
                ],
                "author_detail": {
                    "name": "Marzieh Fadaee"
                },
                "author": "Marzieh Fadaee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19806v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19806v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19805v1",
                "updated": "2025-10-22T17:40:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    40,
                    17,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T17:40:17Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    40,
                    17,
                    2,
                    295,
                    0
                ],
                "title": "Next Generation Cloud-native In-Memory Stores: From Redis to Valkey and\n  Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next Generation Cloud-native In-Memory Stores: From Redis to Valkey and\n  Beyond"
                },
                "summary": "In-memory key-value datastores have become indispensable building blocks of\nmodern cloud-native infrastructures, yet their evolution faces scalability,\ncompatibility, and sustainability constraints. The current literature lacks an\nexperimental evaluation of state-of-the-art tools in the domain. This study\naddressed this timely gap by benchmarking Redis alternatives and systematically\nevaluating Valkey, KeyDB, and Garnet under realistic workloads within\nKubernetes deployments. The results demonstrate clear trade-offs among the\nbenchmarked data systems. Our study presents a comprehensive performance and\nviability assessment of the emerging in-memory key-value stores. Metrics\ninclude throughput, tail latency, CPU and memory efficiency, and migration\ncomplexity. We highlight trade-offs between performance, compatibility, and\nlong-term viability, including project maturity, community support, and\nsustained development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-memory key-value datastores have become indispensable building blocks of\nmodern cloud-native infrastructures, yet their evolution faces scalability,\ncompatibility, and sustainability constraints. The current literature lacks an\nexperimental evaluation of state-of-the-art tools in the domain. This study\naddressed this timely gap by benchmarking Redis alternatives and systematically\nevaluating Valkey, KeyDB, and Garnet under realistic workloads within\nKubernetes deployments. The results demonstrate clear trade-offs among the\nbenchmarked data systems. Our study presents a comprehensive performance and\nviability assessment of the emerging in-memory key-value stores. Metrics\ninclude throughput, tail latency, CPU and memory efficiency, and migration\ncomplexity. We highlight trade-offs between performance, compatibility, and\nlong-term viability, including project maturity, community support, and\nsustained development."
                },
                "authors": [
                    {
                        "name": "Carl-Johan Fauvelle Munck af Rosensch\"old"
                    },
                    {
                        "name": "Feras M. Awaysheh"
                    },
                    {
                        "name": "Ahmad Awad"
                    }
                ],
                "author_detail": {
                    "name": "Ahmad Awad"
                },
                "author": "Ahmad Awad",
                "arxiv_comment": "10 pages, 5 figures, 2 algorithms, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19802v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19802v1",
                "updated": "2025-10-22T17:38:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    38,
                    35,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T17:38:35Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    38,
                    35,
                    2,
                    295,
                    0
                ],
                "title": "Class-Aware Prototype Learning with Negative Contrast for Test-Time\n  Adaptation of Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Class-Aware Prototype Learning with Negative Contrast for Test-Time\n  Adaptation of Vision-Language Models"
                },
                "summary": "Vision-Language Models (VLMs) demonstrate impressive zero-shot generalization\nthrough large-scale image-text pretraining, yet their performance can drop once\nthe deployment distribution diverges from the training distribution. To address\nthis, Test-Time Adaptation (TTA) methods update models using unlabeled target\ndata. However, existing approaches often ignore two key challenges: prototype\ndegradation in long-tailed distributions and confusion between semantically\nsimilar classes. To tackle these issues, we propose \\textbf{C}lass-Aware\n\\textbf{P}rototype \\textbf{L}earning with \\textbf{N}egative\n\\textbf{C}ontrast(\\textbf{CPL-NC}), a lightweight TTA framework designed\nspecifically for VLMs to enhance generalization under distribution shifts.\nCPL-NC introduces a \\textit{Class-Aware Prototype Cache} Module that\ndynamically adjusts per-class capacity based on test-time frequency and\nactivation history, with a rejuvenation mechanism for inactive classes to\nretain rare-category knowledge. Additionally, a \\textit{Negative Contrastive\nLearning} Mechanism identifies and constrains hard visual-textual negatives to\nimprove class separability. The framework employs asymmetric optimization,\nrefining only textual prototypes while anchoring on stable visual features.\nExperiments on 15 benchmarks show that CPL-NC consistently outperforms prior\nTTA methods across both ResNet-50 and ViT-B/16 backbones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) demonstrate impressive zero-shot generalization\nthrough large-scale image-text pretraining, yet their performance can drop once\nthe deployment distribution diverges from the training distribution. To address\nthis, Test-Time Adaptation (TTA) methods update models using unlabeled target\ndata. However, existing approaches often ignore two key challenges: prototype\ndegradation in long-tailed distributions and confusion between semantically\nsimilar classes. To tackle these issues, we propose \\textbf{C}lass-Aware\n\\textbf{P}rototype \\textbf{L}earning with \\textbf{N}egative\n\\textbf{C}ontrast(\\textbf{CPL-NC}), a lightweight TTA framework designed\nspecifically for VLMs to enhance generalization under distribution shifts.\nCPL-NC introduces a \\textit{Class-Aware Prototype Cache} Module that\ndynamically adjusts per-class capacity based on test-time frequency and\nactivation history, with a rejuvenation mechanism for inactive classes to\nretain rare-category knowledge. Additionally, a \\textit{Negative Contrastive\nLearning} Mechanism identifies and constrains hard visual-textual negatives to\nimprove class separability. The framework employs asymmetric optimization,\nrefining only textual prototypes while anchoring on stable visual features.\nExperiments on 15 benchmarks show that CPL-NC consistently outperforms prior\nTTA methods across both ResNet-50 and ViT-B/16 backbones."
                },
                "authors": [
                    {
                        "name": "Xiaozhen Qiao"
                    },
                    {
                        "name": "Jingkai Zhao"
                    },
                    {
                        "name": "Yuqiu Jiang"
                    },
                    {
                        "name": "Xianda Guo"
                    },
                    {
                        "name": "Zhe Sun"
                    },
                    {
                        "name": "Hongyuan Zhang"
                    },
                    {
                        "name": "Xuelong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuelong Li"
                },
                "author": "Xuelong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19802v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19802v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19801v1",
                "updated": "2025-10-22T17:37:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    37,
                    46,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T17:37:46Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    37,
                    46,
                    2,
                    295,
                    0
                ],
                "title": "The Feasibility of Training Sovereign Language Models in the Global\n  South: A Study of Brazil and Mexico",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Feasibility of Training Sovereign Language Models in the Global\n  South: A Study of Brazil and Mexico"
                },
                "summary": "The rapid escalation of computational requirements for training large-scale\nlanguage models has reinforced structural asymmetries between high-capacity\njurisdictions and countries in the Global South. This paper examines the\ntechnical and fiscal feasibility of sovereign-scale language model training in\nBrazil and Mexico under conditions of constrained hardware access, energy\navailability, and fiscal ceilings. Using a dual-axis design that varies\naccelerator generation (NVIDIA H100 vs. A100) and training duration (90 vs. 150\ndays), we estimate compute demand, energy consumption, capital expenditures,\nand regulatory compatibility for the training of a 10-trillion-token model. Our\nfindings show that while all configurations remain below export-control and\nelectrical infrastructure thresholds, fiscal viability is determined by\nhardware efficiency. H100-based scenarios achieve training feasibility at a\ntotal cost of 8-14 million USD, while A100 deployments require 19-32 million\nUSD due to higher energy and hardware demand. We argue that extending training\ntimelines should be treated as a policy lever to mitigate hardware constraints,\nenabling the production of usable, auditable, and locally aligned models\nwithout competing at the global frontier. This study contributes to the\ndiscourse on AI compute governance and technological sovereignty by\nhighlighting context-sensitive strategies that allow middle-income countries to\nestablish sustainable and strategically sufficient AI capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid escalation of computational requirements for training large-scale\nlanguage models has reinforced structural asymmetries between high-capacity\njurisdictions and countries in the Global South. This paper examines the\ntechnical and fiscal feasibility of sovereign-scale language model training in\nBrazil and Mexico under conditions of constrained hardware access, energy\navailability, and fiscal ceilings. Using a dual-axis design that varies\naccelerator generation (NVIDIA H100 vs. A100) and training duration (90 vs. 150\ndays), we estimate compute demand, energy consumption, capital expenditures,\nand regulatory compatibility for the training of a 10-trillion-token model. Our\nfindings show that while all configurations remain below export-control and\nelectrical infrastructure thresholds, fiscal viability is determined by\nhardware efficiency. H100-based scenarios achieve training feasibility at a\ntotal cost of 8-14 million USD, while A100 deployments require 19-32 million\nUSD due to higher energy and hardware demand. We argue that extending training\ntimelines should be treated as a policy lever to mitigate hardware constraints,\nenabling the production of usable, auditable, and locally aligned models\nwithout competing at the global frontier. This study contributes to the\ndiscourse on AI compute governance and technological sovereignty by\nhighlighting context-sensitive strategies that allow middle-income countries to\nestablish sustainable and strategically sufficient AI capabilities."
                },
                "authors": [
                    {
                        "name": "Sandra Malagon"
                    },
                    {
                        "name": "Monica A. Ulloa Ruiz"
                    },
                    {
                        "name": "Tatiana Elizabeth Sandoval Plaza"
                    },
                    {
                        "name": "Gabriel Rafael Rosario Bolívar"
                    },
                    {
                        "name": "Valentina García Mesa"
                    },
                    {
                        "name": "Ivanna Alvarado Morales"
                    }
                ],
                "author_detail": {
                    "name": "Ivanna Alvarado Morales"
                },
                "arxiv_affiliation": "Carreras con Impacto",
                "author": "Ivanna Alvarado Morales",
                "arxiv_comment": "11 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.4.1; K.4.2; I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19799v1",
                "updated": "2025-10-22T17:35:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    35,
                    13,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T17:35:13Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    35,
                    13,
                    2,
                    295,
                    0
                ],
                "title": "Integrating Transparent Models, LLMs, and Practitioner-in-the-Loop: A\n  Case of Nonprofit Program Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Transparent Models, LLMs, and Practitioner-in-the-Loop: A\n  Case of Nonprofit Program Evaluation"
                },
                "summary": "Public and nonprofit organizations often hesitate to adopt AI tools because\nmost models are opaque even though standard approaches typically analyze\naggregate patterns rather than offering actionable, case-level guidance. This\nstudy tests a practitioner-in-the-loop workflow that pairs transparent\ndecision-tree models with large language models (LLMs) to improve predictive\naccuracy, interpretability, and the generation of practical insights. Using\ndata from an ongoing college-success program, we build interpretable decision\ntrees to surface key predictors. We then provide each tree's structure to an\nLLM, enabling it to reproduce case-level predictions grounded in the\ntransparent models. Practitioners participate throughout feature engineering,\nmodel design, explanation review, and usability assessment, ensuring that field\nexpertise informs the analysis at every stage. Results show that integrating\ntransparent models, LLMs, and practitioner input yields accurate, trustworthy,\nand actionable case-level evaluations, offering a viable pathway for\nresponsible AI adoption in the public and nonprofit sectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Public and nonprofit organizations often hesitate to adopt AI tools because\nmost models are opaque even though standard approaches typically analyze\naggregate patterns rather than offering actionable, case-level guidance. This\nstudy tests a practitioner-in-the-loop workflow that pairs transparent\ndecision-tree models with large language models (LLMs) to improve predictive\naccuracy, interpretability, and the generation of practical insights. Using\ndata from an ongoing college-success program, we build interpretable decision\ntrees to surface key predictors. We then provide each tree's structure to an\nLLM, enabling it to reproduce case-level predictions grounded in the\ntransparent models. Practitioners participate throughout feature engineering,\nmodel design, explanation review, and usability assessment, ensuring that field\nexpertise informs the analysis at every stage. Results show that integrating\ntransparent models, LLMs, and practitioner input yields accurate, trustworthy,\nand actionable case-level evaluations, offering a viable pathway for\nresponsible AI adoption in the public and nonprofit sectors."
                },
                "authors": [
                    {
                        "name": "Ji Ma"
                    },
                    {
                        "name": "Albert Casella"
                    }
                ],
                "author_detail": {
                    "name": "Albert Casella"
                },
                "author": "Albert Casella",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19791v1",
                "updated": "2025-10-22T17:26:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    26,
                    5,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T17:26:05Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    26,
                    5,
                    2,
                    295,
                    0
                ],
                "title": "ToolDreamer: Instilling LLM Reasoning Into Tool Retrievers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolDreamer: Instilling LLM Reasoning Into Tool Retrievers"
                },
                "summary": "Tool calling has become increasingly popular for Large Language Models\n(LLMs). However, for large tool sets, the resulting tokens would exceed the\nLLM's context window limit, making it impossible to include every tool. Hence,\nan external retriever is used to provide LLMs with the most relevant tools for\na query. Existing retrieval models rank tools based on the similarity between a\nuser query and a tool description (TD). This leads to suboptimal retrieval as\nuser requests are often poorly aligned with the language of TD. To remedy the\nissue, we propose ToolDreamer, a framework to condition retriever models to\nfetch tools based on hypothetical (synthetic) TD generated using an LLM, i.e.,\ndescription of tools that the LLM feels will be potentially useful for the\nquery. The framework enables a more natural alignment between queries and tools\nwithin the language space of TD's. We apply ToolDreamer on the ToolRet dataset\nand show that our method improves the performance of sparse and dense\nretrievers with and without training, thus showcasing its flexibility. Through\nour proposed framework, our aim is to offload a portion of the reasoning burden\nto the retriever so that the LLM may effectively handle a large collection of\ntools without inundating its context window.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool calling has become increasingly popular for Large Language Models\n(LLMs). However, for large tool sets, the resulting tokens would exceed the\nLLM's context window limit, making it impossible to include every tool. Hence,\nan external retriever is used to provide LLMs with the most relevant tools for\na query. Existing retrieval models rank tools based on the similarity between a\nuser query and a tool description (TD). This leads to suboptimal retrieval as\nuser requests are often poorly aligned with the language of TD. To remedy the\nissue, we propose ToolDreamer, a framework to condition retriever models to\nfetch tools based on hypothetical (synthetic) TD generated using an LLM, i.e.,\ndescription of tools that the LLM feels will be potentially useful for the\nquery. The framework enables a more natural alignment between queries and tools\nwithin the language space of TD's. We apply ToolDreamer on the ToolRet dataset\nand show that our method improves the performance of sparse and dense\nretrievers with and without training, thus showcasing its flexibility. Through\nour proposed framework, our aim is to offload a portion of the reasoning burden\nto the retriever so that the LLM may effectively handle a large collection of\ntools without inundating its context window."
                },
                "authors": [
                    {
                        "name": "Saptarshi Sengupta"
                    },
                    {
                        "name": "Zhengyu Zhou"
                    },
                    {
                        "name": "Jun Araki"
                    },
                    {
                        "name": "Xingbo Wang"
                    },
                    {
                        "name": "Bingqing Wang"
                    },
                    {
                        "name": "Suhang Wang"
                    },
                    {
                        "name": "Zhe Feng"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Feng"
                },
                "author": "Zhe Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19782v1",
                "updated": "2025-10-22T17:16:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    16,
                    23,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T17:16:23Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    16,
                    23,
                    2,
                    295,
                    0
                ],
                "title": "Adapting Multilingual Models to Code-Mixed Tasks via Model Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting Multilingual Models to Code-Mixed Tasks via Model Merging"
                },
                "summary": "We study model merging as a practical alternative to conventional adaptation\nstrategies for code-mixed NLP. Starting from a multilingual base model, we: (i)\nperform continued pre-training (CPT) on unlabeled code-mixed text to obtain an\nadapted checkpoint, (ii) merge checkpoint with the base model, and (iii)\nfine-tune (FT) on the downstream task data. We evaluate our approach for\nsentence classification (sentiment and hate speech) task in English-Hindi\n(En-Hi) and English-Spanish (En-Es) using XLM-R and Llama-3.2-1B models. Our\nresults show that merged models consistently outperform full fine-tuning and\nCPT->FT. We observe gains of 2--5 points in F1 over full fine-tuning and ~1-2\npoints over CPT->FT, indicating that unlabeled data is leveraged more\neffectively via merging than via CPT alone. Zero-/few-shot prompting with\nlarger LLMs (e.g., Llama-3.3-70B) lags behind fine-tuned and merged\ncheckpoints, underscoring limits of in-context learning for code-mixed inputs.\nWe further test cross-pair transfer by training on En-Hi and evaluating on\nEn-Ta and En-Ml: merged checkpoints transfer more strongly than\nmonolingual-English baselines (e.g., TV/TIES variants reaching 0.65-0.68 F1 vs\n0.61-0.63 for full fine-tuning), suggesting that code-mixed knowledge is a more\nreliable substrate for low-resource pairs. We conclude with adaptation recipes\nmatched to common data regimes (labeled only; labeled+unlabeled; transfer-only)\nand discuss limitations and scaling considerations for broader tasks and larger\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study model merging as a practical alternative to conventional adaptation\nstrategies for code-mixed NLP. Starting from a multilingual base model, we: (i)\nperform continued pre-training (CPT) on unlabeled code-mixed text to obtain an\nadapted checkpoint, (ii) merge checkpoint with the base model, and (iii)\nfine-tune (FT) on the downstream task data. We evaluate our approach for\nsentence classification (sentiment and hate speech) task in English-Hindi\n(En-Hi) and English-Spanish (En-Es) using XLM-R and Llama-3.2-1B models. Our\nresults show that merged models consistently outperform full fine-tuning and\nCPT->FT. We observe gains of 2--5 points in F1 over full fine-tuning and ~1-2\npoints over CPT->FT, indicating that unlabeled data is leveraged more\neffectively via merging than via CPT alone. Zero-/few-shot prompting with\nlarger LLMs (e.g., Llama-3.3-70B) lags behind fine-tuned and merged\ncheckpoints, underscoring limits of in-context learning for code-mixed inputs.\nWe further test cross-pair transfer by training on En-Hi and evaluating on\nEn-Ta and En-Ml: merged checkpoints transfer more strongly than\nmonolingual-English baselines (e.g., TV/TIES variants reaching 0.65-0.68 F1 vs\n0.61-0.63 for full fine-tuning), suggesting that code-mixed knowledge is a more\nreliable substrate for low-resource pairs. We conclude with adaptation recipes\nmatched to common data regimes (labeled only; labeled+unlabeled; transfer-only)\nand discuss limitations and scaling considerations for broader tasks and larger\nmodels."
                },
                "authors": [
                    {
                        "name": "Prashant Kodali"
                    },
                    {
                        "name": "Vaishnavi Shivkumar"
                    },
                    {
                        "name": "Swarang Joshi"
                    },
                    {
                        "name": "Monojit Choudhary"
                    },
                    {
                        "name": "Ponnurangam Kumaraguru"
                    },
                    {
                        "name": "Manish Shrivastava"
                    }
                ],
                "author_detail": {
                    "name": "Manish Shrivastava"
                },
                "author": "Manish Shrivastava",
                "arxiv_comment": "9 pages, 5 tables, CODS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08800v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08800v2",
                "updated": "2025-10-22T17:14:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    14,
                    8,
                    2,
                    295,
                    0
                ],
                "published": "2025-06-10T13:47:22Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    13,
                    47,
                    22,
                    1,
                    161,
                    0
                ],
                "title": "Measuring Data Science Automation: A Survey of Evaluation Tools for AI\n  Assistants and Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Data Science Automation: A Survey of Evaluation Tools for AI\n  Assistants and Agents"
                },
                "summary": "Data science aims to extract insights from data to support decision-making\nprocesses. Recently, Large Language Models (LLMs) have been increasingly used\nas assistants for data science, by suggesting ideas, techniques and small code\nsnippets, or for the interpretation of results and reporting. Proper automation\nof some data-science activities is now promised by the rise of LLM agents,\ni.e., AI systems powered by an LLM equipped with additional affordances--such\nas code execution and knowledge bases--that can perform self-directed actions\nand interact with digital environments. In this paper, we survey the evaluation\nof LLM assistants and agents for data science. We find (1) a dominant focus on\na small subset of goal-oriented activities, largely ignoring data management\nand exploratory activities; (2) a concentration on pure assistance or fully\nautonomous agents, without considering intermediate levels of human-AI\ncollaboration; and (3) an emphasis on human substitution, therefore neglecting\nthe possibility of higher levels of automation thanks to task transformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data science aims to extract insights from data to support decision-making\nprocesses. Recently, Large Language Models (LLMs) have been increasingly used\nas assistants for data science, by suggesting ideas, techniques and small code\nsnippets, or for the interpretation of results and reporting. Proper automation\nof some data-science activities is now promised by the rise of LLM agents,\ni.e., AI systems powered by an LLM equipped with additional affordances--such\nas code execution and knowledge bases--that can perform self-directed actions\nand interact with digital environments. In this paper, we survey the evaluation\nof LLM assistants and agents for data science. We find (1) a dominant focus on\na small subset of goal-oriented activities, largely ignoring data management\nand exploratory activities; (2) a concentration on pure assistance or fully\nautonomous agents, without considering intermediate levels of human-AI\ncollaboration; and (3) an emphasis on human substitution, therefore neglecting\nthe possibility of higher levels of automation thanks to task transformation."
                },
                "authors": [
                    {
                        "name": "Irene Testini"
                    },
                    {
                        "name": "José Hernández-Orallo"
                    },
                    {
                        "name": "Lorenzo Pacchiardi"
                    }
                ],
                "author_detail": {
                    "name": "Lorenzo Pacchiardi"
                },
                "author": "Lorenzo Pacchiardi",
                "arxiv_comment": "Published in Transactions of Machine Learning Research (TMLR),\n  10/2025 https://openreview.net/forum?id=MB0TCLfLn1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08800v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08800v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19778v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19778v1",
                "updated": "2025-10-22T17:11:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    11,
                    49,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T17:11:49Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    11,
                    49,
                    2,
                    295,
                    0
                ],
                "title": "GaLLoP: Gradient-based Sparse Learning on Low-Magnitude Parameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GaLLoP: Gradient-based Sparse Learning on Low-Magnitude Parameters"
                },
                "summary": "Sparse fine-tuning techniques adapt LLMs to downstream tasks by only tuning a\nsparse subset of model parameters. However, the effectiveness of sparse\nadaptation depends on optimally selecting the model parameters to be\nfine-tuned. In this work, we introduce a novel sparse fine-tuning technique\nnamed GaLLoP: Gradient-based Sparse Learning on Low-Magnitude Parameters, which\nfine-tunes only those model parameters which have the largest gradient\nmagnitudes on downstream tasks and the smallest pre-trained magnitudes,\nintuitively prioritizing parameters that are highly task-relevant, but\nminimally disruptive to pre-trained knowledge. Our experimentation with LLaMA3\n8B and Gemma 2B as base models shows that GaLLoP consistently improves or\nmatches the in-distribution as well as out-of-distribution performance obtained\nvia the usage of other leading parameter-efficient fine-tuning techniques,\nincluding LoRA, DoRA, and SAFT. Our analysis demonstrates that GaLLoP mitigates\ncatastrophic forgetting and memorization of task data, as important pre-trained\nparameters remain unchanged, and stabilizes performance relative to other\nfine-tuning techniques, robustly generalizing across most random seeds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse fine-tuning techniques adapt LLMs to downstream tasks by only tuning a\nsparse subset of model parameters. However, the effectiveness of sparse\nadaptation depends on optimally selecting the model parameters to be\nfine-tuned. In this work, we introduce a novel sparse fine-tuning technique\nnamed GaLLoP: Gradient-based Sparse Learning on Low-Magnitude Parameters, which\nfine-tunes only those model parameters which have the largest gradient\nmagnitudes on downstream tasks and the smallest pre-trained magnitudes,\nintuitively prioritizing parameters that are highly task-relevant, but\nminimally disruptive to pre-trained knowledge. Our experimentation with LLaMA3\n8B and Gemma 2B as base models shows that GaLLoP consistently improves or\nmatches the in-distribution as well as out-of-distribution performance obtained\nvia the usage of other leading parameter-efficient fine-tuning techniques,\nincluding LoRA, DoRA, and SAFT. Our analysis demonstrates that GaLLoP mitigates\ncatastrophic forgetting and memorization of task data, as important pre-trained\nparameters remain unchanged, and stabilizes performance relative to other\nfine-tuning techniques, robustly generalizing across most random seeds."
                },
                "authors": [
                    {
                        "name": "Anand Choudhary"
                    },
                    {
                        "name": "Yasser Sulaıman"
                    },
                    {
                        "name": "Lukas Mauch"
                    },
                    {
                        "name": "Ghouthi Boukli Hacene"
                    },
                    {
                        "name": "Fabien Cardinaux"
                    },
                    {
                        "name": "Antoine Bosselut"
                    }
                ],
                "author_detail": {
                    "name": "Antoine Bosselut"
                },
                "author": "Antoine Bosselut",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19778v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19778v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19777v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19777v1",
                "updated": "2025-10-22T17:11:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    11,
                    30,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T17:11:30Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    11,
                    30,
                    2,
                    295,
                    0
                ],
                "title": "BOSQTGEN: Breaking the Sound Barrier in Test Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BOSQTGEN: Breaking the Sound Barrier in Test Generation"
                },
                "summary": "Modern software is increasingly built by composing APIs, elevating the API\ncontract to a critical role. Inadequate contracts, however, lead to mismatched\nexpectations and failures, creating a pressing need for robust conformance\ntesting. Current test generation techniques are hindered by key challenges:\npolyglot systems, source code inaccessibility, a cost-reliability trade-off,\nand, most critically, the difficulty of generating structured inputs.\n  We introduce BOSQTGEN, a novel black-box methodology and tool for API test\ngeneration. BOSQTGEN utilizes a novel approach for decomposing API\nspecifications into primitives, using LLMs to suggest coherent strata for them,\nand employing combinatorial testing to efficiently sample over these values.\nThis approach ensures coverage of critical interactions while avoiding the\nredundancy of random sampling.\n  The resulting BOSQTGEN system achieves an average of 82% code coverage on\nRESTful benchmarks, often a 20% or more increase over prior state-of-the-art\nsystems and nearing parity with hand-written test suites. Providing a fully\nAPI-driven approach to test generation, enables developers to automatically\ncreate high-quality test cases for validation or test-driven development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software is increasingly built by composing APIs, elevating the API\ncontract to a critical role. Inadequate contracts, however, lead to mismatched\nexpectations and failures, creating a pressing need for robust conformance\ntesting. Current test generation techniques are hindered by key challenges:\npolyglot systems, source code inaccessibility, a cost-reliability trade-off,\nand, most critically, the difficulty of generating structured inputs.\n  We introduce BOSQTGEN, a novel black-box methodology and tool for API test\ngeneration. BOSQTGEN utilizes a novel approach for decomposing API\nspecifications into primitives, using LLMs to suggest coherent strata for them,\nand employing combinatorial testing to efficiently sample over these values.\nThis approach ensures coverage of critical interactions while avoiding the\nredundancy of random sampling.\n  The resulting BOSQTGEN system achieves an average of 82% code coverage on\nRESTful benchmarks, often a 20% or more increase over prior state-of-the-art\nsystems and nearing parity with hand-written test suites. Providing a fully\nAPI-driven approach to test generation, enables developers to automatically\ncreate high-quality test cases for validation or test-driven development."
                },
                "authors": [
                    {
                        "name": "S M Sadrul Islam Asif"
                    },
                    {
                        "name": "James Chen"
                    },
                    {
                        "name": "Earl T. Barr"
                    },
                    {
                        "name": "Mark Marron"
                    }
                ],
                "author_detail": {
                    "name": "Mark Marron"
                },
                "author": "Mark Marron",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19777v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19777v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19776v1",
                "updated": "2025-10-22T17:11:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    11,
                    9,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T17:11:09Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    11,
                    9,
                    2,
                    295,
                    0
                ],
                "title": "Exoplanetary radio emission predictions and detectability in the SKA era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exoplanetary radio emission predictions and detectability in the SKA era"
                },
                "summary": "Radio observations provide a window into a planet's interior and play a\ncrucial role in studying its atmosphere and surface, key factors to find\npotential habitability. The discovery of thousands of exoplanets, together with\nadvances in radio astronomy through the Square Kilometre Array (SKA), motivates\nthe search for planetary-scale radio emissions. Here, we employ the radiometric\nBode's law (RBL) and machine learning techniques to analyze a dataset of 1330\nconfirmed exoplanets, aiming to estimate their potential radio emission.\nPermutation Importance (PI) and SHapley Additive exPlanations (SHAP) analyses\nindicate that a planet's mass, radius, orbital semi-major axis, and distance\nfrom Earth are sufficient to dependably forecast its radio flux and frequency.\nThe random forest model accurately reproduces these radio characteristics,\nconfirming its reliability for exoplanetary radio predictions. Considering\nobservational constraints, we find that 64 exoplanets could generate signals\ndetectable by the SKA, 52 of which remain observable in the intermediate AA*\ndeployment. Among these, MASCARA-1 b stands out with a predicted flux of 7.209\nmJy at 135.1 MHz, making it an excellent SKA-Low target. Meanwhile, WASP-18 b,\nwith a flux of 18.638 mJy peaking at 812.9 MHz, is the most promising candidate\nfor SKA-Mid. These results show that the SKA can detect gas giants, such as\nMASCARA-1 b (SNR>400) and WASP-18 b (SNR>4236), within feasible integration\ntimes. Additionally, we identify four candidates (HATS-18 b, WASP-12 b,\nWASP-103 b, and WASP-121 b) that are likely affected by radio quenching,\nhighlighting the importance of considering this effect in target selection for\nobservation campaigns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radio observations provide a window into a planet's interior and play a\ncrucial role in studying its atmosphere and surface, key factors to find\npotential habitability. The discovery of thousands of exoplanets, together with\nadvances in radio astronomy through the Square Kilometre Array (SKA), motivates\nthe search for planetary-scale radio emissions. Here, we employ the radiometric\nBode's law (RBL) and machine learning techniques to analyze a dataset of 1330\nconfirmed exoplanets, aiming to estimate their potential radio emission.\nPermutation Importance (PI) and SHapley Additive exPlanations (SHAP) analyses\nindicate that a planet's mass, radius, orbital semi-major axis, and distance\nfrom Earth are sufficient to dependably forecast its radio flux and frequency.\nThe random forest model accurately reproduces these radio characteristics,\nconfirming its reliability for exoplanetary radio predictions. Considering\nobservational constraints, we find that 64 exoplanets could generate signals\ndetectable by the SKA, 52 of which remain observable in the intermediate AA*\ndeployment. Among these, MASCARA-1 b stands out with a predicted flux of 7.209\nmJy at 135.1 MHz, making it an excellent SKA-Low target. Meanwhile, WASP-18 b,\nwith a flux of 18.638 mJy peaking at 812.9 MHz, is the most promising candidate\nfor SKA-Mid. These results show that the SKA can detect gas giants, such as\nMASCARA-1 b (SNR>400) and WASP-18 b (SNR>4236), within feasible integration\ntimes. Additionally, we identify four candidates (HATS-18 b, WASP-12 b,\nWASP-103 b, and WASP-121 b) that are likely affected by radio quenching,\nhighlighting the importance of considering this effect in target selection for\nobservation campaigns."
                },
                "authors": [
                    {
                        "name": "Mahdiyar Mousavi-Sadr"
                    },
                    {
                        "name": "Fatemeh S. Tabatabaei"
                    },
                    {
                        "name": "Alexander Wolszczan"
                    },
                    {
                        "name": "Ghassem Gozaliasl"
                    }
                ],
                "author_detail": {
                    "name": "Ghassem Gozaliasl"
                },
                "author": "Ghassem Gozaliasl",
                "arxiv_comment": "14 pages, 8 figures. Submitted to MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19771v1",
                "updated": "2025-10-22T17:00:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    0,
                    45,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T17:00:45Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    17,
                    0,
                    45,
                    2,
                    295,
                    0
                ],
                "title": "Beyond Reactivity: Measuring Proactive Problem Solving in LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Reactivity: Measuring Proactive Problem Solving in LLM Agents"
                },
                "summary": "LLM-based agents are increasingly moving towards proactivity: rather than\nawaiting instruction, they exercise agency to anticipate user needs and solve\nthem autonomously. However, evaluating proactivity is challenging; current\nbenchmarks are constrained to localized context, limiting their ability to test\nreasoning across sources and longer time horizons. To address this gap, we\npresent PROBE (Proactive Resolution Of BottlEnecks). PROBE decomposes\nproactivity as a pipeline of three core capabilities: (1) searching for\nunspecified issues, (2) identifying specific bottlenecks, and (3) executing\nappropriate resolutions. We apply PROBE to evaluate leading LLMs and popular\nagentic frameworks, showing that even state-of-the-art models struggle to solve\nthis benchmark. Computing our consistent measurements across frontier LLMs and\nagents, we find that the best end-to-end performance of 40% is achieved by both\nGPT-5 and Claude Opus-4.1. Additionally, we demonstrate the relative\ncapabilities of each model and analyze mutual failure modes. Our results\nhighlight the current limitations of autonomous action in agentic systems, and\nexpose promising future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agents are increasingly moving towards proactivity: rather than\nawaiting instruction, they exercise agency to anticipate user needs and solve\nthem autonomously. However, evaluating proactivity is challenging; current\nbenchmarks are constrained to localized context, limiting their ability to test\nreasoning across sources and longer time horizons. To address this gap, we\npresent PROBE (Proactive Resolution Of BottlEnecks). PROBE decomposes\nproactivity as a pipeline of three core capabilities: (1) searching for\nunspecified issues, (2) identifying specific bottlenecks, and (3) executing\nappropriate resolutions. We apply PROBE to evaluate leading LLMs and popular\nagentic frameworks, showing that even state-of-the-art models struggle to solve\nthis benchmark. Computing our consistent measurements across frontier LLMs and\nagents, we find that the best end-to-end performance of 40% is achieved by both\nGPT-5 and Claude Opus-4.1. Additionally, we demonstrate the relative\ncapabilities of each model and analyze mutual failure modes. Our results\nhighlight the current limitations of autonomous action in agentic systems, and\nexpose promising future research directions."
                },
                "authors": [
                    {
                        "name": "Gil Pasternak"
                    },
                    {
                        "name": "Dheeraj Rajagopal"
                    },
                    {
                        "name": "Julia White"
                    },
                    {
                        "name": "Dhruv Atreja"
                    },
                    {
                        "name": "Matthew Thomas"
                    },
                    {
                        "name": "George Hurn-Maloney"
                    },
                    {
                        "name": "Ash Lewis"
                    }
                ],
                "author_detail": {
                    "name": "Ash Lewis"
                },
                "author": "Ash Lewis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19767v1",
                "updated": "2025-10-22T16:56:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    56,
                    1,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T16:56:01Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    56,
                    1,
                    2,
                    295,
                    0
                ],
                "title": "SmartSwitch: Advancing LLM Reasoning by Overcoming Underthinking via\n  Promoting Deeper Thought Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmartSwitch: Advancing LLM Reasoning by Overcoming Underthinking via\n  Promoting Deeper Thought Exploration"
                },
                "summary": "The long chain-of-thought (LongCoT) capability is central to the recent\nbreakthroughs achieved by large language models in complex reasoning tasks.\nHowever, the accompanying issue of ''underthinking'', where models exhibit\nshallow reasoning by frequently switching thoughts without sufficient\nexploration, limits both performance and token efficiency. To address this\nproblem, we propose a simple yet effective reasoning strategy: the SmartSwitch\ninference framework. This framework can be easily integrated into any large\nlanguage model as a plug-and-play solution, continuously monitoring the model's\nreasoning process to detect underthinking and guide it toward deeper\nexploration of promising but overlooked thoughts. Specifically, the perception\nmodule identifies points where thoughts switch and evaluates the potential of\nthe preceding thought using an off-the-shelf process reward model (PRM). If a\nhigh-potential thought is found to be prematurely abandoned, the intervention\nmodule interrupts the ongoing inference, backtracks to the point before the\nswitch, and inserts a \"deepening prompt\" to encourage further exploration along\nthat promising path. Extensive experiments on challenging mathematical\nreasoning benchmarks demonstrate that our method significantly enhances the\nperformance of various large language models of different sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The long chain-of-thought (LongCoT) capability is central to the recent\nbreakthroughs achieved by large language models in complex reasoning tasks.\nHowever, the accompanying issue of ''underthinking'', where models exhibit\nshallow reasoning by frequently switching thoughts without sufficient\nexploration, limits both performance and token efficiency. To address this\nproblem, we propose a simple yet effective reasoning strategy: the SmartSwitch\ninference framework. This framework can be easily integrated into any large\nlanguage model as a plug-and-play solution, continuously monitoring the model's\nreasoning process to detect underthinking and guide it toward deeper\nexploration of promising but overlooked thoughts. Specifically, the perception\nmodule identifies points where thoughts switch and evaluates the potential of\nthe preceding thought using an off-the-shelf process reward model (PRM). If a\nhigh-potential thought is found to be prematurely abandoned, the intervention\nmodule interrupts the ongoing inference, backtracks to the point before the\nswitch, and inserts a \"deepening prompt\" to encourage further exploration along\nthat promising path. Extensive experiments on challenging mathematical\nreasoning benchmarks demonstrate that our method significantly enhances the\nperformance of various large language models of different sizes."
                },
                "authors": [
                    {
                        "name": "Xichen Zhang"
                    },
                    {
                        "name": "Sitong Wu"
                    },
                    {
                        "name": "Haoru Tan"
                    },
                    {
                        "name": "Shaozuo Yu"
                    },
                    {
                        "name": "Yinghao Zhu"
                    },
                    {
                        "name": "Ziyi He"
                    },
                    {
                        "name": "Jiaya Jia"
                    }
                ],
                "author_detail": {
                    "name": "Jiaya Jia"
                },
                "author": "Jiaya Jia",
                "arxiv_comment": "Code: https://github.com/dvlab-research/SmartSwitch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19747v1",
                "updated": "2025-10-22T16:41:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    41,
                    16,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T16:41:16Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    41,
                    16,
                    2,
                    295,
                    0
                ],
                "title": "Review of Tools for Zero-Code LLM Based Application Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Review of Tools for Zero-Code LLM Based Application Development"
                },
                "summary": "Large Language Models (LLMs) are transforming software creation by enabling\nzero code development platforms. Our survey reviews recent platforms that let\nusers build applications without writing code, by leveraging LLMs as the brains\nof the development process. We adopt a broad survey methodology, categorizing\nplatforms based on key dimensions such as interface style, backend integration,\noutput type, and extensibility. We analyze both dedicated LLM based app\nbuilders (OpenAI's custom GPTs, Bolt.new, Dust.tt, Flowise, Cognosys) and\ngeneral no code platforms (e.g., Bubble, Glide) that integrate LLM\ncapabilities. We present a taxonomy categorizing these platforms by their\ninterface (conversational, visual, etc.), supported LLM backends, output type\n(chatbot, full application, workflow), and degree of extensibility. Core\nfeatures such as autonomous agents, memory management, workflow orchestration,\nand API integrations are in scope of the survey. We provide a detailed\ncomparison, highlighting each platform's strengths and limitations. Trade offs\n(customizability, scalability, vendor lock-in) are discussed in comparison with\ntraditional and low code development approaches. Finally, we outline future\ndirections, including multimodal interfaces, on device LLMs, and improved\norchestration for democratizing app creation with AI. Our findings indicate\nthat while zero code LLM platforms greatly reduce the barrier to creating AI\npowered applications, they still face challenges in flexibility and\nreliability. Overall, the landscape is rapidly evolving, offering exciting\nopportunities to empower non programmers to create sophisticated software.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are transforming software creation by enabling\nzero code development platforms. Our survey reviews recent platforms that let\nusers build applications without writing code, by leveraging LLMs as the brains\nof the development process. We adopt a broad survey methodology, categorizing\nplatforms based on key dimensions such as interface style, backend integration,\noutput type, and extensibility. We analyze both dedicated LLM based app\nbuilders (OpenAI's custom GPTs, Bolt.new, Dust.tt, Flowise, Cognosys) and\ngeneral no code platforms (e.g., Bubble, Glide) that integrate LLM\ncapabilities. We present a taxonomy categorizing these platforms by their\ninterface (conversational, visual, etc.), supported LLM backends, output type\n(chatbot, full application, workflow), and degree of extensibility. Core\nfeatures such as autonomous agents, memory management, workflow orchestration,\nand API integrations are in scope of the survey. We provide a detailed\ncomparison, highlighting each platform's strengths and limitations. Trade offs\n(customizability, scalability, vendor lock-in) are discussed in comparison with\ntraditional and low code development approaches. Finally, we outline future\ndirections, including multimodal interfaces, on device LLMs, and improved\norchestration for democratizing app creation with AI. Our findings indicate\nthat while zero code LLM platforms greatly reduce the barrier to creating AI\npowered applications, they still face challenges in flexibility and\nreliability. Overall, the landscape is rapidly evolving, offering exciting\nopportunities to empower non programmers to create sophisticated software."
                },
                "authors": [
                    {
                        "name": "Priyaranjan Pattnayak"
                    },
                    {
                        "name": "Hussain Bohra"
                    }
                ],
                "author_detail": {
                    "name": "Hussain Bohra"
                },
                "author": "Hussain Bohra",
                "arxiv_comment": "Accepted in 6th World Conference on Artificial Intelligence: Advances\n  and Applications (WCAIAA 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16949v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16949v5",
                "updated": "2025-10-22T16:32:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    32,
                    56,
                    2,
                    295,
                    0
                ],
                "published": "2025-08-23T08:47:31Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    8,
                    47,
                    31,
                    5,
                    235,
                    0
                ],
                "title": "Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement\n  Learning for General LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement\n  Learning for General LLM Reasoning"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have underscored the\npotential of Reinforcement Learning (RL) to facilitate the emergence of\nreasoning capabilities. Despite the encouraging results, a fundamental dilemma\npersists as RL improvement relies on learning from high-quality samples, yet\nthe exploration for such samples remains bounded by the inherent limitations of\nLLMs. This, in effect, creates an undesirable cycle in which what cannot be\nexplored cannot be learned. In this work, we propose Rubric-Scaffolded\nReinforcement Learning (RuscaRL), a novel instructional scaffolding framework\ndesigned to break the exploration bottleneck for general LLM reasoning.\nSpecifically, RuscaRL introduces checklist-style rubrics as (1) explicit\nscaffolding for exploration during rollout generation, where different rubrics\nare provided as external guidance within task instructions to steer diverse\nhigh-quality responses. This guidance is gradually decayed over time,\nencouraging the model to internalize the underlying reasoning patterns; (2)\nverifiable rewards for exploitation during model training, where we can obtain\nrobust LLM-as-a-Judge scores using rubrics as references, enabling effective RL\non general reasoning tasks. Extensive experiments demonstrate the superiority\nof the proposed RuscaRL across various benchmarks, effectively expanding\nreasoning boundaries under the Best-of-N evaluation. Notably, RuscaRL\nsignificantly boosts Qwen2.5-7B-Instruct from 23.6 to 50.3 on HealthBench-500,\nsurpassing GPT-4.1. Furthermore, our fine-tuned variant on\nQwen3-30B-A3B-Instruct achieves 61.1 on HealthBench-500, outperforming leading\nLLMs including OpenAI-o3. Our code is available at\nhttps://github.com/IANNXANG/RuscaRL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have underscored the\npotential of Reinforcement Learning (RL) to facilitate the emergence of\nreasoning capabilities. Despite the encouraging results, a fundamental dilemma\npersists as RL improvement relies on learning from high-quality samples, yet\nthe exploration for such samples remains bounded by the inherent limitations of\nLLMs. This, in effect, creates an undesirable cycle in which what cannot be\nexplored cannot be learned. In this work, we propose Rubric-Scaffolded\nReinforcement Learning (RuscaRL), a novel instructional scaffolding framework\ndesigned to break the exploration bottleneck for general LLM reasoning.\nSpecifically, RuscaRL introduces checklist-style rubrics as (1) explicit\nscaffolding for exploration during rollout generation, where different rubrics\nare provided as external guidance within task instructions to steer diverse\nhigh-quality responses. This guidance is gradually decayed over time,\nencouraging the model to internalize the underlying reasoning patterns; (2)\nverifiable rewards for exploitation during model training, where we can obtain\nrobust LLM-as-a-Judge scores using rubrics as references, enabling effective RL\non general reasoning tasks. Extensive experiments demonstrate the superiority\nof the proposed RuscaRL across various benchmarks, effectively expanding\nreasoning boundaries under the Best-of-N evaluation. Notably, RuscaRL\nsignificantly boosts Qwen2.5-7B-Instruct from 23.6 to 50.3 on HealthBench-500,\nsurpassing GPT-4.1. Furthermore, our fine-tuned variant on\nQwen3-30B-A3B-Instruct achieves 61.1 on HealthBench-500, outperforming leading\nLLMs including OpenAI-o3. Our code is available at\nhttps://github.com/IANNXANG/RuscaRL."
                },
                "authors": [
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Sunzhu Li"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Wenkai Fang"
                    },
                    {
                        "name": "Kongcheng Zhang"
                    },
                    {
                        "name": "Jiale Zhao"
                    },
                    {
                        "name": "Jingwen Yang"
                    },
                    {
                        "name": "Yihe Zhou"
                    },
                    {
                        "name": "Jianwei Lv"
                    },
                    {
                        "name": "Tongya Zheng"
                    },
                    {
                        "name": "Hengtong Lu"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Yan Xie"
                    },
                    {
                        "name": "Mingli Song"
                    }
                ],
                "author_detail": {
                    "name": "Mingli Song"
                },
                "author": "Mingli Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16949v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16949v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11791v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11791v2",
                "updated": "2025-10-22T16:27:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    27,
                    29,
                    2,
                    295,
                    0
                ],
                "published": "2025-06-13T13:54:30Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    54,
                    30,
                    4,
                    164,
                    0
                ],
                "title": "SEC-bench: Automated Benchmarking of LLM Agents on Real-World Software\n  Security Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEC-bench: Automated Benchmarking of LLM Agents on Real-World Software\n  Security Tasks"
                },
                "summary": "Rigorous security-focused evaluation of large language model (LLM) agents is\nimperative for establishing trust in their safe deployment throughout the\nsoftware development lifecycle. However, existing benchmarks largely rely on\nsynthetic challenges or simplified vulnerability datasets that fail to capture\nthe complexity and ambiguity encountered by security engineers in practice. We\nintroduce SEC-bench, the first fully automated benchmarking framework for\nevaluating LLM agents on authentic security engineering tasks. SEC-bench\nemploys a novel multi-agent scaffold that automatically constructs code\nrepositories with harnesses, reproduces vulnerabilities in isolated\nenvironments, and generates gold patches for reliable evaluation. Our framework\nautomatically creates high-quality software vulnerability datasets with\nreproducible artifacts at a cost of only $0.87 per instance. Using SEC-bench,\nwe implement two critical software security tasks to rigorously evaluate LLM\nagents' capabilities: proof-of-concept (PoC) generation and vulnerability\npatching. A comprehensive evaluation of state-of-the-art LLM code agents\nreveals significant performance gaps, achieving at most 18.0% success in PoC\ngeneration and 34.0% in vulnerability patching on our complete dataset. These\nresults highlight the crucial steps needed toward developing LLM agents that\nare more practical, intelligent, and autonomous for security engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rigorous security-focused evaluation of large language model (LLM) agents is\nimperative for establishing trust in their safe deployment throughout the\nsoftware development lifecycle. However, existing benchmarks largely rely on\nsynthetic challenges or simplified vulnerability datasets that fail to capture\nthe complexity and ambiguity encountered by security engineers in practice. We\nintroduce SEC-bench, the first fully automated benchmarking framework for\nevaluating LLM agents on authentic security engineering tasks. SEC-bench\nemploys a novel multi-agent scaffold that automatically constructs code\nrepositories with harnesses, reproduces vulnerabilities in isolated\nenvironments, and generates gold patches for reliable evaluation. Our framework\nautomatically creates high-quality software vulnerability datasets with\nreproducible artifacts at a cost of only $0.87 per instance. Using SEC-bench,\nwe implement two critical software security tasks to rigorously evaluate LLM\nagents' capabilities: proof-of-concept (PoC) generation and vulnerability\npatching. A comprehensive evaluation of state-of-the-art LLM code agents\nreveals significant performance gaps, achieving at most 18.0% success in PoC\ngeneration and 34.0% in vulnerability patching on our complete dataset. These\nresults highlight the crucial steps needed toward developing LLM agents that\nare more practical, intelligent, and autonomous for security engineering."
                },
                "authors": [
                    {
                        "name": "Hwiwon Lee"
                    },
                    {
                        "name": "Ziqi Zhang"
                    },
                    {
                        "name": "Hanxiao Lu"
                    },
                    {
                        "name": "Lingming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lingming Zhang"
                },
                "author": "Lingming Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11791v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11791v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19733v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19733v2",
                "updated": "2025-10-23T09:50:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    9,
                    50,
                    7,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-22T16:25:43Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    25,
                    43,
                    2,
                    295,
                    0
                ],
                "title": "Zhyper: Factorized Hypernetworks for Conditioned LLM Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zhyper: Factorized Hypernetworks for Conditioned LLM Fine-Tuning"
                },
                "summary": "Large Language Model (LLM) conditioning refers to instructing an LLM to\ngenerate content in accordance with the norms and values of a specific culture,\nbeliefs of a particular political orientation, or any desired text-specified\nsemantic conditioning. Unfortunately, prompt engineering does not ensure that\nLLMs behave in accordance with a desired conditioning due to the inductive bias\nof the pre-training and alignment datasets. Prior works have focused on\nfine-tuning LLMs by directly conditioning the LoRA weights; however, such\nmethods introduce a large number of parameters. As a remedy, we propose Zhyper,\na parameter-efficient factorized hypernetwork framework that generates\ncontext-aware LoRA adapters from textual descriptions. Experiments on multiple\nbenchmarks show that Zhyper achieves competitive performance with up to 26x\nfewer parameters than the state-of-the-art baselines. Furthermore, we extend\nZhyper to cultural alignment, demonstrating improved generalization to\nout-of-domain settings and a better capturing of fine-grained contextual\nvalues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) conditioning refers to instructing an LLM to\ngenerate content in accordance with the norms and values of a specific culture,\nbeliefs of a particular political orientation, or any desired text-specified\nsemantic conditioning. Unfortunately, prompt engineering does not ensure that\nLLMs behave in accordance with a desired conditioning due to the inductive bias\nof the pre-training and alignment datasets. Prior works have focused on\nfine-tuning LLMs by directly conditioning the LoRA weights; however, such\nmethods introduce a large number of parameters. As a remedy, we propose Zhyper,\na parameter-efficient factorized hypernetwork framework that generates\ncontext-aware LoRA adapters from textual descriptions. Experiments on multiple\nbenchmarks show that Zhyper achieves competitive performance with up to 26x\nfewer parameters than the state-of-the-art baselines. Furthermore, we extend\nZhyper to cultural alignment, demonstrating improved generalization to\nout-of-domain settings and a better capturing of fine-grained contextual\nvalues."
                },
                "authors": [
                    {
                        "name": "M. H. I. Abdalla"
                    },
                    {
                        "name": "Zhipin Wang"
                    },
                    {
                        "name": "Christian Frey"
                    },
                    {
                        "name": "Steffen Eger"
                    },
                    {
                        "name": "Josif Grabocka"
                    }
                ],
                "author_detail": {
                    "name": "Josif Grabocka"
                },
                "author": "Josif Grabocka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19733v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19733v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01069v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01069v2",
                "updated": "2025-10-22T16:24:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    24,
                    57,
                    2,
                    295,
                    0
                ],
                "published": "2024-12-02T03:08:18Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    3,
                    8,
                    18,
                    0,
                    337,
                    0
                ],
                "title": "The Promise and Peril of Generative AI: Evidence from GPT as Sell-Side\n  Analysts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Promise and Peril of Generative AI: Evidence from GPT as Sell-Side\n  Analysts"
                },
                "summary": "Large language models (LLMs) promise to democratize financial analysis by\nreducing information-processing costs. Yet equal access does not ensure equal\noutcomes, as the locus of friction may shift from processing information to\nevaluating model outputs. We study GPT's earnings forecasts following corporate\nearnings releases and document two patterns. First, GPT's narrative attention\nis consistent and human-like but not always associated with higher forecast\naccuracy. Second, its quantitative reasoning varies substantially across\ncontexts, challenging the view that LLMs are uniformly weak at numerical tasks.\nBuilding on these insights, we propose a diagnostic framework that links\nforecast accuracy to observable processing features (i.e., narrative focus,\nnumerical reasoning, and self-assessed confidence). These indicators serve as\nproxies for this new form of information friction and alert investors when to\nexercise caution. Our study has implications for information frictions,\nregulatory oversight, and the economics of AI-mediated financial markets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) promise to democratize financial analysis by\nreducing information-processing costs. Yet equal access does not ensure equal\noutcomes, as the locus of friction may shift from processing information to\nevaluating model outputs. We study GPT's earnings forecasts following corporate\nearnings releases and document two patterns. First, GPT's narrative attention\nis consistent and human-like but not always associated with higher forecast\naccuracy. Second, its quantitative reasoning varies substantially across\ncontexts, challenging the view that LLMs are uniformly weak at numerical tasks.\nBuilding on these insights, we propose a diagnostic framework that links\nforecast accuracy to observable processing features (i.e., narrative focus,\nnumerical reasoning, and self-assessed confidence). These indicators serve as\nproxies for this new form of information friction and alert investors when to\nexercise caution. Our study has implications for information frictions,\nregulatory oversight, and the economics of AI-mediated financial markets."
                },
                "authors": [
                    {
                        "name": "Edward Li"
                    },
                    {
                        "name": "Min Shen"
                    },
                    {
                        "name": "Zhiyuan Tu"
                    },
                    {
                        "name": "Dexin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Dexin Zhou"
                },
                "author": "Dexin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01069v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01069v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19731v1",
                "updated": "2025-10-22T16:22:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    22,
                    31,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T16:22:31Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    22,
                    31,
                    2,
                    295,
                    0
                ],
                "title": "Bridging Earth and Space: A Survey on HAPS for Non-Terrestrial Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Earth and Space: A Survey on HAPS for Non-Terrestrial Networks"
                },
                "summary": "HAPS are emerging as key enablers in the evolution of 6G wireless networks,\nbridging terrestrial and non-terrestrial infrastructures. Operating in the\nstratosphere, HAPS can provide wide-area coverage, low-latency,\nenergy-efficient broadband communications with flexible deployment options for\ndiverse applications. This survey delivers a comprehensive overview of HAPS use\ncases, technologies, and integration strategies within the 6G ecosystem. The\nroles of HAPS in extending connectivity to underserved regions, supporting\ndynamic backhauling, enabling massive IoT, and delivering reliable low-latency\ncommunications for autonomous and immersive services are discussed. The paper\nreviews state-of-the-art architectures for terrestrial and non-terrestrial\nnetwork integration, highlights recent field trials. Furthermore, key enabling\ntechnologies such as channel modeling, AI-driven resource allocation,\ninterference control, mobility management, and energy-efficient communications\nare examined. The paper also outlines open research challenges. By addressing\nexisting gaps in the literature, this survey positions HAPS as a foundational\ncomponent of globally integrated, resilient, and sustainable 6G networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HAPS are emerging as key enablers in the evolution of 6G wireless networks,\nbridging terrestrial and non-terrestrial infrastructures. Operating in the\nstratosphere, HAPS can provide wide-area coverage, low-latency,\nenergy-efficient broadband communications with flexible deployment options for\ndiverse applications. This survey delivers a comprehensive overview of HAPS use\ncases, technologies, and integration strategies within the 6G ecosystem. The\nroles of HAPS in extending connectivity to underserved regions, supporting\ndynamic backhauling, enabling massive IoT, and delivering reliable low-latency\ncommunications for autonomous and immersive services are discussed. The paper\nreviews state-of-the-art architectures for terrestrial and non-terrestrial\nnetwork integration, highlights recent field trials. Furthermore, key enabling\ntechnologies such as channel modeling, AI-driven resource allocation,\ninterference control, mobility management, and energy-efficient communications\nare examined. The paper also outlines open research challenges. By addressing\nexisting gaps in the literature, this survey positions HAPS as a foundational\ncomponent of globally integrated, resilient, and sustainable 6G networks."
                },
                "authors": [
                    {
                        "name": "G. Svistunov"
                    },
                    {
                        "name": "A. Akhtarshenas"
                    },
                    {
                        "name": "D. López-Pérez"
                    },
                    {
                        "name": "M. Giordani"
                    },
                    {
                        "name": "G. Geraci"
                    },
                    {
                        "name": "H. Yanikomeroglu"
                    }
                ],
                "author_detail": {
                    "name": "H. Yanikomeroglu"
                },
                "arxiv_affiliation": "Carleton University",
                "author": "H. Yanikomeroglu",
                "arxiv_comment": "30 pages. This work has been submitted to IEEE Communications Surveys\n  & Tutorials (under review)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18129v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18129v2",
                "updated": "2025-10-22T16:12:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    12,
                    30,
                    2,
                    295,
                    0
                ],
                "published": "2025-03-23T16:20:14Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    16,
                    20,
                    14,
                    6,
                    82,
                    0
                ],
                "title": "GeoBenchX: Benchmarking LLMs in Agent Solving Multistep Geospatial Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GeoBenchX: Benchmarking LLMs in Agent Solving Multistep Geospatial Tasks"
                },
                "summary": "This paper establishes a benchmark for evaluating tool-calling capabilities\nof large language models (LLMs) on multi-step geospatial tasks relevant to\ncommercial GIS practitioners. We assess eight commercial LLMs (Claude Sonnet\n3.5 and 4, Claude Haiku 3.5, Gemini 2.0 Flash, Gemini 2.5 Pro Preview, GPT-4o,\nGPT-4.1 and o4-mini) using a simple tool-calling agent equipped with 23\ngeospatial functions. Our benchmark comprises tasks in four categories of\nincreasing complexity, with both solvable and intentionally unsolvable tasks to\ntest rejection accuracy. We develop a LLM-as-Judge evaluation framework to\ncompare agent solutions against reference solutions. Results show o4-mini and\nClaude 3.5 Sonnet achieve the best overall performance, OpenAI's GPT-4.1,\nGPT-4o and Google's Gemini 2.5 Pro Preview do not fall far behind, but the last\ntwo are more efficient in identifying unsolvable tasks. Claude Sonnet 4, due\nits preference to provide any solution rather than reject a task, proved to be\nless accurate. We observe significant differences in token usage, with\nAnthropic models consuming more tokens than competitors. Common errors include\nmisunderstanding geometrical relationships, relying on outdated knowledge, and\ninefficient data manipulation. The resulting benchmark set, evaluation\nframework, and data generation pipeline are released as open-source resources\n(available at https://github.com/Solirinai/GeoBenchX), providing one more\nstandardized method for the ongoing evaluation of LLMs for GeoAI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper establishes a benchmark for evaluating tool-calling capabilities\nof large language models (LLMs) on multi-step geospatial tasks relevant to\ncommercial GIS practitioners. We assess eight commercial LLMs (Claude Sonnet\n3.5 and 4, Claude Haiku 3.5, Gemini 2.0 Flash, Gemini 2.5 Pro Preview, GPT-4o,\nGPT-4.1 and o4-mini) using a simple tool-calling agent equipped with 23\ngeospatial functions. Our benchmark comprises tasks in four categories of\nincreasing complexity, with both solvable and intentionally unsolvable tasks to\ntest rejection accuracy. We develop a LLM-as-Judge evaluation framework to\ncompare agent solutions against reference solutions. Results show o4-mini and\nClaude 3.5 Sonnet achieve the best overall performance, OpenAI's GPT-4.1,\nGPT-4o and Google's Gemini 2.5 Pro Preview do not fall far behind, but the last\ntwo are more efficient in identifying unsolvable tasks. Claude Sonnet 4, due\nits preference to provide any solution rather than reject a task, proved to be\nless accurate. We observe significant differences in token usage, with\nAnthropic models consuming more tokens than competitors. Common errors include\nmisunderstanding geometrical relationships, relying on outdated knowledge, and\ninefficient data manipulation. The resulting benchmark set, evaluation\nframework, and data generation pipeline are released as open-source resources\n(available at https://github.com/Solirinai/GeoBenchX), providing one more\nstandardized method for the ongoing evaluation of LLMs for GeoAI."
                },
                "authors": [
                    {
                        "name": "Varvara Krechetova"
                    },
                    {
                        "name": "Denis Kochedykov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Kochedykov"
                },
                "author": "Denis Kochedykov",
                "arxiv_comment": "Github with code and benchmark set:\n  https://github.com/Solirinai/GeoBenchX",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18129v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18129v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07364v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07364v3",
                "updated": "2025-10-22T16:02:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    2,
                    22,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-08T17:58:28Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    58,
                    28,
                    2,
                    281,
                    0
                ],
                "title": "Base Models Know How to Reason, Thinking Models Learn When",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Base Models Know How to Reason, Thinking Models Learn When"
                },
                "summary": "Why do thinking language models like DeepSeek R1 outperform their base\ncounterparts? Despite consistent performance gains, it remains unclear to what\nextent thinking models learn entirely new reasoning capabilities or repurpose\npre-existing base model ones. In this work, we propose a hybrid model where we\nactivate reasoning mechanisms in base models at the right time to elicit\nthinking-model-level reasoning chains, implying that thinking models exploit\nalready existing capabilities. To ground our analysis, we introduce an\nunsupervised, bottom-up approach for uncovering human-interpretable reasoning\nbehaviors in thinking models. This approach provides an unbiased method to\ndiscover reasoning behaviors without imposing manual or LLM-derived\nassumptions. Across three base and four thinking models, using GSM8K and\nMATH500, our hybrid model recovers up to 91% of the performance gap to thinking\nmodels without any weight updates while steering only 12% of tokens.\nConcretely, our empirical setup provides a simple, causal way to test the\neffectiveness of existing reasoning mechanisms in base models by invoking them\ndirectly and measuring the resulting task performance. More broadly, these\nresults reframe our understanding of how thinking models are trained:\npre-training is when models acquire most of their reasoning mechanisms, and\npost-training teaches efficient deployment of these mechanisms at the right\ntime, enabling efficient use of their inference-time compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why do thinking language models like DeepSeek R1 outperform their base\ncounterparts? Despite consistent performance gains, it remains unclear to what\nextent thinking models learn entirely new reasoning capabilities or repurpose\npre-existing base model ones. In this work, we propose a hybrid model where we\nactivate reasoning mechanisms in base models at the right time to elicit\nthinking-model-level reasoning chains, implying that thinking models exploit\nalready existing capabilities. To ground our analysis, we introduce an\nunsupervised, bottom-up approach for uncovering human-interpretable reasoning\nbehaviors in thinking models. This approach provides an unbiased method to\ndiscover reasoning behaviors without imposing manual or LLM-derived\nassumptions. Across three base and four thinking models, using GSM8K and\nMATH500, our hybrid model recovers up to 91% of the performance gap to thinking\nmodels without any weight updates while steering only 12% of tokens.\nConcretely, our empirical setup provides a simple, causal way to test the\neffectiveness of existing reasoning mechanisms in base models by invoking them\ndirectly and measuring the resulting task performance. More broadly, these\nresults reframe our understanding of how thinking models are trained:\npre-training is when models acquire most of their reasoning mechanisms, and\npost-training teaches efficient deployment of these mechanisms at the right\ntime, enabling efficient use of their inference-time compute."
                },
                "authors": [
                    {
                        "name": "Constantin Venhoff"
                    },
                    {
                        "name": "Iván Arcuschin"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Arthur Conmy"
                    },
                    {
                        "name": "Neel Nanda"
                    }
                ],
                "author_detail": {
                    "name": "Neel Nanda"
                },
                "author": "Neel Nanda",
                "arxiv_comment": "10 pages, Accepted to the Mechanistic Interpretability Workshop at\n  NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07364v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07364v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01735v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01735v3",
                "updated": "2025-10-22T16:01:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    1,
                    3,
                    2,
                    295,
                    0
                ],
                "published": "2024-10-02T16:46:38Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    46,
                    38,
                    2,
                    276,
                    0
                ],
                "title": "LASeR: Learning to Adaptively Select Reward Models with Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LASeR: Learning to Adaptively Select Reward Models with Multi-Armed\n  Bandits"
                },
                "summary": "Reward Models (RMs) are crucial to aligning large language models (LLMs), but\nthe degree to which an RM specialized to one task (e.g. writing) generalizes to\nnew tasks (e.g. math) is often not known a priori, often making using only one\nfixed RM to train LLMs suboptimal. However, optimizing LLMs with multiple RMs\nsimultaneously can incur a prohibitively high computational cost and lead to\nconflicting signals from different RMs that may degrade performance. To address\nthese challenges, we introduce LASeR (Learning to Adaptively Select Rewards),\nwhich frames reward model selection as a multi-armed bandit problem,\nefficiently and iteratively training LLMs using multiple RMs by selecting the\nmost well-suited RM for each instance. On commonsense and math reasoning tasks,\nwe show that LASeR boosts iterative LLM training, improving the absolute\naverage accuracy of Llama-3-8B over three datasets by 2.67% over an ensemble of\nRM scores while also showing superior efficiency (e.g., a 2x speedup).\nMoreover, on WildChat (open-ended instruction-following tasks), LASeR leads to\na 72.69% AlpacaEval win rate over the RM score ensemble baseline. Extending to\nlong-context generation, LASeR improves by 2.96 F1 points (avg.) on\nsingle-document QA tasks and 2.97 F1 points on few-shot learning over the RM\nscore ensemble baseline with best-of-n sampling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward Models (RMs) are crucial to aligning large language models (LLMs), but\nthe degree to which an RM specialized to one task (e.g. writing) generalizes to\nnew tasks (e.g. math) is often not known a priori, often making using only one\nfixed RM to train LLMs suboptimal. However, optimizing LLMs with multiple RMs\nsimultaneously can incur a prohibitively high computational cost and lead to\nconflicting signals from different RMs that may degrade performance. To address\nthese challenges, we introduce LASeR (Learning to Adaptively Select Rewards),\nwhich frames reward model selection as a multi-armed bandit problem,\nefficiently and iteratively training LLMs using multiple RMs by selecting the\nmost well-suited RM for each instance. On commonsense and math reasoning tasks,\nwe show that LASeR boosts iterative LLM training, improving the absolute\naverage accuracy of Llama-3-8B over three datasets by 2.67% over an ensemble of\nRM scores while also showing superior efficiency (e.g., a 2x speedup).\nMoreover, on WildChat (open-ended instruction-following tasks), LASeR leads to\na 72.69% AlpacaEval win rate over the RM score ensemble baseline. Extending to\nlong-context generation, LASeR improves by 2.96 F1 points (avg.) on\nsingle-document QA tasks and 2.97 F1 points on few-shot learning over the RM\nscore ensemble baseline with best-of-n sampling."
                },
                "authors": [
                    {
                        "name": "Duy Nguyen"
                    },
                    {
                        "name": "Archiki Prasad"
                    },
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "NeurIPS 2025 camera-ready. First two authors contributed equally.\n  Code: https://github.com/duykhuongnguyen/LASeR-MAB",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01735v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01735v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19710v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19710v1",
                "updated": "2025-10-22T15:58:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    58,
                    44,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T15:58:44Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    58,
                    44,
                    2,
                    295,
                    0
                ],
                "title": "SEMPO: Lightweight Foundation Models for Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEMPO: Lightweight Foundation Models for Time Series Forecasting"
                },
                "summary": "The recent boom of large pre-trained models witnesses remarkable success in\ndeveloping foundation models (FMs) for time series forecasting. Despite\nimpressive performance across diverse downstream forecasting tasks, existing\ntime series FMs possess massive network architectures and require substantial\npre-training on large-scale datasets, which significantly hinders their\ndeployment in resource-constrained environments. In response to this growing\ntension between versatility and affordability, we propose SEMPO, a novel\nlightweight foundation model that requires pretraining on relatively\nsmall-scale data, yet exhibits strong general time series forecasting.\nConcretely, SEMPO comprises two key modules: 1) energy-aware SpEctral\ndecomposition module, that substantially improves the utilization of\npre-training data by modeling not only the high-energy frequency signals but\nalso the low-energy yet informative frequency signals that are ignored in\ncurrent methods; and 2) Mixture-of-PrOmpts enabled Transformer, that learns\nheterogeneous temporal patterns through small dataset-specific prompts and\nadaptively routes time series tokens to prompt-based experts for\nparameter-efficient model adaptation across different datasets and domains.\nEquipped with these modules, SEMPO significantly reduces both pre-training data\nscale and model size, while achieving strong generalization. Extensive\nexperiments on two large-scale benchmarks covering 16 datasets demonstrate the\nsuperior performance of SEMPO in both zero-shot and few-shot forecasting\nscenarios compared with state-of-the-art methods. Code and data are available\nat https://github.com/mala-lab/SEMPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent boom of large pre-trained models witnesses remarkable success in\ndeveloping foundation models (FMs) for time series forecasting. Despite\nimpressive performance across diverse downstream forecasting tasks, existing\ntime series FMs possess massive network architectures and require substantial\npre-training on large-scale datasets, which significantly hinders their\ndeployment in resource-constrained environments. In response to this growing\ntension between versatility and affordability, we propose SEMPO, a novel\nlightweight foundation model that requires pretraining on relatively\nsmall-scale data, yet exhibits strong general time series forecasting.\nConcretely, SEMPO comprises two key modules: 1) energy-aware SpEctral\ndecomposition module, that substantially improves the utilization of\npre-training data by modeling not only the high-energy frequency signals but\nalso the low-energy yet informative frequency signals that are ignored in\ncurrent methods; and 2) Mixture-of-PrOmpts enabled Transformer, that learns\nheterogeneous temporal patterns through small dataset-specific prompts and\nadaptively routes time series tokens to prompt-based experts for\nparameter-efficient model adaptation across different datasets and domains.\nEquipped with these modules, SEMPO significantly reduces both pre-training data\nscale and model size, while achieving strong generalization. Extensive\nexperiments on two large-scale benchmarks covering 16 datasets demonstrate the\nsuperior performance of SEMPO in both zero-shot and few-shot forecasting\nscenarios compared with state-of-the-art methods. Code and data are available\nat https://github.com/mala-lab/SEMPO."
                },
                "authors": [
                    {
                        "name": "Hui He"
                    },
                    {
                        "name": "Kun Yi"
                    },
                    {
                        "name": "Yuanchi Ma"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Zhendong Niu"
                    },
                    {
                        "name": "Guansong Pang"
                    }
                ],
                "author_detail": {
                    "name": "Guansong Pang"
                },
                "author": "Guansong Pang",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19710v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19710v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19698v1",
                "updated": "2025-10-22T15:50:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    50,
                    4,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T15:50:04Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    50,
                    4,
                    2,
                    295,
                    0
                ],
                "title": "RLIE: Rule Generation with Logistic Regression, Iterative Refinement,\n  and Evaluation for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RLIE: Rule Generation with Logistic Regression, Iterative Refinement,\n  and Evaluation for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) can propose rules in natural language,\nsidestepping the need for a predefined predicate space in traditional rule\nlearning. Yet many LLM-based approaches ignore interactions among rules, and\nthe opportunity to couple LLMs with probabilistic rule learning for robust\ninference remains underexplored. We present RLIE, a unified framework that\nintegrates LLMs with probabilistic modeling to learn a set of weighted rules.\nRLIE has four stages: (1) Rule generation, where an LLM proposes and filters\ncandidates; (2) Logistic regression, which learns probabilistic weights for\nglobal selection and calibration; (3) Iterative refinement, which updates the\nrule set using prediction errors; and (4) Evaluation, which compares the\nweighted rule set as a direct classifier with methods that inject rules into an\nLLM. We evaluate multiple inference strategies on real-world datasets. Applying\nrules directly with their learned weights yields superior performance, whereas\nprompting LLMs with the rules, weights, and logistic-model outputs surprisingly\ndegrades accuracy. This supports the view that LLMs excel at semantic\ngeneration and interpretation but are less reliable for precise probabilistic\nintegration. RLIE clarifies the potential and limitations of LLMs for inductive\nreasoning and couples them with classic probabilistic rule combination methods\nto enable more reliable neuro-symbolic reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can propose rules in natural language,\nsidestepping the need for a predefined predicate space in traditional rule\nlearning. Yet many LLM-based approaches ignore interactions among rules, and\nthe opportunity to couple LLMs with probabilistic rule learning for robust\ninference remains underexplored. We present RLIE, a unified framework that\nintegrates LLMs with probabilistic modeling to learn a set of weighted rules.\nRLIE has four stages: (1) Rule generation, where an LLM proposes and filters\ncandidates; (2) Logistic regression, which learns probabilistic weights for\nglobal selection and calibration; (3) Iterative refinement, which updates the\nrule set using prediction errors; and (4) Evaluation, which compares the\nweighted rule set as a direct classifier with methods that inject rules into an\nLLM. We evaluate multiple inference strategies on real-world datasets. Applying\nrules directly with their learned weights yields superior performance, whereas\nprompting LLMs with the rules, weights, and logistic-model outputs surprisingly\ndegrades accuracy. This supports the view that LLMs excel at semantic\ngeneration and interpretation but are less reliable for precise probabilistic\nintegration. RLIE clarifies the potential and limitations of LLMs for inductive\nreasoning and couples them with classic probabilistic rule combination methods\nto enable more reliable neuro-symbolic reasoning."
                },
                "authors": [
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Hua XU"
                    },
                    {
                        "name": "Zhangyi Hu"
                    },
                    {
                        "name": "Yutao Yue"
                    }
                ],
                "author_detail": {
                    "name": "Yutao Yue"
                },
                "author": "Yutao Yue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19689v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19689v1",
                "updated": "2025-10-22T15:37:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    37,
                    42,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T15:37:42Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    37,
                    42,
                    2,
                    295,
                    0
                ],
                "title": "Serverless GPU Architecture for Enterprise HR Analytics: A\n  Production-Scale BDaaS Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless GPU Architecture for Enterprise HR Analytics: A\n  Production-Scale BDaaS Implementation"
                },
                "summary": "Industrial and government organizations increasingly depend on data-driven\nanalytics for workforce, finance, and regulated decision processes, where\ntimeliness, cost efficiency, and compliance are critical. Distributed\nframeworks such as Spark and Flink remain effective for massive-scale batch or\nstreaming analytics but introduce coordination complexity and auditing\noverheads that misalign with moderate-scale, latency-sensitive inference.\nMeanwhile, cloud providers now offer serverless GPUs, and models such as TabNet\nenable interpretable tabular ML, motivating new deployment blueprints for\nregulated environments. In this paper, we present a production-oriented Big\nData as a Service (BDaaS) blueprint that integrates a single-node serverless\nGPU runtime with TabNet. The design leverages GPU acceleration for throughput,\nserverless elasticity for cost reduction, and feature-mask interpretability for\nIL4/FIPS compliance. We conduct benchmarks on the HR, Adult, and BLS datasets,\ncomparing our approach against Spark and CPU baselines. Our results show that\nGPU pipelines achieve up to 4.5x higher throughput, 98x lower latency, and 90%\nlower cost per 1K inferences compared to Spark baselines, while compliance\nmechanisms add only ~5.7 ms latency with p99 < 22 ms. Interpretability remains\nstable under peak load, ensuring reliable auditability. Taken together, these\nfindings provide a compliance-aware benchmark, a reproducible Helm-packaged\nblueprint, and a decision framework that demonstrate the practicality of\nsecure, interpretable, and cost-efficient serverless GPU analytics for\nregulated enterprise and government settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Industrial and government organizations increasingly depend on data-driven\nanalytics for workforce, finance, and regulated decision processes, where\ntimeliness, cost efficiency, and compliance are critical. Distributed\nframeworks such as Spark and Flink remain effective for massive-scale batch or\nstreaming analytics but introduce coordination complexity and auditing\noverheads that misalign with moderate-scale, latency-sensitive inference.\nMeanwhile, cloud providers now offer serverless GPUs, and models such as TabNet\nenable interpretable tabular ML, motivating new deployment blueprints for\nregulated environments. In this paper, we present a production-oriented Big\nData as a Service (BDaaS) blueprint that integrates a single-node serverless\nGPU runtime with TabNet. The design leverages GPU acceleration for throughput,\nserverless elasticity for cost reduction, and feature-mask interpretability for\nIL4/FIPS compliance. We conduct benchmarks on the HR, Adult, and BLS datasets,\ncomparing our approach against Spark and CPU baselines. Our results show that\nGPU pipelines achieve up to 4.5x higher throughput, 98x lower latency, and 90%\nlower cost per 1K inferences compared to Spark baselines, while compliance\nmechanisms add only ~5.7 ms latency with p99 < 22 ms. Interpretability remains\nstable under peak load, ensuring reliable auditability. Taken together, these\nfindings provide a compliance-aware benchmark, a reproducible Helm-packaged\nblueprint, and a decision framework that demonstrate the practicality of\nsecure, interpretable, and cost-efficient serverless GPU analytics for\nregulated enterprise and government settings."
                },
                "authors": [
                    {
                        "name": "Guilin Zhang"
                    },
                    {
                        "name": "Wulan Guo"
                    },
                    {
                        "name": "Ziqi Tan"
                    },
                    {
                        "name": "Srinivas Vippagunta"
                    },
                    {
                        "name": "Suchitra Raman"
                    },
                    {
                        "name": "Shreeshankar Chatterjee"
                    },
                    {
                        "name": "Ju Lin"
                    },
                    {
                        "name": "Shang Liu"
                    },
                    {
                        "name": "Mary Schladenhauffen"
                    },
                    {
                        "name": "Jeffrey Luo"
                    },
                    {
                        "name": "Hailong Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hailong Jiang"
                },
                "author": "Hailong Jiang",
                "arxiv_comment": "10 pages, 7 figures, 4 tables. Accepted to IEEE BigData 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19689v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19689v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.4; H.3.4; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19687v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19687v1",
                "updated": "2025-10-22T15:35:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    35,
                    0,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T15:35:00Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    35,
                    0,
                    2,
                    295,
                    0
                ],
                "title": "Are Large Language Models Sensitive to the Motives Behind Communication?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Large Language Models Sensitive to the Motives Behind Communication?"
                },
                "summary": "Human communication is motivated: people speak, write, and create content\nwith a particular communicative intent in mind. As a result, information that\nlarge language models (LLMs) and AI agents process is inherently framed by\nhumans' intentions and incentives. People are adept at navigating such nuanced\ninformation: we routinely identify benevolent or self-serving motives in order\nto decide what statements to trust. For LLMs to be effective in the real world,\nthey too must critically evaluate content by factoring in the motivations of\nthe source -- for instance, weighing the credibility of claims made in a sales\npitch. In this paper, we undertake a comprehensive study of whether LLMs have\nthis capacity for motivational vigilance. We first employ controlled\nexperiments from cognitive science to verify that LLMs' behavior is consistent\nwith rational models of learning from motivated testimony, and find they\nsuccessfully discount information from biased sources in a human-like manner.\nWe then extend our evaluation to sponsored online adverts, a more naturalistic\nreflection of LLM agents' information ecosystems. In these settings, we find\nthat LLMs' inferences do not track the rational models' predictions nearly as\nclosely -- partly due to additional information that distracts them from\nvigilance-relevant considerations. However, a simple steering intervention that\nboosts the salience of intentions and incentives substantially increases the\ncorrespondence between LLMs and the rational model. These results suggest that\nLLMs possess a basic sensitivity to the motivations of others, but generalizing\nto novel real-world settings will require further improvements to these models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human communication is motivated: people speak, write, and create content\nwith a particular communicative intent in mind. As a result, information that\nlarge language models (LLMs) and AI agents process is inherently framed by\nhumans' intentions and incentives. People are adept at navigating such nuanced\ninformation: we routinely identify benevolent or self-serving motives in order\nto decide what statements to trust. For LLMs to be effective in the real world,\nthey too must critically evaluate content by factoring in the motivations of\nthe source -- for instance, weighing the credibility of claims made in a sales\npitch. In this paper, we undertake a comprehensive study of whether LLMs have\nthis capacity for motivational vigilance. We first employ controlled\nexperiments from cognitive science to verify that LLMs' behavior is consistent\nwith rational models of learning from motivated testimony, and find they\nsuccessfully discount information from biased sources in a human-like manner.\nWe then extend our evaluation to sponsored online adverts, a more naturalistic\nreflection of LLM agents' information ecosystems. In these settings, we find\nthat LLMs' inferences do not track the rational models' predictions nearly as\nclosely -- partly due to additional information that distracts them from\nvigilance-relevant considerations. However, a simple steering intervention that\nboosts the salience of intentions and incentives substantially increases the\ncorrespondence between LLMs and the rational model. These results suggest that\nLLMs possess a basic sensitivity to the motivations of others, but generalizing\nto novel real-world settings will require further improvements to these models."
                },
                "authors": [
                    {
                        "name": "Addison J. Wu"
                    },
                    {
                        "name": "Ryan Liu"
                    },
                    {
                        "name": "Kerem Oktar"
                    },
                    {
                        "name": "Theodore R. Sumers"
                    },
                    {
                        "name": "Thomas L. Griffiths"
                    }
                ],
                "author_detail": {
                    "name": "Thomas L. Griffiths"
                },
                "author": "Thomas L. Griffiths",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19687v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19687v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18524v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18524v2",
                "updated": "2025-10-22T15:27:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    27,
                    6,
                    2,
                    295,
                    0
                ],
                "published": "2025-05-24T05:40:38Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    5,
                    40,
                    38,
                    5,
                    144,
                    0
                ],
                "title": "metaTextGrad: Automatically optimizing language model optimizers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "metaTextGrad: Automatically optimizing language model optimizers"
                },
                "summary": "Large language models (LLMs) are increasingly used in learning algorithms,\nevaluations, and optimization tasks. Recent studies have shown that using\nLLM-based optimizers to automatically optimize model prompts, demonstrations,\npredictions themselves, or other components can significantly enhance the\nperformance of AI systems, as demonstrated by frameworks such as DSPy and\nTextGrad. However, optimizers built on language models themselves are usually\ndesigned by humans with manual design choices; optimizers themselves are not\noptimized. Moreover, these optimizers are general purpose by design, to be\nuseful to a broad audience, and are not tailored for specific tasks. To address\nthese challenges, we propose metaTextGrad, which focuses on designing a\nmeta-optimizer to further enhance existing optimizers and align them to be good\noptimizers for a given task. Our approach consists of two key components: a\nmeta prompt optimizer and a meta structure optimizer. The combination of these\ntwo significantly improves performance across multiple benchmarks, achieving an\naverage absolute performance improvement of up to 6% compared to the best\nbaseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used in learning algorithms,\nevaluations, and optimization tasks. Recent studies have shown that using\nLLM-based optimizers to automatically optimize model prompts, demonstrations,\npredictions themselves, or other components can significantly enhance the\nperformance of AI systems, as demonstrated by frameworks such as DSPy and\nTextGrad. However, optimizers built on language models themselves are usually\ndesigned by humans with manual design choices; optimizers themselves are not\noptimized. Moreover, these optimizers are general purpose by design, to be\nuseful to a broad audience, and are not tailored for specific tasks. To address\nthese challenges, we propose metaTextGrad, which focuses on designing a\nmeta-optimizer to further enhance existing optimizers and align them to be good\noptimizers for a given task. Our approach consists of two key components: a\nmeta prompt optimizer and a meta structure optimizer. The combination of these\ntwo significantly improves performance across multiple benchmarks, achieving an\naverage absolute performance improvement of up to 6% compared to the best\nbaseline."
                },
                "authors": [
                    {
                        "name": "Guowei Xu"
                    },
                    {
                        "name": "Mert Yuksekgonul"
                    },
                    {
                        "name": "Carlos Guestrin"
                    },
                    {
                        "name": "James Zou"
                    }
                ],
                "author_detail": {
                    "name": "James Zou"
                },
                "author": "James Zou",
                "arxiv_comment": "21 pages, 2 figures",
                "arxiv_journal_ref": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18524v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18524v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02511v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02511v2",
                "updated": "2025-10-22T15:27:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    27,
                    3,
                    2,
                    295,
                    0
                ],
                "published": "2025-08-04T15:17:13Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    17,
                    13,
                    0,
                    216,
                    0
                ],
                "title": "Test-time Prompt Intervention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time Prompt Intervention"
                },
                "summary": "Test-time compute has led to remarkable success in the large language model\n(LLM) community, particularly for complex tasks, where longer chains of thought\n(CoTs) are generated to enhance reasoning capabilities. However, growing\nevidence reveals that such reasoning models often produce CoTs plagued by\nexcessive redundancy, including unnecessary verification steps and repetitive\nreasoning shifts. The root cause lies in post-training of them that overly rely\non outcome reward paradigms, as the data of process reward paradigms, which\nregulate intermediate reasoning steps, is difficult to construct at scale. To\naddress this, we propose PI, a novel framework for Test-time Prompt\nIntervention. PI provides an interface to dynamically guide and regulate\nreasoning paths during inference through timely (When module) and proper (How\nmodule) interventions and post-intervention sampling (Which module). This\nallows human problem-solving expertise and cognitive science principles to be\nseamlessly integrated into LLMs' reasoning processes, enhancing controllability\nand interpretability. Extensive experiments across multiple models and datasets\ndemonstrate that PI significantly shortens CoTs while reducing hallucination,\nyielding more concise and reliable reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time compute has led to remarkable success in the large language model\n(LLM) community, particularly for complex tasks, where longer chains of thought\n(CoTs) are generated to enhance reasoning capabilities. However, growing\nevidence reveals that such reasoning models often produce CoTs plagued by\nexcessive redundancy, including unnecessary verification steps and repetitive\nreasoning shifts. The root cause lies in post-training of them that overly rely\non outcome reward paradigms, as the data of process reward paradigms, which\nregulate intermediate reasoning steps, is difficult to construct at scale. To\naddress this, we propose PI, a novel framework for Test-time Prompt\nIntervention. PI provides an interface to dynamically guide and regulate\nreasoning paths during inference through timely (When module) and proper (How\nmodule) interventions and post-intervention sampling (Which module). This\nallows human problem-solving expertise and cognitive science principles to be\nseamlessly integrated into LLMs' reasoning processes, enhancing controllability\nand interpretability. Extensive experiments across multiple models and datasets\ndemonstrate that PI significantly shortens CoTs while reducing hallucination,\nyielding more concise and reliable reasoning."
                },
                "authors": [
                    {
                        "name": "Chenxu Yang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Mz Dai"
                    },
                    {
                        "name": "Dingyu Yao"
                    },
                    {
                        "name": "Mingyu Zheng"
                    },
                    {
                        "name": "Minghui Chen"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Weiping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Wang"
                },
                "author": "Weiping Wang",
                "arxiv_comment": "24 pages, 20 figures, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02511v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02511v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19676v1",
                "updated": "2025-10-22T15:22:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    22,
                    15,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T15:22:15Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    22,
                    15,
                    2,
                    295,
                    0
                ],
                "title": "CircuitGuard: Mitigating LLM Memorization in RTL Code Generation Against\n  IP Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CircuitGuard: Mitigating LLM Memorization in RTL Code Generation Against\n  IP Leakage"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success in generative\ntasks, including register-transfer level (RTL) hardware synthesis. However,\ntheir tendency to memorize training data poses critical risks when proprietary\nor security-sensitive designs are unintentionally exposed during inference.\nWhile prior work has examined memorization in natural language, RTL introduces\nunique challenges: In RTL, structurally different implementations (e.g.,\nbehavioral vs. gate-level descriptions) can realize the same hardware, leading\nto intellectual property (IP) leakage (full or partial) even without verbatim\noverlap. Conversely, even small syntactic variations (e.g., operator precedence\nor blocking vs. non-blocking assignments) can drastically alter circuit\nbehavior, making correctness preservation especially challenging. In this work,\nwe systematically study memorization in RTL code generation and propose\nCircuitGuard, a defense strategy that balances leakage reduction with\ncorrectness preservation. CircuitGuard (1) introduces a novel RTL-aware\nsimilarity metric that captures both structural and functional equivalence\nbeyond surface-level overlap, and (2) develops an activation-level steering\nmethod that identifies and attenuates transformer components most responsible\nfor memorization. Our empirical evaluation demonstrates that CircuitGuard\nidentifies (and isolates) 275 memorization-critical features across layers\n18-28 of Llama 3.1-8B model, achieving up to 80% reduction in semantic\nsimilarity to proprietary patterns while maintaining generation quality.\nCircuitGuard further shows 78-85% cross-domain transfer effectiveness, enabling\nrobust memorization mitigation across circuit categories without retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success in generative\ntasks, including register-transfer level (RTL) hardware synthesis. However,\ntheir tendency to memorize training data poses critical risks when proprietary\nor security-sensitive designs are unintentionally exposed during inference.\nWhile prior work has examined memorization in natural language, RTL introduces\nunique challenges: In RTL, structurally different implementations (e.g.,\nbehavioral vs. gate-level descriptions) can realize the same hardware, leading\nto intellectual property (IP) leakage (full or partial) even without verbatim\noverlap. Conversely, even small syntactic variations (e.g., operator precedence\nor blocking vs. non-blocking assignments) can drastically alter circuit\nbehavior, making correctness preservation especially challenging. In this work,\nwe systematically study memorization in RTL code generation and propose\nCircuitGuard, a defense strategy that balances leakage reduction with\ncorrectness preservation. CircuitGuard (1) introduces a novel RTL-aware\nsimilarity metric that captures both structural and functional equivalence\nbeyond surface-level overlap, and (2) develops an activation-level steering\nmethod that identifies and attenuates transformer components most responsible\nfor memorization. Our empirical evaluation demonstrates that CircuitGuard\nidentifies (and isolates) 275 memorization-critical features across layers\n18-28 of Llama 3.1-8B model, achieving up to 80% reduction in semantic\nsimilarity to proprietary patterns while maintaining generation quality.\nCircuitGuard further shows 78-85% cross-domain transfer effectiveness, enabling\nrobust memorization mitigation across circuit categories without retraining."
                },
                "authors": [
                    {
                        "name": "Nowfel Mashnoor"
                    },
                    {
                        "name": "Mohammad Akyash"
                    },
                    {
                        "name": "Hadi Kamali"
                    },
                    {
                        "name": "Kimia Azar"
                    }
                ],
                "author_detail": {
                    "name": "Kimia Azar"
                },
                "author": "Kimia Azar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19675v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19675v1",
                "updated": "2025-10-22T15:21:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    21,
                    5,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T15:21:05Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    21,
                    5,
                    2,
                    295,
                    0
                ],
                "title": "Study of Training Dynamics for Memory-Constrained Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Study of Training Dynamics for Memory-Constrained Fine-Tuning"
                },
                "summary": "Memory-efficient training of deep neural networks has become increasingly\nimportant as models grow larger while deployment environments impose strict\nresource constraints. We propose TraDy, a novel transfer learning scheme\nleveraging two key insights: layer importance for updates is\narchitecture-dependent and determinable a priori, while dynamic stochastic\nchannel selection provides superior gradient approximation compared to static\napproaches. We introduce a dynamic channel selection approach that\nstochastically resamples channels between epochs within preselected layers.\nExtensive experiments demonstrate TraDy achieves state-of-the-art performance\nacross various downstream tasks and architectures while maintaining strict\nmemory constraints, achieving up to 99% activation sparsity, 95% weight\nderivative sparsity, and 97% reduction in FLOPs for weight derivative\ncomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-efficient training of deep neural networks has become increasingly\nimportant as models grow larger while deployment environments impose strict\nresource constraints. We propose TraDy, a novel transfer learning scheme\nleveraging two key insights: layer importance for updates is\narchitecture-dependent and determinable a priori, while dynamic stochastic\nchannel selection provides superior gradient approximation compared to static\napproaches. We introduce a dynamic channel selection approach that\nstochastically resamples channels between epochs within preselected layers.\nExtensive experiments demonstrate TraDy achieves state-of-the-art performance\nacross various downstream tasks and architectures while maintaining strict\nmemory constraints, achieving up to 99% activation sparsity, 95% weight\nderivative sparsity, and 97% reduction in FLOPs for weight derivative\ncomputation."
                },
                "authors": [
                    {
                        "name": "Aël Quélennec"
                    },
                    {
                        "name": "Nour Hezbri"
                    },
                    {
                        "name": "Pavlo Mozharovskyi"
                    },
                    {
                        "name": "Van-Tam Nguyen"
                    },
                    {
                        "name": "Enzo Tartaglione"
                    }
                ],
                "author_detail": {
                    "name": "Enzo Tartaglione"
                },
                "author": "Enzo Tartaglione",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19675v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19675v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19670v1",
                "updated": "2025-10-22T15:16:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    16,
                    56,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T15:16:56Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    16,
                    56,
                    2,
                    295,
                    0
                ],
                "title": "CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware\n  Cloud-Edge Cooperation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware\n  Cloud-Edge Cooperation"
                },
                "summary": "We present CoSense-LLM, an edge-first framework that turns continuous\nmultimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and\nlightweight vision) into compact, verifiable semantic tokens and coordinates\nwith large language models under explicit latency, energy, bandwidth, and\nprivacy constraints. CoSense-LLM has four parts: (i) SenseFusion, a lightweight\nencoder that aligns sensor embeddings with language and compresses them into\nshort discrete code sequences; (ii) Edge-RAG, a local hybrid retrieval layer\nthat grounds generation in site specific policies and notes; (iii)\nPromptRouter, a cost and uncertainty aware policy that selects edge only\ngeneration, edge plus retrieval, or compact cloud escalation; and (iv) Secure\nExecution, an auditable redaction path that enforces data minimization so raw\nwaveforms never leave the device. The system works with modern serving\noptimizations, including paged or streaming KV caches, FlashAttention style\nkernels, speculative decoding, and quantized LoRA adapters, and supports on\ndevice personalization and federated updates under non IID drift. Across home,\noffice, and clinic deployments, CoSense-LLM delivers grounded explanations\nwhile meeting tight service level objectives: it sustains sub second (p95) end\nto end latency on edge dominant paths, reduces inter tier token and bandwidth\ncosts by preferring local retrieval grounded responses, and preserves privacy\nby transmitting only discrete codes and redacted metadata. Ablations show that\nEdge-RAG improves factual consistency and reduces contradictions, calibrated\nuncertainty enables selective abstention and controlled escalations, and KV\nplus decoding accelerators lower energy per decision. The results support an\nedge first design that treats semantics, privacy, and predictable latency as co\nequal goals for large model deployments in interference prone environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present CoSense-LLM, an edge-first framework that turns continuous\nmultimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and\nlightweight vision) into compact, verifiable semantic tokens and coordinates\nwith large language models under explicit latency, energy, bandwidth, and\nprivacy constraints. CoSense-LLM has four parts: (i) SenseFusion, a lightweight\nencoder that aligns sensor embeddings with language and compresses them into\nshort discrete code sequences; (ii) Edge-RAG, a local hybrid retrieval layer\nthat grounds generation in site specific policies and notes; (iii)\nPromptRouter, a cost and uncertainty aware policy that selects edge only\ngeneration, edge plus retrieval, or compact cloud escalation; and (iv) Secure\nExecution, an auditable redaction path that enforces data minimization so raw\nwaveforms never leave the device. The system works with modern serving\noptimizations, including paged or streaming KV caches, FlashAttention style\nkernels, speculative decoding, and quantized LoRA adapters, and supports on\ndevice personalization and federated updates under non IID drift. Across home,\noffice, and clinic deployments, CoSense-LLM delivers grounded explanations\nwhile meeting tight service level objectives: it sustains sub second (p95) end\nto end latency on edge dominant paths, reduces inter tier token and bandwidth\ncosts by preferring local retrieval grounded responses, and preserves privacy\nby transmitting only discrete codes and redacted metadata. Ablations show that\nEdge-RAG improves factual consistency and reduces contradictions, calibrated\nuncertainty enables selective abstention and controlled escalations, and KV\nplus decoding accelerators lower energy per decision. The results support an\nedge first design that treats semantics, privacy, and predictable latency as co\nequal goals for large model deployments in interference prone environments."
                },
                "authors": [
                    {
                        "name": "Hasan Akgul"
                    },
                    {
                        "name": "Mari Eplik"
                    },
                    {
                        "name": "Javier Rojas"
                    },
                    {
                        "name": "Aina Binti Abdullah"
                    },
                    {
                        "name": "Pieter van der Merwe"
                    }
                ],
                "author_detail": {
                    "name": "Pieter van der Merwe"
                },
                "author": "Pieter van der Merwe",
                "arxiv_comment": "19 pages,8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; C.2.4; C.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07836v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07836v6",
                "updated": "2025-10-22T15:16:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    16,
                    35,
                    2,
                    295,
                    0
                ],
                "published": "2024-10-10T11:52:07Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    52,
                    7,
                    3,
                    284,
                    0
                ],
                "title": "Masked Generative Priors Improve World Models Sequence Modelling\n  Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Generative Priors Improve World Models Sequence Modelling\n  Capabilities"
                },
                "summary": "Deep Reinforcement Learning (RL) has become the leading approach for creating\nartificial agents in complex environments. Model-based approaches, which are RL\nmethods with world models that predict environment dynamics, are among the most\npromising directions for improving data efficiency, forming a critical step\ntoward bridging the gap between research and real-world deployment. In\nparticular, world models enhance sample efficiency by learning in imagination,\nwhich involves training a generative sequence model of the environment in a\nself-supervised manner. Recently, Masked Generative Modelling has emerged as a\nmore efficient and superior inductive bias for modelling and generating token\nsequences. Building on the Efficient Stochastic Transformer-based World Models\n(STORM) architecture, we replace the traditional MLP prior with a Masked\nGenerative Prior (e.g., MaskGIT Prior) and introduce GIT-STORM. We evaluate our\nmodel on two downstream tasks: reinforcement learning and video prediction.\nGIT-STORM demonstrates substantial performance gains in RL tasks on the Atari\n100k benchmark. Moreover, we apply Transformer-based World Models to continuous\naction environments for the first time, addressing a significant gap in prior\nresearch. To achieve this, we employ a state mixer function that integrates\nlatent state representations with actions, enabling our model to handle\ncontinuous control tasks. We validate this approach through qualitative and\nquantitative analyses on the DeepMind Control Suite, showcasing the\neffectiveness of Transformer-based World Models in this new domain. Our results\nhighlight the versatility and efficacy of the MaskGIT dynamics prior, paving\nthe way for more accurate world models and effective RL policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Reinforcement Learning (RL) has become the leading approach for creating\nartificial agents in complex environments. Model-based approaches, which are RL\nmethods with world models that predict environment dynamics, are among the most\npromising directions for improving data efficiency, forming a critical step\ntoward bridging the gap between research and real-world deployment. In\nparticular, world models enhance sample efficiency by learning in imagination,\nwhich involves training a generative sequence model of the environment in a\nself-supervised manner. Recently, Masked Generative Modelling has emerged as a\nmore efficient and superior inductive bias for modelling and generating token\nsequences. Building on the Efficient Stochastic Transformer-based World Models\n(STORM) architecture, we replace the traditional MLP prior with a Masked\nGenerative Prior (e.g., MaskGIT Prior) and introduce GIT-STORM. We evaluate our\nmodel on two downstream tasks: reinforcement learning and video prediction.\nGIT-STORM demonstrates substantial performance gains in RL tasks on the Atari\n100k benchmark. Moreover, we apply Transformer-based World Models to continuous\naction environments for the first time, addressing a significant gap in prior\nresearch. To achieve this, we employ a state mixer function that integrates\nlatent state representations with actions, enabling our model to handle\ncontinuous control tasks. We validate this approach through qualitative and\nquantitative analyses on the DeepMind Control Suite, showcasing the\neffectiveness of Transformer-based World Models in this new domain. Our results\nhighlight the versatility and efficacy of the MaskGIT dynamics prior, paving\nthe way for more accurate world models and effective RL policies."
                },
                "authors": [
                    {
                        "name": "Cristian Meo"
                    },
                    {
                        "name": "Mircea Lica"
                    },
                    {
                        "name": "Zarif Ikram"
                    },
                    {
                        "name": "Akihiro Nakano"
                    },
                    {
                        "name": "Vedant Shah"
                    },
                    {
                        "name": "Aniket Rajiv Didolkar"
                    },
                    {
                        "name": "Dianbo Liu"
                    },
                    {
                        "name": "Anirudh Goyal"
                    },
                    {
                        "name": "Justin Dauwels"
                    }
                ],
                "author_detail": {
                    "name": "Justin Dauwels"
                },
                "author": "Justin Dauwels",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07836v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07836v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19669v1",
                "updated": "2025-10-22T15:16:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    16,
                    6,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T15:16:06Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    16,
                    6,
                    2,
                    295,
                    0
                ],
                "title": "DiffAdapt: Difficulty-Adaptive Reasoning for Token-Efficient LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffAdapt: Difficulty-Adaptive Reasoning for Token-Efficient LLM\n  Inference"
                },
                "summary": "Recent reasoning Large Language Models (LLMs) demonstrate remarkable\nproblem-solving abilities but often generate long thinking traces whose utility\nis unclear. Our work aims to improve their efficiency, enabling them to reach\nhigh performance without overthinking. First, we analyze the entropy of token\nprobabilities in reasoning traces. Across three models, we observe a consistent\nU-shaped entropy pattern: high entropy on easy problems despite high accuracy,\nlow entropy on problems with medium difficulty, and high entropy on hard\nproblems reflecting uncertainty. Specifically, we notice 22--25\\% entropy\nreduction from easy to medium difficulty regions, suggesting an {overthinking}\nphenomenon on easy instances. Building on these insights, we introduce\n\\textbf{DiffAdapt}, a lightweight framework that selects Easy/Normal/Hard\ninference strategies per question based on their difficulty and reasoning trace\nentropy. Each inference strategy consists of a fixed prompt, temperature and\nmaximum token length. In contrast to existing efficiency optimization methods,\nour approach does not fine-tune base LLM but a small probe that classifies\nLLM's final hidden state, allowing inexpensive adaptation. We comprehensively\nevaluate our method on five models and eight benchmarks. Our method achieves\ncomparable or improved accuracy while reducing token usage by up to 22.4\\%,\nestablishing a practical path toward compute-efficient reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent reasoning Large Language Models (LLMs) demonstrate remarkable\nproblem-solving abilities but often generate long thinking traces whose utility\nis unclear. Our work aims to improve their efficiency, enabling them to reach\nhigh performance without overthinking. First, we analyze the entropy of token\nprobabilities in reasoning traces. Across three models, we observe a consistent\nU-shaped entropy pattern: high entropy on easy problems despite high accuracy,\nlow entropy on problems with medium difficulty, and high entropy on hard\nproblems reflecting uncertainty. Specifically, we notice 22--25\\% entropy\nreduction from easy to medium difficulty regions, suggesting an {overthinking}\nphenomenon on easy instances. Building on these insights, we introduce\n\\textbf{DiffAdapt}, a lightweight framework that selects Easy/Normal/Hard\ninference strategies per question based on their difficulty and reasoning trace\nentropy. Each inference strategy consists of a fixed prompt, temperature and\nmaximum token length. In contrast to existing efficiency optimization methods,\nour approach does not fine-tune base LLM but a small probe that classifies\nLLM's final hidden state, allowing inexpensive adaptation. We comprehensively\nevaluate our method on five models and eight benchmarks. Our method achieves\ncomparable or improved accuracy while reducing token usage by up to 22.4\\%,\nestablishing a practical path toward compute-efficient reasoning."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Eunsol Choi"
                    }
                ],
                "author_detail": {
                    "name": "Eunsol Choi"
                },
                "author": "Eunsol Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19668v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19668v1",
                "updated": "2025-10-22T15:13:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    13,
                    52,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T15:13:52Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    13,
                    52,
                    2,
                    295,
                    0
                ],
                "title": "Unraveling Emotions with Pre-Trained Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unraveling Emotions with Pre-Trained Models"
                },
                "summary": "Transformer models have significantly advanced the field of emotion\nrecognition. However, there are still open challenges when exploring open-ended\nqueries for Large Language Models (LLMs). Although current models offer good\nresults, automatic emotion analysis in open texts presents significant\nchallenges, such as contextual ambiguity, linguistic variability, and\ndifficulty interpreting complex emotional expressions. These limitations make\nthe direct application of generalist models difficult. Accordingly, this work\ncompares the effectiveness of fine-tuning and prompt engineering in emotion\ndetection in three distinct scenarios: (i) performance of fine-tuned\npre-trained models and general-purpose LLMs using simple prompts; (ii)\neffectiveness of different emotion prompt designs with LLMs; and (iii) impact\nof emotion grouping techniques on these models. Experimental tests attain\nmetrics above 70% with a fine-tuned pre-trained model for emotion recognition.\nMoreover, the findings highlight that LLMs require structured prompt\nengineering and emotion grouping to enhance their performance. These\nadvancements improve sentiment analysis, human-computer interaction, and\nunderstanding of user behavior across various domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer models have significantly advanced the field of emotion\nrecognition. However, there are still open challenges when exploring open-ended\nqueries for Large Language Models (LLMs). Although current models offer good\nresults, automatic emotion analysis in open texts presents significant\nchallenges, such as contextual ambiguity, linguistic variability, and\ndifficulty interpreting complex emotional expressions. These limitations make\nthe direct application of generalist models difficult. Accordingly, this work\ncompares the effectiveness of fine-tuning and prompt engineering in emotion\ndetection in three distinct scenarios: (i) performance of fine-tuned\npre-trained models and general-purpose LLMs using simple prompts; (ii)\neffectiveness of different emotion prompt designs with LLMs; and (iii) impact\nof emotion grouping techniques on these models. Experimental tests attain\nmetrics above 70% with a fine-tuned pre-trained model for emotion recognition.\nMoreover, the findings highlight that LLMs require structured prompt\nengineering and emotion grouping to enhance their performance. These\nadvancements improve sentiment analysis, human-computer interaction, and\nunderstanding of user behavior across various domains."
                },
                "authors": [
                    {
                        "name": "Alejandro Pajón-Sanmartín"
                    },
                    {
                        "name": "Francisco De Arriba-Pérez"
                    },
                    {
                        "name": "Silvia García-Méndez"
                    },
                    {
                        "name": "Fátima Leal"
                    },
                    {
                        "name": "Benedita Malheiro"
                    },
                    {
                        "name": "Juan Carlos Burguillo-Rial"
                    }
                ],
                "author_detail": {
                    "name": "Juan Carlos Burguillo-Rial"
                },
                "author": "Juan Carlos Burguillo-Rial",
                "arxiv_doi": "10.1109/ACCESS.2025.3623877",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ACCESS.2025.3623877",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.19668v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19668v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06884v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06884v2",
                "updated": "2025-10-22T15:07:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    7,
                    2,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-08T10:57:59Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    10,
                    57,
                    59,
                    2,
                    281,
                    0
                ],
                "title": "Memory-Augmented Generative AI for Real-time Wireless Prediction in\n  Dynamic Industrial Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Augmented Generative AI for Real-time Wireless Prediction in\n  Dynamic Industrial Environments"
                },
                "summary": "Accurate and real-time prediction of wireless channel conditions,\nparticularly the Signal-to-Interference-plus-Noise Ratio (SINR), is a\nfoundational requirement for enabling Ultra-Reliable Low-Latency Communication\n(URLLC) in highly dynamic Industry 4.0 environments. Traditional physics-based\nor statistical models fail to cope with the spatio-temporal complexities\nintroduced by mobile obstacles and transient interference inherent to smart\nwarehouses. To address this, we introduce Evo-WISVA (Evolutionary Wireless\nInfrastructure for Smart Warehouse using VAE), a novel synergistic deep\nlearning architecture that functions as a lightweight 2D predictive digital\ntwin of the radio environment. Evo-WISVA integrates a memory-augmented\nVariational Autoencoder (VAE) featuring an Attention-driven Latent Memory\nModule (LMM) for robust, context-aware spatial feature extraction, with a\nConvolutional Long Short-Term Memory (ConvLSTM) network for precise temporal\nforecasting and sequential refinement. The entire pipeline is optimized\nend-to-end via a joint loss function, ensuring optimal feature alignment\nbetween the generative and predictive components. Rigorous experimental\nevaluation conducted on a high-fidelity ns-3-generated industrial warehouse\ndataset demonstrates that Evo-WISVA significantly surpasses state-of-the-art\nbaselines, achieving up to a 47.6\\% reduction in average reconstruction error.\nCrucially, the model exhibits exceptional generalization capacity to unseen\nenvironments with vastly increased dynamic complexity (up to ten simultaneously\nmoving obstacles) while maintaining amortized computational efficiency\nessential for real-time deployment. Evo-WISVA establishes a foundational\ntechnology for proactive wireless resource management, enabling autonomous\noptimization and advancing the realization of predictive digital twins in\nindustrial communication networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate and real-time prediction of wireless channel conditions,\nparticularly the Signal-to-Interference-plus-Noise Ratio (SINR), is a\nfoundational requirement for enabling Ultra-Reliable Low-Latency Communication\n(URLLC) in highly dynamic Industry 4.0 environments. Traditional physics-based\nor statistical models fail to cope with the spatio-temporal complexities\nintroduced by mobile obstacles and transient interference inherent to smart\nwarehouses. To address this, we introduce Evo-WISVA (Evolutionary Wireless\nInfrastructure for Smart Warehouse using VAE), a novel synergistic deep\nlearning architecture that functions as a lightweight 2D predictive digital\ntwin of the radio environment. Evo-WISVA integrates a memory-augmented\nVariational Autoencoder (VAE) featuring an Attention-driven Latent Memory\nModule (LMM) for robust, context-aware spatial feature extraction, with a\nConvolutional Long Short-Term Memory (ConvLSTM) network for precise temporal\nforecasting and sequential refinement. The entire pipeline is optimized\nend-to-end via a joint loss function, ensuring optimal feature alignment\nbetween the generative and predictive components. Rigorous experimental\nevaluation conducted on a high-fidelity ns-3-generated industrial warehouse\ndataset demonstrates that Evo-WISVA significantly surpasses state-of-the-art\nbaselines, achieving up to a 47.6\\% reduction in average reconstruction error.\nCrucially, the model exhibits exceptional generalization capacity to unseen\nenvironments with vastly increased dynamic complexity (up to ten simultaneously\nmoving obstacles) while maintaining amortized computational efficiency\nessential for real-time deployment. Evo-WISVA establishes a foundational\ntechnology for proactive wireless resource management, enabling autonomous\noptimization and advancing the realization of predictive digital twins in\nindustrial communication networks."
                },
                "authors": [
                    {
                        "name": "Rahul Gulia"
                    },
                    {
                        "name": "Amlan Ganguly"
                    },
                    {
                        "name": "Michael E. Kuhl"
                    },
                    {
                        "name": "Ehsan Rashedi"
                    },
                    {
                        "name": "Clark Hochgraf"
                    }
                ],
                "author_detail": {
                    "name": "Clark Hochgraf"
                },
                "author": "Clark Hochgraf",
                "arxiv_comment": "Found a fundamental error (data leakage) in cross-validation setup\n  affecting both papers. This issue compromises the model training and results,\n  making performance claims unreliable and potentially misleading. We request\n  withdrawal of current versions (v1) to prevent the dissemination of incorrect\n  scientific findings. Corrected versions will be submitted later",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06884v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06884v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19661v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19661v1",
                "updated": "2025-10-22T15:06:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    6,
                    26,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T15:06:26Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    6,
                    26,
                    2,
                    295,
                    0
                ],
                "title": "AgentSense: LLMs Empower Generalizable and Explainable Web-Based\n  Participatory Urban Sensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentSense: LLMs Empower Generalizable and Explainable Web-Based\n  Participatory Urban Sensing"
                },
                "summary": "Web-based participatory urban sensing has emerged as a vital approach for\nmodern urban management by leveraging mobile individuals as distributed\nsensors. However, existing urban sensing systems struggle with limited\ngeneralization across diverse urban scenarios and poor interpretability in\ndecision-making. In this work, we introduce AgentSense, a hybrid, training-free\nframework that integrates large language models (LLMs) into participatory urban\nsensing through a multi-agent evolution system. AgentSense initially employs\nclassical planner to generate baseline solutions and then iteratively refines\nthem to adapt sensing task assignments to dynamic urban conditions and\nheterogeneous worker preferences, while producing natural language explanations\nthat enhance transparency and trust. Extensive experiments across two\nlarge-scale mobility datasets and seven types of dynamic disturbances\ndemonstrate that AgentSense offers distinct advantages in adaptivity and\nexplainability over traditional methods. Furthermore, compared to single-agent\nLLM baselines, our approach outperforms in both performance and robustness,\nwhile delivering more reasonable and transparent explanations. These results\nposition AgentSense as a significant advancement towards deploying adaptive and\nexplainable urban sensing systems on the web.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web-based participatory urban sensing has emerged as a vital approach for\nmodern urban management by leveraging mobile individuals as distributed\nsensors. However, existing urban sensing systems struggle with limited\ngeneralization across diverse urban scenarios and poor interpretability in\ndecision-making. In this work, we introduce AgentSense, a hybrid, training-free\nframework that integrates large language models (LLMs) into participatory urban\nsensing through a multi-agent evolution system. AgentSense initially employs\nclassical planner to generate baseline solutions and then iteratively refines\nthem to adapt sensing task assignments to dynamic urban conditions and\nheterogeneous worker preferences, while producing natural language explanations\nthat enhance transparency and trust. Extensive experiments across two\nlarge-scale mobility datasets and seven types of dynamic disturbances\ndemonstrate that AgentSense offers distinct advantages in adaptivity and\nexplainability over traditional methods. Furthermore, compared to single-agent\nLLM baselines, our approach outperforms in both performance and robustness,\nwhile delivering more reasonable and transparent explanations. These results\nposition AgentSense as a significant advancement towards deploying adaptive and\nexplainable urban sensing systems on the web."
                },
                "authors": [
                    {
                        "name": "Xusen Guo"
                    },
                    {
                        "name": "Mingxing Peng"
                    },
                    {
                        "name": "Xixuan Hao"
                    },
                    {
                        "name": "Xingchen Zou"
                    },
                    {
                        "name": "Qiongyan Wang"
                    },
                    {
                        "name": "Sijie Ruan"
                    },
                    {
                        "name": "Yuxuan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yuxuan Liang"
                },
                "author": "Yuxuan Liang",
                "arxiv_comment": "13 pages, 10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19661v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19655v1",
                "updated": "2025-10-22T14:58:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    58,
                    16,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T14:58:16Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    58,
                    16,
                    2,
                    295,
                    0
                ],
                "title": "LaViRA: Language-Vision-Robot Actions Translation for Zero-Shot Vision\n  Language Navigation in Continuous Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaViRA: Language-Vision-Robot Actions Translation for Zero-Shot Vision\n  Language Navigation in Continuous Environments"
                },
                "summary": "Zero-shot Vision-and-Language Navigation in Continuous Environments (VLN-CE)\nrequires an agent to navigate unseen environments based on natural language\ninstructions without any prior training. Current methods face a critical\ntrade-off: either rely on environment-specific waypoint predictors that limit\nscene generalization, or underutilize the reasoning capabilities of large\nmodels during navigation. We introduce LaViRA, a simple yet effective zero-shot\nframework that addresses this dilemma by decomposing action into a\ncoarse-to-fine hierarchy: Language Action for high-level planning, Vision\nAction for perceptual grounding, and Robot Action for robust navigation. This\nmodular decomposition allows us to leverage the distinct strengths of different\nscales of Multimodal Large Language Models (MLLMs) at each stage, creating a\nsystem that is powerful in its reasoning, grounding and practical control.\nLaViRA significantly outperforms existing state-of-the-art methods on the\nVLN-CE benchmark, demonstrating superior generalization capabilities in unseen\nenvironments, while maintaining transparency and efficiency for real-world\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot Vision-and-Language Navigation in Continuous Environments (VLN-CE)\nrequires an agent to navigate unseen environments based on natural language\ninstructions without any prior training. Current methods face a critical\ntrade-off: either rely on environment-specific waypoint predictors that limit\nscene generalization, or underutilize the reasoning capabilities of large\nmodels during navigation. We introduce LaViRA, a simple yet effective zero-shot\nframework that addresses this dilemma by decomposing action into a\ncoarse-to-fine hierarchy: Language Action for high-level planning, Vision\nAction for perceptual grounding, and Robot Action for robust navigation. This\nmodular decomposition allows us to leverage the distinct strengths of different\nscales of Multimodal Large Language Models (MLLMs) at each stage, creating a\nsystem that is powerful in its reasoning, grounding and practical control.\nLaViRA significantly outperforms existing state-of-the-art methods on the\nVLN-CE benchmark, demonstrating superior generalization capabilities in unseen\nenvironments, while maintaining transparency and efficiency for real-world\ndeployment."
                },
                "authors": [
                    {
                        "name": "Hongyu Ding"
                    },
                    {
                        "name": "Ziming Xu"
                    },
                    {
                        "name": "Yudong Fang"
                    },
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Zixuan Chen"
                    },
                    {
                        "name": "Jieqi Shi"
                    },
                    {
                        "name": "Jing Huo"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yang Gao"
                },
                "author": "Yang Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19644v1",
                "updated": "2025-10-22T14:49:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    49,
                    21,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T14:49:21Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    49,
                    21,
                    2,
                    295,
                    0
                ],
                "title": "LLavaCode: Compressed Code Representations for Retrieval-Augmented Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLavaCode: Compressed Code Representations for Retrieval-Augmented Code\n  Generation"
                },
                "summary": "Retrieval-augmented generation has emerged as one of the most effective\napproaches for code completion, particularly when context from a surrounding\nrepository is essential. However, incorporating context significantly extends\nsequence length, leading to slower inference - a critical limitation for\ninteractive settings such as IDEs. In this work, we introduce LlavaCode, a\nframework that compresses code into compact, semantically rich representations\ninterpretable by code LLM, enhancing generation quality while reducing the\nretrieved context to only a few compressed single-token vectors. Using a small\nprojector module we can significantly increase the EM and ES metrics of coding\nmodel with negligible latency increase. Our experiments demonstrate that\ncompressed context enables 20-38% reduction in Time-to-First-Token (TTFT) on\nline completion tasks compared to full-RAG pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation has emerged as one of the most effective\napproaches for code completion, particularly when context from a surrounding\nrepository is essential. However, incorporating context significantly extends\nsequence length, leading to slower inference - a critical limitation for\ninteractive settings such as IDEs. In this work, we introduce LlavaCode, a\nframework that compresses code into compact, semantically rich representations\ninterpretable by code LLM, enhancing generation quality while reducing the\nretrieved context to only a few compressed single-token vectors. Using a small\nprojector module we can significantly increase the EM and ES metrics of coding\nmodel with negligible latency increase. Our experiments demonstrate that\ncompressed context enables 20-38% reduction in Time-to-First-Token (TTFT) on\nline completion tasks compared to full-RAG pipelines."
                },
                "authors": [
                    {
                        "name": "Daria Cherniuk"
                    },
                    {
                        "name": "Nikita Sukhorukov"
                    },
                    {
                        "name": "Nikita Sushko"
                    },
                    {
                        "name": "Daniil Gusak"
                    },
                    {
                        "name": "Danil Sivtsov"
                    },
                    {
                        "name": "Elena Tutubalina"
                    },
                    {
                        "name": "Evgeny Frolov"
                    }
                ],
                "author_detail": {
                    "name": "Evgeny Frolov"
                },
                "author": "Evgeny Frolov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19641v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19641v1",
                "updated": "2025-10-22T14:40:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    40,
                    24,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T14:40:24Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    40,
                    24,
                    2,
                    295,
                    0
                ],
                "title": "Style Attack Disguise: When Fonts Become a Camouflage for Adversarial\n  Intent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Style Attack Disguise: When Fonts Become a Camouflage for Adversarial\n  Intent"
                },
                "summary": "With social media growth, users employ stylistic fonts and font-like emoji to\nexpress individuality, creating visually appealing text that remains\nhuman-readable. However, these fonts introduce hidden vulnerabilities in NLP\nmodels: while humans easily read stylistic text, models process these\ncharacters as distinct tokens, causing interference. We identify this\nhuman-model perception gap and propose a style-based attack, Style Attack\nDisguise (SAD). We design two sizes: light for query efficiency and strong for\nsuperior attack performance. Experiments on sentiment classification and\nmachine translation across traditional models, LLMs, and commercial services\ndemonstrate SAD's strong attack performance. We also show SAD's potential\nthreats to multimodal tasks including text-to-image and text-to-speech\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With social media growth, users employ stylistic fonts and font-like emoji to\nexpress individuality, creating visually appealing text that remains\nhuman-readable. However, these fonts introduce hidden vulnerabilities in NLP\nmodels: while humans easily read stylistic text, models process these\ncharacters as distinct tokens, causing interference. We identify this\nhuman-model perception gap and propose a style-based attack, Style Attack\nDisguise (SAD). We design two sizes: light for query efficiency and strong for\nsuperior attack performance. Experiments on sentiment classification and\nmachine translation across traditional models, LLMs, and commercial services\ndemonstrate SAD's strong attack performance. We also show SAD's potential\nthreats to multimodal tasks including text-to-image and text-to-speech\ngeneration."
                },
                "authors": [
                    {
                        "name": "Yangshijie Zhang"
                    },
                    {
                        "name": "Xinda Wang"
                    },
                    {
                        "name": "Jialin Liu"
                    },
                    {
                        "name": "Wenqiang Wang"
                    },
                    {
                        "name": "Zhicong Ma"
                    },
                    {
                        "name": "Xingxing Jia"
                    }
                ],
                "author_detail": {
                    "name": "Xingxing Jia"
                },
                "author": "Xingxing Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19641v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10088v2",
                "updated": "2025-10-22T14:34:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    34,
                    49,
                    2,
                    295,
                    0
                ],
                "published": "2025-01-17T10:15:03Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    10,
                    15,
                    3,
                    4,
                    17,
                    0
                ],
                "title": "A recursive Bayesian neural network for constitutive modeling of sands\n  under monotonic and cyclic loading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A recursive Bayesian neural network for constitutive modeling of sands\n  under monotonic and cyclic loading"
                },
                "summary": "In geotechnical engineering, constitutive models are central to capturing\nsoil behavior across diverse drainage conditions, stress paths,and loading\nhistories. While data driven deep learning (DL) approaches have shown promise\nas alternatives to traditional constitutive formulations, their deployment\nrequires models that are both accurate and capable of quantifying predictive\nuncertainty. This study introduces a recursive Bayesian neural network (rBNN)\nframework that unifies temporal sequence learning with generalized Bayesian\ninference to achieve both predictive accuracy and rigorous uncertainty\nquantification. A key innovation is the incorporation of a sliding window\nrecursive structure that enables the model to effectively capture path\ndependent soil responses under monotonic and cyclic loading. By treating\nnetwork parameters as random variables and inferring their posterior\ndistributions via generalized variational inference, the rBNN produces well\ncalibrated confidence intervals alongside point predictions.The framework is\nvalidated against four datasets spanning both simulated and experimental\ntriaxial tests: monotonic loading using a Hardening Soil model simulation and\n28 CD tests on Baskarp sand, and cyclic loading using an exponential\nconstitutive simulation of CD CU tests and 37 experimental cyclic CU tests on\nOttawa F65 sand. This progression from monotonic to cyclic and from simulated\nto experimental data demonstrates the adaptability of the proposed approach\nacross varying levels of data fidelity and complexity. Comparative analyses\nwith LSTM, Encoder Decoder,and GRU architectures highlight that rBNN not only\nachieves competitive predictive accuracy but also provides reliable confidence\nintervals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In geotechnical engineering, constitutive models are central to capturing\nsoil behavior across diverse drainage conditions, stress paths,and loading\nhistories. While data driven deep learning (DL) approaches have shown promise\nas alternatives to traditional constitutive formulations, their deployment\nrequires models that are both accurate and capable of quantifying predictive\nuncertainty. This study introduces a recursive Bayesian neural network (rBNN)\nframework that unifies temporal sequence learning with generalized Bayesian\ninference to achieve both predictive accuracy and rigorous uncertainty\nquantification. A key innovation is the incorporation of a sliding window\nrecursive structure that enables the model to effectively capture path\ndependent soil responses under monotonic and cyclic loading. By treating\nnetwork parameters as random variables and inferring their posterior\ndistributions via generalized variational inference, the rBNN produces well\ncalibrated confidence intervals alongside point predictions.The framework is\nvalidated against four datasets spanning both simulated and experimental\ntriaxial tests: monotonic loading using a Hardening Soil model simulation and\n28 CD tests on Baskarp sand, and cyclic loading using an exponential\nconstitutive simulation of CD CU tests and 37 experimental cyclic CU tests on\nOttawa F65 sand. This progression from monotonic to cyclic and from simulated\nto experimental data demonstrates the adaptability of the proposed approach\nacross varying levels of data fidelity and complexity. Comparative analyses\nwith LSTM, Encoder Decoder,and GRU architectures highlight that rBNN not only\nachieves competitive predictive accuracy but also provides reliable confidence\nintervals."
                },
                "authors": [
                    {
                        "name": "Toiba Noor"
                    },
                    {
                        "name": "Soban Nasir Lone"
                    },
                    {
                        "name": "G. V. Ramana"
                    },
                    {
                        "name": "Rajdip Nayek"
                    }
                ],
                "author_detail": {
                    "name": "Rajdip Nayek"
                },
                "author": "Rajdip Nayek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08666v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08666v3",
                "updated": "2025-10-22T14:33:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    33,
                    49,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-09T16:19:42Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    19,
                    42,
                    3,
                    282,
                    0
                ],
                "title": "dInfer: An Efficient Inference Framework for Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "dInfer: An Efficient Inference Framework for Diffusion Language Models"
                },
                "summary": "Diffusion-based large language models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs, leveraging denoising-based generation\nto enable inherent parallelism. Even more and more open-sourced dLLM models\nemerge, yet their widespread adoption remains constrained by the lack of a\nstandardized and efficient inference framework. We present dInfer, an efficient\nand extensible framework for dLLM inference. dInfer decomposes the inference\npipeline into four modular components--model, diffusion iteration manager,\ndecoding strategy, and KV-cache manager--and integrates novel algorithms for\neach component alongside system-level optimizations. Through this combination\nof algorithmic innovations and system enhancements, dInfer achieves substantial\nefficiency gains without compromising output quality on LLaDA-MoE. At batch\nsize 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800\ntokens per second across six benchmarks on $8\\times$ H800 GPUs. Compared to\nprior systems, dInfer delivers a $10\\times$ speedup over Fast-dLLM while\nmaintaining similar model performance. Even compared to the AR model (with a\ncomparable number of activation parameters and performance) QWen2.5-3B, which\nis highly optimized with the latest vLLM inference engine, dInfer still\ndelivers a $2$-$3\\times$ speedup. The implementation of dInfer is open-sourced\nat https://github.com/inclusionAI/dInfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs, leveraging denoising-based generation\nto enable inherent parallelism. Even more and more open-sourced dLLM models\nemerge, yet their widespread adoption remains constrained by the lack of a\nstandardized and efficient inference framework. We present dInfer, an efficient\nand extensible framework for dLLM inference. dInfer decomposes the inference\npipeline into four modular components--model, diffusion iteration manager,\ndecoding strategy, and KV-cache manager--and integrates novel algorithms for\neach component alongside system-level optimizations. Through this combination\nof algorithmic innovations and system enhancements, dInfer achieves substantial\nefficiency gains without compromising output quality on LLaDA-MoE. At batch\nsize 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800\ntokens per second across six benchmarks on $8\\times$ H800 GPUs. Compared to\nprior systems, dInfer delivers a $10\\times$ speedup over Fast-dLLM while\nmaintaining similar model performance. Even compared to the AR model (with a\ncomparable number of activation parameters and performance) QWen2.5-3B, which\nis highly optimized with the latest vLLM inference engine, dInfer still\ndelivers a $2$-$3\\times$ speedup. The implementation of dInfer is open-sourced\nat https://github.com/inclusionAI/dInfer."
                },
                "authors": [
                    {
                        "name": "Yuxin Ma"
                    },
                    {
                        "name": "Lun Du"
                    },
                    {
                        "name": "Lanning Wei"
                    },
                    {
                        "name": "Kun Chen"
                    },
                    {
                        "name": "Qian Xu"
                    },
                    {
                        "name": "Kangyu Wang"
                    },
                    {
                        "name": "Guofeng Feng"
                    },
                    {
                        "name": "Guoshan Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Xiaojing Qi"
                    },
                    {
                        "name": "Xinyuan Zhang"
                    },
                    {
                        "name": "Zhen Tao"
                    },
                    {
                        "name": "Haibo Feng"
                    },
                    {
                        "name": "Ziyun Jiang"
                    },
                    {
                        "name": "Ying Xu"
                    },
                    {
                        "name": "Zenan Huang"
                    },
                    {
                        "name": "Yihong Zhuang"
                    },
                    {
                        "name": "Haokai Xu"
                    },
                    {
                        "name": "Jiaqi Hu"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    },
                    {
                        "name": "Junbo Zhao"
                    },
                    {
                        "name": "Jianguo Li"
                    },
                    {
                        "name": "Da Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Da Zheng"
                },
                "author": "Da Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08666v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08666v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19631v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19631v1",
                "updated": "2025-10-22T14:28:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    28,
                    33,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T14:28:33Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    28,
                    33,
                    2,
                    295,
                    0
                ],
                "title": "HSCodeComp: A Realistic and Expert-level Benchmark for Deep Search\n  Agents in Hierarchical Rule Application",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HSCodeComp: A Realistic and Expert-level Benchmark for Deep Search\n  Agents in Hierarchical Rule Application"
                },
                "summary": "Effective deep search agents must not only access open-domain and\ndomain-specific knowledge but also apply complex rules-such as legal clauses,\nmedical manuals and tariff rules. These rules often feature vague boundaries\nand implicit logic relationships, making precise application challenging for\nagents. However, this critical capability is largely overlooked by current\nagent benchmarks.\n  To fill this gap, we introduce HSCodeComp, the first realistic, expert-level\ne-commerce benchmark designed to evaluate deep search agents in hierarchical\nrule application. In this task, the deep reasoning process of agents is guided\nby these rules to predict 10-digit Harmonized System Code (HSCode) of products\nwith noisy but realistic descriptions. These codes, established by the World\nCustoms Organization, are vital for global supply chain efficiency. Built from\nreal-world data collected from large-scale e-commerce platforms, our proposed\nHSCodeComp comprises 632 product entries spanning diverse product categories,\nwith these HSCodes annotated by several human experts.\n  Extensive experimental results on several state-of-the-art LLMs, open-source,\nand closed-source agents reveal a huge performance gap: best agent achieves\nonly 46.8% 10-digit accuracy, far below human experts at 95.0%. Besides,\ndetailed analysis demonstrates the challenges of hierarchical rule application,\nand test-time scaling fails to improve performance further.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective deep search agents must not only access open-domain and\ndomain-specific knowledge but also apply complex rules-such as legal clauses,\nmedical manuals and tariff rules. These rules often feature vague boundaries\nand implicit logic relationships, making precise application challenging for\nagents. However, this critical capability is largely overlooked by current\nagent benchmarks.\n  To fill this gap, we introduce HSCodeComp, the first realistic, expert-level\ne-commerce benchmark designed to evaluate deep search agents in hierarchical\nrule application. In this task, the deep reasoning process of agents is guided\nby these rules to predict 10-digit Harmonized System Code (HSCode) of products\nwith noisy but realistic descriptions. These codes, established by the World\nCustoms Organization, are vital for global supply chain efficiency. Built from\nreal-world data collected from large-scale e-commerce platforms, our proposed\nHSCodeComp comprises 632 product entries spanning diverse product categories,\nwith these HSCodes annotated by several human experts.\n  Extensive experimental results on several state-of-the-art LLMs, open-source,\nand closed-source agents reveal a huge performance gap: best agent achieves\nonly 46.8% 10-digit accuracy, far below human experts at 95.0%. Besides,\ndetailed analysis demonstrates the challenges of hierarchical rule application,\nand test-time scaling fails to improve performance further."
                },
                "authors": [
                    {
                        "name": "Yiqian Yang"
                    },
                    {
                        "name": "Tian Lan"
                    },
                    {
                        "name": "Qianghuai Jia"
                    },
                    {
                        "name": "Li Zhu"
                    },
                    {
                        "name": "Hui Jiang"
                    },
                    {
                        "name": "Hang Zhu"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Weihua Luo"
                    },
                    {
                        "name": "Kaifu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kaifu Zhang"
                },
                "author": "Kaifu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19631v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19631v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19628v1",
                "updated": "2025-10-22T14:23:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    23,
                    50,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T14:23:50Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    23,
                    50,
                    2,
                    295,
                    0
                ],
                "title": "CrossNews-UA: A Cross-lingual News Semantic Similarity Benchmark for\n  Ukrainian, Polish, Russian, and English",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CrossNews-UA: A Cross-lingual News Semantic Similarity Benchmark for\n  Ukrainian, Polish, Russian, and English"
                },
                "summary": "In the era of social networks and rapid misinformation spread, news analysis\nremains a critical task. Detecting fake news across multiple languages,\nparticularly beyond English, poses significant challenges. Cross-lingual news\ncomparison offers a promising approach to verify information by leveraging\nexternal sources in different languages (Chen and Shu, 2024). However, existing\ndatasets for cross-lingual news analysis (Chen et al., 2022a) were manually\ncurated by journalists and experts, limiting their scalability and adaptability\nto new languages. In this work, we address this gap by introducing a scalable,\nexplainable crowdsourcing pipeline for cross-lingual news similarity\nassessment. Using this pipeline, we collected a novel dataset CrossNews-UA of\nnews pairs in Ukrainian as a central language with linguistically and\ncontextually relevant languages-Polish, Russian, and English. Each news pair is\nannotated for semantic similarity with detailed justifications based on the 4W\ncriteria (Who, What, Where, When). We further tested a range of models, from\ntraditional bag-of-words, Transformer-based architectures to large language\nmodels (LLMs). Our results highlight the challenges in multilingual news\nanalysis and offer insights into models performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of social networks and rapid misinformation spread, news analysis\nremains a critical task. Detecting fake news across multiple languages,\nparticularly beyond English, poses significant challenges. Cross-lingual news\ncomparison offers a promising approach to verify information by leveraging\nexternal sources in different languages (Chen and Shu, 2024). However, existing\ndatasets for cross-lingual news analysis (Chen et al., 2022a) were manually\ncurated by journalists and experts, limiting their scalability and adaptability\nto new languages. In this work, we address this gap by introducing a scalable,\nexplainable crowdsourcing pipeline for cross-lingual news similarity\nassessment. Using this pipeline, we collected a novel dataset CrossNews-UA of\nnews pairs in Ukrainian as a central language with linguistically and\ncontextually relevant languages-Polish, Russian, and English. Each news pair is\nannotated for semantic similarity with detailed justifications based on the 4W\ncriteria (Who, What, Where, When). We further tested a range of models, from\ntraditional bag-of-words, Transformer-based architectures to large language\nmodels (LLMs). Our results highlight the challenges in multilingual news\nanalysis and offer insights into models performance."
                },
                "authors": [
                    {
                        "name": "Daryna Dementieva"
                    },
                    {
                        "name": "Evgeniya Sukhodolskaya"
                    },
                    {
                        "name": "Alexander Fraser"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Fraser"
                },
                "author": "Alexander Fraser",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17138v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17138v2",
                "updated": "2025-10-22T14:21:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    21,
                    7,
                    2,
                    295,
                    0
                ],
                "published": "2025-09-21T16:02:42Z",
                "published_parsed": [
                    2025,
                    9,
                    21,
                    16,
                    2,
                    42,
                    6,
                    264,
                    0
                ],
                "title": "Analyzing Memory Effects in Large Language Models through the lens of\n  Cognitive Psychology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing Memory Effects in Large Language Models through the lens of\n  Cognitive Psychology"
                },
                "summary": "Memory, a fundamental component of human cognition, exhibits adaptive yet\nfallible characteristics as illustrated by Schacter's memory \"sins\".These\ncognitive phenomena have been studied extensively in psychology and\nneuroscience, but the extent to which artificial systems, specifically Large\nLanguage Models (LLMs), emulate these cognitive phenomena remains\nunderexplored. This study uses human memory research as a lens for\nunderstanding LLMs and systematically investigates human memory effects in\nstate-of-the-art LLMs using paradigms drawn from psychological research. We\nevaluate seven key memory phenomena, comparing human behavior to LLM\nperformance. Both people and models remember less when overloaded with\ninformation (list length effect) and remember better with repeated exposure\n(list strength effect). They also show similar difficulties when retrieving\noverlapping information, where storing too many similar facts leads to\nconfusion (fan effect). Like humans, LLMs are susceptible to falsely\n\"remembering\" words that were never shown but are related to others (false\nmemories), and they can apply prior learning to new, related situations\n(cross-domain generalization). However, LLMs differ in two key ways: they are\nless influenced by the order in which information is presented (positional\nbias) and more robust when processing random or meaningless material (nonsense\neffect). These results reveal both alignments and divergences in how LLMs and\nhumans reconstruct memory. The findings help clarify how memory-like behavior\nin LLMs echoes core features of human cognition, while also highlighting the\narchitectural differences that lead to distinct patterns of error and success.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory, a fundamental component of human cognition, exhibits adaptive yet\nfallible characteristics as illustrated by Schacter's memory \"sins\".These\ncognitive phenomena have been studied extensively in psychology and\nneuroscience, but the extent to which artificial systems, specifically Large\nLanguage Models (LLMs), emulate these cognitive phenomena remains\nunderexplored. This study uses human memory research as a lens for\nunderstanding LLMs and systematically investigates human memory effects in\nstate-of-the-art LLMs using paradigms drawn from psychological research. We\nevaluate seven key memory phenomena, comparing human behavior to LLM\nperformance. Both people and models remember less when overloaded with\ninformation (list length effect) and remember better with repeated exposure\n(list strength effect). They also show similar difficulties when retrieving\noverlapping information, where storing too many similar facts leads to\nconfusion (fan effect). Like humans, LLMs are susceptible to falsely\n\"remembering\" words that were never shown but are related to others (false\nmemories), and they can apply prior learning to new, related situations\n(cross-domain generalization). However, LLMs differ in two key ways: they are\nless influenced by the order in which information is presented (positional\nbias) and more robust when processing random or meaningless material (nonsense\neffect). These results reveal both alignments and divergences in how LLMs and\nhumans reconstruct memory. The findings help clarify how memory-like behavior\nin LLMs echoes core features of human cognition, while also highlighting the\narchitectural differences that lead to distinct patterns of error and success."
                },
                "authors": [
                    {
                        "name": "Zhaoyang Cao"
                    },
                    {
                        "name": "Lael Schooler"
                    },
                    {
                        "name": "Reza Zafarani"
                    }
                ],
                "author_detail": {
                    "name": "Reza Zafarani"
                },
                "author": "Reza Zafarani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17138v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17138v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19616v1",
                "updated": "2025-10-22T14:12:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    12,
                    0,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T14:12:00Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    12,
                    0,
                    2,
                    295,
                    0
                ],
                "title": "PBBQ: A Persian Bias Benchmark Dataset Curated with Human-AI\n  Collaboration for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PBBQ: A Persian Bias Benchmark Dataset Curated with Human-AI\n  Collaboration for Large Language Models"
                },
                "summary": "With the increasing adoption of large language models (LLMs), ensuring their\nalignment with social norms has become a critical concern. While prior research\nhas examined bias detection in various languages, there remains a significant\ngap in resources addressing social biases within Persian cultural contexts. In\nthis work, we introduce PBBQ, a comprehensive benchmark dataset designed to\nevaluate social biases in Persian LLMs. Our benchmark, which encompasses 16\ncultural categories, was developed through questionnaires completed by 250\ndiverse individuals across multiple demographics, in close collaboration with\nsocial science experts to ensure its validity. The resulting PBBQ dataset\ncontains over 37,000 carefully curated questions, providing a foundation for\nthe evaluation and mitigation of bias in Persian language models. We benchmark\nseveral open-source LLMs, a closed-source model, and Persian-specific\nfine-tuned models on PBBQ. Our findings reveal that current LLMs exhibit\nsignificant social biases across Persian culture. Additionally, by comparing\nmodel outputs to human responses, we observe that LLMs often replicate human\nbias patterns, highlighting the complex interplay between learned\nrepresentations and cultural stereotypes.Upon acceptance of the paper, our PBBQ\ndataset will be publicly available for use in future work. Content warning:\nThis paper contains unsafe content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing adoption of large language models (LLMs), ensuring their\nalignment with social norms has become a critical concern. While prior research\nhas examined bias detection in various languages, there remains a significant\ngap in resources addressing social biases within Persian cultural contexts. In\nthis work, we introduce PBBQ, a comprehensive benchmark dataset designed to\nevaluate social biases in Persian LLMs. Our benchmark, which encompasses 16\ncultural categories, was developed through questionnaires completed by 250\ndiverse individuals across multiple demographics, in close collaboration with\nsocial science experts to ensure its validity. The resulting PBBQ dataset\ncontains over 37,000 carefully curated questions, providing a foundation for\nthe evaluation and mitigation of bias in Persian language models. We benchmark\nseveral open-source LLMs, a closed-source model, and Persian-specific\nfine-tuned models on PBBQ. Our findings reveal that current LLMs exhibit\nsignificant social biases across Persian culture. Additionally, by comparing\nmodel outputs to human responses, we observe that LLMs often replicate human\nbias patterns, highlighting the complex interplay between learned\nrepresentations and cultural stereotypes.Upon acceptance of the paper, our PBBQ\ndataset will be publicly available for use in future work. Content warning:\nThis paper contains unsafe content."
                },
                "authors": [
                    {
                        "name": "Farhan Farsi"
                    },
                    {
                        "name": "Shayan Bali"
                    },
                    {
                        "name": "Fatemeh Valeh"
                    },
                    {
                        "name": "Parsa Ghofrani"
                    },
                    {
                        "name": "Alireza Pakniat"
                    },
                    {
                        "name": "Kian Kashfipour"
                    },
                    {
                        "name": "Amir H. Payberah"
                    }
                ],
                "author_detail": {
                    "name": "Amir H. Payberah"
                },
                "author": "Amir H. Payberah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19615v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19615v1",
                "updated": "2025-10-22T14:11:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    11,
                    44,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T14:11:44Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    11,
                    44,
                    2,
                    295,
                    0
                ],
                "title": "FidelityGPT: Correcting Decompilation Distortions with Retrieval\n  Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FidelityGPT: Correcting Decompilation Distortions with Retrieval\n  Augmented Generation"
                },
                "summary": "Decompilation converts machine code into human-readable form, enabling\nanalysis and debugging without source code. However, fidelity issues often\ndegrade the readability and semantic accuracy of decompiled output. Existing\nmethods, such as variable renaming or structural simplification, provide\npartial improvements but lack robust detection and correction, particularly for\ncomplex closed-source binaries. We present FidelityGPT, a framework that\nenhances decompiled code accuracy and readability by systematically detecting\nand correcting semantic distortions. FidelityGPT introduces distortion-aware\nprompt templates tailored to closed-source settings and integrates\nRetrieval-Augmented Generation (RAG) with a dynamic semantic intensity\nalgorithm to locate distorted lines and retrieve semantically similar code from\na database. A variable dependency algorithm further mitigates long-context\nlimitations by analyzing redundant variables and integrating their dependencies\ninto the prompt context. Evaluated on 620 function pairs from a binary\nsimilarity benchmark, FidelityGPT achieved an average detection accuracy of 89%\nand a precision of 83%. Compared to the state-of-the-art DeGPT (Fix Rate 83%,\nCorrected Fix Rate 37%), FidelityGPT attained 94% FR and 64% CFR, demonstrating\nsignificant gains in accuracy and readability. These results highlight its\npotential to advance LLM-based decompilation and reverse engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decompilation converts machine code into human-readable form, enabling\nanalysis and debugging without source code. However, fidelity issues often\ndegrade the readability and semantic accuracy of decompiled output. Existing\nmethods, such as variable renaming or structural simplification, provide\npartial improvements but lack robust detection and correction, particularly for\ncomplex closed-source binaries. We present FidelityGPT, a framework that\nenhances decompiled code accuracy and readability by systematically detecting\nand correcting semantic distortions. FidelityGPT introduces distortion-aware\nprompt templates tailored to closed-source settings and integrates\nRetrieval-Augmented Generation (RAG) with a dynamic semantic intensity\nalgorithm to locate distorted lines and retrieve semantically similar code from\na database. A variable dependency algorithm further mitigates long-context\nlimitations by analyzing redundant variables and integrating their dependencies\ninto the prompt context. Evaluated on 620 function pairs from a binary\nsimilarity benchmark, FidelityGPT achieved an average detection accuracy of 89%\nand a precision of 83%. Compared to the state-of-the-art DeGPT (Fix Rate 83%,\nCorrected Fix Rate 37%), FidelityGPT attained 94% FR and 64% CFR, demonstrating\nsignificant gains in accuracy and readability. These results highlight its\npotential to advance LLM-based decompilation and reverse engineering."
                },
                "authors": [
                    {
                        "name": "Zhiping Zhou"
                    },
                    {
                        "name": "Xiaohong Li"
                    },
                    {
                        "name": "Ruitao Feng"
                    },
                    {
                        "name": "Yao Zhang"
                    },
                    {
                        "name": "Yuekang Li"
                    },
                    {
                        "name": "Wenbu Feng"
                    },
                    {
                        "name": "Yunqian Wang"
                    },
                    {
                        "name": "Yuqing Li"
                    }
                ],
                "author_detail": {
                    "name": "Yuqing Li"
                },
                "author": "Yuqing Li",
                "arxiv_doi": "10.14722/ndss.2026.230989",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.14722/ndss.2026.230989",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.19615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19615v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15268v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15268v2",
                "updated": "2025-10-22T14:04:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    4,
                    44,
                    2,
                    295,
                    0
                ],
                "published": "2025-07-21T06:13:53Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    6,
                    13,
                    53,
                    0,
                    202,
                    0
                ],
                "title": "IM-Chat: A Multi-agent LLM Framework Integrating Tool-Calling and\n  Diffusion Modeling for Knowledge Transfer in Injection Molding Industry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IM-Chat: A Multi-agent LLM Framework Integrating Tool-Calling and\n  Diffusion Modeling for Knowledge Transfer in Injection Molding Industry"
                },
                "summary": "The injection molding industry faces critical challenges in preserving and\ntransferring field knowledge, particularly as experienced workers retire and\nmultilingual barriers hinder effective communication. This study introduces\nIM-Chat, a multi-agent framework based on large language models (LLMs),\ndesigned to facilitate knowledge transfer in injection molding. IM-Chat\nintegrates both limited documented knowledge (e.g., troubleshooting tables,\nmanuals) and extensive field data modeled through a data-driven process\ncondition generator that infers optimal manufacturing settings from\nenvironmental inputs such as temperature and humidity, enabling robust and\ncontext-aware task resolution. By adopting a retrieval-augmented generation\n(RAG) strategy and tool-calling agents within a modular architecture, IM-Chat\nensures adaptability without the need for fine-tuning. Performance was assessed\nacross 100 single-tool and 60 hybrid tasks for GPT-4o, GPT-4o-mini, and\nGPT-3.5-turbo by domain experts using a 10-point rubric focused on relevance\nand correctness, and was further supplemented by automated evaluation using\nGPT-4o guided by a domain-adapted instruction prompt. The evaluation results\nindicate that more capable models tend to achieve higher accuracy, particularly\nin complex, tool-integrated scenarios. In addition, compared with the\nfine-tuned single-agent LLM, IM-Chat demonstrated superior accuracy,\nparticularly in quantitative reasoning, and greater scalability in handling\nmultiple information sources. Overall, these findings demonstrate the viability\nof multi-agent LLM systems for industrial knowledge workflows and establish\nIM-Chat as a scalable and generalizable approach to AI-assisted decision\nsupport in manufacturing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The injection molding industry faces critical challenges in preserving and\ntransferring field knowledge, particularly as experienced workers retire and\nmultilingual barriers hinder effective communication. This study introduces\nIM-Chat, a multi-agent framework based on large language models (LLMs),\ndesigned to facilitate knowledge transfer in injection molding. IM-Chat\nintegrates both limited documented knowledge (e.g., troubleshooting tables,\nmanuals) and extensive field data modeled through a data-driven process\ncondition generator that infers optimal manufacturing settings from\nenvironmental inputs such as temperature and humidity, enabling robust and\ncontext-aware task resolution. By adopting a retrieval-augmented generation\n(RAG) strategy and tool-calling agents within a modular architecture, IM-Chat\nensures adaptability without the need for fine-tuning. Performance was assessed\nacross 100 single-tool and 60 hybrid tasks for GPT-4o, GPT-4o-mini, and\nGPT-3.5-turbo by domain experts using a 10-point rubric focused on relevance\nand correctness, and was further supplemented by automated evaluation using\nGPT-4o guided by a domain-adapted instruction prompt. The evaluation results\nindicate that more capable models tend to achieve higher accuracy, particularly\nin complex, tool-integrated scenarios. In addition, compared with the\nfine-tuned single-agent LLM, IM-Chat demonstrated superior accuracy,\nparticularly in quantitative reasoning, and greater scalability in handling\nmultiple information sources. Overall, these findings demonstrate the viability\nof multi-agent LLM systems for industrial knowledge workflows and establish\nIM-Chat as a scalable and generalizable approach to AI-assisted decision\nsupport in manufacturing."
                },
                "authors": [
                    {
                        "name": "Junhyeong Lee"
                    },
                    {
                        "name": "Joon-Young Kim"
                    },
                    {
                        "name": "Heekyu Kim"
                    },
                    {
                        "name": "Inhyo Lee"
                    },
                    {
                        "name": "Seunghwa Ryu"
                    }
                ],
                "author_detail": {
                    "name": "Seunghwa Ryu"
                },
                "author": "Seunghwa Ryu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15268v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15268v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19611v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19611v1",
                "updated": "2025-10-22T14:04:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    4,
                    42,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T14:04:42Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    4,
                    42,
                    2,
                    295,
                    0
                ],
                "title": "A Climate-Aware Deep Learning Framework for Generalizable Epidemic\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Climate-Aware Deep Learning Framework for Generalizable Epidemic\n  Forecasting"
                },
                "summary": "Precise outbreak forecasting of infectious diseases is essential for\neffective public health responses and epidemic control. The increased\navailability of machine learning (ML) methods for time-series forecasting\npresents an enticing avenue to enhance outbreak forecasting. Though the\nCOVID-19 outbreak demonstrated the value of applying ML models to predict\nepidemic profiles, using ML models to forecast endemic diseases remains\nunderexplored. In this work, we present ForecastNet-XCL (an ensemble model\nbased on XGBoost+CNN+BiLSTM), a deep learning hybrid framework designed to\naddresses this gap by creating accurate multi-week RSV forecasts up to 100\nweeks in advance based on climate and temporal data, without access to\nreal-time surveillance on RSV. The framework combines high-resolution feature\nlearning with long-range temporal dependency capturing mechanisms, bolstered by\nan autoregressive module trained on climate-controlled lagged relations.\nStochastic inference returns probabilistic intervals to inform decision-making.\nEvaluated across 34 U.S. states, ForecastNet-XCL reliably outperformed\nstatistical baselines, individual neural nets, and conventional ensemble\nmethods in both within- and cross-state scenarios, sustaining accuracy over\nextended forecast horizons. Training on climatologically diverse datasets\nenhanced generalization furthermore, particularly in locations having irregular\nor biennial RSV patterns. ForecastNet-XCL's efficiency, performance, and\nuncertainty-aware design make it a deployable early-warning tool amid\nescalating climate pressures and constrained surveillance resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precise outbreak forecasting of infectious diseases is essential for\neffective public health responses and epidemic control. The increased\navailability of machine learning (ML) methods for time-series forecasting\npresents an enticing avenue to enhance outbreak forecasting. Though the\nCOVID-19 outbreak demonstrated the value of applying ML models to predict\nepidemic profiles, using ML models to forecast endemic diseases remains\nunderexplored. In this work, we present ForecastNet-XCL (an ensemble model\nbased on XGBoost+CNN+BiLSTM), a deep learning hybrid framework designed to\naddresses this gap by creating accurate multi-week RSV forecasts up to 100\nweeks in advance based on climate and temporal data, without access to\nreal-time surveillance on RSV. The framework combines high-resolution feature\nlearning with long-range temporal dependency capturing mechanisms, bolstered by\nan autoregressive module trained on climate-controlled lagged relations.\nStochastic inference returns probabilistic intervals to inform decision-making.\nEvaluated across 34 U.S. states, ForecastNet-XCL reliably outperformed\nstatistical baselines, individual neural nets, and conventional ensemble\nmethods in both within- and cross-state scenarios, sustaining accuracy over\nextended forecast horizons. Training on climatologically diverse datasets\nenhanced generalization furthermore, particularly in locations having irregular\nor biennial RSV patterns. ForecastNet-XCL's efficiency, performance, and\nuncertainty-aware design make it a deployable early-warning tool amid\nescalating climate pressures and constrained surveillance resources."
                },
                "authors": [
                    {
                        "name": "Jinpyo Hong"
                    },
                    {
                        "name": "Rachel E. Baker"
                    }
                ],
                "author_detail": {
                    "name": "Rachel E. Baker"
                },
                "author": "Rachel E. Baker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19611v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19611v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24625v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24625v3",
                "updated": "2025-10-22T14:04:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    4,
                    1,
                    2,
                    295,
                    0
                ],
                "published": "2025-05-30T14:16:41Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    16,
                    41,
                    4,
                    150,
                    0
                ],
                "title": "Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision\n  Geometry Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision\n  Geometry Priors"
                },
                "summary": "Previous research has investigated the application of Multimodal Large\nLanguage Models (MLLMs) in understanding 3D scenes by interpreting them as\nvideos. These approaches generally depend on comprehensive 3D data inputs, such\nas point clouds or reconstructed Bird's-Eye View (BEV) maps. In our research,\nwe advance this field by enhancing the capability of MLLMs to understand and\nreason in 3D spaces directly from video data, without the need for additional\n3D input. We propose a novel and efficient method called the Video-3D Geometry\nLarge Language Model (VG LLM). Our approach utilizes a 3D visual geometry\nencoder to extract 3D prior information from video sequences. This information\nis then integrated with visual tokens and input into the MLLM. Extensive\nexperiments have shown that our method has achieved substantial improvements in\nvarious tasks related to 3D scene understanding and spatial reasoning, all\ndirectly learned from video sources. Impressively, our 4B model, which does not\nrely on explicit 3D data inputs, achieves competitive results compared to\nexisting state-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the\nVSI-Bench evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous research has investigated the application of Multimodal Large\nLanguage Models (MLLMs) in understanding 3D scenes by interpreting them as\nvideos. These approaches generally depend on comprehensive 3D data inputs, such\nas point clouds or reconstructed Bird's-Eye View (BEV) maps. In our research,\nwe advance this field by enhancing the capability of MLLMs to understand and\nreason in 3D spaces directly from video data, without the need for additional\n3D input. We propose a novel and efficient method called the Video-3D Geometry\nLarge Language Model (VG LLM). Our approach utilizes a 3D visual geometry\nencoder to extract 3D prior information from video sequences. This information\nis then integrated with visual tokens and input into the MLLM. Extensive\nexperiments have shown that our method has achieved substantial improvements in\nvarious tasks related to 3D scene understanding and spatial reasoning, all\ndirectly learned from video sources. Impressively, our 4B model, which does not\nrely on explicit 3D data inputs, achieves competitive results compared to\nexisting state-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the\nVSI-Bench evaluations."
                },
                "authors": [
                    {
                        "name": "Duo Zheng"
                    },
                    {
                        "name": "Shijia Huang"
                    },
                    {
                        "name": "Yanyang Li"
                    },
                    {
                        "name": "Liwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Liwei Wang"
                },
                "author": "Liwei Wang",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24625v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24625v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16206v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16206v2",
                "updated": "2025-10-22T13:56:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    13,
                    56,
                    17,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-17T20:38:12Z",
                "published_parsed": [
                    2025,
                    10,
                    17,
                    20,
                    38,
                    12,
                    4,
                    290,
                    0
                ],
                "title": "The Right to Be Remembered: Preserving Maximally Truthful Digital Memory\n  in the Age of AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Right to Be Remembered: Preserving Maximally Truthful Digital Memory\n  in the Age of AI"
                },
                "summary": "Since the rapid expansion of large language models (LLMs), people have begun\nto rely on them for information retrieval. While traditional search engines\ndisplay ranked lists of sources shaped by search engine optimization (SEO),\nadvertising, and personalization, LLMs typically provide a synthesized response\nthat feels singular and authoritative. While both approaches carry risks of\nbias and omission, LLMs may amplify the effect by collapsing multiple\nperspectives into one answer, reducing users ability or inclination to compare\nalternatives. This concentrates power over information in a few LLM vendors\nwhose systems effectively shape what is remembered and what is overlooked. As a\nresult, certain narratives, individuals or groups, may be disproportionately\nsuppressed, while others are disproportionately elevated. Over time, this\ncreates a new threat: the gradual erasure of those with limited digital\npresence, and the amplification of those already prominent, reshaping\ncollective memory. To address these concerns, this paper presents a concept of\nthe Right To Be Remembered (RTBR) which encompasses minimizing the risk of\nAI-driven information omission, embracing the right of fair treatment, while\nensuring that the generated content would be maximally truthful.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the rapid expansion of large language models (LLMs), people have begun\nto rely on them for information retrieval. While traditional search engines\ndisplay ranked lists of sources shaped by search engine optimization (SEO),\nadvertising, and personalization, LLMs typically provide a synthesized response\nthat feels singular and authoritative. While both approaches carry risks of\nbias and omission, LLMs may amplify the effect by collapsing multiple\nperspectives into one answer, reducing users ability or inclination to compare\nalternatives. This concentrates power over information in a few LLM vendors\nwhose systems effectively shape what is remembered and what is overlooked. As a\nresult, certain narratives, individuals or groups, may be disproportionately\nsuppressed, while others are disproportionately elevated. Over time, this\ncreates a new threat: the gradual erasure of those with limited digital\npresence, and the amplification of those already prominent, reshaping\ncollective memory. To address these concerns, this paper presents a concept of\nthe Right To Be Remembered (RTBR) which encompasses minimizing the risk of\nAI-driven information omission, embracing the right of fair treatment, while\nensuring that the generated content would be maximally truthful."
                },
                "authors": [
                    {
                        "name": "Alex Zhavoronkov"
                    },
                    {
                        "name": "Dominika Wilczok"
                    },
                    {
                        "name": "Roman Yampolskiy"
                    }
                ],
                "author_detail": {
                    "name": "Roman Yampolskiy"
                },
                "author": "Roman Yampolskiy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16206v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16206v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13878v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13878v2",
                "updated": "2025-10-22T13:55:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    13,
                    55,
                    29,
                    2,
                    295,
                    0
                ],
                "published": "2025-05-20T03:32:37Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    3,
                    32,
                    37,
                    1,
                    140,
                    0
                ],
                "title": "InfiFPO: Implicit Model Fusion via Preference Optimization in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiFPO: Implicit Model Fusion via Preference Optimization in Large\n  Language Models"
                },
                "summary": "Model fusion combines multiple Large Language Models (LLMs) with different\nstrengths into a more powerful, integrated model through lightweight training\nmethods. Existing works on model fusion focus primarily on supervised\nfine-tuning (SFT), leaving preference alignment (PA) --a critical phase for\nenhancing LLM performance--largely unexplored. The current few fusion methods\non PA phase, like WRPO, simplify the process by utilizing only response outputs\nfrom source models while discarding their probability information. To address\nthis limitation, we propose InfiFPO, a preference optimization method for\nimplicit model fusion. InfiFPO replaces the reference model in Direct\nPreference Optimization (DPO) with a fused source model that synthesizes\nmulti-source probabilities at the sequence level, circumventing complex\nvocabulary alignment challenges in previous works and meanwhile maintaining the\nprobability information. By introducing probability clipping and max-margin\nfusion strategies, InfiFPO enables the pivot model to align with human\npreferences while effectively distilling knowledge from source models.\nComprehensive experiments on 11 widely-used benchmarks demonstrate that InfiFPO\nconsistently outperforms existing model fusion and preference optimization\nmethods. When using Phi-4 as the pivot model, InfiFPO improve its average\nperformance from 79.95 to 83.33 on 11 benchmarks, significantly improving its\ncapabilities in mathematics, coding, and reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model fusion combines multiple Large Language Models (LLMs) with different\nstrengths into a more powerful, integrated model through lightweight training\nmethods. Existing works on model fusion focus primarily on supervised\nfine-tuning (SFT), leaving preference alignment (PA) --a critical phase for\nenhancing LLM performance--largely unexplored. The current few fusion methods\non PA phase, like WRPO, simplify the process by utilizing only response outputs\nfrom source models while discarding their probability information. To address\nthis limitation, we propose InfiFPO, a preference optimization method for\nimplicit model fusion. InfiFPO replaces the reference model in Direct\nPreference Optimization (DPO) with a fused source model that synthesizes\nmulti-source probabilities at the sequence level, circumventing complex\nvocabulary alignment challenges in previous works and meanwhile maintaining the\nprobability information. By introducing probability clipping and max-margin\nfusion strategies, InfiFPO enables the pivot model to align with human\npreferences while effectively distilling knowledge from source models.\nComprehensive experiments on 11 widely-used benchmarks demonstrate that InfiFPO\nconsistently outperforms existing model fusion and preference optimization\nmethods. When using Phi-4 as the pivot model, InfiFPO improve its average\nperformance from 79.95 to 83.33 on 11 benchmarks, significantly improving its\ncapabilities in mathematics, coding, and reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Yanggan Gu"
                    },
                    {
                        "name": "Yuanyi Wang"
                    },
                    {
                        "name": "Zhaoyi Yan"
                    },
                    {
                        "name": "Yiming Zhang"
                    },
                    {
                        "name": "Qi Zhou"
                    },
                    {
                        "name": "Fei Wu"
                    },
                    {
                        "name": "Hongxia Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hongxia Yang"
                },
                "author": "Hongxia Yang",
                "arxiv_journal_ref": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13878v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13878v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19599v1",
                "updated": "2025-10-22T13:52:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    13,
                    52,
                    19,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T13:52:19Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    13,
                    52,
                    19,
                    2,
                    295,
                    0
                ],
                "title": "XBench: A Comprehensive Benchmark for Visual-Language Explanations in\n  Chest Radiography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XBench: A Comprehensive Benchmark for Visual-Language Explanations in\n  Chest Radiography"
                },
                "summary": "Vision-language models (VLMs) have recently shown remarkable zero-shot\nperformance in medical image understanding, yet their grounding ability, the\nextent to which textual concepts align with visual evidence, remains\nunderexplored. In the medical domain, however, reliable grounding is essential\nfor interpretability and clinical adoption. In this work, we present the first\nsystematic benchmark for evaluating cross-modal interpretability in chest\nX-rays across seven CLIP-style VLM variants. We generate visual explanations\nusing cross-attention and similarity-based localization maps, and\nquantitatively assess their alignment with radiologist-annotated regions across\nmultiple pathologies. Our analysis reveals that: (1) while all VLM variants\ndemonstrate reasonable localization for large and well-defined pathologies,\ntheir performance substantially degrades for small or diffuse lesions; (2)\nmodels that are pretrained on chest X-ray-specific datasets exhibit improved\nalignment compared to those trained on general-domain data. (3) The overall\nrecognition ability and grounding ability of the model are strongly correlated.\nThese findings underscore that current VLMs, despite their strong recognition\nability, still fall short in clinically reliable grounding, highlighting the\nneed for targeted interpretability benchmarks before deployment in medical\npractice. XBench code is available at\nhttps://github.com/Roypic/Benchmarkingattention",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) have recently shown remarkable zero-shot\nperformance in medical image understanding, yet their grounding ability, the\nextent to which textual concepts align with visual evidence, remains\nunderexplored. In the medical domain, however, reliable grounding is essential\nfor interpretability and clinical adoption. In this work, we present the first\nsystematic benchmark for evaluating cross-modal interpretability in chest\nX-rays across seven CLIP-style VLM variants. We generate visual explanations\nusing cross-attention and similarity-based localization maps, and\nquantitatively assess their alignment with radiologist-annotated regions across\nmultiple pathologies. Our analysis reveals that: (1) while all VLM variants\ndemonstrate reasonable localization for large and well-defined pathologies,\ntheir performance substantially degrades for small or diffuse lesions; (2)\nmodels that are pretrained on chest X-ray-specific datasets exhibit improved\nalignment compared to those trained on general-domain data. (3) The overall\nrecognition ability and grounding ability of the model are strongly correlated.\nThese findings underscore that current VLMs, despite their strong recognition\nability, still fall short in clinically reliable grounding, highlighting the\nneed for targeted interpretability benchmarks before deployment in medical\npractice. XBench code is available at\nhttps://github.com/Roypic/Benchmarkingattention"
                },
                "authors": [
                    {
                        "name": "Haozhe Luo"
                    },
                    {
                        "name": "Shelley Zixin Shu"
                    },
                    {
                        "name": "Ziyu Zhou"
                    },
                    {
                        "name": "Sebastian Otalora"
                    },
                    {
                        "name": "Mauricio Reyes"
                    }
                ],
                "author_detail": {
                    "name": "Mauricio Reyes"
                },
                "author": "Mauricio Reyes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19597v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19597v2",
                "updated": "2025-10-23T05:56:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    5,
                    56,
                    32,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-22T13:48:36Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    13,
                    48,
                    36,
                    2,
                    295,
                    0
                ],
                "title": "CBDiff:Conditional Bernoulli Diffusion Models for Image Forgery\n  Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CBDiff:Conditional Bernoulli Diffusion Models for Image Forgery\n  Localization"
                },
                "summary": "Image Forgery Localization (IFL) is a crucial task in image forensics, aimed\nat accurately identifying manipulated or tampered regions within an image at\nthe pixel level. Existing methods typically generate a single deterministic\nlocalization map, which often lacks the precision and reliability required for\nhigh-stakes applications such as forensic analysis and security surveillance.\nTo enhance the credibility of predictions and mitigate the risk of errors, we\nintroduce an advanced Conditional Bernoulli Diffusion Model (CBDiff). Given a\nforged image, CBDiff generates multiple diverse and plausible localization\nmaps, thereby offering a richer and more comprehensive representation of the\nforgery distribution. This approach addresses the uncertainty and variability\ninherent in tampered regions. Furthermore, CBDiff innovatively incorporates\nBernoulli noise into the diffusion process to more faithfully reflect the\ninherent binary and sparse properties of forgery masks. Additionally, CBDiff\nintroduces a Time-Step Cross-Attention (TSCAttention), which is specifically\ndesigned to leverage semantic feature guidance with temporal steps to improve\nmanipulation detection. Extensive experiments on eight publicly benchmark\ndatasets demonstrate that CBDiff significantly outperforms existing\nstate-of-the-art methods, highlighting its strong potential for real-world\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image Forgery Localization (IFL) is a crucial task in image forensics, aimed\nat accurately identifying manipulated or tampered regions within an image at\nthe pixel level. Existing methods typically generate a single deterministic\nlocalization map, which often lacks the precision and reliability required for\nhigh-stakes applications such as forensic analysis and security surveillance.\nTo enhance the credibility of predictions and mitigate the risk of errors, we\nintroduce an advanced Conditional Bernoulli Diffusion Model (CBDiff). Given a\nforged image, CBDiff generates multiple diverse and plausible localization\nmaps, thereby offering a richer and more comprehensive representation of the\nforgery distribution. This approach addresses the uncertainty and variability\ninherent in tampered regions. Furthermore, CBDiff innovatively incorporates\nBernoulli noise into the diffusion process to more faithfully reflect the\ninherent binary and sparse properties of forgery masks. Additionally, CBDiff\nintroduces a Time-Step Cross-Attention (TSCAttention), which is specifically\ndesigned to leverage semantic feature guidance with temporal steps to improve\nmanipulation detection. Extensive experiments on eight publicly benchmark\ndatasets demonstrate that CBDiff significantly outperforms existing\nstate-of-the-art methods, highlighting its strong potential for real-world\ndeployment."
                },
                "authors": [
                    {
                        "name": "Zhou Lei"
                    },
                    {
                        "name": "Pan Gang"
                    },
                    {
                        "name": "Wang Jiahao"
                    },
                    {
                        "name": "Sun Di"
                    }
                ],
                "author_detail": {
                    "name": "Sun Di"
                },
                "author": "Sun Di",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19597v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19597v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19955v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19955v3",
                "updated": "2025-10-22T13:33:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    13,
                    33,
                    52,
                    2,
                    295,
                    0
                ],
                "published": "2025-05-26T13:18:37Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    13,
                    18,
                    37,
                    0,
                    146,
                    0
                ],
                "title": "MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research"
                },
                "summary": "Recent advancements in AI agents have demonstrated their growing potential to\ndrive and support scientific discovery. In this work, we introduce MLR-Bench, a\ncomprehensive benchmark for evaluating AI agents on open-ended machine learning\nresearch. MLR-Bench includes three key components: (1) 201 research tasks\nsourced from NeurIPS, ICLR, and ICML workshops covering diverse ML topics; (2)\nMLR-Judge, an automated evaluation framework combining LLM-based reviewers with\ncarefully designed review rubrics to assess research quality; and (3)\nMLR-Agent, a modular agent scaffold capable of completing research tasks\nthrough four stages: idea generation, proposal formulation, experimentation,\nand paper writing. Our framework supports both stepwise assessment across these\ndistinct research stages, and end-to-end evaluation of the final research\npaper. We then use MLR-Bench to evaluate six frontier LLMs and an advanced\ncoding agent, finding that while LLMs are effective at generating coherent\nideas and well-structured papers, current coding agents frequently (e.g., in\n80% of the cases) produce fabricated or invalidated experimental\nresults--posing a major barrier to scientific reliability. We validate\nMLR-Judge through human evaluation, showing high agreement with expert\nreviewers, supporting its potential as a scalable tool for research evaluation.\nWe open-source MLR-Bench to help the community benchmark, diagnose, and improve\nAI research agents toward trustworthy and transparent scientific discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in AI agents have demonstrated their growing potential to\ndrive and support scientific discovery. In this work, we introduce MLR-Bench, a\ncomprehensive benchmark for evaluating AI agents on open-ended machine learning\nresearch. MLR-Bench includes three key components: (1) 201 research tasks\nsourced from NeurIPS, ICLR, and ICML workshops covering diverse ML topics; (2)\nMLR-Judge, an automated evaluation framework combining LLM-based reviewers with\ncarefully designed review rubrics to assess research quality; and (3)\nMLR-Agent, a modular agent scaffold capable of completing research tasks\nthrough four stages: idea generation, proposal formulation, experimentation,\nand paper writing. Our framework supports both stepwise assessment across these\ndistinct research stages, and end-to-end evaluation of the final research\npaper. We then use MLR-Bench to evaluate six frontier LLMs and an advanced\ncoding agent, finding that while LLMs are effective at generating coherent\nideas and well-structured papers, current coding agents frequently (e.g., in\n80% of the cases) produce fabricated or invalidated experimental\nresults--posing a major barrier to scientific reliability. We validate\nMLR-Judge through human evaluation, showing high agreement with expert\nreviewers, supporting its potential as a scalable tool for research evaluation.\nWe open-source MLR-Bench to help the community benchmark, diagnose, and improve\nAI research agents toward trustworthy and transparent scientific discovery."
                },
                "authors": [
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Miao Xiong"
                    },
                    {
                        "name": "Yujie Lu"
                    },
                    {
                        "name": "Wei Han"
                    },
                    {
                        "name": "Ailin Deng"
                    },
                    {
                        "name": "Yufei He"
                    },
                    {
                        "name": "Jiaying Wu"
                    },
                    {
                        "name": "Yibo Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "arxiv_comment": "49 pages, 9 figures. Accepted by NeurIPS 2025 D&B Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19955v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19955v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19579v1",
                "updated": "2025-10-22T13:29:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    13,
                    29,
                    32,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T13:29:32Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    13,
                    29,
                    32,
                    2,
                    295,
                    0
                ],
                "title": "Multi-modal Co-learning for Earth Observation: Enhancing single-modality\n  models via modality collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal Co-learning for Earth Observation: Enhancing single-modality\n  models via modality collaboration"
                },
                "summary": "Multi-modal co-learning is emerging as an effective paradigm in machine\nlearning, enabling models to collaboratively learn from different modalities to\nenhance single-modality predictions. Earth Observation (EO) represents a\nquintessential domain for multi-modal data analysis, wherein diverse remote\nsensors collect data to sense our planet. This unprecedented volume of data\nintroduces novel challenges. Specifically, the access to the same sensor\nmodalities at both training and inference stages becomes increasingly complex\nbased on real-world constraints affecting remote sensing platforms. In this\ncontext, multi-modal co-learning presents a promising strategy to leverage the\nvast amount of sensor-derived data available at the training stage to improve\nsingle-modality models for inference-time deployment. Most current research\nefforts focus on designing customized solutions for either particular\ndownstream tasks or specific modalities available at the inference stage. To\naddress this, we propose a novel multi-modal co-learning framework capable of\ngeneralizing across various tasks without targeting a specific modality for\ninference. Our approach combines contrastive and modality discriminative\nlearning together to guide single-modality models to structure the internal\nmodel manifold into modality-shared and modality-specific information. We\nevaluate our framework on four EO benchmarks spanning classification and\nregression tasks across different sensor modalities, where only one of the\nmodalities available during training is accessible at inference time. Our\nresults demonstrate consistent predictive improvements over state-of-the-art\napproaches from the recent machine learning and computer vision literature, as\nwell as EO-specific methods. The obtained findings validate our framework in\nthe single-modality inference scenarios across a diverse range of EO\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal co-learning is emerging as an effective paradigm in machine\nlearning, enabling models to collaboratively learn from different modalities to\nenhance single-modality predictions. Earth Observation (EO) represents a\nquintessential domain for multi-modal data analysis, wherein diverse remote\nsensors collect data to sense our planet. This unprecedented volume of data\nintroduces novel challenges. Specifically, the access to the same sensor\nmodalities at both training and inference stages becomes increasingly complex\nbased on real-world constraints affecting remote sensing platforms. In this\ncontext, multi-modal co-learning presents a promising strategy to leverage the\nvast amount of sensor-derived data available at the training stage to improve\nsingle-modality models for inference-time deployment. Most current research\nefforts focus on designing customized solutions for either particular\ndownstream tasks or specific modalities available at the inference stage. To\naddress this, we propose a novel multi-modal co-learning framework capable of\ngeneralizing across various tasks without targeting a specific modality for\ninference. Our approach combines contrastive and modality discriminative\nlearning together to guide single-modality models to structure the internal\nmodel manifold into modality-shared and modality-specific information. We\nevaluate our framework on four EO benchmarks spanning classification and\nregression tasks across different sensor modalities, where only one of the\nmodalities available during training is accessible at inference time. Our\nresults demonstrate consistent predictive improvements over state-of-the-art\napproaches from the recent machine learning and computer vision literature, as\nwell as EO-specific methods. The obtained findings validate our framework in\nthe single-modality inference scenarios across a diverse range of EO\napplications."
                },
                "authors": [
                    {
                        "name": "Francisco Mena"
                    },
                    {
                        "name": "Dino Ienco"
                    },
                    {
                        "name": "Cassio F. Dantas"
                    },
                    {
                        "name": "Roberto Interdonato"
                    },
                    {
                        "name": "Andreas Dengel"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Dengel"
                },
                "author": "Andreas Dengel",
                "arxiv_comment": "Accepted at the Machine Learning journal, CfP: Discovery Science 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19577v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19577v1",
                "updated": "2025-10-22T13:28:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    13,
                    28,
                    36,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T13:28:36Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    13,
                    28,
                    36,
                    2,
                    295,
                    0
                ],
                "title": "gem5 Co-Pilot: AI Assistant Agent for Architectural Design Space\n  Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "gem5 Co-Pilot: AI Assistant Agent for Architectural Design Space\n  Exploration"
                },
                "summary": "Generative AI is increasing the productivity of software and hardware\ndevelopment across many application domains. In this work, we utilize the power\nof Large Language Models (LLMs) to develop a co-pilot agent for assisting gem5\nusers with automating design space exploration. Computer architecture design\nspace exploration is complex and time-consuming, given that numerous parameter\nsettings and simulation statistics must be analyzed before improving the\ncurrent design. The emergence of LLMs has significantly accelerated the\nanalysis of long-text data as well as smart decision making, two key functions\nin a successful design space exploration task. In this project, we first build\ngem5 Co-Pilot, an AI agent assistant for gem5, which comes with a webpage-GUI\nfor smooth user interaction, agent automation, and result summarization. We\nalso implemented a language for design space exploration, as well as a Design\nSpace Database (DSDB). With DSDB, gem5 Co-Pilot effectively implements a\nRetrieval Augmented Generation system for gem5 design space exploration. We\nexperiment on cost-constraint optimization with four cost ranges and compare\nour results with two baseline models. Results show that gem5 Co-Pilot can\nquickly identify optimal parameters for specific design constraints based on\nperformance and cost, with limited user interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI is increasing the productivity of software and hardware\ndevelopment across many application domains. In this work, we utilize the power\nof Large Language Models (LLMs) to develop a co-pilot agent for assisting gem5\nusers with automating design space exploration. Computer architecture design\nspace exploration is complex and time-consuming, given that numerous parameter\nsettings and simulation statistics must be analyzed before improving the\ncurrent design. The emergence of LLMs has significantly accelerated the\nanalysis of long-text data as well as smart decision making, two key functions\nin a successful design space exploration task. In this project, we first build\ngem5 Co-Pilot, an AI agent assistant for gem5, which comes with a webpage-GUI\nfor smooth user interaction, agent automation, and result summarization. We\nalso implemented a language for design space exploration, as well as a Design\nSpace Database (DSDB). With DSDB, gem5 Co-Pilot effectively implements a\nRetrieval Augmented Generation system for gem5 design space exploration. We\nexperiment on cost-constraint optimization with four cost ranges and compare\nour results with two baseline models. Results show that gem5 Co-Pilot can\nquickly identify optimal parameters for specific design constraints based on\nperformance and cost, with limited user interaction."
                },
                "authors": [
                    {
                        "name": "Zuoming Fu"
                    },
                    {
                        "name": "Alex Manley"
                    },
                    {
                        "name": "Mohammad Alian"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Alian"
                },
                "author": "Mohammad Alian",
                "arxiv_comment": "Accepted by CAMS25, October, 2025, Seoul, Republic of Korea",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19577v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19577v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01675v2",
                "updated": "2025-10-22T13:17:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    13,
                    17,
                    36,
                    2,
                    295,
                    0
                ],
                "published": "2025-03-03T15:48:01Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    15,
                    48,
                    1,
                    0,
                    62,
                    0
                ],
                "title": "Using (Not-so) Large Language Models to Generate Simulation Models in a\n  Formal DSL: A Study on Reaction Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using (Not-so) Large Language Models to Generate Simulation Models in a\n  Formal DSL: A Study on Reaction Networks"
                },
                "summary": "Formal languages are an integral part of modeling and simulation. They allow\nthe distillation of knowledge into concise simulation models amenable to\nautomatic execution, interpretation, and analysis. However, the arguably most\nhumanly accessible means of expressing models is through natural language,\nwhich is not easily interpretable by computers. Here, we evaluate how a Large\nLanguage Model (LLM) might be used for formalizing natural language into\nsimulation models. Existing studies only explored using very large LLMs, like\nthe commercial GPT models, without fine-tuning model weights. To close this\ngap, we show how an open-weights, 7B-parameter Mistral model can be fine-tuned\nto translate natural language descriptions to reaction network models in a\ndomain-specific language, offering a self-hostable, compute-efficient, and\nmemory efficient alternative. To this end, we develop a synthetic data\ngenerator to serve as the basis for fine-tuning and evaluation. Our\nquantitative evaluation shows that our fine-tuned Mistral model can recover the\nground truth simulation model in up to 84.5% of cases. In addition, our\nsmall-scale user study demonstrates the model's practical potential for\none-time generation as well as interactive modeling in various domains. While\npromising, in its current form, the fine-tuned small LLM cannot catch up with\nlarge LLMs. We conclude that higher-quality training data are required, and\nexpect future small and open-source LLMs to offer new opportunities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal languages are an integral part of modeling and simulation. They allow\nthe distillation of knowledge into concise simulation models amenable to\nautomatic execution, interpretation, and analysis. However, the arguably most\nhumanly accessible means of expressing models is through natural language,\nwhich is not easily interpretable by computers. Here, we evaluate how a Large\nLanguage Model (LLM) might be used for formalizing natural language into\nsimulation models. Existing studies only explored using very large LLMs, like\nthe commercial GPT models, without fine-tuning model weights. To close this\ngap, we show how an open-weights, 7B-parameter Mistral model can be fine-tuned\nto translate natural language descriptions to reaction network models in a\ndomain-specific language, offering a self-hostable, compute-efficient, and\nmemory efficient alternative. To this end, we develop a synthetic data\ngenerator to serve as the basis for fine-tuning and evaluation. Our\nquantitative evaluation shows that our fine-tuned Mistral model can recover the\nground truth simulation model in up to 84.5% of cases. In addition, our\nsmall-scale user study demonstrates the model's practical potential for\none-time generation as well as interactive modeling in various domains. While\npromising, in its current form, the fine-tuned small LLM cannot catch up with\nlarge LLMs. We conclude that higher-quality training data are required, and\nexpect future small and open-source LLMs to offer new opportunities."
                },
                "authors": [
                    {
                        "name": "Justin N. Kreikemeyer"
                    },
                    {
                        "name": "Miłosz Jankowski"
                    },
                    {
                        "name": "Pia Wilsdorf"
                    },
                    {
                        "name": "Adelinde M. Uhrmacher"
                    }
                ],
                "author_detail": {
                    "name": "Adelinde M. Uhrmacher"
                },
                "author": "Adelinde M. Uhrmacher",
                "arxiv_doi": "10.1145/3733719",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3733719",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.01675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "27 pages, 5 figures; supplemental material available at\n  https://doi.org/10.1145/3733719",
                "arxiv_journal_ref": "ACM Trans. Model. Comput. Simul. 35, 4, Article 31 (October 2025),\n  27 pages",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05346v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05346v2",
                "updated": "2025-10-22T13:08:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    13,
                    8,
                    29,
                    2,
                    295,
                    0
                ],
                "published": "2025-09-02T14:21:59Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    14,
                    21,
                    59,
                    1,
                    245,
                    0
                ],
                "title": "Benchmarking Large Language Models for Personalized Guidance in\n  AI-Enhanced Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Large Language Models for Personalized Guidance in\n  AI-Enhanced Learning"
                },
                "summary": "While Large Language Models (LLMs) are increasingly envisioned as intelligent\nassistants for personalized learning, systematic head-to-head evaluations in\nauthentic learning scenarios remain scarce. This study presents an empirical\ncomparison of three state-of-the-art LLMs on a tutoring task simulating a\nrealistic learning setting. Using a dataset containing a student's responses to\nten mixed-format questions with correctness labels, each model was asked to (i)\nanalyze the quiz to identify underlying knowledge components, (ii) infer the\nstudent's mastery profile, and (iii) generate targeted guidance for\nimprovement. To mitigate subjectivity and evaluator bias, Gemini was employed\nas a virtual judge to perform pairwise comparisons across multiple dimensions:\naccuracy, clarity, actionability, and appropriateness. Results analyzed via the\nBradley-Terry model reveal that GPT-4o is generally preferred, producing\nfeedback that is more informative and better structured than its counterparts,\nwhereas DeepSeek-V3 and GLM-4.5 demonstrate intermittent strengths but lower\nconsistency. These findings highlight the feasibility of deploying LLMs as\nadvanced teaching assistants for individualized support and provide\nmethodological insights for subsequent empirical research on LLM-driven\npersonalized learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) are increasingly envisioned as intelligent\nassistants for personalized learning, systematic head-to-head evaluations in\nauthentic learning scenarios remain scarce. This study presents an empirical\ncomparison of three state-of-the-art LLMs on a tutoring task simulating a\nrealistic learning setting. Using a dataset containing a student's responses to\nten mixed-format questions with correctness labels, each model was asked to (i)\nanalyze the quiz to identify underlying knowledge components, (ii) infer the\nstudent's mastery profile, and (iii) generate targeted guidance for\nimprovement. To mitigate subjectivity and evaluator bias, Gemini was employed\nas a virtual judge to perform pairwise comparisons across multiple dimensions:\naccuracy, clarity, actionability, and appropriateness. Results analyzed via the\nBradley-Terry model reveal that GPT-4o is generally preferred, producing\nfeedback that is more informative and better structured than its counterparts,\nwhereas DeepSeek-V3 and GLM-4.5 demonstrate intermittent strengths but lower\nconsistency. These findings highlight the feasibility of deploying LLMs as\nadvanced teaching assistants for individualized support and provide\nmethodological insights for subsequent empirical research on LLM-driven\npersonalized learning."
                },
                "authors": [
                    {
                        "name": "Bo Yuan"
                    },
                    {
                        "name": "Jiazi Hu"
                    }
                ],
                "author_detail": {
                    "name": "Jiazi Hu"
                },
                "author": "Jiazi Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05346v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05346v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.18608v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.18608v3",
                "updated": "2025-10-22T12:54:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    12,
                    54,
                    22,
                    2,
                    295,
                    0
                ],
                "published": "2023-10-28T06:31:06Z",
                "published_parsed": [
                    2023,
                    10,
                    28,
                    6,
                    31,
                    6,
                    5,
                    301,
                    0
                ],
                "title": "Embedding in Recommender Systems: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedding in Recommender Systems: A Survey"
                },
                "summary": "Recommender systems have become an essential component of many online\nplatforms, providing personalized recommendations to users. A crucial aspect is\nembedding techniques that convert the high-dimensional discrete features, such\nas user and item IDs, into low-dimensional continuous vectors, which can\nenhance the recommendation performance. Embedding techniques have\nrevolutionized the capture of complex entity relationships, generating\nsignificant research interest. This survey presents a comprehensive analysis of\nrecent advances in recommender system embedding techniques. We examine\ncentralized embedding approaches across matrix, sequential, and graph\nstructures. In matrix-based scenarios, collaborative filtering generates\nembeddings that effectively model user-item preferences, particularly in sparse\ndata environments. For sequential data, we explore various approaches including\nrecurrent neural networks and self-supervised methods such as contrastive and\ngenerative learning. In graph-structured contexts, we analyze techniques like\nnode2vec that leverage network relationships, along with applicable\nself-supervised methods. Our survey addresses critical scalability challenges\nin embedding methods and explores innovative directions in recommender systems.\nWe introduce emerging approaches, including AutoML, hashing techniques, and\nquantization methods, to enhance performance while reducing computational\ncomplexity. Additionally, we examine the promising role of Large Language\nModels (LLMs) in embedding enhancement. Through detailed discussion of various\narchitectures and methodologies, this survey aims to provide a thorough\noverview of state-of-the-art embedding techniques in recommender systems, while\nhighlighting key challenges and future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems have become an essential component of many online\nplatforms, providing personalized recommendations to users. A crucial aspect is\nembedding techniques that convert the high-dimensional discrete features, such\nas user and item IDs, into low-dimensional continuous vectors, which can\nenhance the recommendation performance. Embedding techniques have\nrevolutionized the capture of complex entity relationships, generating\nsignificant research interest. This survey presents a comprehensive analysis of\nrecent advances in recommender system embedding techniques. We examine\ncentralized embedding approaches across matrix, sequential, and graph\nstructures. In matrix-based scenarios, collaborative filtering generates\nembeddings that effectively model user-item preferences, particularly in sparse\ndata environments. For sequential data, we explore various approaches including\nrecurrent neural networks and self-supervised methods such as contrastive and\ngenerative learning. In graph-structured contexts, we analyze techniques like\nnode2vec that leverage network relationships, along with applicable\nself-supervised methods. Our survey addresses critical scalability challenges\nin embedding methods and explores innovative directions in recommender systems.\nWe introduce emerging approaches, including AutoML, hashing techniques, and\nquantization methods, to enhance performance while reducing computational\ncomplexity. Additionally, we examine the promising role of Large Language\nModels (LLMs) in embedding enhancement. Through detailed discussion of various\narchitectures and methodologies, this survey aims to provide a thorough\noverview of state-of-the-art embedding techniques in recommender systems, while\nhighlighting key challenges and future research directions."
                },
                "authors": [
                    {
                        "name": "Maolin Wang"
                    },
                    {
                        "name": "Xinjian Zhao"
                    },
                    {
                        "name": "Wanyu Wang"
                    },
                    {
                        "name": "Sheng Zhang"
                    },
                    {
                        "name": "Jiansheng Li"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Binhao Wang"
                    },
                    {
                        "name": "Shucheng Zhou"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Ruocheng Guo"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhao"
                },
                "author": "Xiangyu Zhao",
                "arxiv_comment": "47 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.18608v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.18608v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19521v1",
                "updated": "2025-10-22T12:19:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    12,
                    19,
                    58,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T12:19:58Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    12,
                    19,
                    58,
                    2,
                    295,
                    0
                ],
                "title": "Network-Centric Anomaly Filtering and Spoofer localization for 5G-NR\n  Localization in LAWNs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network-Centric Anomaly Filtering and Spoofer localization for 5G-NR\n  Localization in LAWNs"
                },
                "summary": "This paper investigates security vulnerabilities and countermeasures for 3rd\nGeneration Partnership Project (3GPP) Fifth Generation New Radio (5G-NR) Time\nDifference of Arrival (TDoA)-based unmanned aerial vehicle (UAV) localization\nin low-altitude urban environments. We first optimize node selection strategies\nunder Air to Ground (A2G) channel conditions, proving that optimal selection\ndepends on UAV altitude and deployment density. We propose lightweight User\nEquipment (UE)-assisted that reduce overhead while enhancing accuracy. We then\nexpose critical security vulnerabilities by introducing merged-peak spoofing\nattacks where rogue UAVs transmit multiple lower-power pulses that merge with\nlegitimate signals, bypassing existing detection methods. Through theoretical\nmodeling and sensitivity analysis, we quantify how synchronization quality and\ngeometric factors determine spoofing success probability, revealing fundamental\nweaknesses in current 3GPP positioning frameworks. To address these\nvulnerabilities, we design a network-centric anomaly detection framework at the\nLocalization Management Function (LMF) using existing 3GPP-specified\nparameters, coupled with a recursive gradient descent-based robust localization\nalgorithm that filters anomaly data while estimating UAV position. Our unified\nframework simultaneously provides robust victim localization and spoofer\nlocalization-capabilities not integrated in existing literature. Extensive\nsimulations validate the effectiveness of both optimization and security\nmechanisms for 3GPP-compliant UAV positioning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates security vulnerabilities and countermeasures for 3rd\nGeneration Partnership Project (3GPP) Fifth Generation New Radio (5G-NR) Time\nDifference of Arrival (TDoA)-based unmanned aerial vehicle (UAV) localization\nin low-altitude urban environments. We first optimize node selection strategies\nunder Air to Ground (A2G) channel conditions, proving that optimal selection\ndepends on UAV altitude and deployment density. We propose lightweight User\nEquipment (UE)-assisted that reduce overhead while enhancing accuracy. We then\nexpose critical security vulnerabilities by introducing merged-peak spoofing\nattacks where rogue UAVs transmit multiple lower-power pulses that merge with\nlegitimate signals, bypassing existing detection methods. Through theoretical\nmodeling and sensitivity analysis, we quantify how synchronization quality and\ngeometric factors determine spoofing success probability, revealing fundamental\nweaknesses in current 3GPP positioning frameworks. To address these\nvulnerabilities, we design a network-centric anomaly detection framework at the\nLocalization Management Function (LMF) using existing 3GPP-specified\nparameters, coupled with a recursive gradient descent-based robust localization\nalgorithm that filters anomaly data while estimating UAV position. Our unified\nframework simultaneously provides robust victim localization and spoofer\nlocalization-capabilities not integrated in existing literature. Extensive\nsimulations validate the effectiveness of both optimization and security\nmechanisms for 3GPP-compliant UAV positioning."
                },
                "authors": [
                    {
                        "name": "Zexin Fang"
                    },
                    {
                        "name": "Bin Han"
                    },
                    {
                        "name": "Zhu Han"
                    },
                    {
                        "name": "Hans D. Schotten"
                    }
                ],
                "author_detail": {
                    "name": "Hans D. Schotten"
                },
                "author": "Hans D. Schotten",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16551v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16551v3",
                "updated": "2025-10-22T12:15:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    12,
                    15,
                    26,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-18T15:46:11Z",
                "published_parsed": [
                    2025,
                    10,
                    18,
                    15,
                    46,
                    11,
                    5,
                    291,
                    0
                ],
                "title": "From Reviews to Actionable Insights: An LLM-Based Approach for Attribute\n  and Feature Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Reviews to Actionable Insights: An LLM-Based Approach for Attribute\n  and Feature Extraction"
                },
                "summary": "This research proposes a systematic, large language model (LLM) approach for\nextracting product and service attributes, features, and associated sentiments\nfrom customer reviews. Grounded in marketing theory, the framework\ndistinguishes perceptual attributes from actionable features, producing\ninterpretable and managerially actionable insights. We apply the methodology to\n20,000 Yelp reviews of Starbucks stores and evaluate eight prompt variants on a\nrandom subset of reviews. Model performance is assessed through agreement with\nhuman annotations and predictive validity for customer ratings. Results show\nhigh consistency between LLMs and human coders and strong predictive validity,\nconfirming the reliability of the approach. Human coders required a median of\nsix minutes per review, whereas the LLM processed each in two seconds,\ndelivering comparable insights at a scale unattainable through manual coding.\nManagerially, the analysis identifies attributes and features that most\nstrongly influence customer satisfaction and their associated sentiments,\nenabling firms to pinpoint \"joy points,\" address \"pain points,\" and design\ntargeted interventions. We demonstrate how structured review data can power an\nactionable marketing dashboard that tracks sentiment over time and across\nstores, benchmarks performance, and highlights high-leverage features for\nimprovement. Simulations indicate that enhancing sentiment for key service\nfeatures could yield 1-2% average revenue gains per store.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research proposes a systematic, large language model (LLM) approach for\nextracting product and service attributes, features, and associated sentiments\nfrom customer reviews. Grounded in marketing theory, the framework\ndistinguishes perceptual attributes from actionable features, producing\ninterpretable and managerially actionable insights. We apply the methodology to\n20,000 Yelp reviews of Starbucks stores and evaluate eight prompt variants on a\nrandom subset of reviews. Model performance is assessed through agreement with\nhuman annotations and predictive validity for customer ratings. Results show\nhigh consistency between LLMs and human coders and strong predictive validity,\nconfirming the reliability of the approach. Human coders required a median of\nsix minutes per review, whereas the LLM processed each in two seconds,\ndelivering comparable insights at a scale unattainable through manual coding.\nManagerially, the analysis identifies attributes and features that most\nstrongly influence customer satisfaction and their associated sentiments,\nenabling firms to pinpoint \"joy points,\" address \"pain points,\" and design\ntargeted interventions. We demonstrate how structured review data can power an\nactionable marketing dashboard that tracks sentiment over time and across\nstores, benchmarks performance, and highlights high-leverage features for\nimprovement. Simulations indicate that enhancing sentiment for key service\nfeatures could yield 1-2% average revenue gains per store."
                },
                "authors": [
                    {
                        "name": "Khaled Boughanmi"
                    },
                    {
                        "name": "Kamel Jedidi"
                    },
                    {
                        "name": "Nour Jedidi"
                    }
                ],
                "author_detail": {
                    "name": "Nour Jedidi"
                },
                "author": "Nour Jedidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16551v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16551v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19514v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19514v1",
                "updated": "2025-10-22T12:09:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    12,
                    9,
                    50,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T12:09:50Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    12,
                    9,
                    50,
                    2,
                    295,
                    0
                ],
                "title": "From Prototypes to Sparse ECG Explanations: SHAP-Driven Counterfactuals\n  for Multivariate Time-Series Multi-class Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Prototypes to Sparse ECG Explanations: SHAP-Driven Counterfactuals\n  for Multivariate Time-Series Multi-class Classification"
                },
                "summary": "In eXplainable Artificial Intelligence (XAI), instance-based explanations for\ntime series have gained increasing attention due to their potential for\nactionable and interpretable insights in domains such as healthcare. Addressing\nthe challenges of explainability of state-of-the-art models, we propose a\nprototype-driven framework for generating sparse counterfactual explanations\ntailored to 12-lead ECG classification models. Our method employs SHAP-based\nthresholds to identify critical signal segments and convert them into interval\nrules, uses Dynamic Time Warping (DTW) and medoid clustering to extract\nrepresentative prototypes, and aligns these prototypes to query R-peaks for\ncoherence with the sample being explained. The framework generates\ncounterfactuals that modify only 78% of the original signal while maintaining\n81.3% validity across all classes and achieving 43% improvement in temporal\nstability. We evaluate three variants of our approach, Original, Sparse, and\nAligned Sparse, with class-specific performance ranging from 98.9% validity for\nmyocardial infarction (MI) to challenges with hypertrophy (HYP) detection\n(13.2%). This approach supports near realtime generation (< 1 second) of\nclinically valid counterfactuals and provides a foundation for interactive\nexplanation platforms. Our findings establish design principles for\nphysiologically-aware counterfactual explanations in AI-based diagnosis systems\nand outline pathways toward user-controlled explanation interfaces for clinical\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In eXplainable Artificial Intelligence (XAI), instance-based explanations for\ntime series have gained increasing attention due to their potential for\nactionable and interpretable insights in domains such as healthcare. Addressing\nthe challenges of explainability of state-of-the-art models, we propose a\nprototype-driven framework for generating sparse counterfactual explanations\ntailored to 12-lead ECG classification models. Our method employs SHAP-based\nthresholds to identify critical signal segments and convert them into interval\nrules, uses Dynamic Time Warping (DTW) and medoid clustering to extract\nrepresentative prototypes, and aligns these prototypes to query R-peaks for\ncoherence with the sample being explained. The framework generates\ncounterfactuals that modify only 78% of the original signal while maintaining\n81.3% validity across all classes and achieving 43% improvement in temporal\nstability. We evaluate three variants of our approach, Original, Sparse, and\nAligned Sparse, with class-specific performance ranging from 98.9% validity for\nmyocardial infarction (MI) to challenges with hypertrophy (HYP) detection\n(13.2%). This approach supports near realtime generation (< 1 second) of\nclinically valid counterfactuals and provides a foundation for interactive\nexplanation platforms. Our findings establish design principles for\nphysiologically-aware counterfactual explanations in AI-based diagnosis systems\nand outline pathways toward user-controlled explanation interfaces for clinical\ndeployment."
                },
                "authors": [
                    {
                        "name": "Maciej Mozolewski"
                    },
                    {
                        "name": "Betül Bayrak"
                    },
                    {
                        "name": "Kerstin Bach"
                    },
                    {
                        "name": "Grzegorz J. Nalepa"
                    }
                ],
                "author_detail": {
                    "name": "Grzegorz J. Nalepa"
                },
                "author": "Grzegorz J. Nalepa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19514v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19514v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15568v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15568v5",
                "updated": "2025-10-22T12:05:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    12,
                    5,
                    11,
                    2,
                    295,
                    0
                ],
                "published": "2025-08-21T13:42:49Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    13,
                    42,
                    49,
                    3,
                    233,
                    0
                ],
                "title": "Backpropagation-Free Test-Time Adaptation via Probabilistic Gaussian\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backpropagation-Free Test-Time Adaptation via Probabilistic Gaussian\n  Alignment"
                },
                "summary": "Test-time adaptation (TTA) enhances the zero-shot robustness under\ndistribution shifts by leveraging unlabeled test data during inference. Despite\nnotable advances, several challenges still limit its broader applicability.\nFirst, most methods rely on backpropagation or iterative optimization, which\nlimits scalability and hinders real-time deployment. Second, they lack explicit\nmodeling of class-conditional feature distributions. This modeling is crucial\nfor producing reliable decision boundaries and calibrated predictions, but it\nremains underexplored due to the lack of both source data and supervision at\ntest time. In this paper, we propose ADAPT, an Advanced Distribution-Aware and\nbackPropagation-free Test-time adaptation method. We reframe TTA as a Gaussian\nprobabilistic inference task by modeling class-conditional likelihoods using\ngradually updated class means and a shared covariance matrix. This enables\nclosed-form, training-free inference. To correct potential likelihood bias, we\nintroduce lightweight regularization guided by CLIP priors and a historical\nknowledge bank. ADAPT requires no source data, no gradient updates, and no full\naccess to target data, supporting both online and transductive settings.\nExtensive experiments across diverse benchmarks demonstrate that our method\nachieves state-of-the-art performance under a wide range of distribution shifts\nwith superior scalability and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation (TTA) enhances the zero-shot robustness under\ndistribution shifts by leveraging unlabeled test data during inference. Despite\nnotable advances, several challenges still limit its broader applicability.\nFirst, most methods rely on backpropagation or iterative optimization, which\nlimits scalability and hinders real-time deployment. Second, they lack explicit\nmodeling of class-conditional feature distributions. This modeling is crucial\nfor producing reliable decision boundaries and calibrated predictions, but it\nremains underexplored due to the lack of both source data and supervision at\ntest time. In this paper, we propose ADAPT, an Advanced Distribution-Aware and\nbackPropagation-free Test-time adaptation method. We reframe TTA as a Gaussian\nprobabilistic inference task by modeling class-conditional likelihoods using\ngradually updated class means and a shared covariance matrix. This enables\nclosed-form, training-free inference. To correct potential likelihood bias, we\nintroduce lightweight regularization guided by CLIP priors and a historical\nknowledge bank. ADAPT requires no source data, no gradient updates, and no full\naccess to target data, supporting both online and transductive settings.\nExtensive experiments across diverse benchmarks demonstrate that our method\nachieves state-of-the-art performance under a wide range of distribution shifts\nwith superior scalability and robustness."
                },
                "authors": [
                    {
                        "name": "Youjia Zhang"
                    },
                    {
                        "name": "Youngeun Kim"
                    },
                    {
                        "name": "Young-Geun Choi"
                    },
                    {
                        "name": "Hongyeob Kim"
                    },
                    {
                        "name": "Huiling Liu"
                    },
                    {
                        "name": "Sungeun Hong"
                    }
                ],
                "author_detail": {
                    "name": "Sungeun Hong"
                },
                "author": "Sungeun Hong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15568v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15568v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19507v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19507v1",
                "updated": "2025-10-22T12:03:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    12,
                    3,
                    43,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T12:03:43Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    12,
                    3,
                    43,
                    2,
                    295,
                    0
                ],
                "title": "Teaming LLMs to Detect and Mitigate Hallucinations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaming LLMs to Detect and Mitigate Hallucinations"
                },
                "summary": "Recent work has demonstrated state-of-the-art results in large language model\n(LLM) hallucination detection and mitigation through consistency-based\napproaches which involve aggregating multiple responses sampled from a single\nLLM for a given prompt. These approaches help offset limitations stemming from\nthe imperfect data on which LLMs are trained, which includes biases and\nunder-representation of information required at deployment time among other\nlimitations which can lead to hallucinations. We show that extending these\nsingle-model consistency methods to combine responses from multiple LLMs with\ndifferent training data, training schemes and model architectures can result in\nsubstantial further improvements in hallucination detection and mitigation\ncapabilities beyond their single-model consistency counterparts. We evaluate\nthis \\emph{consortium consistency} approach across many model teams from a pool\nof 15 LLMs and explore under what conditions it is beneficial to team together\ndifferent LLMs in this manner. Further, we show that these performance\nimprovements often come with reduced inference costs, offsetting a significant\ndrawback with single-model consistency methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has demonstrated state-of-the-art results in large language model\n(LLM) hallucination detection and mitigation through consistency-based\napproaches which involve aggregating multiple responses sampled from a single\nLLM for a given prompt. These approaches help offset limitations stemming from\nthe imperfect data on which LLMs are trained, which includes biases and\nunder-representation of information required at deployment time among other\nlimitations which can lead to hallucinations. We show that extending these\nsingle-model consistency methods to combine responses from multiple LLMs with\ndifferent training data, training schemes and model architectures can result in\nsubstantial further improvements in hallucination detection and mitigation\ncapabilities beyond their single-model consistency counterparts. We evaluate\nthis \\emph{consortium consistency} approach across many model teams from a pool\nof 15 LLMs and explore under what conditions it is beneficial to team together\ndifferent LLMs in this manner. Further, we show that these performance\nimprovements often come with reduced inference costs, offsetting a significant\ndrawback with single-model consistency methods."
                },
                "authors": [
                    {
                        "name": "Demian Till"
                    },
                    {
                        "name": "John Smeaton"
                    },
                    {
                        "name": "Peter Haubrick"
                    },
                    {
                        "name": "Gouse Saheb"
                    },
                    {
                        "name": "Florian Graef"
                    },
                    {
                        "name": "David Berman"
                    }
                ],
                "author_detail": {
                    "name": "David Berman"
                },
                "author": "David Berman",
                "arxiv_comment": "Accepted to NeurIPS 2025 workshop on Reliable ML from Unreliable Data",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19507v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12718v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12718v2",
                "updated": "2025-10-22T12:00:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    12,
                    0,
                    52,
                    2,
                    295,
                    0
                ],
                "published": "2024-12-17T09:33:06Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    33,
                    6,
                    1,
                    352,
                    0
                ],
                "title": "ASAP: Advancing Semantic Alignment Promotes Multi-Modal Manipulation\n  Detecting and Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASAP: Advancing Semantic Alignment Promotes Multi-Modal Manipulation\n  Detecting and Grounding"
                },
                "summary": "We present ASAP, a new framework for detecting and grounding multi-modal\nmedia manipulation (DGM4).Upon thorough examination, we observe that accurate\nfine-grained cross-modal semantic alignment between the image and text is vital\nfor accurately manipulation detection and grounding. While existing DGM4\nmethods pay rare attention to the cross-modal alignment, hampering the accuracy\nof manipulation detecting to step further. To remedy this issue, this work\ntargets to advance the semantic alignment learning to promote this task.\nParticularly, we utilize the off-the-shelf Multimodal Large-Language Models\n(MLLMs) and Large Language Models (LLMs) to construct paired image-text pairs,\nespecially for the manipulated instances. Subsequently, a cross-modal alignment\nlearning is performed to enhance the semantic alignment. Besides the explicit\nauxiliary clues, we further design a Manipulation-Guided Cross Attention (MGCA)\nto provide implicit guidance for augmenting the manipulation perceiving. With\nthe grounding truth available during training, MGCA encourages the model to\nconcentrate more on manipulated components while downplaying normal ones,\nenhancing the model's ability to capture manipulations. Extensive experiments\nare conducted on the DGM4 dataset, the results demonstrate that our model can\nsurpass the comparison method with a clear margin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present ASAP, a new framework for detecting and grounding multi-modal\nmedia manipulation (DGM4).Upon thorough examination, we observe that accurate\nfine-grained cross-modal semantic alignment between the image and text is vital\nfor accurately manipulation detection and grounding. While existing DGM4\nmethods pay rare attention to the cross-modal alignment, hampering the accuracy\nof manipulation detecting to step further. To remedy this issue, this work\ntargets to advance the semantic alignment learning to promote this task.\nParticularly, we utilize the off-the-shelf Multimodal Large-Language Models\n(MLLMs) and Large Language Models (LLMs) to construct paired image-text pairs,\nespecially for the manipulated instances. Subsequently, a cross-modal alignment\nlearning is performed to enhance the semantic alignment. Besides the explicit\nauxiliary clues, we further design a Manipulation-Guided Cross Attention (MGCA)\nto provide implicit guidance for augmenting the manipulation perceiving. With\nthe grounding truth available during training, MGCA encourages the model to\nconcentrate more on manipulated components while downplaying normal ones,\nenhancing the model's ability to capture manipulations. Extensive experiments\nare conducted on the DGM4 dataset, the results demonstrate that our model can\nsurpass the comparison method with a clear margin."
                },
                "authors": [
                    {
                        "name": "Zhenxing Zhang"
                    },
                    {
                        "name": "Yaxiong Wang"
                    },
                    {
                        "name": "Lechao Cheng"
                    },
                    {
                        "name": "Zhun Zhong"
                    },
                    {
                        "name": "Dan Guo"
                    },
                    {
                        "name": "Meng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Meng Wang"
                },
                "author": "Meng Wang",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12718v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12718v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "Multimedia",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19506v1",
                "updated": "2025-10-22T12:00:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    12,
                    0,
                    21,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T12:00:21Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    12,
                    0,
                    21,
                    2,
                    295,
                    0
                ],
                "title": "Lookahead Routing for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lookahead Routing for Large Language Models"
                },
                "summary": "Large language model (LLM) routers improve the efficiency of multi-model\nsystems by directing each query to the most appropriate model while leveraging\nthe diverse strengths of heterogeneous LLMs. Most existing approaches frame\nrouting as a classification problem based solely on the input query. While this\nreduces overhead by avoiding inference across all models, it overlooks valuable\ninformation that could be gleaned from potential outputs and fails to capture\nimplicit intent or contextual nuances that often emerge only during response\ngeneration. These limitations can result in suboptimal routing decisions,\nparticularly for complex or ambiguous queries that require deeper semantic\nunderstanding. To address this challenge, we propose Lookahead, a routing\nframework that \"foresees\" potential model outputs by predicting their latent\nrepresentations and uses these predictions to guide model selection, thus\nenabling more informed routing without full inference. Within this framework,\nwe implement two approaches based on causal and masked language models.\nEmpirical evaluations across seven public benchmarks - spanning instruction\nfollowing, mathematical reasoning, and code generation - show that Lookahead\nconsistently outperforms existing routing baselines, achieving an average\nperformance gain of 7.7% over the state-of-the-art. Our code is available at\nhttps://github.com/huangcb01/lookahead-routing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) routers improve the efficiency of multi-model\nsystems by directing each query to the most appropriate model while leveraging\nthe diverse strengths of heterogeneous LLMs. Most existing approaches frame\nrouting as a classification problem based solely on the input query. While this\nreduces overhead by avoiding inference across all models, it overlooks valuable\ninformation that could be gleaned from potential outputs and fails to capture\nimplicit intent or contextual nuances that often emerge only during response\ngeneration. These limitations can result in suboptimal routing decisions,\nparticularly for complex or ambiguous queries that require deeper semantic\nunderstanding. To address this challenge, we propose Lookahead, a routing\nframework that \"foresees\" potential model outputs by predicting their latent\nrepresentations and uses these predictions to guide model selection, thus\nenabling more informed routing without full inference. Within this framework,\nwe implement two approaches based on causal and masked language models.\nEmpirical evaluations across seven public benchmarks - spanning instruction\nfollowing, mathematical reasoning, and code generation - show that Lookahead\nconsistently outperforms existing routing baselines, achieving an average\nperformance gain of 7.7% over the state-of-the-art. Our code is available at\nhttps://github.com/huangcb01/lookahead-routing."
                },
                "authors": [
                    {
                        "name": "Canbin Huang"
                    },
                    {
                        "name": "Tianyuan Shi"
                    },
                    {
                        "name": "Yuhua Zhu"
                    },
                    {
                        "name": "Ruijun Chen"
                    },
                    {
                        "name": "Xiaojun Quan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Quan"
                },
                "author": "Xiaojun Quan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08878v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08878v3",
                "updated": "2025-10-22T11:58:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    11,
                    58,
                    26,
                    2,
                    295,
                    0
                ],
                "published": "2024-08-02T17:42:03Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    17,
                    42,
                    3,
                    4,
                    215,
                    0
                ],
                "title": "Knowledge Prompting: How Knowledge Engineers Use Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Prompting: How Knowledge Engineers Use Large Language Models"
                },
                "summary": "Despite many advances in knowledge engineering (KE), challenges remain in\nareas such as engineering knowledge graphs (KGs) at scale, keeping up with\nevolving domain knowledge, multilingualism, and multimodality. Recently, KE has\nused LLMs to support semi-automatic tasks, but the most effective use of LLMs\nto support knowledge engineers across the KE activites is still in its infancy.\nTo explore the vision of LLM copilots for KE and change existing KE practices,\nwe conducted a multimethod study during a KE hackathon. We investigated\nparticipants' views on the use of LLMs, the challenges they face, the skills\nthey may need to integrate LLMs into their practices, and how they use LLMs\nresponsibly. We found participants felt LLMs could contribute to improving\nefficiency when engineering KGs, but presented increased challenges around the\nalready complex issues of evaluating the KE tasks. We discovered prompting to\nbe a useful but undervalued skill for knowledge engineers working with LLMs,\nand note that natural language processing skills may become more relevant\nacross more roles in KG construction. Integrating LLMs into KE tasks needs to\nbe mindful of potential risks and harms related to responsible AI. Given the\nlimited ethical training, most knowledge engineers receive solutions such as\nour suggested `KG cards' based on data cards could be a useful guide for KG\nconstruction. Our findings can support designers of KE AI copilots, KE\nresearchers, and practitioners using advanced AI to develop trustworthy\napplications, propose new methodologies for KE and operate new technologies\nresponsibly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite many advances in knowledge engineering (KE), challenges remain in\nareas such as engineering knowledge graphs (KGs) at scale, keeping up with\nevolving domain knowledge, multilingualism, and multimodality. Recently, KE has\nused LLMs to support semi-automatic tasks, but the most effective use of LLMs\nto support knowledge engineers across the KE activites is still in its infancy.\nTo explore the vision of LLM copilots for KE and change existing KE practices,\nwe conducted a multimethod study during a KE hackathon. We investigated\nparticipants' views on the use of LLMs, the challenges they face, the skills\nthey may need to integrate LLMs into their practices, and how they use LLMs\nresponsibly. We found participants felt LLMs could contribute to improving\nefficiency when engineering KGs, but presented increased challenges around the\nalready complex issues of evaluating the KE tasks. We discovered prompting to\nbe a useful but undervalued skill for knowledge engineers working with LLMs,\nand note that natural language processing skills may become more relevant\nacross more roles in KG construction. Integrating LLMs into KE tasks needs to\nbe mindful of potential risks and harms related to responsible AI. Given the\nlimited ethical training, most knowledge engineers receive solutions such as\nour suggested `KG cards' based on data cards could be a useful guide for KG\nconstruction. Our findings can support designers of KE AI copilots, KE\nresearchers, and practitioners using advanced AI to develop trustworthy\napplications, propose new methodologies for KE and operate new technologies\nresponsibly."
                },
                "authors": [
                    {
                        "name": "Elisavet Koutsiana"
                    },
                    {
                        "name": "Johanna Walker"
                    },
                    {
                        "name": "Michelle Nwachukwu"
                    },
                    {
                        "name": "Bohui Zhang"
                    },
                    {
                        "name": "Albert Meroño-Peñuela"
                    },
                    {
                        "name": "Elena Simperl"
                    }
                ],
                "author_detail": {
                    "name": "Elena Simperl"
                },
                "author": "Elena Simperl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08878v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08878v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19498v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19498v1",
                "updated": "2025-10-22T11:50:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    11,
                    50,
                    0,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T11:50:00Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    11,
                    50,
                    0,
                    2,
                    295,
                    0
                ],
                "title": "Energy-Efficient and Dequantization-Free Q-LLMs: A Spiking Neural\n  Network Approach to Salient Value Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy-Efficient and Dequantization-Free Q-LLMs: A Spiking Neural\n  Network Approach to Salient Value Mitigation"
                },
                "summary": "In the era of large language models (LLMs), weight-activation quantization\nhelps fit models on edge device by reducing memory and compute bit-widths.\nHowever, three challenges persist for energy constrained hardware: (1) even\nafter quantization, multiply-accumulate (MAC) operations remain unavoidable and\ncontinue to dominate energy consumption; (2) dequantization (or\nper-tensor/channel rescaling) introduces extra arithmetic and data movement,\nincreasing latency and energy; (3) uniform parameters bit widths clip salient\nvalues-while intra-channel mixed precision is generally impractical on current\nmatrix hardware and memory. In contrast, brain-inspired Spiking Neural Networks\n(SNNs), owing to their binary spike-based information representation and the\nIntegrate-and-Fire (IF) paradigm, naturally support mixed-precision storage and\nenergy-efficient computation by replacing complex MACs with temporal Accumulate\n(ACCs). Motivated by this property, we propose SpikeQuant, which selectively\napplies mixed-precision quantization to activations with salient values and\nre-encodes them into binary spike counts, thereby enabling dynamic mixed\nstorage of different bitwidths. Furthermore, by embedding the quantization\nscale into the threshold of the IF mechanism, our approach performs\nenergy-efficient linear transformations on weights and activations while\navoiding explicit dequantization. Experimental results demonstrate that\nSpikeQuant consistently achieves near-FP16 perplexity under W4A4 quantization\nwhile reducing energy cost by up to 4.6 times compared to existing methods,\nhighlighting its effectiveness for accurate and energy-efficient LLM\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of large language models (LLMs), weight-activation quantization\nhelps fit models on edge device by reducing memory and compute bit-widths.\nHowever, three challenges persist for energy constrained hardware: (1) even\nafter quantization, multiply-accumulate (MAC) operations remain unavoidable and\ncontinue to dominate energy consumption; (2) dequantization (or\nper-tensor/channel rescaling) introduces extra arithmetic and data movement,\nincreasing latency and energy; (3) uniform parameters bit widths clip salient\nvalues-while intra-channel mixed precision is generally impractical on current\nmatrix hardware and memory. In contrast, brain-inspired Spiking Neural Networks\n(SNNs), owing to their binary spike-based information representation and the\nIntegrate-and-Fire (IF) paradigm, naturally support mixed-precision storage and\nenergy-efficient computation by replacing complex MACs with temporal Accumulate\n(ACCs). Motivated by this property, we propose SpikeQuant, which selectively\napplies mixed-precision quantization to activations with salient values and\nre-encodes them into binary spike counts, thereby enabling dynamic mixed\nstorage of different bitwidths. Furthermore, by embedding the quantization\nscale into the threshold of the IF mechanism, our approach performs\nenergy-efficient linear transformations on weights and activations while\navoiding explicit dequantization. Experimental results demonstrate that\nSpikeQuant consistently achieves near-FP16 perplexity under W4A4 quantization\nwhile reducing energy cost by up to 4.6 times compared to existing methods,\nhighlighting its effectiveness for accurate and energy-efficient LLM\ndeployment."
                },
                "authors": [
                    {
                        "name": "Chenyu Wang"
                    },
                    {
                        "name": "Zhanglu Yan"
                    },
                    {
                        "name": "Zhi Zhou"
                    },
                    {
                        "name": "Xu Chen"
                    },
                    {
                        "name": "Weng-Fai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Weng-Fai Wong"
                },
                "author": "Weng-Fai Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19498v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19497v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19497v1",
                "updated": "2025-10-22T11:45:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    11,
                    45,
                    44,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T11:45:44Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    11,
                    45,
                    44,
                    2,
                    295,
                    0
                ],
                "title": "Modeling realistic human behavior using generative agents in a\n  multimodal transport system: Software architecture and Application to\n  Toulouse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling realistic human behavior using generative agents in a\n  multimodal transport system: Software architecture and Application to\n  Toulouse"
                },
                "summary": "Modeling realistic human behaviour to understand people's mode choices in\norder to propose personalised mobility solutions remains challenging. This\npaper presents an architecture for modeling realistic human mobility behavior\nin complex multimodal transport systems, demonstrated through a case study in\nToulouse, France. We apply Large Language Models (LLMs) within an agent-based\nsimulation to capture decision-making in a real urban setting. The framework\nintegrates the GAMA simulation platform with an LLM-based generative agent,\nalong with General Transit Feed Specification (GTFS) data for public transport,\nand OpenTripPlanner for multimodal routing. GAMA platform models the\ninteractive transport environment, providing visualization and dynamic agent\ninteractions while eliminating the need to construct the simulation environment\nfrom scratch. This design enables a stronger focus on developing generative\nagents and evaluating their performance in transport decision-making processes.\nOver a simulated month, results show that agents not only make context-aware\ntransport decisions but also form habits over time. We conclude that combining\nLLMs with agent-based simulation offers a promising direction for advancing\nintelligent transportation systems and personalised multimodal mobility\nsolutions. We also discuss some limitations of this approach and outline future\nwork on scaling to larger regions, integrating real-time data, and refining\nmemory models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling realistic human behaviour to understand people's mode choices in\norder to propose personalised mobility solutions remains challenging. This\npaper presents an architecture for modeling realistic human mobility behavior\nin complex multimodal transport systems, demonstrated through a case study in\nToulouse, France. We apply Large Language Models (LLMs) within an agent-based\nsimulation to capture decision-making in a real urban setting. The framework\nintegrates the GAMA simulation platform with an LLM-based generative agent,\nalong with General Transit Feed Specification (GTFS) data for public transport,\nand OpenTripPlanner for multimodal routing. GAMA platform models the\ninteractive transport environment, providing visualization and dynamic agent\ninteractions while eliminating the need to construct the simulation environment\nfrom scratch. This design enables a stronger focus on developing generative\nagents and evaluating their performance in transport decision-making processes.\nOver a simulated month, results show that agents not only make context-aware\ntransport decisions but also form habits over time. We conclude that combining\nLLMs with agent-based simulation offers a promising direction for advancing\nintelligent transportation systems and personalised multimodal mobility\nsolutions. We also discuss some limitations of this approach and outline future\nwork on scaling to larger regions, integrating real-time data, and refining\nmemory models."
                },
                "authors": [
                    {
                        "name": "Trung-Dung Vu"
                    },
                    {
                        "name": "Benoit Gaudou"
                    },
                    {
                        "name": "Kamaldeep Singh Oberoi"
                    }
                ],
                "author_detail": {
                    "name": "Kamaldeep Singh Oberoi"
                },
                "author": "Kamaldeep Singh Oberoi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19497v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02289v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02289v2",
                "updated": "2025-10-22T11:43:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    11,
                    43,
                    7,
                    2,
                    295,
                    0
                ],
                "published": "2024-12-03T09:06:57Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    9,
                    6,
                    57,
                    1,
                    338,
                    0
                ],
                "title": "Learn More by Using Less: Distributed Learning with Energy-Constrained\n  Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learn More by Using Less: Distributed Learning with Energy-Constrained\n  Devices"
                },
                "summary": "Federated Learning (FL) has emerged as a solution for distributed model\ntraining across decentralized, privacy-preserving devices, but the different\nenergy capacities of participating devices (system heterogeneity) constrain\nreal-world implementations. These energy limitations not only reduce model\naccuracy but also increase dropout rates, impacting on convergence in practical\nFL deployments. In this work, we propose LeanFed, an energy-aware FL framework\ndesigned to optimize client selection and training workloads on\nbattery-constrained devices. LeanFed leverages adaptive data usage by\ndynamically adjusting the fraction of local data each device utilizes during\ntraining, thereby maximizing device participation across communication rounds\nwhile ensuring they do not run out of battery during the process. We rigorously\nevaluate LeanFed against traditional FedAvg on CIFAR-10 and CIFAR-100 datasets,\nsimulating various levels of data heterogeneity and device participation rates.\nResults show that LeanFed consistently enhances model accuracy and stability,\nparticularly in settings with high data heterogeneity and limited battery life,\nby mitigating client dropout and extending device availability. This approach\ndemonstrates the potential of energy-efficient, privacy-preserving FL in\nreal-world, large-scale applications, setting a foundation for robust and\nsustainable pervasive AI on resource-constrained networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) has emerged as a solution for distributed model\ntraining across decentralized, privacy-preserving devices, but the different\nenergy capacities of participating devices (system heterogeneity) constrain\nreal-world implementations. These energy limitations not only reduce model\naccuracy but also increase dropout rates, impacting on convergence in practical\nFL deployments. In this work, we propose LeanFed, an energy-aware FL framework\ndesigned to optimize client selection and training workloads on\nbattery-constrained devices. LeanFed leverages adaptive data usage by\ndynamically adjusting the fraction of local data each device utilizes during\ntraining, thereby maximizing device participation across communication rounds\nwhile ensuring they do not run out of battery during the process. We rigorously\nevaluate LeanFed against traditional FedAvg on CIFAR-10 and CIFAR-100 datasets,\nsimulating various levels of data heterogeneity and device participation rates.\nResults show that LeanFed consistently enhances model accuracy and stability,\nparticularly in settings with high data heterogeneity and limited battery life,\nby mitigating client dropout and extending device availability. This approach\ndemonstrates the potential of energy-efficient, privacy-preserving FL in\nreal-world, large-scale applications, setting a foundation for robust and\nsustainable pervasive AI on resource-constrained networks."
                },
                "authors": [
                    {
                        "name": "Roberto Pereira"
                    },
                    {
                        "name": "Cristian J. Vaca-Rubio"
                    },
                    {
                        "name": "Luis Blanco"
                    }
                ],
                "author_detail": {
                    "name": "Luis Blanco"
                },
                "author": "Luis Blanco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02289v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02289v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19491v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19491v1",
                "updated": "2025-10-22T11:35:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    11,
                    35,
                    51,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T11:35:51Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    11,
                    35,
                    51,
                    2,
                    295,
                    0
                ],
                "title": "Cross-Chain Sealed-Bid Auctions Using Confidential Compute Blockchains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Chain Sealed-Bid Auctions Using Confidential Compute Blockchains"
                },
                "summary": "Sealed-bid auctions ensure fair competition and efficient allocation but are\noften deployed on centralized infrastructure, enabling opaque manipulation.\nPublic blockchains eliminate central control, yet their inherent transparency\nconflicts with the confidentiality required for sealed bidding. Prior attempts\nstruggle to reconcile privacy, verifiability, and scalability without relying\non trusted intermediaries, multi-round protocols, or expensive cryptography. We\npresent a sealed-bid auction protocol that executes sensitive bidding logic on\na Trusted Execution Environment (TEE)-backed confidential compute blockchain\nwhile retaining settlement and enforcement on a public chain. Bidders commit\nfunds to enclave-generated escrow addresses, ensuring confidentiality and\nbinding commitments. After the deadline, any party can trigger resolution: the\nconfidential blockchain determines the winner through verifiable off-chain\ncomputation and issues signed settlement transactions for execution on the\npublic chain. Our design provides security, privacy, and scalability without\ntrusted third parties or protocol modifications. We implement it on SUAVE with\nEthereum settlement, evaluate its scalability and trust assumptions, and\ndemonstrate deployment with minimal integration on existing infrastructure",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sealed-bid auctions ensure fair competition and efficient allocation but are\noften deployed on centralized infrastructure, enabling opaque manipulation.\nPublic blockchains eliminate central control, yet their inherent transparency\nconflicts with the confidentiality required for sealed bidding. Prior attempts\nstruggle to reconcile privacy, verifiability, and scalability without relying\non trusted intermediaries, multi-round protocols, or expensive cryptography. We\npresent a sealed-bid auction protocol that executes sensitive bidding logic on\na Trusted Execution Environment (TEE)-backed confidential compute blockchain\nwhile retaining settlement and enforcement on a public chain. Bidders commit\nfunds to enclave-generated escrow addresses, ensuring confidentiality and\nbinding commitments. After the deadline, any party can trigger resolution: the\nconfidential blockchain determines the winner through verifiable off-chain\ncomputation and issues signed settlement transactions for execution on the\npublic chain. Our design provides security, privacy, and scalability without\ntrusted third parties or protocol modifications. We implement it on SUAVE with\nEthereum settlement, evaluate its scalability and trust assumptions, and\ndemonstrate deployment with minimal integration on existing infrastructure"
                },
                "authors": [
                    {
                        "name": "Jonas Gebele"
                    },
                    {
                        "name": "Timm Mutzel"
                    },
                    {
                        "name": "Burak Oez"
                    },
                    {
                        "name": "Florian Matthes"
                    }
                ],
                "author_detail": {
                    "name": "Florian Matthes"
                },
                "author": "Florian Matthes",
                "arxiv_doi": "10.1145/3733815.3764043",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3733815.3764043",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.19491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19491v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19482v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19482v1",
                "updated": "2025-10-22T11:20:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    11,
                    20,
                    47,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T11:20:47Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    11,
                    20,
                    47,
                    2,
                    295,
                    0
                ],
                "title": "ELUTQ: Efficient LUT-Aware Quantization for Deploying Large Language\n  Models on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELUTQ: Efficient LUT-Aware Quantization for Deploying Large Language\n  Models on Edge Devices"
                },
                "summary": "The deployment of Large Language Models (LLMs) on CPU-based edge devices is\ncrucial for enabling on-device intelligence and expanding AI accessibility.\nHowever, it remains challenging due to limited memory and computational\nresources. During edge inference, memory usage and latency are the primary\nbottlenecks. Although weight quantization can effectively reduce memory\nconsumption, existing hardware-friendly approaches often rely on uniform\nquantization, which poorly fits weight distributions and incurs high\ndequantization overhead at low bit widths. To address these limitations, we\npropose ELUTQ, an efficient quantization framework introducing a novel\nquantization format, Hierarchical Linear Quantization (HLQ). HLQ better\ncaptures the statistical characteristics of weights without increasing the\ncomputational cost of Bit-serial LUT-based GEMM operations, thereby eliminating\ndequantization overhead. It is orthogonal to existing quantization algorithms\nand can be seamlessly integrated into various quantization pipelines. For\nefficient on-device deployment, ELUTQ provides optimized CPU kernels for\nend-to-end inference. Experiments show that for LLaMA3-8B, HLQ reduces\nperplexity by about 8% at 3-bit and 85% at 2-bit precision under post-training\nquantization, completing quantization within one hour. With efficient\nfinetuning, HLQ further improves 2-bit performance within two hours. In terms\nof inference efficiency, our 2-bit LLaMA2-7B achieves over 25 tokens/s on an\nApple M2 chip (4 threads, batch size = 1).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Large Language Models (LLMs) on CPU-based edge devices is\ncrucial for enabling on-device intelligence and expanding AI accessibility.\nHowever, it remains challenging due to limited memory and computational\nresources. During edge inference, memory usage and latency are the primary\nbottlenecks. Although weight quantization can effectively reduce memory\nconsumption, existing hardware-friendly approaches often rely on uniform\nquantization, which poorly fits weight distributions and incurs high\ndequantization overhead at low bit widths. To address these limitations, we\npropose ELUTQ, an efficient quantization framework introducing a novel\nquantization format, Hierarchical Linear Quantization (HLQ). HLQ better\ncaptures the statistical characteristics of weights without increasing the\ncomputational cost of Bit-serial LUT-based GEMM operations, thereby eliminating\ndequantization overhead. It is orthogonal to existing quantization algorithms\nand can be seamlessly integrated into various quantization pipelines. For\nefficient on-device deployment, ELUTQ provides optimized CPU kernels for\nend-to-end inference. Experiments show that for LLaMA3-8B, HLQ reduces\nperplexity by about 8% at 3-bit and 85% at 2-bit precision under post-training\nquantization, completing quantization within one hour. With efficient\nfinetuning, HLQ further improves 2-bit performance within two hours. In terms\nof inference efficiency, our 2-bit LLaMA2-7B achieves over 25 tokens/s on an\nApple M2 chip (4 threads, batch size = 1)."
                },
                "authors": [
                    {
                        "name": "Xin Nie"
                    },
                    {
                        "name": "Liang Dong"
                    },
                    {
                        "name": "HaiCheng Zhang"
                    },
                    {
                        "name": "JiaWang Xiao"
                    },
                    {
                        "name": "G. Sun"
                    }
                ],
                "author_detail": {
                    "name": "G. Sun"
                },
                "author": "G. Sun",
                "arxiv_comment": "19 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19482v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19482v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21589v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21589v5",
                "updated": "2025-10-22T11:09:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    11,
                    9,
                    23,
                    2,
                    295,
                    0
                ],
                "published": "2025-08-29T12:47:27Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    12,
                    47,
                    27,
                    4,
                    241,
                    0
                ],
                "title": "Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM\n  Fine-Tuning via Closed-Loop Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM\n  Fine-Tuning via Closed-Loop Learning"
                },
                "summary": "Supervised Fine-Tuning (SFT) Large Language Models (LLM) fundamentally rely\non high-quality training data. While data selection and data synthesis are two\ncommon strategies to improve data quality, existing approaches often face\nlimitations in static dataset curation that fail to adapt to evolving model\ncapabilities. In this paper, we introduce Middo, a self-evolving Model-informed\ndynamic data optimization framework that uses model-aware data selection and\ncontext-preserving data refinement. Unlike conventional one-off\nfiltering/synthesis methods, our framework establishes a closed-loop\noptimization system: (1) A self-referential diagnostic module proactively\nidentifies suboptimal samples through tri-axial model signals - loss patterns\n(complexity), embedding cluster dynamics (diversity), and self-alignment scores\n(quality); (2) An adaptive optimization engine then transforms suboptimal\nsamples into pedagogically valuable training points while preserving semantic\nintegrity; (3) This optimization process continuously evolves with model\ncapability through dynamic learning principles. Experiments on multiple\nbenchmarks demonstrate that our Middo consistently enhances the quality of seed\ndata and boosts LLM's performance with improving accuracy by 7.15% on average\nwhile maintaining the original dataset scale. This work establishes a new\nparadigm for sustainable LLM training through dynamic human-AI co-evolution of\ndata and models. Our datasets, models, and code are publicly available at\nhttps://github.com/Word2VecT/Middo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised Fine-Tuning (SFT) Large Language Models (LLM) fundamentally rely\non high-quality training data. While data selection and data synthesis are two\ncommon strategies to improve data quality, existing approaches often face\nlimitations in static dataset curation that fail to adapt to evolving model\ncapabilities. In this paper, we introduce Middo, a self-evolving Model-informed\ndynamic data optimization framework that uses model-aware data selection and\ncontext-preserving data refinement. Unlike conventional one-off\nfiltering/synthesis methods, our framework establishes a closed-loop\noptimization system: (1) A self-referential diagnostic module proactively\nidentifies suboptimal samples through tri-axial model signals - loss patterns\n(complexity), embedding cluster dynamics (diversity), and self-alignment scores\n(quality); (2) An adaptive optimization engine then transforms suboptimal\nsamples into pedagogically valuable training points while preserving semantic\nintegrity; (3) This optimization process continuously evolves with model\ncapability through dynamic learning principles. Experiments on multiple\nbenchmarks demonstrate that our Middo consistently enhances the quality of seed\ndata and boosts LLM's performance with improving accuracy by 7.15% on average\nwhile maintaining the original dataset scale. This work establishes a new\nparadigm for sustainable LLM training through dynamic human-AI co-evolution of\ndata and models. Our datasets, models, and code are publicly available at\nhttps://github.com/Word2VecT/Middo."
                },
                "authors": [
                    {
                        "name": "Zinan Tang"
                    },
                    {
                        "name": "Xin Gao"
                    },
                    {
                        "name": "Qizhi Pei"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Mengzhang Cai"
                    },
                    {
                        "name": "Jiang Wu"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Lijun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Lijun Wu"
                },
                "author": "Lijun Wu",
                "arxiv_comment": "Accepted by EMNLP 2025 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21589v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21589v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18572v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18572v2",
                "updated": "2025-10-22T10:52:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    10,
                    52,
                    26,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-21T12:28:11Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    28,
                    11,
                    1,
                    294,
                    0
                ],
                "title": "Forward to Hell? On the Potentials of Misusing Transparent DNS\n  Forwarders in Reflective Amplification Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forward to Hell? On the Potentials of Misusing Transparent DNS\n  Forwarders in Reflective Amplification Attacks"
                },
                "summary": "The DNS infrastructure is infamous for facilitating reflective amplification\nattacks. Various countermeasures such as server shielding, access control, rate\nlimiting, and protocol restrictions have been implemented. Still, the threat\nremains throughout the deployment of DNS servers. In this paper, we report on\nand evaluate the often unnoticed threat that derives from transparent DNS\nforwarders, a widely deployed, incompletely functional set of DNS components.\nTransparent DNS forwarders transfer DNS requests without rebuilding packets\nwith correct source addresses. As such, transparent forwarders feed DNS\nrequests into (mainly powerful and anycasted) open recursive resolvers, which\nthereby can be misused to participate unwillingly in distributed reflective\namplification attacks. We show how transparent forwarders raise severe threats\nto the Internet infrastructure. They easily circumvent rate limiting and\nachieve an additional, scalable impact via the DNS anycast infrastructure. We\nempirically verify this scaling behavior up to a factor of 14. Transparent\nforwarders can also assist in bypassing firewall rules that protect recursive\nresolvers, making these shielded infrastructure entities part of the global DNS\nattack surface.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The DNS infrastructure is infamous for facilitating reflective amplification\nattacks. Various countermeasures such as server shielding, access control, rate\nlimiting, and protocol restrictions have been implemented. Still, the threat\nremains throughout the deployment of DNS servers. In this paper, we report on\nand evaluate the often unnoticed threat that derives from transparent DNS\nforwarders, a widely deployed, incompletely functional set of DNS components.\nTransparent DNS forwarders transfer DNS requests without rebuilding packets\nwith correct source addresses. As such, transparent forwarders feed DNS\nrequests into (mainly powerful and anycasted) open recursive resolvers, which\nthereby can be misused to participate unwillingly in distributed reflective\namplification attacks. We show how transparent forwarders raise severe threats\nto the Internet infrastructure. They easily circumvent rate limiting and\nachieve an additional, scalable impact via the DNS anycast infrastructure. We\nempirically verify this scaling behavior up to a factor of 14. Transparent\nforwarders can also assist in bypassing firewall rules that protect recursive\nresolvers, making these shielded infrastructure entities part of the global DNS\nattack surface."
                },
                "authors": [
                    {
                        "name": "Maynard Koch"
                    },
                    {
                        "name": "Florian Dolzmann"
                    },
                    {
                        "name": "Thomas C. Schmidt"
                    },
                    {
                        "name": "Matthias Wählisch"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Wählisch"
                },
                "author": "Matthias Wählisch",
                "arxiv_doi": "10.1145/3719027.3765096",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3719027.3765096",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.18572v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18572v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of ACM CCS 2025",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19462v1",
                "updated": "2025-10-22T10:50:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    10,
                    50,
                    22,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T10:50:22Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    10,
                    50,
                    22,
                    2,
                    295,
                    0
                ],
                "title": "AegisMCP: Online Graph Intrusion Detection for Tool-Augmented LLMs on\n  Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AegisMCP: Online Graph Intrusion Detection for Tool-Augmented LLMs on\n  Edge Devices"
                },
                "summary": "In this work, we study security of Model Context Protocol (MCP) agent\ntoolchains and their applications in smart homes. We introduce AegisMCP, a\nprotocol-level intrusion detector. Our contributions are: (i) a minimal attack\nsuite spanning instruction-driven escalation, chain-of-tool exfiltration,\nmalicious MCP server registration, and persistence; (ii) NEBULA-Schema\n(Network-Edge Behavioral Learning for Untrusted LLM Agents), a reusable\nprotocol-level instrumentation that represents MCP activity as a streaming\nheterogeneous temporal graph over agents, MCP servers, tools, devices, remotes,\nand sessions; and (iii) a CPU-only streaming detector that fuses novelty,\nsession-DAG structure, and attribute cues for near-real-time edge inference,\nwith optional fusion of local prompt-guardrail signals. On an emulated\nsmart-home testbed spanning multiple MCP stacks and a physical bench, AegisMCP\nachieves sub-second per-window model inference and end-to-end alerting. The\nlatency of AegisMCP is consistently sub-second on Intel N150-class edge\nhardware, while outperforming traffic-only and sequence baselines; ablations\nconfirm the importance of DAG and install/permission signals. We release code,\nschemas, and generators for reproducible evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we study security of Model Context Protocol (MCP) agent\ntoolchains and their applications in smart homes. We introduce AegisMCP, a\nprotocol-level intrusion detector. Our contributions are: (i) a minimal attack\nsuite spanning instruction-driven escalation, chain-of-tool exfiltration,\nmalicious MCP server registration, and persistence; (ii) NEBULA-Schema\n(Network-Edge Behavioral Learning for Untrusted LLM Agents), a reusable\nprotocol-level instrumentation that represents MCP activity as a streaming\nheterogeneous temporal graph over agents, MCP servers, tools, devices, remotes,\nand sessions; and (iii) a CPU-only streaming detector that fuses novelty,\nsession-DAG structure, and attribute cues for near-real-time edge inference,\nwith optional fusion of local prompt-guardrail signals. On an emulated\nsmart-home testbed spanning multiple MCP stacks and a physical bench, AegisMCP\nachieves sub-second per-window model inference and end-to-end alerting. The\nlatency of AegisMCP is consistently sub-second on Intel N150-class edge\nhardware, while outperforming traffic-only and sequence baselines; ablations\nconfirm the importance of DAG and install/permission signals. We release code,\nschemas, and generators for reproducible evaluation."
                },
                "authors": [
                    {
                        "name": "Zhonghao Zhan"
                    },
                    {
                        "name": "Amir Al Sadi"
                    },
                    {
                        "name": "Krinos Li"
                    },
                    {
                        "name": "Hamed Haddadi"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Haddadi"
                },
                "author": "Hamed Haddadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12892v2",
                "updated": "2025-10-22T10:27:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    10,
                    27,
                    32,
                    2,
                    295,
                    0
                ],
                "published": "2025-08-18T12:49:23Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    12,
                    49,
                    23,
                    0,
                    230,
                    0
                ],
                "title": "A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge\n  AI-assisted RAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge\n  AI-assisted RAN"
                },
                "summary": "Artificial intelligence approaches for base-band processing for radio\nreceivers have demonstrated significant performance gains. Most of the proposed\nmethods are characterized by high compute and memory requirements, hindering\ntheir deployment at the edge of the Radio Access Networks (RAN) and limiting\ntheir scalability to large bandwidths and many antenna 6G systems. In this\npaper, we propose a low-complexity, model-driven neural network-based receiver,\ndesigned for multi-user multiple-input multiple-output (MU-MIMO) systems and\nsuitable for implementation at the RAN edge. The proposed solution is compliant\nwith the 5G New Radio (5G NR), and supports different modulation schemes,\nbandwidths, number of users, and number of base-station antennas with a single\ntrained model without the need for further training. Numerical simulations of\nthe Physical Uplink Shared Channel (PUSCH) processing show that the proposed\nsolution outperforms the state-of-the-art methods in terms of achievable\nTransport Block Error Rate (TBLER), while reducing the Floating Point\nOperations (FLOPs) by 66$\\times$, and the learnable parameters by 396$\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence approaches for base-band processing for radio\nreceivers have demonstrated significant performance gains. Most of the proposed\nmethods are characterized by high compute and memory requirements, hindering\ntheir deployment at the edge of the Radio Access Networks (RAN) and limiting\ntheir scalability to large bandwidths and many antenna 6G systems. In this\npaper, we propose a low-complexity, model-driven neural network-based receiver,\ndesigned for multi-user multiple-input multiple-output (MU-MIMO) systems and\nsuitable for implementation at the RAN edge. The proposed solution is compliant\nwith the 5G New Radio (5G NR), and supports different modulation schemes,\nbandwidths, number of users, and number of base-station antennas with a single\ntrained model without the need for further training. Numerical simulations of\nthe Physical Uplink Shared Channel (PUSCH) processing show that the proposed\nsolution outperforms the state-of-the-art methods in terms of achievable\nTransport Block Error Rate (TBLER), while reducing the Floating Point\nOperations (FLOPs) by 66$\\times$, and the learnable parameters by 396$\\times$."
                },
                "authors": [
                    {
                        "name": "Mahdi Abdollahpour"
                    },
                    {
                        "name": "Marco Bertuletti"
                    },
                    {
                        "name": "Yichao Zhang"
                    },
                    {
                        "name": "Yawei Li"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Alessandro Vanelli-Coralli"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Vanelli-Coralli"
                },
                "author": "Alessandro Vanelli-Coralli",
                "arxiv_comment": "Accepted to IEEE GLOBECOM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03158v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03158v3",
                "updated": "2025-10-22T10:20:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    10,
                    20,
                    6,
                    2,
                    295,
                    0
                ],
                "published": "2025-02-05T13:31:38Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    13,
                    31,
                    38,
                    2,
                    36,
                    0
                ],
                "title": "Strategizing with AI: Insights from a Beauty Contest Experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strategizing with AI: Insights from a Beauty Contest Experiment"
                },
                "summary": "A $p$-beauty contest is a wide class of games of guessing the most popular\nstrategy among other players. In particular, guessing a fraction of a mean of\nnumbers chosen by all players is a classic behavioral experiment designed to\ntest iterative reasoning patterns among various groups of people. The previous\nliterature reveals that the level of sophistication of the opponents is an\nimportant factor affecting the outcome of the game. Smarter decision makers\nchoose strategies that are closer to theoretical Nash equilibrium and\ndemonstrate faster convergence to equilibrium in iterated contests with\ninformation revelation. We replicate a series of classic experiments by running\nvirtual experiments with large language models (LLMs) who play against various\ngroups of virtual players. Our results show that LLMs recognize strategic\ncontext of the game and demonstrate expected adaptability to the changing set\nof parameters. LLMs systematically behave in a more sophisticated way compared\nto the participants of the original experiments. All LLMs still fail to\nidentify dominant strategies in a two-player game. Our results contribute to\nthe discussion on the accuracy of modeling human economic agents by artificial\nintelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A $p$-beauty contest is a wide class of games of guessing the most popular\nstrategy among other players. In particular, guessing a fraction of a mean of\nnumbers chosen by all players is a classic behavioral experiment designed to\ntest iterative reasoning patterns among various groups of people. The previous\nliterature reveals that the level of sophistication of the opponents is an\nimportant factor affecting the outcome of the game. Smarter decision makers\nchoose strategies that are closer to theoretical Nash equilibrium and\ndemonstrate faster convergence to equilibrium in iterated contests with\ninformation revelation. We replicate a series of classic experiments by running\nvirtual experiments with large language models (LLMs) who play against various\ngroups of virtual players. Our results show that LLMs recognize strategic\ncontext of the game and demonstrate expected adaptability to the changing set\nof parameters. LLMs systematically behave in a more sophisticated way compared\nto the participants of the original experiments. All LLMs still fail to\nidentify dominant strategies in a two-player game. Our results contribute to\nthe discussion on the accuracy of modeling human economic agents by artificial\nintelligence."
                },
                "authors": [
                    {
                        "name": "Iuliia Alekseenko"
                    },
                    {
                        "name": "Dmitry Dagaev"
                    },
                    {
                        "name": "Sofia Paklina"
                    },
                    {
                        "name": "Petr Parshakov"
                    }
                ],
                "author_detail": {
                    "name": "Petr Parshakov"
                },
                "author": "Petr Parshakov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03158v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03158v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14909v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14909v2",
                "updated": "2025-10-22T10:17:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    10,
                    17,
                    9,
                    2,
                    295,
                    0
                ],
                "published": "2025-07-20T10:48:07Z",
                "published_parsed": [
                    2025,
                    7,
                    20,
                    10,
                    48,
                    7,
                    6,
                    201,
                    0
                ],
                "title": "The Endless Tuning. An Artificial Intelligence Design To Avoid Human\n  Replacement and Trace Back Responsibilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Endless Tuning. An Artificial Intelligence Design To Avoid Human\n  Replacement and Trace Back Responsibilities"
                },
                "summary": "The Endless Tuning is a design method for a reliable deployment of artificial\nintelligence based on a double mirroring process, which pursues both the goals\nof avoiding human replacement and filling the so-called responsibility gap\n(Matthias 2004). Originally depicted in (Fabris et al. 2024) and ensuing the\nrelational approach urged therein, it was then actualized in a protocol,\nimplemented in three prototypical applications regarding decision-making\nprocesses (respectively: loan granting, pneumonia diagnosis, and art style\nrecognition) and tested with such as many domain experts. Step by step\nillustrating the protocol, giving insights concretely showing a different voice\n(Gilligan 1993) in the ethics of artificial intelligence, a philosophical\naccount of technical choices (e.g., a reversed and hermeneutic deployment of\nXAI algorithms) will be provided in the present study together with the results\nof the experiments, focusing on user experience rather than statistical\naccuracy. Even thoroughly employing deep learning models, full control was\nperceived by the interviewees in the decision-making setting, while it appeared\nthat a bridge can be built between accountability and liability in case of\ndamage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Endless Tuning is a design method for a reliable deployment of artificial\nintelligence based on a double mirroring process, which pursues both the goals\nof avoiding human replacement and filling the so-called responsibility gap\n(Matthias 2004). Originally depicted in (Fabris et al. 2024) and ensuing the\nrelational approach urged therein, it was then actualized in a protocol,\nimplemented in three prototypical applications regarding decision-making\nprocesses (respectively: loan granting, pneumonia diagnosis, and art style\nrecognition) and tested with such as many domain experts. Step by step\nillustrating the protocol, giving insights concretely showing a different voice\n(Gilligan 1993) in the ethics of artificial intelligence, a philosophical\naccount of technical choices (e.g., a reversed and hermeneutic deployment of\nXAI algorithms) will be provided in the present study together with the results\nof the experiments, focusing on user experience rather than statistical\naccuracy. Even thoroughly employing deep learning models, full control was\nperceived by the interviewees in the decision-making setting, while it appeared\nthat a bridge can be built between accountability and liability in case of\ndamage."
                },
                "authors": [
                    {
                        "name": "Elio Grande"
                    }
                ],
                "author_detail": {
                    "name": "Elio Grande"
                },
                "author": "Elio Grande",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14909v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14909v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15695v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15695v3",
                "updated": "2025-10-22T10:13:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    10,
                    13,
                    27,
                    2,
                    295,
                    0
                ],
                "published": "2025-05-21T16:09:44Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    9,
                    44,
                    2,
                    141,
                    0
                ],
                "title": "Can Large Language Models be Effective Online Opinion Miners?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models be Effective Online Opinion Miners?"
                },
                "summary": "The surge of user-generated online content presents a wealth of insights into\ncustomer preferences and market trends. However, the highly diverse, complex,\nand context-rich nature of such contents poses significant challenges to\ntraditional opinion mining approaches. To address this, we introduce Online\nOpinion Mining Benchmark (OOMB), a novel dataset and evaluation protocol\ndesigned to assess the ability of large language models (LLMs) to mine opinions\neffectively from diverse and intricate online environments. OOMB provides\nextensive (entity, feature, opinion) tuple annotations and a comprehensive\nopinion-centric summary that highlights key opinion topics within each content,\nthereby enabling the evaluation of both the extractive and abstractive\ncapabilities of models. Through our proposed benchmark, we conduct a\ncomprehensive analysis of which aspects remain challenging and where LLMs\nexhibit adaptability, to explore whether they can effectively serve as opinion\nminers in realistic online scenarios. This study lays the foundation for\nLLM-based opinion mining and discusses directions for future research in this\nfield.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The surge of user-generated online content presents a wealth of insights into\ncustomer preferences and market trends. However, the highly diverse, complex,\nand context-rich nature of such contents poses significant challenges to\ntraditional opinion mining approaches. To address this, we introduce Online\nOpinion Mining Benchmark (OOMB), a novel dataset and evaluation protocol\ndesigned to assess the ability of large language models (LLMs) to mine opinions\neffectively from diverse and intricate online environments. OOMB provides\nextensive (entity, feature, opinion) tuple annotations and a comprehensive\nopinion-centric summary that highlights key opinion topics within each content,\nthereby enabling the evaluation of both the extractive and abstractive\ncapabilities of models. Through our proposed benchmark, we conduct a\ncomprehensive analysis of which aspects remain challenging and where LLMs\nexhibit adaptability, to explore whether they can effectively serve as opinion\nminers in realistic online scenarios. This study lays the foundation for\nLLM-based opinion mining and discusses directions for future research in this\nfield."
                },
                "authors": [
                    {
                        "name": "Ryang Heo"
                    },
                    {
                        "name": "Yongsik Seo"
                    },
                    {
                        "name": "Junseong Lee"
                    },
                    {
                        "name": "Dongha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongha Lee"
                },
                "author": "Dongha Lee",
                "arxiv_comment": "Accepted to EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15695v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15695v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19438v1",
                "updated": "2025-10-22T10:11:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    10,
                    11,
                    5,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T10:11:05Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    10,
                    11,
                    5,
                    2,
                    295,
                    0
                ],
                "title": "AutoMT: A Multi-Agent LLM Framework for Automated Metamorphic Testing of\n  Autonomous Driving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoMT: A Multi-Agent LLM Framework for Automated Metamorphic Testing of\n  Autonomous Driving Systems"
                },
                "summary": "Autonomous Driving Systems (ADS) are safety-critical, where failures can be\nsevere. While Metamorphic Testing (MT) is effective for fault detection in ADS,\nexisting methods rely heavily on manual effort and lack automation. We present\nAutoMT, a multi-agent MT framework powered by Large Language Models (LLMs) that\nautomates the extraction of Metamorphic Relations (MRs) from local traffic\nrules and the generation of valid follow-up test cases. AutoMT leverages LLMs\nto extract MRs from traffic rules in Gherkin syntax using a predefined\nontology. A vision-language agent analyzes scenarios, and a search agent\nretrieves suitable MRs from a RAG-based database to generate follow-up cases\nvia computer vision. Experiments show that AutoMT achieves up to 5 x higher\ntest diversity in follow-up case generation compared to the best baseline\n(manual expert-defined MRs) in terms of validation rate, and detects up to\n20.55% more behavioral violations. While manual MT relies on a fixed set of\npredefined rules, AutoMT automatically extracts diverse metamorphic relations\nthat augment real-world datasets and help uncover corner cases often missed\nduring in-field testing and data collection. Its modular architecture\nseparating MR extraction, filtering, and test generation supports integration\ninto industrial pipelines and potentially enables simulation-based testing to\nsystematically cover underrepresented or safety-critical scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Driving Systems (ADS) are safety-critical, where failures can be\nsevere. While Metamorphic Testing (MT) is effective for fault detection in ADS,\nexisting methods rely heavily on manual effort and lack automation. We present\nAutoMT, a multi-agent MT framework powered by Large Language Models (LLMs) that\nautomates the extraction of Metamorphic Relations (MRs) from local traffic\nrules and the generation of valid follow-up test cases. AutoMT leverages LLMs\nto extract MRs from traffic rules in Gherkin syntax using a predefined\nontology. A vision-language agent analyzes scenarios, and a search agent\nretrieves suitable MRs from a RAG-based database to generate follow-up cases\nvia computer vision. Experiments show that AutoMT achieves up to 5 x higher\ntest diversity in follow-up case generation compared to the best baseline\n(manual expert-defined MRs) in terms of validation rate, and detects up to\n20.55% more behavioral violations. While manual MT relies on a fixed set of\npredefined rules, AutoMT automatically extracts diverse metamorphic relations\nthat augment real-world datasets and help uncover corner cases often missed\nduring in-field testing and data collection. Its modular architecture\nseparating MR extraction, filtering, and test generation supports integration\ninto industrial pipelines and potentially enables simulation-based testing to\nsystematically cover underrepresented or safety-critical scenarios."
                },
                "authors": [
                    {
                        "name": "Linfeng Liang"
                    },
                    {
                        "name": "Chenkai Tan"
                    },
                    {
                        "name": "Yao Deng"
                    },
                    {
                        "name": "Yingfeng Cai"
                    },
                    {
                        "name": "T. Y Chen"
                    },
                    {
                        "name": "Xi Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Xi Zheng"
                },
                "author": "Xi Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18167v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18167v4",
                "updated": "2025-10-22T09:57:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    9,
                    57,
                    18,
                    2,
                    295,
                    0
                ],
                "published": "2025-06-22T20:45:26Z",
                "published_parsed": [
                    2025,
                    6,
                    22,
                    20,
                    45,
                    26,
                    6,
                    173,
                    0
                ],
                "title": "Understanding Reasoning in Thinking Language Models via Steering Vectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Reasoning in Thinking Language Models via Steering Vectors"
                },
                "summary": "Recent advances in large language models (LLMs) have led to the development\nof thinking language models that generate extensive internal reasoning chains\nbefore producing responses. While these models achieve improved performance,\ncontrolling their reasoning processes remains challenging. This work presents a\nsteering approach for thinking LLMs by analyzing and manipulating specific\nreasoning behaviors in DeepSeek-R1-Distill models. Through a systematic\nexperiment on 500 tasks across 10 diverse categories, we identify several\nreasoning behaviors exhibited by thinking models, including expressing\nuncertainty, generating examples for hypothesis validation, and backtracking in\nreasoning chains. We demonstrate that these behaviors are mediated by linear\ndirections in the model's activation space and can be controlled using steering\nvectors. By extracting and applying these vectors, we provide a method to\nmodulate specific aspects of the model's reasoning process, such as its\ntendency to backtrack or express uncertainty. Our approach offers practical\ntools for steering reasoning processes in thinking models in a controlled and\ninterpretable manner. We validate our steering method using three\nDeepSeek-R1-Distill models, demonstrating consistent control across different\nmodel architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have led to the development\nof thinking language models that generate extensive internal reasoning chains\nbefore producing responses. While these models achieve improved performance,\ncontrolling their reasoning processes remains challenging. This work presents a\nsteering approach for thinking LLMs by analyzing and manipulating specific\nreasoning behaviors in DeepSeek-R1-Distill models. Through a systematic\nexperiment on 500 tasks across 10 diverse categories, we identify several\nreasoning behaviors exhibited by thinking models, including expressing\nuncertainty, generating examples for hypothesis validation, and backtracking in\nreasoning chains. We demonstrate that these behaviors are mediated by linear\ndirections in the model's activation space and can be controlled using steering\nvectors. By extracting and applying these vectors, we provide a method to\nmodulate specific aspects of the model's reasoning process, such as its\ntendency to backtrack or express uncertainty. Our approach offers practical\ntools for steering reasoning processes in thinking models in a controlled and\ninterpretable manner. We validate our steering method using three\nDeepSeek-R1-Distill models, demonstrating consistent control across different\nmodel architectures."
                },
                "authors": [
                    {
                        "name": "Constantin Venhoff"
                    },
                    {
                        "name": "Iván Arcuschin"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Arthur Conmy"
                    },
                    {
                        "name": "Neel Nanda"
                    }
                ],
                "author_detail": {
                    "name": "Neel Nanda"
                },
                "author": "Neel Nanda",
                "arxiv_comment": "Accepted to the Workshop on Reasoning and Planning for Large Language\n  Models at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18167v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18167v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19429v1",
                "updated": "2025-10-22T09:57:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    9,
                    57,
                    2,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T09:57:02Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    9,
                    57,
                    2,
                    2,
                    295,
                    0
                ],
                "title": "NeSyPr: Neurosymbolic Proceduralization For Efficient Embodied Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeSyPr: Neurosymbolic Proceduralization For Efficient Embodied Reasoning"
                },
                "summary": "We address the challenge of adopting language models (LMs) for embodied tasks\nin dynamic environments, where online access to large-scale inference engines\nor symbolic planners is constrained due to latency, connectivity, and resource\nlimitations. To this end, we present NeSyPr, a novel embodied reasoning\nframework that compiles knowledge via neurosymbolic proceduralization, thereby\nequipping LM-based agents with structured, adaptive, and timely reasoning\ncapabilities. In NeSyPr, task-specific plans are first explicitly generated by\na symbolic tool leveraging its declarative knowledge. These plans are then\ntransformed into composable procedural representations that encode the plans'\nimplicit production rules, enabling the resulting composed procedures to be\nseamlessly integrated into the LM's inference process. This neurosymbolic\nproceduralization abstracts and generalizes multi-step symbolic structured\npath-finding and reasoning into single-step LM inference, akin to human\nknowledge compilation. It supports efficient test-time inference without\nrelying on external symbolic guidance, making it well suited for deployment in\nlatency-sensitive and resource-constrained physical systems. We evaluate NeSyPr\non the embodied benchmarks PDDLGym, VirtualHome, and ALFWorld, demonstrating\nits efficient reasoning capabilities over large-scale reasoning models and a\nsymbolic planner, while using more compact LMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of adopting language models (LMs) for embodied tasks\nin dynamic environments, where online access to large-scale inference engines\nor symbolic planners is constrained due to latency, connectivity, and resource\nlimitations. To this end, we present NeSyPr, a novel embodied reasoning\nframework that compiles knowledge via neurosymbolic proceduralization, thereby\nequipping LM-based agents with structured, adaptive, and timely reasoning\ncapabilities. In NeSyPr, task-specific plans are first explicitly generated by\na symbolic tool leveraging its declarative knowledge. These plans are then\ntransformed into composable procedural representations that encode the plans'\nimplicit production rules, enabling the resulting composed procedures to be\nseamlessly integrated into the LM's inference process. This neurosymbolic\nproceduralization abstracts and generalizes multi-step symbolic structured\npath-finding and reasoning into single-step LM inference, akin to human\nknowledge compilation. It supports efficient test-time inference without\nrelying on external symbolic guidance, making it well suited for deployment in\nlatency-sensitive and resource-constrained physical systems. We evaluate NeSyPr\non the embodied benchmarks PDDLGym, VirtualHome, and ALFWorld, demonstrating\nits efficient reasoning capabilities over large-scale reasoning models and a\nsymbolic planner, while using more compact LMs."
                },
                "authors": [
                    {
                        "name": "Wonje Choi"
                    },
                    {
                        "name": "Jooyoung Kim"
                    },
                    {
                        "name": "Honguk Woo"
                    }
                ],
                "author_detail": {
                    "name": "Honguk Woo"
                },
                "author": "Honguk Woo",
                "arxiv_comment": "Accepted at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19423v1",
                "updated": "2025-10-22T09:45:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    9,
                    45,
                    11,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T09:45:11Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    9,
                    45,
                    11,
                    2,
                    295,
                    0
                ],
                "title": "MSC-Bench: A Rigorous Benchmark for Multi-Server Tool Orchestration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSC-Bench: A Rigorous Benchmark for Multi-Server Tool Orchestration"
                },
                "summary": "We introduce MSC-Bench, a large-scale benchmark for evaluating multi-hop,\nend-to-end tool orchestration by LLM agents in a hierarchical Model-Context\nProtocol (MCP) ecosystem. Existing benchmarks often evaluate tools in\nisolation, ignoring challenges such as functional overlap and cross-server\norchestration, leading to overly optimistic assessments. MSC-Bench addresses\nthese gaps by constructing ground truth through 'equal function sets', allowing\nobjective metrics such as F1 score and reducing the dependency on\nLLM-as-a-judge evaluation. Organized as a five-level curriculum, it\nsystematically tests agent capabilities from single-tool orchestration to\ncomplex cross-server planning, and robustness to out-of-scope requests.\nExperiments reveal that rigid hierarchies can hinder performance without\nco-designed strategies, and even state-of-the-art agents exhibit systemic\nweaknesses in robustness. MSC-Bench provides a diagnostic framework to expose\nthese limitations and guide the development of more capable and efficient\ntool-using agents. The benchmark and resources are publicly available at\nhttps://github.com/snooow1029/MSC_Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MSC-Bench, a large-scale benchmark for evaluating multi-hop,\nend-to-end tool orchestration by LLM agents in a hierarchical Model-Context\nProtocol (MCP) ecosystem. Existing benchmarks often evaluate tools in\nisolation, ignoring challenges such as functional overlap and cross-server\norchestration, leading to overly optimistic assessments. MSC-Bench addresses\nthese gaps by constructing ground truth through 'equal function sets', allowing\nobjective metrics such as F1 score and reducing the dependency on\nLLM-as-a-judge evaluation. Organized as a five-level curriculum, it\nsystematically tests agent capabilities from single-tool orchestration to\ncomplex cross-server planning, and robustness to out-of-scope requests.\nExperiments reveal that rigid hierarchies can hinder performance without\nco-designed strategies, and even state-of-the-art agents exhibit systemic\nweaknesses in robustness. MSC-Bench provides a diagnostic framework to expose\nthese limitations and guide the development of more capable and efficient\ntool-using agents. The benchmark and resources are publicly available at\nhttps://github.com/snooow1029/MSC_Bench."
                },
                "authors": [
                    {
                        "name": "Jia-Kai Dong"
                    },
                    {
                        "name": "I-Wei Huang"
                    },
                    {
                        "name": "Chun-Tin Wu"
                    },
                    {
                        "name": "Yi-Tien Tsai"
                    }
                ],
                "author_detail": {
                    "name": "Yi-Tien Tsai"
                },
                "author": "Yi-Tien Tsai",
                "arxiv_comment": "under ACL Rolling Review 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.1; I.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19422v1",
                "updated": "2025-10-22T09:44:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    9,
                    44,
                    36,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T09:44:36Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    9,
                    44,
                    36,
                    2,
                    295,
                    0
                ],
                "title": "LLM Unlearning with LLM Beliefs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Unlearning with LLM Beliefs"
                },
                "summary": "Large language models trained on vast corpora inherently risk memorizing\nsensitive or harmful content, which may later resurface in their outputs.\nPrevailing unlearning methods generally rely on gradient ascent and its\nvariants to lower the probability of specific target responses. However, we\nfind that this strategy induces a critical side effect: probability mass is\nredistributed into high-likelihood regions, often corresponding to semantically\nrelated rephrasings of the targets. We refer to this as the squeezing effect,\nwhich explains why many methods yield merely spurious unlearning, a problem\nfurther obscured by automated metrics (e.g., ROUGE, truth ratio) that misreport\nactual success. To address this, we propose a bootstrapping (BS) framework that\nexplicitly links the squeezing effect with the model's own high-confidence\ngenerations, namely its model beliefs. Since model beliefs inherently capture\nthe very high-likelihood regions where probability mass is squeezed,\nincorporating them into the unlearning objective directly counters the\nsqueezing effect. By jointly suppressing both target responses and model\nbeliefs, BS-T (token) attenuates high-probability tokens, whereas BS-S\n(sequence) removes entire high-confidence generations, together achieving more\nthorough forgetting while preserving utility. Extensive experiments across\ndiverse benchmarks with various model families confirm the effectiveness of our\napproach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models trained on vast corpora inherently risk memorizing\nsensitive or harmful content, which may later resurface in their outputs.\nPrevailing unlearning methods generally rely on gradient ascent and its\nvariants to lower the probability of specific target responses. However, we\nfind that this strategy induces a critical side effect: probability mass is\nredistributed into high-likelihood regions, often corresponding to semantically\nrelated rephrasings of the targets. We refer to this as the squeezing effect,\nwhich explains why many methods yield merely spurious unlearning, a problem\nfurther obscured by automated metrics (e.g., ROUGE, truth ratio) that misreport\nactual success. To address this, we propose a bootstrapping (BS) framework that\nexplicitly links the squeezing effect with the model's own high-confidence\ngenerations, namely its model beliefs. Since model beliefs inherently capture\nthe very high-likelihood regions where probability mass is squeezed,\nincorporating them into the unlearning objective directly counters the\nsqueezing effect. By jointly suppressing both target responses and model\nbeliefs, BS-T (token) attenuates high-probability tokens, whereas BS-S\n(sequence) removes entire high-confidence generations, together achieving more\nthorough forgetting while preserving utility. Extensive experiments across\ndiverse benchmarks with various model families confirm the effectiveness of our\napproach."
                },
                "authors": [
                    {
                        "name": "Kemou Li"
                    },
                    {
                        "name": "Qizhou Wang"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Fengpeng Li"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Bo Han"
                    },
                    {
                        "name": "Jiantao Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jiantao Zhou"
                },
                "author": "Jiantao Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19420v1",
                "updated": "2025-10-22T09:43:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    9,
                    43,
                    32,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T09:43:32Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    9,
                    43,
                    32,
                    2,
                    295,
                    0
                ],
                "title": "Monitoring LLM-based Multi-Agent Systems Against Corruptions via Node\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monitoring LLM-based Multi-Agent Systems Against Corruptions via Node\n  Evaluation"
                },
                "summary": "Large Language Model (LLM)-based Multi-Agent Systems (MAS) have become a\npopular paradigm of AI applications. However, trustworthiness issues in MAS\nremain a critical concern. Unlike challenges in single-agent systems, MAS\ninvolve more complex communication processes, making them susceptible to\ncorruption attacks. To mitigate this issue, several defense mechanisms have\nbeen developed based on the graph representation of MAS, where agents represent\nnodes and communications form edges. Nevertheless, these methods predominantly\nfocus on static graph defense, attempting to either detect attacks in a fixed\ngraph structure or optimize a static topology with certain defensive\ncapabilities. To address this limitation, we propose a dynamic defense paradigm\nfor MAS graph structures, which continuously monitors communication within the\nMAS graph, then dynamically adjusts the graph topology, accurately disrupts\nmalicious communications, and effectively defends against evolving and diverse\ndynamic attacks. Experimental results in increasingly complex and dynamic MAS\nenvironments demonstrate that our method significantly outperforms existing MAS\ndefense mechanisms, contributing an effective guardrail for their trustworthy\napplications. Our code is available at\nhttps://github.com/ChengcanWu/Monitoring-LLM-Based-Multi-Agent-Systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based Multi-Agent Systems (MAS) have become a\npopular paradigm of AI applications. However, trustworthiness issues in MAS\nremain a critical concern. Unlike challenges in single-agent systems, MAS\ninvolve more complex communication processes, making them susceptible to\ncorruption attacks. To mitigate this issue, several defense mechanisms have\nbeen developed based on the graph representation of MAS, where agents represent\nnodes and communications form edges. Nevertheless, these methods predominantly\nfocus on static graph defense, attempting to either detect attacks in a fixed\ngraph structure or optimize a static topology with certain defensive\ncapabilities. To address this limitation, we propose a dynamic defense paradigm\nfor MAS graph structures, which continuously monitors communication within the\nMAS graph, then dynamically adjusts the graph topology, accurately disrupts\nmalicious communications, and effectively defends against evolving and diverse\ndynamic attacks. Experimental results in increasingly complex and dynamic MAS\nenvironments demonstrate that our method significantly outperforms existing MAS\ndefense mechanisms, contributing an effective guardrail for their trustworthy\napplications. Our code is available at\nhttps://github.com/ChengcanWu/Monitoring-LLM-Based-Multi-Agent-Systems."
                },
                "authors": [
                    {
                        "name": "Chengcan Wu"
                    },
                    {
                        "name": "Zhixin Zhang"
                    },
                    {
                        "name": "Mingqian Xu"
                    },
                    {
                        "name": "Zeming Wei"
                    },
                    {
                        "name": "Meng Sun"
                    }
                ],
                "author_detail": {
                    "name": "Meng Sun"
                },
                "author": "Meng Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19414v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19414v1",
                "updated": "2025-10-22T09:34:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    9,
                    34,
                    31,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T09:34:31Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    9,
                    34,
                    31,
                    2,
                    295,
                    0
                ],
                "title": "EchoFake: A Replay-Aware Dataset for Practical Speech Deepfake Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EchoFake: A Replay-Aware Dataset for Practical Speech Deepfake Detection"
                },
                "summary": "The growing prevalence of speech deepfakes has raised serious concerns,\nparticularly in real-world scenarios such as telephone fraud and identity\ntheft. While many anti-spoofing systems have demonstrated promising performance\non lab-generated synthetic speech, they often fail when confronted with\nphysical replay attacks-a common and low-cost form of attack used in practical\nsettings. Our experiments show that models trained on existing datasets exhibit\nsevere performance degradation, with average accuracy dropping to 59.6% when\nevaluated on replayed audio. To bridge this gap, we present EchoFake, a\ncomprehensive dataset comprising more than 120 hours of audio from over 13,000\nspeakers, featuring both cutting-edge zero-shot text-to-speech (TTS) speech and\nphysical replay recordings collected under varied devices and real-world\nenvironmental settings. Additionally, we evaluate three baseline detection\nmodels and show that models trained on EchoFake achieve lower average EERs\nacross datasets, indicating better generalization. By introducing more\npractical challenges relevant to real-world deployment, EchoFake offers a more\nrealistic foundation for advancing spoofing detection methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing prevalence of speech deepfakes has raised serious concerns,\nparticularly in real-world scenarios such as telephone fraud and identity\ntheft. While many anti-spoofing systems have demonstrated promising performance\non lab-generated synthetic speech, they often fail when confronted with\nphysical replay attacks-a common and low-cost form of attack used in practical\nsettings. Our experiments show that models trained on existing datasets exhibit\nsevere performance degradation, with average accuracy dropping to 59.6% when\nevaluated on replayed audio. To bridge this gap, we present EchoFake, a\ncomprehensive dataset comprising more than 120 hours of audio from over 13,000\nspeakers, featuring both cutting-edge zero-shot text-to-speech (TTS) speech and\nphysical replay recordings collected under varied devices and real-world\nenvironmental settings. Additionally, we evaluate three baseline detection\nmodels and show that models trained on EchoFake achieve lower average EERs\nacross datasets, indicating better generalization. By introducing more\npractical challenges relevant to real-world deployment, EchoFake offers a more\nrealistic foundation for advancing spoofing detection methods."
                },
                "authors": [
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Yihuan Huang"
                    },
                    {
                        "name": "Yanzhen Ren"
                    }
                ],
                "author_detail": {
                    "name": "Yanzhen Ren"
                },
                "author": "Yanzhen Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19414v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19414v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19410v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19410v1",
                "updated": "2025-10-22T09:28:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    9,
                    28,
                    18,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T09:28:18Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    9,
                    28,
                    18,
                    2,
                    295,
                    0
                ],
                "title": "ToMMeR -- Efficient Entity Mention Detection from Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToMMeR -- Efficient Entity Mention Detection from Large Language Models"
                },
                "summary": "Identifying which text spans refer to entities -- mention detection -- is\nboth foundational for information extraction and a known performance\nbottleneck. We introduce ToMMeR, a lightweight model (<300K parameters) probing\nmention detection capabilities from early LLM layers. Across 13 NER benchmarks,\nToMMeR achieves 93\\% recall zero-shot, with over 90\\% precision using an LLM as\na judge showing that ToMMeR rarely produces spurious predictions despite high\nrecall. Cross-model analysis reveals that diverse architectures (14M-15B\nparameters) converge on similar mention boundaries (DICE >75\\%), confirming\nthat mention detection emerges naturally from language modeling. When extended\nwith span classification heads, ToMMeR achieves near SOTA NER performance\n(80-87\\% F1 on standard benchmarks). Our work provides evidence that structured\nentity representations exist in early transformer layers and can be efficiently\nrecovered with minimal parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying which text spans refer to entities -- mention detection -- is\nboth foundational for information extraction and a known performance\nbottleneck. We introduce ToMMeR, a lightweight model (<300K parameters) probing\nmention detection capabilities from early LLM layers. Across 13 NER benchmarks,\nToMMeR achieves 93\\% recall zero-shot, with over 90\\% precision using an LLM as\na judge showing that ToMMeR rarely produces spurious predictions despite high\nrecall. Cross-model analysis reveals that diverse architectures (14M-15B\nparameters) converge on similar mention boundaries (DICE >75\\%), confirming\nthat mention detection emerges naturally from language modeling. When extended\nwith span classification heads, ToMMeR achieves near SOTA NER performance\n(80-87\\% F1 on standard benchmarks). Our work provides evidence that structured\nentity representations exist in early transformer layers and can be efficiently\nrecovered with minimal parameters."
                },
                "authors": [
                    {
                        "name": "Victor Morand"
                    },
                    {
                        "name": "Nadi Tomeh"
                    },
                    {
                        "name": "Josiane Mothe"
                    },
                    {
                        "name": "Benjamin Piwowarski"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Piwowarski"
                },
                "author": "Benjamin Piwowarski",
                "arxiv_comment": "Code is available at https://github.com/VictorMorand/llm2ner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19410v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19410v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26433v2",
                "updated": "2025-10-22T09:12:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    9,
                    12,
                    0,
                    2,
                    295,
                    0
                ],
                "published": "2025-09-30T15:54:08Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    54,
                    8,
                    1,
                    273,
                    0
                ],
                "title": "ACT: Agentic Classification Tree",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACT: Agentic Classification Tree"
                },
                "summary": "When used in high-stakes settings, AI systems are expected to produce\ndecisions that are transparent, interpretable, and auditable, a requirement\nincreasingly expected by regulations. Decision trees such as CART provide clear\nand verifiable rules, but they are restricted to structured tabular data and\ncannot operate directly on unstructured inputs such as text. In practice, large\nlanguage models (LLMs) are widely used for such data, yet prompting strategies\nsuch as chain-of-thought or prompt optimization still rely on free-form\nreasoning, limiting their ability to ensure trustworthy behaviors. We present\nthe Agentic Classification Tree (ACT), which extends decision-tree methodology\nto unstructured inputs by formulating each split as a natural-language\nquestion, refined through impurity-based evaluation and LLM feedback via\nTextGrad. Experiments on text benchmarks show that ACT matches or surpasses\nprompting-based baselines while producing transparent and interpretable\ndecision paths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When used in high-stakes settings, AI systems are expected to produce\ndecisions that are transparent, interpretable, and auditable, a requirement\nincreasingly expected by regulations. Decision trees such as CART provide clear\nand verifiable rules, but they are restricted to structured tabular data and\ncannot operate directly on unstructured inputs such as text. In practice, large\nlanguage models (LLMs) are widely used for such data, yet prompting strategies\nsuch as chain-of-thought or prompt optimization still rely on free-form\nreasoning, limiting their ability to ensure trustworthy behaviors. We present\nthe Agentic Classification Tree (ACT), which extends decision-tree methodology\nto unstructured inputs by formulating each split as a natural-language\nquestion, refined through impurity-based evaluation and LLM feedback via\nTextGrad. Experiments on text benchmarks show that ACT matches or surpasses\nprompting-based baselines while producing transparent and interpretable\ndecision paths."
                },
                "authors": [
                    {
                        "name": "Vincent Grari"
                    },
                    {
                        "name": "Tim Arni"
                    },
                    {
                        "name": "Thibault Laugel"
                    },
                    {
                        "name": "Sylvain Lamprier"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Marcin Detyniecki"
                    }
                ],
                "author_detail": {
                    "name": "Marcin Detyniecki"
                },
                "author": "Marcin Detyniecki",
                "arxiv_comment": "18 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19389v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19389v1",
                "updated": "2025-10-22T09:05:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    9,
                    5,
                    47,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T09:05:47Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    9,
                    5,
                    47,
                    2,
                    295,
                    0
                ],
                "title": "ARA: Adaptive Rank Allocation for Efficient Large Language Model SVD\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARA: Adaptive Rank Allocation for Efficient Large Language Model SVD\n  Compression"
                },
                "summary": "In the field of large language model (LLM) compression, singular value\ndecomposition (SVD) is a widely studied and adopted low-rank decomposition\ntechnique. Since SVD operates exclusively on linear modules, and these modules\nin LLMs are separated by nonlinear components, SVD can only be applied\nindependently to each linear module. Under a global compression ratio\nconstraint, determining the appropriate rank for different linear modules\nbecomes a critical problem. Existing approaches, such as heuristic algorithms\nand mask-based training, have made progress in addressing this challenge.\nHowever, these methods still suffer from several limitations: heuristic\nalgorithms explore the solution space within restricted regions, while\nmask-based training struggles to efficiently capture the relationship between\nsingular value spectra and trainable parameters. More importantly, current\nmethods overlook the key property that the gain function is non-smooth at a\ncompression ratio of 1, which often leads the training process to suboptimal\nlocal minima. To address these issues, we propose an Adaptive Rank Allocation\n(ARA) method. Specifically, (1) ARA introduces a dedicated mask design that\nenables efficient mapping and updating between retained ranks and trainable\nparameters; and (2) it employs an additional loss function to guide parameter\nselection toward globally optimal solutions. Experimental results demonstrate\nthat ARA achieves state-of-the-art performance. On the LLaMA2-7B model with a\n80\\% compression ratio, ARA reduces perplexity on WikiText2 from 8.38 to 6.42\nand improves average zero-shot task accuracy by 9.72 percentage points compared\nwith uniform compression. These results highlight the effectiveness of our\nmethod for rank allocation in SVD-based LLM compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of large language model (LLM) compression, singular value\ndecomposition (SVD) is a widely studied and adopted low-rank decomposition\ntechnique. Since SVD operates exclusively on linear modules, and these modules\nin LLMs are separated by nonlinear components, SVD can only be applied\nindependently to each linear module. Under a global compression ratio\nconstraint, determining the appropriate rank for different linear modules\nbecomes a critical problem. Existing approaches, such as heuristic algorithms\nand mask-based training, have made progress in addressing this challenge.\nHowever, these methods still suffer from several limitations: heuristic\nalgorithms explore the solution space within restricted regions, while\nmask-based training struggles to efficiently capture the relationship between\nsingular value spectra and trainable parameters. More importantly, current\nmethods overlook the key property that the gain function is non-smooth at a\ncompression ratio of 1, which often leads the training process to suboptimal\nlocal minima. To address these issues, we propose an Adaptive Rank Allocation\n(ARA) method. Specifically, (1) ARA introduces a dedicated mask design that\nenables efficient mapping and updating between retained ranks and trainable\nparameters; and (2) it employs an additional loss function to guide parameter\nselection toward globally optimal solutions. Experimental results demonstrate\nthat ARA achieves state-of-the-art performance. On the LLaMA2-7B model with a\n80\\% compression ratio, ARA reduces perplexity on WikiText2 from 8.38 to 6.42\nand improves average zero-shot task accuracy by 9.72 percentage points compared\nwith uniform compression. These results highlight the effectiveness of our\nmethod for rank allocation in SVD-based LLM compression."
                },
                "authors": [
                    {
                        "name": "Lin Xv"
                    },
                    {
                        "name": "Jingsheng Gao"
                    },
                    {
                        "name": "Xian Gao"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Yuzhuo Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yuzhuo Fu"
                },
                "author": "Yuzhuo Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19389v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19389v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16062v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16062v2",
                "updated": "2025-10-22T09:04:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    9,
                    4,
                    12,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-17T02:40:19Z",
                "published_parsed": [
                    2025,
                    10,
                    17,
                    2,
                    40,
                    19,
                    4,
                    290,
                    0
                ],
                "title": "Can LLMs Correct Themselves? A Benchmark of Self-Correction in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Correct Themselves? A Benchmark of Self-Correction in LLMs"
                },
                "summary": "Self-correction of large language models (LLMs) emerges as a critical\ncomponent for enhancing their reasoning performance. Although various\nself-correction methods have been proposed, a comprehensive evaluation of these\nmethods remains largely unexplored, and the question of whether LLMs can truly\ncorrect themselves is a matter of significant interest and concern. In this\nstudy, we introduce CorrectBench, a benchmark developed to evaluate the\neffectiveness of self-correction strategies, including intrinsic, external, and\nfine-tuned approaches, across three tasks: commonsense reasoning, mathematical\nreasoning, and code generation. Our findings reveal that: 1) Self-correction\nmethods can improve accuracy, especially for complex reasoning tasks; 2) Mixing\ndifferent self-correction strategies yields further improvements, though it\nreduces efficiency; 3) Reasoning LLMs (e.g., DeepSeek-R1) have limited\noptimization under additional self-correction methods and have high time costs.\nInterestingly, a comparatively simple chain-of-thought (CoT) baseline\ndemonstrates competitive accuracy and efficiency. These results underscore the\npotential of self-correction to enhance LLM's reasoning performance while\nhighlighting the ongoing challenge of improving their efficiency. Consequently,\nwe advocate for further research focused on optimizing the balance between\nreasoning capabilities and operational efficiency. Project Page:\nhttps://correctbench.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-correction of large language models (LLMs) emerges as a critical\ncomponent for enhancing their reasoning performance. Although various\nself-correction methods have been proposed, a comprehensive evaluation of these\nmethods remains largely unexplored, and the question of whether LLMs can truly\ncorrect themselves is a matter of significant interest and concern. In this\nstudy, we introduce CorrectBench, a benchmark developed to evaluate the\neffectiveness of self-correction strategies, including intrinsic, external, and\nfine-tuned approaches, across three tasks: commonsense reasoning, mathematical\nreasoning, and code generation. Our findings reveal that: 1) Self-correction\nmethods can improve accuracy, especially for complex reasoning tasks; 2) Mixing\ndifferent self-correction strategies yields further improvements, though it\nreduces efficiency; 3) Reasoning LLMs (e.g., DeepSeek-R1) have limited\noptimization under additional self-correction methods and have high time costs.\nInterestingly, a comparatively simple chain-of-thought (CoT) baseline\ndemonstrates competitive accuracy and efficiency. These results underscore the\npotential of self-correction to enhance LLM's reasoning performance while\nhighlighting the ongoing challenge of improving their efficiency. Consequently,\nwe advocate for further research focused on optimizing the balance between\nreasoning capabilities and operational efficiency. Project Page:\nhttps://correctbench.github.io/"
                },
                "authors": [
                    {
                        "name": "Guiyao Tie"
                    },
                    {
                        "name": "Zenghui Yuan"
                    },
                    {
                        "name": "Zeli Zhao"
                    },
                    {
                        "name": "Chaoran Hu"
                    },
                    {
                        "name": "Tianhe Gu"
                    },
                    {
                        "name": "Ruihang Zhang"
                    },
                    {
                        "name": "Sizhe Zhang"
                    },
                    {
                        "name": "Junran Wu"
                    },
                    {
                        "name": "Xiaoyue Tu"
                    },
                    {
                        "name": "Ming Jin"
                    },
                    {
                        "name": "Qingsong Wen"
                    },
                    {
                        "name": "Lixing Chen"
                    },
                    {
                        "name": "Pan Zhou"
                    },
                    {
                        "name": "Lichao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Lichao Sun"
                },
                "author": "Lichao Sun",
                "arxiv_comment": "47 pages, 25 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16062v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16062v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19385v1",
                "updated": "2025-10-22T09:02:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    9,
                    2,
                    37,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T09:02:37Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    9,
                    2,
                    37,
                    2,
                    295,
                    0
                ],
                "title": "CPSVD: Enhancing Large Language Model Compression via Column-Preserving\n  Singular Value Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CPSVD: Enhancing Large Language Model Compression via Column-Preserving\n  Singular Value Decomposition"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) faces a critical\nbottleneck in their immense size, necessitating efficient compression\ntechniques. While Singular Value Decomposition (SVD) is a promising approach,\nexisting SVD-based methods treat the entire parameter matrix uniformly,\noverlooking that SVD approximation errors vary significantly across different\nmatrix parts, which often leads to suboptimal compression. To address this, we\npropose \\textbf{C}olumn-\\textbf{P}reserving \\textbf{S}ingular \\textbf{V}alue\n\\textbf{D}ecomposition (CPSVD), a novel method that refines SVD-based LLM\ncompression by intelligently segmenting the parameter matrix. Unlike\ntraditional SVD, CPSVD identifies and directly preserves matrix columns with\nhigh decomposition errors, applying SVD only to columns with low decomposition\nerrors, while precisely determining the optimal balance point between these two\nstrategies to minimize error. Furthermore, leveraging the inherent\nheterogeneity in decomposition errors across different matrices within an LLM,\nCPSVD adaptively allocates non-uniform compression rates to modules within that\nlayer, while adhering to a target layer-wise compression ratio, thereby further\nenhancing compression performance. Extensive experiments demonstrate that CPSVD\nconsistently outperforms state-of-the-art SVD-based LLM compression methods,\nachieving lower perplexity and higher accuracy on zero-shot tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) faces a critical\nbottleneck in their immense size, necessitating efficient compression\ntechniques. While Singular Value Decomposition (SVD) is a promising approach,\nexisting SVD-based methods treat the entire parameter matrix uniformly,\noverlooking that SVD approximation errors vary significantly across different\nmatrix parts, which often leads to suboptimal compression. To address this, we\npropose \\textbf{C}olumn-\\textbf{P}reserving \\textbf{S}ingular \\textbf{V}alue\n\\textbf{D}ecomposition (CPSVD), a novel method that refines SVD-based LLM\ncompression by intelligently segmenting the parameter matrix. Unlike\ntraditional SVD, CPSVD identifies and directly preserves matrix columns with\nhigh decomposition errors, applying SVD only to columns with low decomposition\nerrors, while precisely determining the optimal balance point between these two\nstrategies to minimize error. Furthermore, leveraging the inherent\nheterogeneity in decomposition errors across different matrices within an LLM,\nCPSVD adaptively allocates non-uniform compression rates to modules within that\nlayer, while adhering to a target layer-wise compression ratio, thereby further\nenhancing compression performance. Extensive experiments demonstrate that CPSVD\nconsistently outperforms state-of-the-art SVD-based LLM compression methods,\nachieving lower perplexity and higher accuracy on zero-shot tasks."
                },
                "authors": [
                    {
                        "name": "Lin Xv"
                    },
                    {
                        "name": "Jingsheng Gao"
                    },
                    {
                        "name": "Xian Gao"
                    },
                    {
                        "name": "Ting Li"
                    },
                    {
                        "name": "Yuzhuo Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yuzhuo Fu"
                },
                "author": "Yuzhuo Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10620v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10620v3",
                "updated": "2025-10-22T08:47:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    8,
                    47,
                    3,
                    2,
                    295,
                    0
                ],
                "published": "2025-03-13T17:57:32Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    57,
                    32,
                    3,
                    72,
                    0
                ],
                "title": "From TOWER to SPIRE: Adding the Speech Modality to a\n  Translation-Specialist LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From TOWER to SPIRE: Adding the Speech Modality to a\n  Translation-Specialist LLM"
                },
                "summary": "We introduce Spire, a speech-augmented language model (LM) capable of both\ntranslating and transcribing speech input from English into 10 other languages\nas well as translating text input in both language directions. Spire integrates\nthe speech modality into an existing multilingual LM via speech discretization\nand continued pre-training using only 42.5K hours of speech. In particular, we\nadopt the pretraining framework of multilingual LMs and treat discretized\nspeech input as an additional translation language. This approach not only\nequips the model with speech capabilities, but also preserves its strong\ntext-based performance. We achieve this using significantly less data than\nexisting speech LMs, demonstrating that discretized speech input integration as\nan additional language is feasible during LM adaptation. We make our code and\nmodels available to the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Spire, a speech-augmented language model (LM) capable of both\ntranslating and transcribing speech input from English into 10 other languages\nas well as translating text input in both language directions. Spire integrates\nthe speech modality into an existing multilingual LM via speech discretization\nand continued pre-training using only 42.5K hours of speech. In particular, we\nadopt the pretraining framework of multilingual LMs and treat discretized\nspeech input as an additional translation language. This approach not only\nequips the model with speech capabilities, but also preserves its strong\ntext-based performance. We achieve this using significantly less data than\nexisting speech LMs, demonstrating that discretized speech input integration as\nan additional language is feasible during LM adaptation. We make our code and\nmodels available to the community."
                },
                "authors": [
                    {
                        "name": "Kshitij Ambilduke"
                    },
                    {
                        "name": "Ben Peters"
                    },
                    {
                        "name": "Sonal Sannigrahi"
                    },
                    {
                        "name": "Anil Keshwani"
                    },
                    {
                        "name": "Tsz Kin Lam"
                    },
                    {
                        "name": "Bruno Martins"
                    },
                    {
                        "name": "André F. T. Martins"
                    },
                    {
                        "name": "Marcely Zanon Boito"
                    }
                ],
                "author_detail": {
                    "name": "Marcely Zanon Boito"
                },
                "author": "Marcely Zanon Boito",
                "arxiv_comment": "EMNLP 2025 (Findings) camera ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10620v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10620v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16708v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16708v2",
                "updated": "2025-10-22T08:45:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    8,
                    45,
                    10,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-19T04:26:51Z",
                "published_parsed": [
                    2025,
                    10,
                    19,
                    4,
                    26,
                    51,
                    6,
                    292,
                    0
                ],
                "title": "Natural Language Processing for Cardiology: A Narrative Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Processing for Cardiology: A Narrative Review"
                },
                "summary": "Cardiovascular diseases are becoming increasingly prevalent in modern\nsociety, with a profound impact on global health and well-being. These\nCardiovascular disorders are complex and multifactorial, influenced by genetic\npredispositions, lifestyle choices, and diverse socioeconomic and clinical\nfactors. Information about these interrelated factors is dispersed across\nmultiple types of textual data, including patient narratives, medical records,\nand scientific literature. Natural language processing (NLP) has emerged as a\npowerful approach for analysing such unstructured data, enabling healthcare\nprofessionals and researchers to gain deeper insights that may transform the\ndiagnosis, treatment, and prevention of cardiac disorders. This review provides\na comprehensive overview of NLP research in cardiology from 2014 to 2025. We\nsystematically searched six literature databases for studies describing NLP\napplications across a range of cardiovascular diseases. After a rigorous\nscreening process, we identified 265 relevant articles. Each study was analysed\nacross multiple dimensions, including NLP paradigms, cardiology-related tasks,\ndisease types, and data sources. Our findings reveal substantial diversity\nwithin these dimensions, reflecting the breadth and evolution of NLP research\nin cardiology. A temporal analysis further highlights methodological trends,\nshowing a progression from rule-based systems to large language models.\nFinally, we discuss key challenges and future directions, such as developing\ninterpretable LLMs and integrating multimodal data. To the best of our\nknowledge, this review represents the most comprehensive synthesis of NLP\nresearch in cardiology to date.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cardiovascular diseases are becoming increasingly prevalent in modern\nsociety, with a profound impact on global health and well-being. These\nCardiovascular disorders are complex and multifactorial, influenced by genetic\npredispositions, lifestyle choices, and diverse socioeconomic and clinical\nfactors. Information about these interrelated factors is dispersed across\nmultiple types of textual data, including patient narratives, medical records,\nand scientific literature. Natural language processing (NLP) has emerged as a\npowerful approach for analysing such unstructured data, enabling healthcare\nprofessionals and researchers to gain deeper insights that may transform the\ndiagnosis, treatment, and prevention of cardiac disorders. This review provides\na comprehensive overview of NLP research in cardiology from 2014 to 2025. We\nsystematically searched six literature databases for studies describing NLP\napplications across a range of cardiovascular diseases. After a rigorous\nscreening process, we identified 265 relevant articles. Each study was analysed\nacross multiple dimensions, including NLP paradigms, cardiology-related tasks,\ndisease types, and data sources. Our findings reveal substantial diversity\nwithin these dimensions, reflecting the breadth and evolution of NLP research\nin cardiology. A temporal analysis further highlights methodological trends,\nshowing a progression from rule-based systems to large language models.\nFinally, we discuss key challenges and future directions, such as developing\ninterpretable LLMs and integrating multimodal data. To the best of our\nknowledge, this review represents the most comprehensive synthesis of NLP\nresearch in cardiology to date."
                },
                "authors": [
                    {
                        "name": "Kailai Yang"
                    },
                    {
                        "name": "Yan Leng"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Tianlin Zhang"
                    },
                    {
                        "name": "Paul Thompson"
                    },
                    {
                        "name": "Bernard Keavney"
                    },
                    {
                        "name": "Maciej Tomaszewski"
                    },
                    {
                        "name": "Sophia Ananiadou"
                    }
                ],
                "author_detail": {
                    "name": "Sophia Ananiadou"
                },
                "author": "Sophia Ananiadou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16708v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19366v1",
                "updated": "2025-10-22T08:40:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    8,
                    40,
                    1,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T08:40:01Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    8,
                    40,
                    1,
                    2,
                    295,
                    0
                ],
                "title": "MoE-Prism: Disentangling Monolithic Experts for Elastic MoE Services via\n  Model-System Co-Designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Prism: Disentangling Monolithic Experts for Elastic MoE Services via\n  Model-System Co-Designs"
                },
                "summary": "Mixture-of-Experts (MoE) models, the state-of-the-art in large-scale AI,\nachieve high quality by sparsely activating parameters. However, their reliance\non routing between a few monolithic experts via a top-k mechanism creates a\n\"quality cliff\", offering only a few coarse-grained operating points. This\ninflexibility forces a difficult trade-off between cost and quality, preventing\nadaptation to diverse Service Level Objectives (SLOs) and leading to\nsignificant resource over-provisioning.\n  This paper introduces MoE-Prism, a model-system co-design that transforms\nrigid MoE models into elastic services. Our methodology is divided into two\nphases. First, an \\emph{Offline Refactoring Engine} systematically deconstructs\nmonolithic experts into fine-grained \"sub-experts.\" This engine employs a\npartitioning optimization solver that uses a metaheuristic-based approach to\ngroup neurons, preserving functional locality without requiring retraining.\nSecond, an \\emph{Online Scheduling Engine} leverages this new elasticity\nthrough QoS-aware scheduling. It implements specialized policies to solve\ncomplex system problems, including maximizing throughput in cloud deployments\nand managing latency-optimized offloading for memory-constrained devices. Our\nevaluation across three different MoE models shows that MoE-Prismprovides over\n4 times more distinct, stable operating points than the baseline. This allows\nan AI service to dynamically improve throughput by up to 19.9\\% under a strict\nlatency budget or reduce latency by up to 10.36\\% under limited resources.\nMoE-Prism provides the critical \"control knob\" to bridge the model-system gap,\nenabling the next generation of adaptive, efficient, and QoS-aware AI services.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models, the state-of-the-art in large-scale AI,\nachieve high quality by sparsely activating parameters. However, their reliance\non routing between a few monolithic experts via a top-k mechanism creates a\n\"quality cliff\", offering only a few coarse-grained operating points. This\ninflexibility forces a difficult trade-off between cost and quality, preventing\nadaptation to diverse Service Level Objectives (SLOs) and leading to\nsignificant resource over-provisioning.\n  This paper introduces MoE-Prism, a model-system co-design that transforms\nrigid MoE models into elastic services. Our methodology is divided into two\nphases. First, an \\emph{Offline Refactoring Engine} systematically deconstructs\nmonolithic experts into fine-grained \"sub-experts.\" This engine employs a\npartitioning optimization solver that uses a metaheuristic-based approach to\ngroup neurons, preserving functional locality without requiring retraining.\nSecond, an \\emph{Online Scheduling Engine} leverages this new elasticity\nthrough QoS-aware scheduling. It implements specialized policies to solve\ncomplex system problems, including maximizing throughput in cloud deployments\nand managing latency-optimized offloading for memory-constrained devices. Our\nevaluation across three different MoE models shows that MoE-Prismprovides over\n4 times more distinct, stable operating points than the baseline. This allows\nan AI service to dynamically improve throughput by up to 19.9\\% under a strict\nlatency budget or reduce latency by up to 10.36\\% under limited resources.\nMoE-Prism provides the critical \"control knob\" to bridge the model-system gap,\nenabling the next generation of adaptive, efficient, and QoS-aware AI services."
                },
                "authors": [
                    {
                        "name": "Xinfeng Xia"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Xiaofeng Hou"
                    },
                    {
                        "name": "Peng Tang"
                    },
                    {
                        "name": "Mingxuan Zhang"
                    },
                    {
                        "name": "Wenfeng Wang"
                    },
                    {
                        "name": "Chao Li"
                    }
                ],
                "author_detail": {
                    "name": "Chao Li"
                },
                "author": "Chao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01349v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01349v4",
                "updated": "2025-10-22T08:36:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    8,
                    36,
                    39,
                    2,
                    295,
                    0
                ],
                "published": "2025-02-03T13:39:28Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    13,
                    39,
                    28,
                    0,
                    34,
                    0
                ],
                "title": "Bias Beware: The Impact of Cognitive Biases on LLM-Driven Product\n  Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias Beware: The Impact of Cognitive Biases on LLM-Driven Product\n  Recommendations"
                },
                "summary": "The advent of Large Language Models (LLMs) has revolutionized product\nrecommenders, yet their susceptibility to adversarial manipulation poses\ncritical challenges, particularly in real-world commercial applications. Our\napproach is the first one to tap into human psychological principles,\nseamlessly modifying product descriptions, making such manipulations hard to\ndetect. In this work, we investigate cognitive biases as black-box adversarial\nstrategies, drawing parallels between their effects on LLMs and human\npurchasing behavior. Through extensive evaluation across models of varying\nscale, we find that certain biases, such as social proof, consistently boost\nproduct recommendation rate and ranking, while others, like scarcity and\nexclusivity, surprisingly reduce visibility. Our results demonstrate that\ncognitive biases are deeply embedded in state-of-the-art LLMs, leading to\nhighly unpredictable behavior in product recommendations and posing significant\nchallenges for effective mitigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) has revolutionized product\nrecommenders, yet their susceptibility to adversarial manipulation poses\ncritical challenges, particularly in real-world commercial applications. Our\napproach is the first one to tap into human psychological principles,\nseamlessly modifying product descriptions, making such manipulations hard to\ndetect. In this work, we investigate cognitive biases as black-box adversarial\nstrategies, drawing parallels between their effects on LLMs and human\npurchasing behavior. Through extensive evaluation across models of varying\nscale, we find that certain biases, such as social proof, consistently boost\nproduct recommendation rate and ranking, while others, like scarcity and\nexclusivity, surprisingly reduce visibility. Our results demonstrate that\ncognitive biases are deeply embedded in state-of-the-art LLMs, leading to\nhighly unpredictable behavior in product recommendations and posing significant\nchallenges for effective mitigation."
                },
                "authors": [
                    {
                        "name": "Giorgos Filandrianos"
                    },
                    {
                        "name": "Angeliki Dimitriou"
                    },
                    {
                        "name": "Maria Lymperaiou"
                    },
                    {
                        "name": "Konstantinos Thomas"
                    },
                    {
                        "name": "Giorgos Stamou"
                    }
                ],
                "author_detail": {
                    "name": "Giorgos Stamou"
                },
                "author": "Giorgos Stamou",
                "arxiv_comment": "Accepted at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01349v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01349v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]