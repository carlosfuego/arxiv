[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.06447v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06447v1",
                "updated": "2025-08-08T16:42:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    42,
                    38,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T16:42:38Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    42,
                    38,
                    4,
                    220,
                    0
                ],
                "title": "SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token\n  Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token\n  Pruning"
                },
                "summary": "Long-context inference for Large Language Models (LLMs) is heavily limited by\nhigh computational demands. While several existing methods optimize attention\ncomputation, they still process the full set of hidden states at each layer,\nlimiting overall efficiency. In this work, we propose SlimInfer, an innovative\nframework that aims to accelerate inference by directly pruning less critical\nprompt tokens during the forward pass. Our key insight is an information\ndiffusion phenomenon: As information from critical tokens propagates through\nlayers, it becomes distributed across the entire sequence. This diffusion\nprocess suggests that LLMs can maintain their semantic integrity when excessive\ntokens, even including these critical ones, are pruned in hidden states.\nMotivated by this, SlimInfer introduces a dynamic fine-grained pruning\nmechanism that accurately removes redundant tokens of hidden state at\nintermediate layers. This layer-wise pruning naturally enables an asynchronous\nKV cache manager that prefetches required token blocks without complex\npredictors, reducing both memory usage and I/O costs. Extensive experiments\nshow that SlimInfer can achieve up to $\\mathbf{2.53\\times}$ time-to-first-token\n(TTFT) speedup and $\\mathbf{1.88\\times}$ end-to-end latency reduction for\nLLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on\nLongBench. Our code will be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context inference for Large Language Models (LLMs) is heavily limited by\nhigh computational demands. While several existing methods optimize attention\ncomputation, they still process the full set of hidden states at each layer,\nlimiting overall efficiency. In this work, we propose SlimInfer, an innovative\nframework that aims to accelerate inference by directly pruning less critical\nprompt tokens during the forward pass. Our key insight is an information\ndiffusion phenomenon: As information from critical tokens propagates through\nlayers, it becomes distributed across the entire sequence. This diffusion\nprocess suggests that LLMs can maintain their semantic integrity when excessive\ntokens, even including these critical ones, are pruned in hidden states.\nMotivated by this, SlimInfer introduces a dynamic fine-grained pruning\nmechanism that accurately removes redundant tokens of hidden state at\nintermediate layers. This layer-wise pruning naturally enables an asynchronous\nKV cache manager that prefetches required token blocks without complex\npredictors, reducing both memory usage and I/O costs. Extensive experiments\nshow that SlimInfer can achieve up to $\\mathbf{2.53\\times}$ time-to-first-token\n(TTFT) speedup and $\\mathbf{1.88\\times}$ end-to-end latency reduction for\nLLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on\nLongBench. Our code will be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Lingkun Long"
                    },
                    {
                        "name": "Rubing Yang"
                    },
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Desheng Hui"
                    },
                    {
                        "name": "Ao Zhou"
                    },
                    {
                        "name": "Jianlei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jianlei Yang"
                },
                "author": "Jianlei Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06447v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06447v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07467v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07467v3",
                "updated": "2025-08-08T14:25:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    25,
                    24,
                    4,
                    220,
                    0
                ],
                "published": "2024-06-11T17:13:18Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    17,
                    13,
                    18,
                    1,
                    163,
                    0
                ],
                "title": "LLM meets ML: Data-efficient Anomaly Detection on Unstable Logs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM meets ML: Data-efficient Anomaly Detection on Unstable Logs"
                },
                "summary": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for \\task, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\n(pp) in F1 score while using much less labeled data (62.87 pp reduction). When\ntrained on the same amount of data as the baselines, FlexLog achieves up to a\n13 pp increase in F1 score on ADFA-U across varying training dataset sizes.\nAdditionally, FlexLog maintains inference time under one second per log\nsequence, making it suitable for most applications, except latency-sensitive\nsystems. Further analysis reveals the positive impact of FlexLog's key\ncomponents: cache, RAG and ensemble learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for \\task, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\n(pp) in F1 score while using much less labeled data (62.87 pp reduction). When\ntrained on the same amount of data as the baselines, FlexLog achieves up to a\n13 pp increase in F1 score on ADFA-U across varying training dataset sizes.\nAdditionally, FlexLog maintains inference time under one second per log\nsequence, making it suitable for most applications, except latency-sensitive\nsystems. Further analysis reveals the positive impact of FlexLog's key\ncomponents: cache, RAG and ensemble learning."
                },
                "authors": [
                    {
                        "name": "Fatemeh Hadadi"
                    },
                    {
                        "name": "Qinghua Xu"
                    },
                    {
                        "name": "Domenico Bianculli"
                    },
                    {
                        "name": "Lionel Briand"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Briand"
                },
                "author": "Lionel Briand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07467v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07467v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06297v1",
                "updated": "2025-08-08T13:19:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    19,
                    30,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T13:19:30Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    19,
                    30,
                    4,
                    220,
                    0
                ],
                "title": "KV Cache Compression for Inference Efficiency in LLMs: A Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Compression for Inference Efficiency in LLMs: A Review"
                },
                "summary": "Withtherapid advancement of large language models (LLMs), the context length\nfor inference has been continuously increasing, leading to an exponential\ngrowth in the demand for Key-Value (KV) caching. This has resulted in a\nsignificant memory bottleneck, limiting the inference efficiency and\nscalability of the models. Therefore, optimizing the KV cache during inference\nis crucial for enhancing performance and efficiency. This review systematically\nexamines current KV cache optimization techniques, including compression\nstrategies such as selective token strategies, quantization, and attention\ncompression. We evaluate the effectiveness, trade-offs, and application\nscenarios of these methods, providing a comprehensive analysis of their impact\non memory usage and inference speed. We focus on identifying the limitations\nand challenges of existing methods, such as compatibility issues with different\nmodels and tasks. Additionally, this review highlights future research\ndirections, including hybrid optimization techniques, adaptive dynamic\nstrategies, and software-hardware co-design. These approaches aim to improve\ninference efficiency and promote the practical application of large language\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Withtherapid advancement of large language models (LLMs), the context length\nfor inference has been continuously increasing, leading to an exponential\ngrowth in the demand for Key-Value (KV) caching. This has resulted in a\nsignificant memory bottleneck, limiting the inference efficiency and\nscalability of the models. Therefore, optimizing the KV cache during inference\nis crucial for enhancing performance and efficiency. This review systematically\nexamines current KV cache optimization techniques, including compression\nstrategies such as selective token strategies, quantization, and attention\ncompression. We evaluate the effectiveness, trade-offs, and application\nscenarios of these methods, providing a comprehensive analysis of their impact\non memory usage and inference speed. We focus on identifying the limitations\nand challenges of existing methods, such as compatibility issues with different\nmodels and tasks. Additionally, this review highlights future research\ndirections, including hybrid optimization techniques, adaptive dynamic\nstrategies, and software-hardware co-design. These approaches aim to improve\ninference efficiency and promote the practical application of large language\nmodels."
                },
                "authors": [
                    {
                        "name": "Yanyu Liu"
                    },
                    {
                        "name": "Jingying Fu"
                    },
                    {
                        "name": "Sixiang Liu"
                    },
                    {
                        "name": "Yitian Zou"
                    },
                    {
                        "name": "You Fu"
                    },
                    {
                        "name": "Jiehan Zhou"
                    },
                    {
                        "name": "Shouhua Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shouhua Zhang"
                },
                "arxiv_affiliation": "University of Oulu",
                "author": "Shouhua Zhang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06160v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06160v1",
                "updated": "2025-08-08T09:29:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    9,
                    29,
                    37,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T09:29:37Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    9,
                    29,
                    37,
                    4,
                    220,
                    0
                ],
                "title": "Fewer Denoising Steps or Cheaper Per-Step Inference: Towards\n  Compute-Optimal Diffusion Model Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fewer Denoising Steps or Cheaper Per-Step Inference: Towards\n  Compute-Optimal Diffusion Model Deployment"
                },
                "summary": "Diffusion models have shown remarkable success across generative tasks, yet\ntheir high computational demands challenge deployment on resource-limited\nplatforms. This paper investigates a critical question for compute-optimal\ndiffusion model deployment: Under a post-training setting without fine-tuning,\nis it more effective to reduce the number of denoising steps or to use a\ncheaper per-step inference? Intuitively, reducing the number of denoising steps\nincreases the variability of the distributions across steps, making the model\nmore sensitive to compression. In contrast, keeping more denoising steps makes\nthe differences smaller, preserving redundancy, and making post-training\ncompression more feasible. To systematically examine this, we propose PostDiff,\na training-free framework for accelerating pre-trained diffusion models by\nreducing redundancy at both the input level and module level in a post-training\nmanner. At the input level, we propose a mixed-resolution denoising scheme\nbased on the insight that reducing generation resolution in early denoising\nsteps can enhance low-frequency components and improve final generation\nfidelity. At the module level, we employ a hybrid module caching strategy to\nreuse computations across denoising steps. Extensive experiments and ablation\nstudies demonstrate that (1) PostDiff can significantly improve the\nfidelity-efficiency trade-off of state-of-the-art diffusion models, and (2) to\nboost efficiency while maintaining decent generation fidelity, reducing\nper-step inference cost is often more effective than reducing the number of\ndenoising steps. Our code is available at\nhttps://github.com/GATECH-EIC/PostDiff.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have shown remarkable success across generative tasks, yet\ntheir high computational demands challenge deployment on resource-limited\nplatforms. This paper investigates a critical question for compute-optimal\ndiffusion model deployment: Under a post-training setting without fine-tuning,\nis it more effective to reduce the number of denoising steps or to use a\ncheaper per-step inference? Intuitively, reducing the number of denoising steps\nincreases the variability of the distributions across steps, making the model\nmore sensitive to compression. In contrast, keeping more denoising steps makes\nthe differences smaller, preserving redundancy, and making post-training\ncompression more feasible. To systematically examine this, we propose PostDiff,\na training-free framework for accelerating pre-trained diffusion models by\nreducing redundancy at both the input level and module level in a post-training\nmanner. At the input level, we propose a mixed-resolution denoising scheme\nbased on the insight that reducing generation resolution in early denoising\nsteps can enhance low-frequency components and improve final generation\nfidelity. At the module level, we employ a hybrid module caching strategy to\nreuse computations across denoising steps. Extensive experiments and ablation\nstudies demonstrate that (1) PostDiff can significantly improve the\nfidelity-efficiency trade-off of state-of-the-art diffusion models, and (2) to\nboost efficiency while maintaining decent generation fidelity, reducing\nper-step inference cost is often more effective than reducing the number of\ndenoising steps. Our code is available at\nhttps://github.com/GATECH-EIC/PostDiff."
                },
                "authors": [
                    {
                        "name": "Zhenbang Du"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Lifu Wang"
                    },
                    {
                        "name": "Jiayi Qian"
                    },
                    {
                        "name": "Xiao Luo"
                    },
                    {
                        "name": "Yingyan"
                    },
                    {
                        "name": "Lin"
                    }
                ],
                "author_detail": {
                    "name": "Lin"
                },
                "arxiv_affiliation": "Celine",
                "author": "Lin",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06160v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06160v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06133v1",
                "updated": "2025-08-08T08:54:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    54,
                    21,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T08:54:21Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    54,
                    21,
                    4,
                    220,
                    0
                ],
                "title": "LLM Serving Optimization with Variable Prefill and Decode Lengths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Serving Optimization with Variable Prefill and Decode Lengths"
                },
                "summary": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency."
                },
                "authors": [
                    {
                        "name": "Meixuan Wang"
                    },
                    {
                        "name": "Yinyu Ye"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06064v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06064v1",
                "updated": "2025-08-08T06:53:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    6,
                    53,
                    50,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T06:53:50Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    6,
                    53,
                    50,
                    4,
                    220,
                    0
                ],
                "title": "A Generic Complete Anytime Beam Search for Optimal Decision Tree",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Generic Complete Anytime Beam Search for Optimal Decision Tree"
                },
                "summary": "Finding an optimal decision tree that minimizes classification error is known\nto be NP-hard. While exact algorithms based on MILP, CP, SAT, or dynamic\nprogramming guarantee optimality, they often suffer from poor anytime behavior\n-- meaning they struggle to find high-quality decision trees quickly when the\nsearch is stopped before completion -- due to unbalanced search space\nexploration. To address this, several anytime extensions of exact methods have\nbeen proposed, such as LDS-DL8.5, Top-k-DL8.5, and Blossom, but they have not\nbeen systematically compared, making it difficult to assess their relative\neffectiveness. In this paper, we propose CA-DL8.5, a generic, complete, and\nanytime beam search algorithm that extends the DL8.5 framework and unifies some\nexisting anytime strategies. In particular, CA-DL8.5 generalizes previous\napproaches LDS-DL8.5 and Top-k-DL8.5, by allowing the integration of various\nheuristics and relaxation mechanisms through a modular design. The algorithm\nreuses DL8.5's efficient branch-and-bound pruning and trie-based caching,\ncombined with a restart-based beam search that gradually relaxes pruning\ncriteria to improve solution quality over time. Our contributions are twofold:\n(1) We introduce this new generic framework for exact and anytime decision tree\nlearning, enabling the incorporation of diverse heuristics and search\nstrategies; (2) We conduct a rigorous empirical comparison of several\ninstantiations of CA-DL8.5 -- based on Purity, Gain, Discrepancy, and Top-k\nheuristics -- using an anytime evaluation metric called the primal gap\nintegral. Experimental results on standard classification benchmarks show that\nCA-DL8.5 using LDS (limited discrepancy) consistently provides the best anytime\nperformance, outperforming both other CA-DL8.5 variants and the Blossom\nalgorithm while maintaining completeness and optimality guarantees.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finding an optimal decision tree that minimizes classification error is known\nto be NP-hard. While exact algorithms based on MILP, CP, SAT, or dynamic\nprogramming guarantee optimality, they often suffer from poor anytime behavior\n-- meaning they struggle to find high-quality decision trees quickly when the\nsearch is stopped before completion -- due to unbalanced search space\nexploration. To address this, several anytime extensions of exact methods have\nbeen proposed, such as LDS-DL8.5, Top-k-DL8.5, and Blossom, but they have not\nbeen systematically compared, making it difficult to assess their relative\neffectiveness. In this paper, we propose CA-DL8.5, a generic, complete, and\nanytime beam search algorithm that extends the DL8.5 framework and unifies some\nexisting anytime strategies. In particular, CA-DL8.5 generalizes previous\napproaches LDS-DL8.5 and Top-k-DL8.5, by allowing the integration of various\nheuristics and relaxation mechanisms through a modular design. The algorithm\nreuses DL8.5's efficient branch-and-bound pruning and trie-based caching,\ncombined with a restart-based beam search that gradually relaxes pruning\ncriteria to improve solution quality over time. Our contributions are twofold:\n(1) We introduce this new generic framework for exact and anytime decision tree\nlearning, enabling the incorporation of diverse heuristics and search\nstrategies; (2) We conduct a rigorous empirical comparison of several\ninstantiations of CA-DL8.5 -- based on Purity, Gain, Discrepancy, and Top-k\nheuristics -- using an anytime evaluation metric called the primal gap\nintegral. Experimental results on standard classification benchmarks show that\nCA-DL8.5 using LDS (limited discrepancy) consistently provides the best anytime\nperformance, outperforming both other CA-DL8.5 variants and the Blossom\nalgorithm while maintaining completeness and optimality guarantees."
                },
                "authors": [
                    {
                        "name": "Harold Silvère Kiossou"
                    },
                    {
                        "name": "Siegfried Nijssen"
                    },
                    {
                        "name": "Pierre Schaus"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Schaus"
                },
                "author": "Pierre Schaus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06064v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06064v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2104.13123v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2104.13123v3",
                "updated": "2025-08-08T06:38:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    6,
                    38,
                    1,
                    4,
                    220,
                    0
                ],
                "published": "2021-04-27T11:55:54Z",
                "published_parsed": [
                    2021,
                    4,
                    27,
                    11,
                    55,
                    54,
                    1,
                    117,
                    0
                ],
                "title": "Affine Springer fibers and depth zero L-packets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affine Springer fibers and depth zero L-packets"
                },
                "summary": "Let $G$ be a connected reductive group over a field $F=\\mathbb{F}_q((t))$\nsplitting over $\\overline{\\mathbb{F}}_q((t))$. Following [KV,DR], a tamely\nunramified Langlands parameter $\\lambda:W_F\\to{}^L\nG(\\overline{\\mathbb{Q}}_{\\ell})$ in general position gives rise to a finite set\n$\\Pi_{\\lambda}$ of irreducible admissible representations of $G(F)$, called the\n$L$-packet.\n  The main goal of this work is to provide a geometric description of\ncharacters $\\chi_{\\pi}$ of $\\pi\\in\\Pi_{\\lambda}$ and of their endoscopic linear\ncombinations $\\chi_{\\lambda}^{\\kappa}$ in terms of homology of affine Springer\nfibers, thus establishing an analog of Lusztig conjectures in this case.\nFurthermore, each $\\chi_{\\lambda}^{\\kappa}$ can be described as the trace of\nFrobenius function of a conjugation equivariant perverse sheaf on the loop\ngroup by the sheaf-function correspondence.\n  As another application, we prove that the sum\n$\\chi_{\\lambda}^{st}:=\\sum_{\\pi\\in\\Pi_{\\lambda}}\\chi_{\\pi}$ is stable and show\nthat the $\\chi_{\\lambda}^{st}$'s are compatible with inner twistings. More\ngenerally, we prove that each $\\chi_{\\lambda}^{\\kappa}$ is\n$\\mathcal{E}_{\\lambda,\\kappa}$-stable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let $G$ be a connected reductive group over a field $F=\\mathbb{F}_q((t))$\nsplitting over $\\overline{\\mathbb{F}}_q((t))$. Following [KV,DR], a tamely\nunramified Langlands parameter $\\lambda:W_F\\to{}^L\nG(\\overline{\\mathbb{Q}}_{\\ell})$ in general position gives rise to a finite set\n$\\Pi_{\\lambda}$ of irreducible admissible representations of $G(F)$, called the\n$L$-packet.\n  The main goal of this work is to provide a geometric description of\ncharacters $\\chi_{\\pi}$ of $\\pi\\in\\Pi_{\\lambda}$ and of their endoscopic linear\ncombinations $\\chi_{\\lambda}^{\\kappa}$ in terms of homology of affine Springer\nfibers, thus establishing an analog of Lusztig conjectures in this case.\nFurthermore, each $\\chi_{\\lambda}^{\\kappa}$ can be described as the trace of\nFrobenius function of a conjugation equivariant perverse sheaf on the loop\ngroup by the sheaf-function correspondence.\n  As another application, we prove that the sum\n$\\chi_{\\lambda}^{st}:=\\sum_{\\pi\\in\\Pi_{\\lambda}}\\chi_{\\pi}$ is stable and show\nthat the $\\chi_{\\lambda}^{st}$'s are compatible with inner twistings. More\ngenerally, we prove that each $\\chi_{\\lambda}^{\\kappa}$ is\n$\\mathcal{E}_{\\lambda,\\kappa}$-stable."
                },
                "authors": [
                    {
                        "name": "Roman Bezrukavnikov"
                    },
                    {
                        "name": "Yakov Varshavsky"
                    }
                ],
                "author_detail": {
                    "name": "Yakov Varshavsky"
                },
                "author": "Yakov Varshavsky",
                "arxiv_comment": "v.3, 96 pages, minor changes in abstract and introduction",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2104.13123v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2104.13123v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.RT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.RT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "22E50, 22E57",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05904v1",
                "updated": "2025-08-07T23:53:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    23,
                    53,
                    31,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T23:53:31Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    23,
                    53,
                    31,
                    3,
                    219,
                    0
                ],
                "title": "Snowpark: Performant, Secure, User-Friendly Data Engineering and AI/ML\n  Next To Your Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Snowpark: Performant, Secure, User-Friendly Data Engineering and AI/ML\n  Next To Your Data"
                },
                "summary": "Snowflake revolutionized data analytics with an elastic architecture that\ndecouples compute and storage, enabling scalable solutions supporting data\narchitectures like data lake, data warehouse, data lakehouse, and data mesh.\nBuilding on this foundation, Snowflake has advanced its AI Data Cloud vision by\nintroducing Snowpark, a managed turnkey solution that supports data engineering\nand AI and ML workloads using Python and other programming languages.\n  This paper outlines Snowpark's design objectives towards high performance,\nstrong security and governance, and ease of use. We detail the architecture of\nSnowpark, highlighting its elastic scalability and seamless integration with\nSnowflake core compute infrastructure. This includes leveraging Snowflake\ncontrol plane for distributed computing and employing a secure sandbox for\nisolating Snowflake SQL workloads from Snowpark executions. Additionally, we\npresent core innovations in Snowpark that drive further performance\nenhancements, such as query initialization latency reduction through Python\npackage caching, improved workload scheduling for customized workloads, and\ndata skew management via efficient row redistribution. Finally, we showcase\nreal-world case studies that illustrate Snowpark's efficiency and effectiveness\nfor large-scale data engineering and AI and ML tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Snowflake revolutionized data analytics with an elastic architecture that\ndecouples compute and storage, enabling scalable solutions supporting data\narchitectures like data lake, data warehouse, data lakehouse, and data mesh.\nBuilding on this foundation, Snowflake has advanced its AI Data Cloud vision by\nintroducing Snowpark, a managed turnkey solution that supports data engineering\nand AI and ML workloads using Python and other programming languages.\n  This paper outlines Snowpark's design objectives towards high performance,\nstrong security and governance, and ease of use. We detail the architecture of\nSnowpark, highlighting its elastic scalability and seamless integration with\nSnowflake core compute infrastructure. This includes leveraging Snowflake\ncontrol plane for distributed computing and employing a secure sandbox for\nisolating Snowflake SQL workloads from Snowpark executions. Additionally, we\npresent core innovations in Snowpark that drive further performance\nenhancements, such as query initialization latency reduction through Python\npackage caching, improved workload scheduling for customized workloads, and\ndata skew management via efficient row redistribution. Finally, we showcase\nreal-world case studies that illustrate Snowpark's efficiency and effectiveness\nfor large-scale data engineering and AI and ML tasks."
                },
                "authors": [
                    {
                        "name": "Brandon Baker"
                    },
                    {
                        "name": "Elliott Brossard"
                    },
                    {
                        "name": "Chenwei Xie"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Deen Liu"
                    },
                    {
                        "name": "Yijun Xie"
                    },
                    {
                        "name": "Arthur Zwiegincew"
                    },
                    {
                        "name": "Nitya Kumar Sharma"
                    },
                    {
                        "name": "Gaurav Jain"
                    },
                    {
                        "name": "Eugene Retunsky"
                    },
                    {
                        "name": "Mike Halcrow"
                    },
                    {
                        "name": "Derek Denny-Brown"
                    },
                    {
                        "name": "Istvan Cseri"
                    },
                    {
                        "name": "Tyler Akidau"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "arxiv_comment": "12 pages, 6 figures, accepted in ICDCS 2025",
                "arxiv_journal_ref": "Proc. 45th IEEE International Conference on Distributed Computing\n  Systems (ICDCS), Glasgow, UK, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05898v1",
                "updated": "2025-08-07T23:11:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    23,
                    11,
                    33,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T23:11:33Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    23,
                    11,
                    33,
                    3,
                    219,
                    0
                ],
                "title": "ETTA: Efficient Test-Time Adaptation for Vision-Language Models through\n  Dynamic Embedding Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETTA: Efficient Test-Time Adaptation for Vision-Language Models through\n  Dynamic Embedding Updates"
                },
                "summary": "Pretrained vision-language models (VLMs) like CLIP show strong zero-shot\nperformance but struggle with generalization under distribution shifts.\nTest-Time Adaptation (TTA) addresses this by adapting VLMs to unlabeled test\ndata in new domains. While some TTA methods rely on prompt-tuning,\ntraining-free cache-based approaches are preferred for efficiency. However,\ncurrent cache-based TTA models store only a limited set of high-confidence\nsamples, restricting the decision boundary to these samples and ignoring the\ninfluence of other incoming test data. To address this, we propose Efficient\nTest-Time Adaptation (ETTA), introducing a Recursive Updating module that\nintegrates all incoming test samples, progressively refining the decision\nboundary. This strategy mimics an unbounded cache, dynamically updating\ncontextual embeddings for improved accuracy with minimal memory and\ncomputational overhead. ETTA also includes an Adaptive Ensemble module to\nreduce prompt dependency in image-to-text scores by dynamically selecting\noptimal prompts for each class. Furthermore, ETTA adaptively combines scores\nfrom both modules based on confidence levels, leveraging their complementary\nstrengths. Extensive experiments on two benchmarks confirm that ETTA surpasses\nthe state-of-the-art TTA models in computational complexity and accuracy,\nsetting a new standard for effective, efficient test-time adaptation. The code\nhas been released at https://github.com/hamidreza-dastmalchi/ETTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretrained vision-language models (VLMs) like CLIP show strong zero-shot\nperformance but struggle with generalization under distribution shifts.\nTest-Time Adaptation (TTA) addresses this by adapting VLMs to unlabeled test\ndata in new domains. While some TTA methods rely on prompt-tuning,\ntraining-free cache-based approaches are preferred for efficiency. However,\ncurrent cache-based TTA models store only a limited set of high-confidence\nsamples, restricting the decision boundary to these samples and ignoring the\ninfluence of other incoming test data. To address this, we propose Efficient\nTest-Time Adaptation (ETTA), introducing a Recursive Updating module that\nintegrates all incoming test samples, progressively refining the decision\nboundary. This strategy mimics an unbounded cache, dynamically updating\ncontextual embeddings for improved accuracy with minimal memory and\ncomputational overhead. ETTA also includes an Adaptive Ensemble module to\nreduce prompt dependency in image-to-text scores by dynamically selecting\noptimal prompts for each class. Furthermore, ETTA adaptively combines scores\nfrom both modules based on confidence levels, leveraging their complementary\nstrengths. Extensive experiments on two benchmarks confirm that ETTA surpasses\nthe state-of-the-art TTA models in computational complexity and accuracy,\nsetting a new standard for effective, efficient test-time adaptation. The code\nhas been released at https://github.com/hamidreza-dastmalchi/ETTA."
                },
                "authors": [
                    {
                        "name": "Hamidreza Dastmalchi"
                    },
                    {
                        "name": "Aijun An"
                    },
                    {
                        "name": "Ali cheraghian"
                    }
                ],
                "author_detail": {
                    "name": "Ali cheraghian"
                },
                "author": "Ali cheraghian",
                "arxiv_comment": "BMVC2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05211v1",
                "updated": "2025-08-07T09:47:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    9,
                    47,
                    21,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T09:47:21Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    9,
                    47,
                    21,
                    3,
                    219,
                    0
                ],
                "title": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization"
                },
                "summary": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference."
                },
                "authors": [
                    {
                        "name": "Sihan Yang"
                    },
                    {
                        "name": "Runsen Xu"
                    },
                    {
                        "name": "Chenhang Cui"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05091v1",
                "updated": "2025-08-07T07:19:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    7,
                    19,
                    2,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T07:19:02Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    7,
                    19,
                    2,
                    3,
                    219,
                    0
                ],
                "title": "PoseGen: In-Context LoRA Finetuning for Pose-Controllable Long Human\n  Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PoseGen: In-Context LoRA Finetuning for Pose-Controllable Long Human\n  Video Generation"
                },
                "summary": "Generating long, temporally coherent videos with precise control over subject\nidentity and motion is a formidable challenge for current diffusion models,\nwhich often suffer from identity drift and are limited to short clips. We\nintroduce PoseGen, a novel framework that generates arbitrarily long videos of\na specific subject from a single reference image and a driving pose sequence.\nOur core innovation is an in-context LoRA finetuning strategy that injects\nsubject appearance at the token level for identity preservation, while\nsimultaneously conditioning on pose information at the channel level for\nfine-grained motion control. To overcome duration limits, PoseGen pioneers an\ninterleaved segment generation method that seamlessly stitches video clips\ntogether, using a shared KV cache mechanism and a specialized transition\nprocess to ensure background consistency and temporal smoothness. Trained on a\nremarkably small 33-hour video dataset, extensive experiments show that PoseGen\nsignificantly outperforms state-of-the-art methods in identity fidelity, pose\naccuracy, and its unique ability to produce coherent, artifact-free videos of\nunlimited duration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating long, temporally coherent videos with precise control over subject\nidentity and motion is a formidable challenge for current diffusion models,\nwhich often suffer from identity drift and are limited to short clips. We\nintroduce PoseGen, a novel framework that generates arbitrarily long videos of\na specific subject from a single reference image and a driving pose sequence.\nOur core innovation is an in-context LoRA finetuning strategy that injects\nsubject appearance at the token level for identity preservation, while\nsimultaneously conditioning on pose information at the channel level for\nfine-grained motion control. To overcome duration limits, PoseGen pioneers an\ninterleaved segment generation method that seamlessly stitches video clips\ntogether, using a shared KV cache mechanism and a specialized transition\nprocess to ensure background consistency and temporal smoothness. Trained on a\nremarkably small 33-hour video dataset, extensive experiments show that PoseGen\nsignificantly outperforms state-of-the-art methods in identity fidelity, pose\naccuracy, and its unique ability to produce coherent, artifact-free videos of\nunlimited duration."
                },
                "authors": [
                    {
                        "name": "Jingxuan He"
                    },
                    {
                        "name": "Busheng Su"
                    },
                    {
                        "name": "Finn Wong"
                    }
                ],
                "author_detail": {
                    "name": "Finn Wong"
                },
                "author": "Finn Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05012v1",
                "updated": "2025-08-07T03:49:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    3,
                    49,
                    56,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T03:49:56Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    3,
                    49,
                    56,
                    3,
                    219,
                    0
                ],
                "title": "Making Prompts First-Class Citizens for Adaptive LLM Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making Prompts First-Class Citizens for Adaptive LLM Pipelines"
                },
                "summary": "Modern LLM pipelines increasingly resemble data-centric systems: they\nretrieve external context, compose intermediate outputs, validate results, and\nadapt based on runtime feedback. Yet, the central element guiding this process\n-- the prompt -- remains a brittle, opaque string, disconnected from the\nsurrounding dataflow. This disconnect limits reuse, optimization, and runtime\ncontrol.\n  In this paper, we describe our vision and an initial design for SPEAR, a\nlanguage and runtime that fills this prompt management gap by making prompts\nstructured, adaptive, and first-class components of the execution model. SPEAR\nenables (1) runtime prompt refinement -- modifying prompts dynamically in\nresponse to execution-time signals such as confidence, latency, or missing\ncontext; and (2) structured prompt management -- organizing prompt fragments\ninto versioned views with support for introspection and logging.\n  SPEAR defines a prompt algebra that governs how prompts are constructed and\nadapted within a pipeline. It supports multiple refinement modes (manual,\nassisted, and automatic), giving developers a balance between control and\nautomation. By treating prompt logic as structured data, SPEAR enables\noptimizations such as operator fusion, prefix caching, and view reuse.\nPreliminary experiments quantify the behavior of different refinement modes\ncompared to static prompts and agentic retries, as well as the impact of\nprompt-level optimizations such as operator fusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLM pipelines increasingly resemble data-centric systems: they\nretrieve external context, compose intermediate outputs, validate results, and\nadapt based on runtime feedback. Yet, the central element guiding this process\n-- the prompt -- remains a brittle, opaque string, disconnected from the\nsurrounding dataflow. This disconnect limits reuse, optimization, and runtime\ncontrol.\n  In this paper, we describe our vision and an initial design for SPEAR, a\nlanguage and runtime that fills this prompt management gap by making prompts\nstructured, adaptive, and first-class components of the execution model. SPEAR\nenables (1) runtime prompt refinement -- modifying prompts dynamically in\nresponse to execution-time signals such as confidence, latency, or missing\ncontext; and (2) structured prompt management -- organizing prompt fragments\ninto versioned views with support for introspection and logging.\n  SPEAR defines a prompt algebra that governs how prompts are constructed and\nadapted within a pipeline. It supports multiple refinement modes (manual,\nassisted, and automatic), giving developers a balance between control and\nautomation. By treating prompt logic as structured data, SPEAR enables\noptimizations such as operator fusion, prefix caching, and view reuse.\nPreliminary experiments quantify the behavior of different refinement modes\ncompared to static prompts and agentic retries, as well as the impact of\nprompt-level optimizations such as operator fusion."
                },
                "authors": [
                    {
                        "name": "Ugur Cetintemel"
                    },
                    {
                        "name": "Shu Chen"
                    },
                    {
                        "name": "Alexander W. Lee"
                    },
                    {
                        "name": "Deepti Raghavan"
                    }
                ],
                "author_detail": {
                    "name": "Deepti Raghavan"
                },
                "author": "Deepti Raghavan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04449v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04449v2",
                "updated": "2025-08-06T16:57:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    57,
                    39,
                    2,
                    218,
                    0
                ],
                "published": "2024-12-05T18:58:03Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "title": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay"
                },
                "summary": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. In this paper, we propose p-MoD, an efficient MLLM\narchitecture that significantly reduces training and inference costs while\nmaintaining model performance. The majority of computation in MLLMs stems from\nthe overwhelming volume of vision tokens processed by the transformer-based\nLLM. Accordingly, we leverage the Mixture-of-Depths (MoD) mechanism, where each\nLLM layer selects essential vision tokens to process while skipping redundant\nones. However, integrating MoD into MLLMs is non-trivial. To address the\nchallenges of training and inference stability as well as limited training\ndata, we adapt the MoD module with two novel designs: tanh-gated weight\nnormalization (TanhNorm) and symmetric token reweighting (STRing). Moreover, we\nobserve that vision tokens exhibit higher redundancy in deeper layers and thus\ndesign a progressive ratio decay (PRD) strategy, which gradually reduces the\ntoken retention ratio layer by layer, employing a shifted cosine schedule. This\ncrucial design fully unleashes the potential of MoD, significantly boosting the\nefficiency and performance of our models. Extensive experiments on two baseline\nmodels across 15 benchmarks show that our model matches or even surpasses the\nperformance of corresponding baselines, while requiring only 55.6% TFLOPs and\n53.7% KV cache storage during inference, and 77.7% GPU hours during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. In this paper, we propose p-MoD, an efficient MLLM\narchitecture that significantly reduces training and inference costs while\nmaintaining model performance. The majority of computation in MLLMs stems from\nthe overwhelming volume of vision tokens processed by the transformer-based\nLLM. Accordingly, we leverage the Mixture-of-Depths (MoD) mechanism, where each\nLLM layer selects essential vision tokens to process while skipping redundant\nones. However, integrating MoD into MLLMs is non-trivial. To address the\nchallenges of training and inference stability as well as limited training\ndata, we adapt the MoD module with two novel designs: tanh-gated weight\nnormalization (TanhNorm) and symmetric token reweighting (STRing). Moreover, we\nobserve that vision tokens exhibit higher redundancy in deeper layers and thus\ndesign a progressive ratio decay (PRD) strategy, which gradually reduces the\ntoken retention ratio layer by layer, employing a shifted cosine schedule. This\ncrucial design fully unleashes the potential of MoD, significantly boosting the\nefficiency and performance of our models. Extensive experiments on two baseline\nmodels across 15 benchmarks show that our model matches or even surpasses the\nperformance of corresponding baselines, while requiring only 55.6% TFLOPs and\n53.7% KV cache storage during inference, and 77.7% GPU hours during training."
                },
                "authors": [
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Desen Meng"
                    },
                    {
                        "name": "Zhengming Zhang"
                    },
                    {
                        "name": "Zhenpeng Huang"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "arxiv_comment": "Accepted by ICCV 2025; Code released at\n  https://github.com/MCG-NJU/p-MoD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04449v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04449v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04581v1",
                "updated": "2025-08-06T16:06:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    6,
                    43,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T16:06:43Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    6,
                    43,
                    2,
                    218,
                    0
                ],
                "title": "Share Your Attention: Transformer Weight Sharing via Matrix-based\n  Dictionary Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Share Your Attention: Transformer Weight Sharing via Matrix-based\n  Dictionary Learning"
                },
                "summary": "Large language models (LLMs) have revolutionized AI applications, yet their\nhigh computational and memory demands hinder their widespread deployment.\nExisting compression techniques focus on intra-block optimizations (e.g.\nlow-rank approximation, attention head pruning), while the repetitive layered\nstructure of transformers implies significant inter-block redundancy - a\ndimension largely unexplored beyond key-value (KV) caching. Inspired by\ndictionary learning in CNNs, we propose a framework for structured weight\nsharing across transformer layers. Our approach decomposes attention projection\nmatrices into shared dictionary atoms, reducing the attention module's\nparameters by 66.7% while achieving on-par performance. Unlike complex methods\nrequiring distillation or architectural changes, MASA (Matrix Atom Sharing in\nAttention) operates as a drop-in replacement - trained with standard optimizers\n- and represents each layer's weights as linear combinations of shared matrix\natoms. Experiments across scales (100M-700M parameters) show that MASA achieves\nbetter benchmark accuracy and perplexity than grouped-query attention (GQA),\nlow-rank baselines and recently proposed Repeat-all-over/Sequential sharing at\ncomparable parameter budgets. Ablation studies confirm robustness to the\ndictionary size and the efficacy of shared representations in capturing\ncross-layer statistical regularities. Extending to Vision Transformers (ViT),\nMASA matches performance metrics on image classification and detection tasks\nwith 66.7% fewer attention parameters. By combining dictionary learning\nstrategies with transformer efficiency, MASA offers a scalable blueprint for\nparameter-efficient models without sacrificing performance. Finally, we\ninvestigate the possibility of employing MASA on pretrained LLMs to reduce\ntheir number of parameters without experiencing any significant drop in their\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized AI applications, yet their\nhigh computational and memory demands hinder their widespread deployment.\nExisting compression techniques focus on intra-block optimizations (e.g.\nlow-rank approximation, attention head pruning), while the repetitive layered\nstructure of transformers implies significant inter-block redundancy - a\ndimension largely unexplored beyond key-value (KV) caching. Inspired by\ndictionary learning in CNNs, we propose a framework for structured weight\nsharing across transformer layers. Our approach decomposes attention projection\nmatrices into shared dictionary atoms, reducing the attention module's\nparameters by 66.7% while achieving on-par performance. Unlike complex methods\nrequiring distillation or architectural changes, MASA (Matrix Atom Sharing in\nAttention) operates as a drop-in replacement - trained with standard optimizers\n- and represents each layer's weights as linear combinations of shared matrix\natoms. Experiments across scales (100M-700M parameters) show that MASA achieves\nbetter benchmark accuracy and perplexity than grouped-query attention (GQA),\nlow-rank baselines and recently proposed Repeat-all-over/Sequential sharing at\ncomparable parameter budgets. Ablation studies confirm robustness to the\ndictionary size and the efficacy of shared representations in capturing\ncross-layer statistical regularities. Extending to Vision Transformers (ViT),\nMASA matches performance metrics on image classification and detection tasks\nwith 66.7% fewer attention parameters. By combining dictionary learning\nstrategies with transformer efficiency, MASA offers a scalable blueprint for\nparameter-efficient models without sacrificing performance. Finally, we\ninvestigate the possibility of employing MASA on pretrained LLMs to reduce\ntheir number of parameters without experiencing any significant drop in their\nperformance."
                },
                "authors": [
                    {
                        "name": "Magauiya Zhussip"
                    },
                    {
                        "name": "Dmitriy Shopkhoev"
                    },
                    {
                        "name": "Ammar Ali"
                    },
                    {
                        "name": "Stamatios Lefkimmiatis"
                    }
                ],
                "author_detail": {
                    "name": "Stamatios Lefkimmiatis"
                },
                "author": "Stamatios Lefkimmiatis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.01516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.01516v3",
                "updated": "2025-08-06T15:38:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    38,
                    6,
                    2,
                    218,
                    0
                ],
                "published": "2023-05-02T15:27:16Z",
                "published_parsed": [
                    2023,
                    5,
                    2,
                    15,
                    27,
                    16,
                    1,
                    122,
                    0
                ],
                "title": "From FASTER to F2: Evolving Concurrent Key-Value Store Designs for Large\n  Skewed Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From FASTER to F2: Evolving Concurrent Key-Value Store Designs for Large\n  Skewed Workloads"
                },
                "summary": "Modern large-scale services such as search engines, messaging platforms, and\nserverless functions, rely on key-value (KV) stores to maintain high\nperformance at scale. When such services are deployed in constrained memory\nenvironments, they present challenging requirements: point operations requiring\nhigh throughput, working sets much larger than main memory, and natural skew in\nkey access patterns. Traditional KV stores, based on LSM- and B-Trees, have\nbeen widely used to handle such use cases, but they often suffer from\nsuboptimal use of modern hardware resources. The FASTER project, developed as a\nhigh-performance open-source KV storage library, has demonstrated remarkable\nsuccess in both in-memory and hybrid storage environments. However, when tasked\nwith serving large skewed workloads, it faced challenges, including high\nindexing and compactions overheads, and inefficient management of\nnon-overlapping read-hot and write-hot working sets.\n  In this paper, we introduce F2 (for FASTER v2), an evolution of FASTER\ndesigned to meet the requirements of large skewed workloads common in industry\napplications. F2 adopts a two-tier record-oriented design to handle\nlarger-than-memory skewed workloads, along with new concurrent latch-free\nmechanisms and components to maximize performance on modern hardware. To\nrealize this design, F2 tackles key challenges and introduces several\ninnovations, including new latch-free algorithms for multi-threaded log\ncompaction, a two-level hash index to reduce indexing overhead for cold\nrecords, and a read-cache for serving read-hot records. Our evaluation shows\nthat F2 achieves 2-11.9x better throughput compared to existing KV stores,\neffectively serving the target workload. F2 is open-source and available as\npart of the FASTER project.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large-scale services such as search engines, messaging platforms, and\nserverless functions, rely on key-value (KV) stores to maintain high\nperformance at scale. When such services are deployed in constrained memory\nenvironments, they present challenging requirements: point operations requiring\nhigh throughput, working sets much larger than main memory, and natural skew in\nkey access patterns. Traditional KV stores, based on LSM- and B-Trees, have\nbeen widely used to handle such use cases, but they often suffer from\nsuboptimal use of modern hardware resources. The FASTER project, developed as a\nhigh-performance open-source KV storage library, has demonstrated remarkable\nsuccess in both in-memory and hybrid storage environments. However, when tasked\nwith serving large skewed workloads, it faced challenges, including high\nindexing and compactions overheads, and inefficient management of\nnon-overlapping read-hot and write-hot working sets.\n  In this paper, we introduce F2 (for FASTER v2), an evolution of FASTER\ndesigned to meet the requirements of large skewed workloads common in industry\napplications. F2 adopts a two-tier record-oriented design to handle\nlarger-than-memory skewed workloads, along with new concurrent latch-free\nmechanisms and components to maximize performance on modern hardware. To\nrealize this design, F2 tackles key challenges and introduces several\ninnovations, including new latch-free algorithms for multi-threaded log\ncompaction, a two-level hash index to reduce indexing overhead for cold\nrecords, and a read-cache for serving read-hot records. Our evaluation shows\nthat F2 achieves 2-11.9x better throughput compared to existing KV stores,\neffectively serving the target workload. F2 is open-source and available as\npart of the FASTER project."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kanellis"
                    },
                    {
                        "name": "Badrish Chandramouli"
                    },
                    {
                        "name": "Ted Hart"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "arxiv_comment": "Proceedings of the VLDB Endowment 18 (VLDB'25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.01516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.01516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04462v1",
                "updated": "2025-08-06T14:02:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    2,
                    10,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T14:02:10Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    2,
                    10,
                    2,
                    218,
                    0
                ],
                "title": "CARD: Cache-Assisted Parallel Speculative Decoding for Efficient Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CARD: Cache-Assisted Parallel Speculative Decoding for Efficient Large\n  Language Model Inference"
                },
                "summary": "Speculative decoding (SD), where an extra draft model first provides multiple\ndraft tokens and the original target model then verifies these tokens in\nparallel, has shown great power for LLM inference acceleration. However,\nexisting SD methods must adhere to the 'draft-then-verify' paradigm, which\nforces drafting and verification processes to execute sequentially during SD,\nresulting in inefficient inference performance and limiting the size of the\ndraft model. Furthermore, once a single token in the candidate sequence is\nrejected during the drafting process, all subsequent candidate tokens must be\ndiscarded, leading to inefficient drafting. To address these challenges, we\npropose a cache-based parallel speculative decoding framework employing a\n'query-and-correct' paradigm. Specifically, CARD decouples drafting and\nverification: the draft model generates candidate tokens to populate a shared\ncache, while the target model concurrently rectifies the draft model's\ngeneration direction. This effectively enables the target model to perform\ninference at speed approaching that of the draft model. Our approach achieves\nup to 4.83 speedup over vanilla decoding without requiring fine-tuning of\neither the draft or target models. Our code is available at\nhttps://github.com/hunzhizi/CARD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD), where an extra draft model first provides multiple\ndraft tokens and the original target model then verifies these tokens in\nparallel, has shown great power for LLM inference acceleration. However,\nexisting SD methods must adhere to the 'draft-then-verify' paradigm, which\nforces drafting and verification processes to execute sequentially during SD,\nresulting in inefficient inference performance and limiting the size of the\ndraft model. Furthermore, once a single token in the candidate sequence is\nrejected during the drafting process, all subsequent candidate tokens must be\ndiscarded, leading to inefficient drafting. To address these challenges, we\npropose a cache-based parallel speculative decoding framework employing a\n'query-and-correct' paradigm. Specifically, CARD decouples drafting and\nverification: the draft model generates candidate tokens to populate a shared\ncache, while the target model concurrently rectifies the draft model's\ngeneration direction. This effectively enables the target model to perform\ninference at speed approaching that of the draft model. Our approach achieves\nup to 4.83 speedup over vanilla decoding without requiring fine-tuning of\neither the draft or target models. Our code is available at\nhttps://github.com/hunzhizi/CARD."
                },
                "authors": [
                    {
                        "name": "Enyu Zhou"
                    },
                    {
                        "name": "Kai Sheng"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Xin He"
                    }
                ],
                "author_detail": {
                    "name": "Xin He"
                },
                "author": "Xin He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24007v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24007v3",
                "updated": "2025-08-06T11:46:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    46,
                    11,
                    2,
                    218,
                    0
                ],
                "published": "2025-03-31T12:32:23Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "title": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting"
                },
                "summary": "In practical time series forecasting, covariates provide rich contextual\ninformation that can potentially enhance the forecast of target variables.\nAlthough some covariates extend into the future forecasting horizon (e.g.,\ncalendar events, discount schedules), most multivariate models fail to leverage\nthis pivotal insight due to the length discrepancy with target variables.\nAdditionally, capturing the dependency between target variables and covariates\nis non-trivial, as models must precisely reflect the local impact of covariates\nwhile also capturing global cross-variate dependencies. To overcome these\nchallenges, we propose CITRAS, a decoder-only Transformer that flexibly\nleverages multiple targets, past covariates, and future covariates. While\npreserving strong autoregressive capabilities, CITRAS introduces two novel\nmechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift and\nAttention Score Smoothing. KV Shift seamlessly incorporates future covariates\ninto the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing refines locally accurate\npatch-wise cross-variate dependencies into global variate-level dependencies by\nsmoothing the past series of attention scores. Experimentally, CITRAS\noutperforms state-of-the-art models on thirteen real-world benchmarks from both\ncovariate-informed and multivariate settings, demonstrating its versatile\nability to leverage cross-variate and cross-time dependencies for improved\nforecasting accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In practical time series forecasting, covariates provide rich contextual\ninformation that can potentially enhance the forecast of target variables.\nAlthough some covariates extend into the future forecasting horizon (e.g.,\ncalendar events, discount schedules), most multivariate models fail to leverage\nthis pivotal insight due to the length discrepancy with target variables.\nAdditionally, capturing the dependency between target variables and covariates\nis non-trivial, as models must precisely reflect the local impact of covariates\nwhile also capturing global cross-variate dependencies. To overcome these\nchallenges, we propose CITRAS, a decoder-only Transformer that flexibly\nleverages multiple targets, past covariates, and future covariates. While\npreserving strong autoregressive capabilities, CITRAS introduces two novel\nmechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift and\nAttention Score Smoothing. KV Shift seamlessly incorporates future covariates\ninto the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing refines locally accurate\npatch-wise cross-variate dependencies into global variate-level dependencies by\nsmoothing the past series of attention scores. Experimentally, CITRAS\noutperforms state-of-the-art models on thirteen real-world benchmarks from both\ncovariate-informed and multivariate settings, demonstrating its versatile\nability to leverage cross-variate and cross-time dependencies for improved\nforecasting accuracy."
                },
                "authors": [
                    {
                        "name": "Yosuke Yamaguchi"
                    },
                    {
                        "name": "Issei Suemitsu"
                    },
                    {
                        "name": "Wenpeng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Wenpeng Wei"
                },
                "author": "Wenpeng Wei",
                "arxiv_comment": "Submission under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24007v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24007v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04257v1",
                "updated": "2025-08-06T09:40:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    9,
                    40,
                    9,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T09:40:09Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    9,
                    40,
                    9,
                    2,
                    218,
                    0
                ],
                "title": "KVSink: Understanding and Enhancing the Preservation of Attention Sinks\n  in KV Cache Quantization for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVSink: Understanding and Enhancing the Preservation of Attention Sinks\n  in KV Cache Quantization for LLMs"
                },
                "summary": "Key-Value (KV) cache quantization has become a widely adopted optimization\ntechnique for efficient large language models (LLMs) inference by reducing KV\ncache memory usage and mitigating memory-bound constraints. Recent studies have\nemphasized the importance of preserving the original precision of KVs for the\nfirst few tokens to ensure the protection of attention sinks. While this\napproach has proven effective in mitigating performance degradation, its\nunderlying principles remain insufficiently understood. Moreover, it fails to\naddress the recent discovery that attention sinks can emerge beyond the initial\ntoken positions. In this work, we elucidate the underlying mechanisms of\nattention sinks during inference by examining their role in the cross-layer\nevolution of extreme activation outliers. Additionally, we provide a\ncomprehensive analysis of the interplay between attention sinks and KV cache\nquantization. Based on our enhanced understanding, we introduce\n\\textit{\\textbf{KVSink}}, a plug-and-play method that effectively predicts sink\ntokens with negligible overhead, enabling more thorough preservation. Extensive\nexperiments demonstrate that KVSink outperforms the existing Preserve-First-N\n(PFN) strategy, offering more effective preservation of attention sinks during\nKV cache quantization. Moreover, when applied to the well-established KVQuant\nmethod, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit\nnumerical outliers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache quantization has become a widely adopted optimization\ntechnique for efficient large language models (LLMs) inference by reducing KV\ncache memory usage and mitigating memory-bound constraints. Recent studies have\nemphasized the importance of preserving the original precision of KVs for the\nfirst few tokens to ensure the protection of attention sinks. While this\napproach has proven effective in mitigating performance degradation, its\nunderlying principles remain insufficiently understood. Moreover, it fails to\naddress the recent discovery that attention sinks can emerge beyond the initial\ntoken positions. In this work, we elucidate the underlying mechanisms of\nattention sinks during inference by examining their role in the cross-layer\nevolution of extreme activation outliers. Additionally, we provide a\ncomprehensive analysis of the interplay between attention sinks and KV cache\nquantization. Based on our enhanced understanding, we introduce\n\\textit{\\textbf{KVSink}}, a plug-and-play method that effectively predicts sink\ntokens with negligible overhead, enabling more thorough preservation. Extensive\nexperiments demonstrate that KVSink outperforms the existing Preserve-First-N\n(PFN) strategy, offering more effective preservation of attention sinks during\nKV cache quantization. Moreover, when applied to the well-established KVQuant\nmethod, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit\nnumerical outliers."
                },
                "authors": [
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Kehong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kehong Yuan"
                },
                "author": "Kehong Yuan",
                "arxiv_comment": "Published as a conference paper at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00370v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00370v2",
                "updated": "2025-08-06T08:32:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    8,
                    32,
                    53,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-01T07:03:16Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    7,
                    3,
                    16,
                    4,
                    213,
                    0
                ],
                "title": "EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level\n  Efficiency for Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level\n  Efficiency for Edge Devices"
                },
                "summary": "Deploying Transformer-based large language models (LLMs) on\nresource-constrained edge devices for long-sequence tasks remains challenging\ndue to the quadratic time complexity of self-attention and growing Key-Value\n(KV) cache demands. While existing KV cache optimizations improve memory\nefficiency, they often fail to reduce time to first token (TTFT) and may\ndegrade performance through token pruning. Alternative sequence modeling\narchitectures address some of these limitations, but typically require full\nretraining and lack infrastructure support. EdgeInfinite offers an efficient\nsolution by fine-tuning only a small subset of parameters, maintaining quality\nwhile reducing both computational and memory costs, including improved TTFT.\nHowever, its instruction-following ability is limited, and it lacks\nmobile-specific optimizations. To address these issues, we propose\nEdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning\n(S-SFT) strategy tailored to long-sequence tasks such as summarization and\nquestion answering. We further optimized EdgeInfinite-Instruct for efficient\ndeployment on edge NPUs by employing fine-grained post-training quantization\n(PTQ) to reduce computational demands while maintaining accuracy, and by\nimplementing a fixed-shape computation graph that balances memory usage and\non-device efficiency through scenario-specific customization of input token and\ncache sizes. Experiments on long-context benchmarks and real-world mobile tasks\nshow that our approach improves domain-specific performance while maintaining\nefficiency on NPU-accelerated edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Transformer-based large language models (LLMs) on\nresource-constrained edge devices for long-sequence tasks remains challenging\ndue to the quadratic time complexity of self-attention and growing Key-Value\n(KV) cache demands. While existing KV cache optimizations improve memory\nefficiency, they often fail to reduce time to first token (TTFT) and may\ndegrade performance through token pruning. Alternative sequence modeling\narchitectures address some of these limitations, but typically require full\nretraining and lack infrastructure support. EdgeInfinite offers an efficient\nsolution by fine-tuning only a small subset of parameters, maintaining quality\nwhile reducing both computational and memory costs, including improved TTFT.\nHowever, its instruction-following ability is limited, and it lacks\nmobile-specific optimizations. To address these issues, we propose\nEdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning\n(S-SFT) strategy tailored to long-sequence tasks such as summarization and\nquestion answering. We further optimized EdgeInfinite-Instruct for efficient\ndeployment on edge NPUs by employing fine-grained post-training quantization\n(PTQ) to reduce computational demands while maintaining accuracy, and by\nimplementing a fixed-shape computation graph that balances memory usage and\non-device efficiency through scenario-specific customization of input token and\ncache sizes. Experiments on long-context benchmarks and real-world mobile tasks\nshow that our approach improves domain-specific performance while maintaining\nefficiency on NPU-accelerated edge devices."
                },
                "authors": [
                    {
                        "name": "Jiyu Chen"
                    },
                    {
                        "name": "Poh Seng Lim"
                    },
                    {
                        "name": "Shuang Peng"
                    },
                    {
                        "name": "Daxiong Luo"
                    },
                    {
                        "name": "JungHau Foo"
                    },
                    {
                        "name": "Yap Deep"
                    },
                    {
                        "name": "Timothy Lee Jun Jie"
                    },
                    {
                        "name": "Kelvin Teh Kae Wen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Danyu Feng"
                    },
                    {
                        "name": "Hao-Yun Chen"
                    },
                    {
                        "name": "Peng-Wen Chen"
                    },
                    {
                        "name": "Fangyuan Li"
                    },
                    {
                        "name": "Xiaoxin Chen"
                    },
                    {
                        "name": "Wong Wai Mun"
                    }
                ],
                "author_detail": {
                    "name": "Wong Wai Mun"
                },
                "author": "Wong Wai Mun",
                "arxiv_comment": "The data and method in the paper need to be re-audited",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00370v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00370v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03974v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03974v2",
                "updated": "2025-08-09T02:33:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    9,
                    2,
                    33,
                    21,
                    5,
                    221,
                    0
                ],
                "published": "2025-08-05T23:47:34Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    23,
                    47,
                    34,
                    1,
                    217,
                    0
                ],
                "title": "Managing Data for Scalable and Interactive Event Sequence Visualization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Managing Data for Scalable and Interactive Event Sequence Visualization"
                },
                "summary": "Parallel event sequences, such as those collected in program execution traces\nand automated manufacturing pipelines, are typically visualized as interactive\nparallel timelines. As the dataset size grows, these charts frequently\nexperience lag during common interactions such as zooming, panning, and\nfiltering. Summarization approaches can improve interaction performance, but at\nthe cost of accuracy in representation. To address this challenge, we introduce\nESeMan (Event Sequence Manager), an event sequence management system designed\nto support interactive rendering of timeline visualizations with tunable\naccuracy. ESeMan employs hierarchical data structures and intelligent caching\nto provide visualizations with only the data necessary to generate accurate\nsummarizations with significantly reduced data fetch time. We evaluate ESeMan's\nquery times against summed area tables, M4 aggregation, and statistical\nsub-sampling on a variety of program execution traces. Our results demonstrate\nESeMan provides better performance, achieving sub-100ms fetch times while\nmaintaining visualization accuracy at the pixel level. We further present our\nbenchmarking harness, enabling future performance evaluations for event\nsequence visualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel event sequences, such as those collected in program execution traces\nand automated manufacturing pipelines, are typically visualized as interactive\nparallel timelines. As the dataset size grows, these charts frequently\nexperience lag during common interactions such as zooming, panning, and\nfiltering. Summarization approaches can improve interaction performance, but at\nthe cost of accuracy in representation. To address this challenge, we introduce\nESeMan (Event Sequence Manager), an event sequence management system designed\nto support interactive rendering of timeline visualizations with tunable\naccuracy. ESeMan employs hierarchical data structures and intelligent caching\nto provide visualizations with only the data necessary to generate accurate\nsummarizations with significantly reduced data fetch time. We evaluate ESeMan's\nquery times against summed area tables, M4 aggregation, and statistical\nsub-sampling on a variety of program execution traces. Our results demonstrate\nESeMan provides better performance, achieving sub-100ms fetch times while\nmaintaining visualization accuracy at the pixel level. We further present our\nbenchmarking harness, enabling future performance evaluations for event\nsequence visualization."
                },
                "authors": [
                    {
                        "name": "Sayef Azad Sakin"
                    },
                    {
                        "name": "Katherine E. Isaacs"
                    }
                ],
                "author_detail": {
                    "name": "Katherine E. Isaacs"
                },
                "author": "Katherine E. Isaacs",
                "arxiv_comment": "The 15th IEEE Workshop on Large Data Analysis and Visualization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03974v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03974v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03837v1",
                "updated": "2025-08-05T18:34:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    18,
                    34,
                    48,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-05T18:34:48Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    18,
                    34,
                    48,
                    1,
                    217,
                    0
                ],
                "title": "Rhea: a Framework for Fast Design and Validation of RTL Cache-Coherent\n  Memory Subsystems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rhea: a Framework for Fast Design and Validation of RTL Cache-Coherent\n  Memory Subsystems"
                },
                "summary": "Designing and validating efficient cache-coherent memory subsystems is a\ncritical yet complex task in the development of modern multi-core\nsystem-on-chip architectures. Rhea is a unified framework that streamlines the\ndesign and system-level validation of RTL cache-coherent memory subsystems. On\nthe design side, Rhea generates synthesizable, highly configurable RTL\nsupporting various architectural parameters. On the validation side, Rhea\nintegrates Verilator's cycle-accurate RTL simulation with gem5's full-system\nsimulation, allowing realistic workloads and operating systems to run alongside\nthe actual RTL under test. We apply Rhea to design MSI-based RTL memory\nsubsystems with one and two levels of private caches and scaling up to sixteen\ncores. Their evaluation with 22 applications from state-of-the-art benchmark\nsuites shows intermediate performance relative to gem5 Ruby's MI and MOESI\nmodels. The hybrid gem5-Verilator co-simulation flow incurs a moderate\nsimulation overhead, up to 2.7 times compared to gem5 MI, but achieves higher\nfidelity by simulating real RTL hardware. This overhead decreases with scale,\ndown to 1.6 times in sixteen-core scenarios. These results demonstrate Rhea's\neffectiveness and scalability in enabling fast development of RTL\ncache-coherent memory subsystem designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing and validating efficient cache-coherent memory subsystems is a\ncritical yet complex task in the development of modern multi-core\nsystem-on-chip architectures. Rhea is a unified framework that streamlines the\ndesign and system-level validation of RTL cache-coherent memory subsystems. On\nthe design side, Rhea generates synthesizable, highly configurable RTL\nsupporting various architectural parameters. On the validation side, Rhea\nintegrates Verilator's cycle-accurate RTL simulation with gem5's full-system\nsimulation, allowing realistic workloads and operating systems to run alongside\nthe actual RTL under test. We apply Rhea to design MSI-based RTL memory\nsubsystems with one and two levels of private caches and scaling up to sixteen\ncores. Their evaluation with 22 applications from state-of-the-art benchmark\nsuites shows intermediate performance relative to gem5 Ruby's MI and MOESI\nmodels. The hybrid gem5-Verilator co-simulation flow incurs a moderate\nsimulation overhead, up to 2.7 times compared to gem5 MI, but achieves higher\nfidelity by simulating real RTL hardware. This overhead decreases with scale,\ndown to 1.6 times in sixteen-core scenarios. These results demonstrate Rhea's\neffectiveness and scalability in enabling fast development of RTL\ncache-coherent memory subsystem designs."
                },
                "authors": [
                    {
                        "name": "Davide Zoni"
                    },
                    {
                        "name": "Andrea Galimberti"
                    },
                    {
                        "name": "Adriano Guarisco"
                    }
                ],
                "author_detail": {
                    "name": "Adriano Guarisco"
                },
                "author": "Adriano Guarisco",
                "arxiv_comment": "9 pages, 13 figures, 1 table, accepted for presentation at 2025\n  International Conference on Computer-Aided Design (ICCAD), Munich, Germany,\n  October 26-30, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07120v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07120v2",
                "updated": "2025-08-05T16:17:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    16,
                    17,
                    1,
                    1,
                    217,
                    0
                ],
                "published": "2025-03-10T09:49:18Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    49,
                    18,
                    0,
                    69,
                    0
                ],
                "title": "FEB-Cache: Frequency-Guided Exposure Bias Reduction for Enhancing\n  Diffusion Transformer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FEB-Cache: Frequency-Guided Exposure Bias Reduction for Enhancing\n  Diffusion Transformer Caching"
                },
                "summary": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis issue, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing why caching damage the generation processes. In this paper, we first\nconfirm that the cache greatly amplifies the exposure bias, resulting in a\ndecline in the generation quality. However, directly applying noise scaling is\nchallenging for this issue due to the non-smoothness of exposure bias. We found\nthat this phenomenon stems from the mismatch between its frequency response\ncharacteristics and the simple cache of Attention and MLP. Since these two\ncomponents exhibit unique preferences for frequency signals, which provides us\nwith a caching strategy to separate Attention and MLP to achieve an enhanced\nfit of exposure bias and reduce it. Based on this, we introduced FEB-Cache, a\njoint caching strategy that aligns with the non-exposed bias diffusion process\n(which gives us a higher performance cap) of caching Attention and MLP based on\nthe frequency-guided cache table. Our approach combines a comprehensive\nunderstanding of the caching mechanism and offers a new perspective on\nleveraging caching to accelerate the diffusion process. Empirical results\nindicate that FEB-Cache optimizes model performance while concurrently\nfacilitating acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis issue, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing why caching damage the generation processes. In this paper, we first\nconfirm that the cache greatly amplifies the exposure bias, resulting in a\ndecline in the generation quality. However, directly applying noise scaling is\nchallenging for this issue due to the non-smoothness of exposure bias. We found\nthat this phenomenon stems from the mismatch between its frequency response\ncharacteristics and the simple cache of Attention and MLP. Since these two\ncomponents exhibit unique preferences for frequency signals, which provides us\nwith a caching strategy to separate Attention and MLP to achieve an enhanced\nfit of exposure bias and reduce it. Based on this, we introduced FEB-Cache, a\njoint caching strategy that aligns with the non-exposed bias diffusion process\n(which gives us a higher performance cap) of caching Attention and MLP based on\nthe frequency-guided cache table. Our approach combines a comprehensive\nunderstanding of the caching mechanism and offers a new perspective on\nleveraging caching to accelerate the diffusion process. Empirical results\nindicate that FEB-Cache optimizes model performance while concurrently\nfacilitating acceleration."
                },
                "authors": [
                    {
                        "name": "Zhen Zou"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07120v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07120v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03321v1",
                "updated": "2025-08-05T11:00:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    11,
                    0,
                    41,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-05T11:00:41Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    11,
                    0,
                    41,
                    1,
                    217,
                    0
                ],
                "title": "Bidirectional TLS Handshake Caching for Constrained Industrial IoT\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bidirectional TLS Handshake Caching for Constrained Industrial IoT\n  Scenarios"
                },
                "summary": "While TLS has become the de-facto standard for end-to-end security, its use\nto secure critical communication in evolving industrial IoT scenarios is\nseverely limited by prevalent resource constraints of devices and networks.\nMost notably, the TLS handshake to establish secure connections incurs\nsignificant bandwidth and processing overhead that often cannot be handled in\nconstrained environments. To alleviate this situation, we present BiTHaC which\nrealizes bidirectional TLS handshake caching by exploiting that significant\nparts of repeated TLS handshakes, especially certificates, are static. Thus,\nredundant information neither needs to be transmitted nor corresponding\ncomputations performed, saving valuable bandwidth and processing resources. By\nimplementing BiTHaC for wolfSSL, we show that we can reduce the bandwidth\nconsumption of TLS handshakes by up to 61.1% and the computational overhead by\nup to 8.5%, while incurring only well-manageable memory overhead and preserving\nthe strict security guarantees of TLS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While TLS has become the de-facto standard for end-to-end security, its use\nto secure critical communication in evolving industrial IoT scenarios is\nseverely limited by prevalent resource constraints of devices and networks.\nMost notably, the TLS handshake to establish secure connections incurs\nsignificant bandwidth and processing overhead that often cannot be handled in\nconstrained environments. To alleviate this situation, we present BiTHaC which\nrealizes bidirectional TLS handshake caching by exploiting that significant\nparts of repeated TLS handshakes, especially certificates, are static. Thus,\nredundant information neither needs to be transmitted nor corresponding\ncomputations performed, saving valuable bandwidth and processing resources. By\nimplementing BiTHaC for wolfSSL, we show that we can reduce the bandwidth\nconsumption of TLS handshakes by up to 61.1% and the computational overhead by\nup to 8.5%, while incurring only well-manageable memory overhead and preserving\nthe strict security guarantees of TLS."
                },
                "authors": [
                    {
                        "name": "Jörn Bodenhausen"
                    },
                    {
                        "name": "Simon Mangel"
                    },
                    {
                        "name": "Thomas Vogt"
                    },
                    {
                        "name": "Martin Henze"
                    }
                ],
                "author_detail": {
                    "name": "Martin Henze"
                },
                "author": "Martin Henze",
                "arxiv_comment": "Accepted for publication in Proceedings of the 2025 IEEE 50th\n  Conference on Local Computer Networks (LCN)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03258v1",
                "updated": "2025-08-05T09:35:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    9,
                    35,
                    52,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-05T09:35:52Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    9,
                    35,
                    52,
                    1,
                    217,
                    0
                ],
                "title": "SmartLLMs Scheduler: A Framework for Cost-Effective LLMs Utilization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmartLLMs Scheduler: A Framework for Cost-Effective LLMs Utilization"
                },
                "summary": "Large Language Models (LLMs) such as GPT-4 and Llama have shown remarkable\ncapabilities in a variety of software engineering tasks. Despite the\nadvancements, their practical deployment faces challenges, including high\nfinancial costs, long response time, and varying performance, especially when\nhandling a large number of queries (jobs). Existing optimization strategies for\ndeploying LLMs for diverse tasks focus on static scheduling, which requires\nextensive training data for performance prediction, increasing the\ncomputational costs and limiting the applicability and flexibility. In this\npaper, we propose the SmartLLMs Scheduler (SLS), a dynamic and cost-effective\nscheduling solution. The key idea is to learn LLMs' performance on diverse\ntasks and incorporate their real-time feedback to update strategies\nperiodically. Specifically, SLS incorporates three key components, including an\nAdaptive Cache Manager, a Performance-Cost Optimized Scheduler, and a Dynamic\nUpdate Manager. The Cache Manager stores the outputs of previously processed\nqueries and employs an adaptive strategy to reduce redundant computations and\nminimize response times. For queries not found in the cache, the Scheduler\ndynamically allocates them to the most suitable LLM based on the predicted\nperformance and cost from models that take both query-specific and LLM-specific\nfeatures as input. The Update Manager continuously refines the cache and\nscheduling strategies based on real-time feedback from the assigned queries to\nenhance decision-making and adapt to evolving task characteristics. To evaluate\nthe effectiveness of SLS, we conduct extensive experiments on two LLM-based\nsoftware engineering tasks, including log parsing and code generation. The\nresults show that SLS significantly outperforms the baseline methods, achieving\nan average performance improvement of 198.82% and an average processing time\nreduction of 63.28%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) such as GPT-4 and Llama have shown remarkable\ncapabilities in a variety of software engineering tasks. Despite the\nadvancements, their practical deployment faces challenges, including high\nfinancial costs, long response time, and varying performance, especially when\nhandling a large number of queries (jobs). Existing optimization strategies for\ndeploying LLMs for diverse tasks focus on static scheduling, which requires\nextensive training data for performance prediction, increasing the\ncomputational costs and limiting the applicability and flexibility. In this\npaper, we propose the SmartLLMs Scheduler (SLS), a dynamic and cost-effective\nscheduling solution. The key idea is to learn LLMs' performance on diverse\ntasks and incorporate their real-time feedback to update strategies\nperiodically. Specifically, SLS incorporates three key components, including an\nAdaptive Cache Manager, a Performance-Cost Optimized Scheduler, and a Dynamic\nUpdate Manager. The Cache Manager stores the outputs of previously processed\nqueries and employs an adaptive strategy to reduce redundant computations and\nminimize response times. For queries not found in the cache, the Scheduler\ndynamically allocates them to the most suitable LLM based on the predicted\nperformance and cost from models that take both query-specific and LLM-specific\nfeatures as input. The Update Manager continuously refines the cache and\nscheduling strategies based on real-time feedback from the assigned queries to\nenhance decision-making and adapt to evolving task characteristics. To evaluate\nthe effectiveness of SLS, we conduct extensive experiments on two LLM-based\nsoftware engineering tasks, including log parsing and code generation. The\nresults show that SLS significantly outperforms the baseline methods, achieving\nan average performance improvement of 198.82% and an average processing time\nreduction of 63.28%."
                },
                "authors": [
                    {
                        "name": "Yueyue Liu"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Yuantian Miao"
                    }
                ],
                "author_detail": {
                    "name": "Yuantian Miao"
                },
                "author": "Yuantian Miao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02240v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02240v2",
                "updated": "2025-08-05T02:13:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    2,
                    13,
                    39,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-04T09:39:31Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    39,
                    31,
                    0,
                    216,
                    0
                ],
                "title": "Forecasting When to Forecast: Accelerating Diffusion Models with\n  Confidence-Gated Taylor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting When to Forecast: Accelerating Diffusion Models with\n  Confidence-Gated Taylor"
                },
                "summary": "Diffusion Transformers (DiTs) have demonstrated remarkable performance in\nvisual generation tasks. However, their low inference speed limits their\ndeployment in low-resource applications. Recent training-free approaches\nexploit the redundancy of features across timesteps by caching and reusing past\nrepresentations to accelerate inference. Building on this idea, TaylorSeer\ninstead uses cached features to predict future ones via Taylor expansion.\nHowever, its module-level prediction across all transformer blocks (e.g.,\nattention or feedforward modules) requires storing fine-grained intermediate\nfeatures, leading to notable memory and computation overhead. Moreover, it\nadopts a fixed caching schedule without considering the varying accuracy of\npredictions across timesteps, which can lead to degraded outputs when\nprediction fails. To address these limitations, we propose a novel approach to\nbetter leverage Taylor-based acceleration. First, we shift the Taylor\nprediction target from the module level to the last block level, significantly\nreducing the number of cached features. Furthermore, observing strong\nsequential dependencies among Transformer blocks, we propose to use the error\nbetween the Taylor-estimated and actual outputs of the first block as an\nindicator of prediction reliability. If the error is small, we trust the Taylor\nprediction for the last block; otherwise, we fall back to full computation,\nthereby enabling a dynamic caching mechanism. Empirical results show that our\nmethod achieves a better balance between speed and quality, achieving a 3.17x\nacceleration on FLUX, 2.36x on DiT, and 4.14x on Wan Video with negligible\nquality drop. The Project Page is\n\\href{https://cg-taylor-acce.github.io/CG-Taylor/}{here.}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have demonstrated remarkable performance in\nvisual generation tasks. However, their low inference speed limits their\ndeployment in low-resource applications. Recent training-free approaches\nexploit the redundancy of features across timesteps by caching and reusing past\nrepresentations to accelerate inference. Building on this idea, TaylorSeer\ninstead uses cached features to predict future ones via Taylor expansion.\nHowever, its module-level prediction across all transformer blocks (e.g.,\nattention or feedforward modules) requires storing fine-grained intermediate\nfeatures, leading to notable memory and computation overhead. Moreover, it\nadopts a fixed caching schedule without considering the varying accuracy of\npredictions across timesteps, which can lead to degraded outputs when\nprediction fails. To address these limitations, we propose a novel approach to\nbetter leverage Taylor-based acceleration. First, we shift the Taylor\nprediction target from the module level to the last block level, significantly\nreducing the number of cached features. Furthermore, observing strong\nsequential dependencies among Transformer blocks, we propose to use the error\nbetween the Taylor-estimated and actual outputs of the first block as an\nindicator of prediction reliability. If the error is small, we trust the Taylor\nprediction for the last block; otherwise, we fall back to full computation,\nthereby enabling a dynamic caching mechanism. Empirical results show that our\nmethod achieves a better balance between speed and quality, achieving a 3.17x\nacceleration on FLUX, 2.36x on DiT, and 4.14x on Wan Video with negligible\nquality drop. The Project Page is\n\\href{https://cg-taylor-acce.github.io/CG-Taylor/}{here.}"
                },
                "authors": [
                    {
                        "name": "Xiaoliu Guan"
                    },
                    {
                        "name": "Lielin Jiang"
                    },
                    {
                        "name": "Hanqi Chen"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Jiaxing Yan"
                    },
                    {
                        "name": "Guanzhong Wang"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Zetao Zhang"
                    },
                    {
                        "name": "Yu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wu"
                },
                "author": "Yu Wu",
                "arxiv_comment": "15 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02240v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02240v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14866v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14866v5",
                "updated": "2025-08-05T00:25:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    0,
                    25,
                    53,
                    1,
                    217,
                    0
                ],
                "published": "2025-04-21T05:27:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    27,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "GainSight: A Unified Framework for Data Lifetime Profiling and\n  Heterogeneous Memory Composition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GainSight: A Unified Framework for Data Lifetime Profiling and\n  Heterogeneous Memory Composition"
                },
                "summary": "As AI workloads drive increasing memory requirements, domain-specific\naccelerators need higher-density on-chip memory beyond what current SRAM\nscaling trends can provide. Simultaneously, the vast amounts of short-lived\ndata in these workloads make SRAM overprovisioned in retention capability. To\naddress this mismatch, we propose a wholesale shift from uniform SRAM arrays to\nheterogeneous on-chip memory, incorporating denser short-term RAM (StRAM)\ndevices whose limited retention times align with transient data lifetimes. To\nfacilitate this shift, we introduce GainSight, the first comprehensive,\nopen-source framework that aligns dynamic, fine-grained workload lifetime\nprofiles with memory device characteristics to enable generation of optimal\nStRAM memory compositions. GainSight combines retargetable profiling backends\nwith an architecture-agnostic analytical frontend. The various backends capture\ncycle-accurate data lifetimes, while the frontend correlates workload patterns\nwith StRAM retention properties to generate optimal memory compositions and\nproject performance. GainSight elevates data lifetime to a first-class design\nconsideration for next-generation AI accelerators, enabling systematic\nexploitation of data transience for improved on-chip memory density and\nefficiency. Applying GainSight to MLPerf Inference and PolyBench workloads\nreveals that 64.3% of first-level GPU cache accesses and 79.01% of systolic\narray scratchpad accesses exhibit sub-microsecond lifetimes suitable for\nhigh-density StRAM, with optimal heterogeneous on-chip memory compositions\nachieving up to 3x active energy and 4x area reductions compared to uniform\nSRAM hierarchies. To facilitate adoption and further research, GainSight is\nopen-sourced at https://gainsight.stanford.edu/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI workloads drive increasing memory requirements, domain-specific\naccelerators need higher-density on-chip memory beyond what current SRAM\nscaling trends can provide. Simultaneously, the vast amounts of short-lived\ndata in these workloads make SRAM overprovisioned in retention capability. To\naddress this mismatch, we propose a wholesale shift from uniform SRAM arrays to\nheterogeneous on-chip memory, incorporating denser short-term RAM (StRAM)\ndevices whose limited retention times align with transient data lifetimes. To\nfacilitate this shift, we introduce GainSight, the first comprehensive,\nopen-source framework that aligns dynamic, fine-grained workload lifetime\nprofiles with memory device characteristics to enable generation of optimal\nStRAM memory compositions. GainSight combines retargetable profiling backends\nwith an architecture-agnostic analytical frontend. The various backends capture\ncycle-accurate data lifetimes, while the frontend correlates workload patterns\nwith StRAM retention properties to generate optimal memory compositions and\nproject performance. GainSight elevates data lifetime to a first-class design\nconsideration for next-generation AI accelerators, enabling systematic\nexploitation of data transience for improved on-chip memory density and\nefficiency. Applying GainSight to MLPerf Inference and PolyBench workloads\nreveals that 64.3% of first-level GPU cache accesses and 79.01% of systolic\narray scratchpad accesses exhibit sub-microsecond lifetimes suitable for\nhigh-density StRAM, with optimal heterogeneous on-chip memory compositions\nachieving up to 3x active energy and 4x area reductions compared to uniform\nSRAM hierarchies. To facilitate adoption and further research, GainSight is\nopen-sourced at https://gainsight.stanford.edu/."
                },
                "authors": [
                    {
                        "name": "Peijing Li"
                    },
                    {
                        "name": "Matthew Hung"
                    },
                    {
                        "name": "Yiming Tan"
                    },
                    {
                        "name": "Konstantin Hoßfeld"
                    },
                    {
                        "name": "Jake Cheng Jiajun"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Lixian Yan"
                    },
                    {
                        "name": "Xinxin Wang"
                    },
                    {
                        "name": "Philip Levis"
                    },
                    {
                        "name": "H. -S. Philip Wong"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "arxiv_comment": "14 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14866v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14866v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.7.1; B.3.1; C.3; I.6; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02558v1",
                "updated": "2025-08-04T16:14:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    14,
                    3,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T16:14:03Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    14,
                    3,
                    0,
                    216,
                    0
                ],
                "title": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction"
                },
                "summary": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10$\\times$ higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10$\\times$ higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness."
                },
                "authors": [
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Ruixiao Li"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Zengfeng Huang"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02401v1",
                "updated": "2025-08-04T13:26:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    26,
                    16,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T13:26:16Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    26,
                    16,
                    0,
                    216,
                    0
                ],
                "title": "CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important\n  Before Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important\n  Before Generation"
                },
                "summary": "Recent advances in large language models (LLMs) have significantly boosted\nlong-context processing. However, the increasing key-value (KV) cache size\nposes critical challenges to memory and execution efficiency. Most KV cache\ncompression methods rely on heuristic token eviction using all attention heads\nin Grouped Query Attention (GQA)-based LLMs. This method ignores the different\nfunctionalities of attention heads, leading to the eviction of critical tokens\nand thus degrades the performance of LLMs.\n  To address the issue above, instead of using all the attention heads in\nGQA-based LLMs to determine important tokens as in the previous work, we first\nidentify the attention heads in each layer that are not only capable of\nretrieving the initial and final tokens of a prompt, but also capable of\nretrieving important tokens within the text and attending to their surrounding\nsemantic context. Afterwards, we exploit such heads to determine the important\ntokens and retain their corresponding KV cache pairs. Furthermore, we analyze\nthe cache eviction error of each layer individually and introduce a\nlayer-adaptive KV cache allocation strategy. Experimental results demonstrate\nthe proposed CompressKV consistently outperforms state-of-the-art approaches\nunder various memory budgets on LongBench and Needle-in-a-Haystack benchmarks.\nOur code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have significantly boosted\nlong-context processing. However, the increasing key-value (KV) cache size\nposes critical challenges to memory and execution efficiency. Most KV cache\ncompression methods rely on heuristic token eviction using all attention heads\nin Grouped Query Attention (GQA)-based LLMs. This method ignores the different\nfunctionalities of attention heads, leading to the eviction of critical tokens\nand thus degrades the performance of LLMs.\n  To address the issue above, instead of using all the attention heads in\nGQA-based LLMs to determine important tokens as in the previous work, we first\nidentify the attention heads in each layer that are not only capable of\nretrieving the initial and final tokens of a prompt, but also capable of\nretrieving important tokens within the text and attending to their surrounding\nsemantic context. Afterwards, we exploit such heads to determine the important\ntokens and retain their corresponding KV cache pairs. Furthermore, we analyze\nthe cache eviction error of each layer individually and introduce a\nlayer-adaptive KV cache allocation strategy. Experimental results demonstrate\nthe proposed CompressKV consistently outperforms state-of-the-art approaches\nunder various memory budgets on LongBench and Needle-in-a-Haystack benchmarks.\nOur code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git."
                },
                "authors": [
                    {
                        "name": "Xiaolin Lin"
                    },
                    {
                        "name": "Jingcun Wang"
                    },
                    {
                        "name": "Olga Kondrateva"
                    },
                    {
                        "name": "Yiyu Shi"
                    },
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Grace Li Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Grace Li Zhang"
                },
                "author": "Grace Li Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02280v1",
                "updated": "2025-08-04T10:51:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    10,
                    51,
                    20,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T10:51:20Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    10,
                    51,
                    20,
                    0,
                    216,
                    0
                ],
                "title": "OnPair: Short Strings Compression for Fast Random Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OnPair: Short Strings Compression for Fast Random Access"
                },
                "summary": "We present OnPair, a dictionary-based compression algorithm designed to meet\nthe needs of in-memory database systems that require both high compression and\nfast random access. Existing methods either achieve strong compression ratios\nat significant computational and memory cost (e.g., BPE) or prioritize speed at\nthe expense of compression quality (e.g., FSST). OnPair bridges this gap by\nemploying a cache-friendly dictionary construction technique that incrementally\nmerges frequent adjacent substrings in a single sequential pass over a data\nsample. This enables fast, memory-efficient training without tracking global\npair positions, as required by traditional BPE. We also introduce OnPair16, a\nvariant that limits dictionary entries to 16 bytes, enabling faster parsing via\noptimized longest prefix matching. Both variants compress strings\nindependently, supporting fine-grained random access without block-level\noverhead. Experiments on real-world datasets show that OnPair and OnPair16\nachieve compression ratios comparable to BPE while significantly improving\ncompression speed and memory usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present OnPair, a dictionary-based compression algorithm designed to meet\nthe needs of in-memory database systems that require both high compression and\nfast random access. Existing methods either achieve strong compression ratios\nat significant computational and memory cost (e.g., BPE) or prioritize speed at\nthe expense of compression quality (e.g., FSST). OnPair bridges this gap by\nemploying a cache-friendly dictionary construction technique that incrementally\nmerges frequent adjacent substrings in a single sequential pass over a data\nsample. This enables fast, memory-efficient training without tracking global\npair positions, as required by traditional BPE. We also introduce OnPair16, a\nvariant that limits dictionary entries to 16 bytes, enabling faster parsing via\noptimized longest prefix matching. Both variants compress strings\nindependently, supporting fine-grained random access without block-level\noverhead. Experiments on real-world datasets show that OnPair and OnPair16\nachieve compression ratios comparable to BPE while significantly improving\ncompression speed and memory usage."
                },
                "authors": [
                    {
                        "name": "Francesco Gargiulo"
                    },
                    {
                        "name": "Rossano Venturini"
                    }
                ],
                "author_detail": {
                    "name": "Rossano Venturini"
                },
                "author": "Rossano Venturini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.4; E.4; H.3.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02215v1",
                "updated": "2025-08-04T09:08:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    8,
                    43,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T09:08:43Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    8,
                    43,
                    0,
                    216,
                    0
                ],
                "title": "LeanK: Learnable K Cache Channel Pruning for Efficient Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeanK: Learnable K Cache Channel Pruning for Efficient Decoding"
                },
                "summary": "Large language models (LLMs) enable long-context tasks but face efficiency\nchallenges due to the growing key-value (KV) cache. We propose LeanK, a\nlearning-based method that prunes unimportant key (K) cache channels by\nleveraging static channel sparsity. With a novel two-stage training process,\nLeanK learns channel-wise static mask that could satisfy specific sparsity\nratio and hardware alignment requirement. LeanK reduces GPU memory and\naccelerates decoding without sacrificing accuracy. Experiments demonstrate up\nto 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel\nenables 1.3x speedup for attention computation. We also provide insights into\nmodel channels and attention heads during long-context inference by analyzing\nthe learned importance distribution. Our code is available at\nhttps://aka.ms/LeanK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) enable long-context tasks but face efficiency\nchallenges due to the growing key-value (KV) cache. We propose LeanK, a\nlearning-based method that prunes unimportant key (K) cache channels by\nleveraging static channel sparsity. With a novel two-stage training process,\nLeanK learns channel-wise static mask that could satisfy specific sparsity\nratio and hardware alignment requirement. LeanK reduces GPU memory and\naccelerates decoding without sacrificing accuracy. Experiments demonstrate up\nto 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel\nenables 1.3x speedup for attention computation. We also provide insights into\nmodel channels and attention heads during long-context inference by analyzing\nthe learned importance distribution. Our code is available at\nhttps://aka.ms/LeanK."
                },
                "authors": [
                    {
                        "name": "Yike Zhang"
                    },
                    {
                        "name": "Zhiyuan He"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Jianyong Wang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19906v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19906v2",
                "updated": "2025-08-04T08:19:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    8,
                    19,
                    26,
                    0,
                    216,
                    0
                ],
                "published": "2025-07-26T10:34:53Z",
                "published_parsed": [
                    2025,
                    7,
                    26,
                    10,
                    34,
                    53,
                    5,
                    207,
                    0
                ],
                "title": "CaliDrop: KV Cache Compression with Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaliDrop: KV Cache Compression with Calibration"
                },
                "summary": "Large Language Models (LLMs) require substantial computational resources\nduring generation. While the Key-Value (KV) cache significantly accelerates\nthis process by storing attention intermediates, its memory footprint grows\nlinearly with sequence length, batch size, and model size, creating a\nbottleneck in long-context scenarios. Various KV cache compression techniques,\nincluding token eviction, quantization, and low-rank projection, have been\nproposed to mitigate this bottleneck, often complementing each other. This\npaper focuses on enhancing token eviction strategies. Token eviction leverages\nthe observation that the attention patterns are often sparse, allowing for the\nremoval of less critical KV entries to save memory. However, this reduction\nusually comes at the cost of notable accuracy degradation, particularly under\nhigh compression ratios. To address this issue, we propose \\textbf{CaliDrop}, a\nnovel strategy that enhances token eviction through calibration. Our\npreliminary experiments show that queries at nearby positions exhibit high\nsimilarity. Building on this observation, CaliDrop performs speculative\ncalibration on the discarded tokens to mitigate the accuracy loss caused by\ntoken eviction. Extensive experiments demonstrate that CaliDrop significantly\nimproves the accuracy of existing token eviction methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require substantial computational resources\nduring generation. While the Key-Value (KV) cache significantly accelerates\nthis process by storing attention intermediates, its memory footprint grows\nlinearly with sequence length, batch size, and model size, creating a\nbottleneck in long-context scenarios. Various KV cache compression techniques,\nincluding token eviction, quantization, and low-rank projection, have been\nproposed to mitigate this bottleneck, often complementing each other. This\npaper focuses on enhancing token eviction strategies. Token eviction leverages\nthe observation that the attention patterns are often sparse, allowing for the\nremoval of less critical KV entries to save memory. However, this reduction\nusually comes at the cost of notable accuracy degradation, particularly under\nhigh compression ratios. To address this issue, we propose \\textbf{CaliDrop}, a\nnovel strategy that enhances token eviction through calibration. Our\npreliminary experiments show that queries at nearby positions exhibit high\nsimilarity. Building on this observation, CaliDrop performs speculative\ncalibration on the discarded tokens to mitigate the accuracy loss caused by\ntoken eviction. Extensive experiments demonstrate that CaliDrop significantly\nimproves the accuracy of existing token eviction methods."
                },
                "authors": [
                    {
                        "name": "Yi Su"
                    },
                    {
                        "name": "Quantong Qiu"
                    },
                    {
                        "name": "Yuechi Zhou"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Qingrong Xia"
                    },
                    {
                        "name": "Ping Li"
                    },
                    {
                        "name": "Xinyu Duan"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19906v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19906v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21492v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21492v2",
                "updated": "2025-08-04T04:48:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    4,
                    48,
                    41,
                    0,
                    216,
                    0
                ],
                "published": "2025-07-29T04:21:11Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    4,
                    21,
                    11,
                    1,
                    210,
                    0
                ],
                "title": "Bridging Cache-Friendliness and Concurrency: A Locality-Optimized\n  In-Memory B-Skiplist",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Cache-Friendliness and Concurrency: A Locality-Optimized\n  In-Memory B-Skiplist"
                },
                "summary": "Skiplists are widely used for in-memory indexing in many key-value stores,\nsuch as RocksDB and LevelDB, due to their ease of implementation and simple\nconcurrency control mechanisms. However, traditional skiplists suffer from poor\ncache locality, as they store only a single element per node, leaving\nperformance on the table. Minimizing last-level cache misses is key to\nmaximizing in-memory index performance, making high cache locality essential.\nIn this paper, we present a practical concurrent B-skiplist that enhances cache\nlocality and performance while preserving the simplicity of traditional\nskiplist structures and concurrency control schemes. Our key contributions\ninclude a top-down, single-pass insertion algorithm for B-skiplists and a\ncorresponding simple and efficient top-down concurrency control scheme. On 128\nthreads, the proposed concurrent B-skiplist achieves between 2x-9x higher\nthroughput compared to state-of-the-art concurrent skiplist implementations,\nincluding Facebook's concurrent skiplist from Folly and the Java\nConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves\ncompetitive (0.9x-1.7x) throughput on point workloads compared to\nstate-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a\nmore complete picture of the performance, we also measure the latency of\nskiplist and tree-based indices and find that the B-skiplist achieves between\n3.5x-103x lower 99% latency compared to other concurrent skiplists and between\n0.85x-64x lower 99% latency compared to tree-based indices on point workloads\nwith inserts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skiplists are widely used for in-memory indexing in many key-value stores,\nsuch as RocksDB and LevelDB, due to their ease of implementation and simple\nconcurrency control mechanisms. However, traditional skiplists suffer from poor\ncache locality, as they store only a single element per node, leaving\nperformance on the table. Minimizing last-level cache misses is key to\nmaximizing in-memory index performance, making high cache locality essential.\nIn this paper, we present a practical concurrent B-skiplist that enhances cache\nlocality and performance while preserving the simplicity of traditional\nskiplist structures and concurrency control schemes. Our key contributions\ninclude a top-down, single-pass insertion algorithm for B-skiplists and a\ncorresponding simple and efficient top-down concurrency control scheme. On 128\nthreads, the proposed concurrent B-skiplist achieves between 2x-9x higher\nthroughput compared to state-of-the-art concurrent skiplist implementations,\nincluding Facebook's concurrent skiplist from Folly and the Java\nConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves\ncompetitive (0.9x-1.7x) throughput on point workloads compared to\nstate-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a\nmore complete picture of the performance, we also measure the latency of\nskiplist and tree-based indices and find that the B-skiplist achieves between\n3.5x-103x lower 99% latency compared to other concurrent skiplists and between\n0.85x-64x lower 99% latency compared to tree-based indices on point workloads\nwith inserts."
                },
                "authors": [
                    {
                        "name": "Yicong Luo"
                    },
                    {
                        "name": "Senhe Hao"
                    },
                    {
                        "name": "Brian Wheatman"
                    },
                    {
                        "name": "Prashant Pandey"
                    },
                    {
                        "name": "Helen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Helen Xu"
                },
                "author": "Helen Xu",
                "arxiv_doi": "10.1145/3754598.3754655",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3754598.3754655",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.21492v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21492v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Original paper was accepted into ICPP 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02930v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02930v4",
                "updated": "2025-08-04T02:47:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    2,
                    47,
                    35,
                    0,
                    216,
                    0
                ],
                "published": "2024-07-03T09:02:05Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    9,
                    2,
                    5,
                    2,
                    185,
                    0
                ],
                "title": "Timely Requesting for Time-Critical Content Users in Decentralized\n  F-RANs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timely Requesting for Time-Critical Content Users in Decentralized\n  F-RANs"
                },
                "summary": "With the rising demand for high-rate and timely communications, fog radio\naccess networks (F-RANs) offer a promising solution. This work investigates age\nof information (AoI) performance in F-RANs, consisting of multiple content\nusers (CUs), enhanced remote radio heads (eRRHs), and content providers (CPs).\nTime-critical CUs need rapid content updates from CPs but cannot communicate\ndirectly with them; instead, eRRHs act as intermediaries. CUs decide whether to\nrequest content from a CP and which eRRH to send the request to, while eRRHs\ndecide whether to command CPs to update content or use cached content. We study\ntwo general classes of policies: (i) oblivious policies, where decision-making\nis independent of historical information, and (ii) non-oblivious policies,\nwhere decisions are influenced by historical information. First, we obtain\nclosed-form expressions for the average AoI of eRRHs under both policy types.\nDue to the complexity of calculating closed-form expressions for CUs, we then\nderive general upper bounds for their average AoI. Next, we identify optimal\npolicies for both types. Under both optimal policies, each CU requests content\nfrom each CP at an equal rate, consolidating all requests to a single eRRH when\ndemand is low or resources are limited, and distributing requests evenly among\neRRHs when demand is high and resources are ample. eRRHs command content from\neach CP at an equal rate under an optimal oblivious policy, while prioritize\nthe CP with the highest age under an optimal non-oblivious policy. Our\nnumerical results validate these theoretical findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rising demand for high-rate and timely communications, fog radio\naccess networks (F-RANs) offer a promising solution. This work investigates age\nof information (AoI) performance in F-RANs, consisting of multiple content\nusers (CUs), enhanced remote radio heads (eRRHs), and content providers (CPs).\nTime-critical CUs need rapid content updates from CPs but cannot communicate\ndirectly with them; instead, eRRHs act as intermediaries. CUs decide whether to\nrequest content from a CP and which eRRH to send the request to, while eRRHs\ndecide whether to command CPs to update content or use cached content. We study\ntwo general classes of policies: (i) oblivious policies, where decision-making\nis independent of historical information, and (ii) non-oblivious policies,\nwhere decisions are influenced by historical information. First, we obtain\nclosed-form expressions for the average AoI of eRRHs under both policy types.\nDue to the complexity of calculating closed-form expressions for CUs, we then\nderive general upper bounds for their average AoI. Next, we identify optimal\npolicies for both types. Under both optimal policies, each CU requests content\nfrom each CP at an equal rate, consolidating all requests to a single eRRH when\ndemand is low or resources are limited, and distributing requests evenly among\neRRHs when demand is high and resources are ample. eRRHs command content from\neach CP at an equal rate under an optimal oblivious policy, while prioritize\nthe CP with the highest age under an optimal non-oblivious policy. Our\nnumerical results validate these theoretical findings."
                },
                "authors": [
                    {
                        "name": "Xingran Chen"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Kun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Kun Yang"
                },
                "author": "Kun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02930v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02930v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02252v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02252v2",
                "updated": "2025-08-04T02:17:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    2,
                    17,
                    56,
                    0,
                    216,
                    0
                ],
                "published": "2024-12-03T08:29:27Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "title": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity"
                },
                "summary": "The rapid expansion of context window sizes in Large Language Models~(LLMs)\nhas enabled them to tackle increasingly complex tasks involving lengthy\ndocuments. However, this progress comes at the cost of a substantial increase\nin memory usage during inference, primarily due to the linear growth of the\nkey-value~(KV) cache. Existing KV cache compression methods often discard less\nrelevant tokens, which can lead to significant performance degradation when\ncritical information is lost. In this paper, we propose \\textsc{PoD}~(Proximal\ntokens over Distant tokens), a novel KV cache compression framework that\nallocates memory according to token importance, retaining less important tokens\nin a more compact, shared form rather than discarding them entirely. Our\napproach is motivated by two key observations: (1) proximal tokens -- those at\nthe beginning and end of the context -- are significantly more important for\nnext-token prediction, and (2) attention scores for distant tokens are highly\nredundant across consecutive layers. Leveraging these insights, \\textsc{PoD}\npreserves the full KV cache for proximal tokens, while for distant tokens, it\nshares key states across layers. Since attention scores are determined by both\nqueries and keys, sharing key states enables multiple layers to reuse a single\nset of keys for distant tokens, substantially reducing KV cache memory without\ndiscarding essential context. We further introduce a lightweight post-training\nadaptation to enable the model to adjust to this new attention-sharing\nstructure. Extensive experiments on both synthetic~(Needle in a Haystack) and\nreal-world long-context benchmarks demonstrate that \\textsc{PoD} reduces KV\ncache memory usage by up to 35\\% without compromising performance. Our method\nis orthogonal to existing token-selection-based techniques and can be combined\nwith them for further KV cache compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid expansion of context window sizes in Large Language Models~(LLMs)\nhas enabled them to tackle increasingly complex tasks involving lengthy\ndocuments. However, this progress comes at the cost of a substantial increase\nin memory usage during inference, primarily due to the linear growth of the\nkey-value~(KV) cache. Existing KV cache compression methods often discard less\nrelevant tokens, which can lead to significant performance degradation when\ncritical information is lost. In this paper, we propose \\textsc{PoD}~(Proximal\ntokens over Distant tokens), a novel KV cache compression framework that\nallocates memory according to token importance, retaining less important tokens\nin a more compact, shared form rather than discarding them entirely. Our\napproach is motivated by two key observations: (1) proximal tokens -- those at\nthe beginning and end of the context -- are significantly more important for\nnext-token prediction, and (2) attention scores for distant tokens are highly\nredundant across consecutive layers. Leveraging these insights, \\textsc{PoD}\npreserves the full KV cache for proximal tokens, while for distant tokens, it\nshares key states across layers. Since attention scores are determined by both\nqueries and keys, sharing key states enables multiple layers to reuse a single\nset of keys for distant tokens, substantially reducing KV cache memory without\ndiscarding essential context. We further introduce a lightweight post-training\nadaptation to enable the model to adjust to this new attention-sharing\nstructure. Extensive experiments on both synthetic~(Needle in a Haystack) and\nreal-world long-context benchmarks demonstrate that \\textsc{PoD} reduces KV\ncache memory usage by up to 35\\% without compromising performance. Our method\nis orthogonal to existing token-selection-based techniques and can be combined\nwith them for further KV cache compression."
                },
                "authors": [
                    {
                        "name": "Da Ma"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Situo Zhang"
                    },
                    {
                        "name": "Yuxun Miao"
                    },
                    {
                        "name": "Su Zhu"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Hongshen Xu"
                    },
                    {
                        "name": "Hanqi Li"
                    },
                    {
                        "name": "Shuai Fan"
                    },
                    {
                        "name": "Lei Pan"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "14 pages, 7 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02252v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02252v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01898v1",
                "updated": "2025-08-03T19:16:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    3,
                    19,
                    16,
                    40,
                    6,
                    215,
                    0
                ],
                "published": "2025-08-03T19:16:40Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    19,
                    16,
                    40,
                    6,
                    215,
                    0
                ],
                "title": "Revenue Optimization in Wireless Video Caching Networks: A\n  Privacy-Preserving Two-Stage Solution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revenue Optimization in Wireless Video Caching Networks: A\n  Privacy-Preserving Two-Stage Solution"
                },
                "summary": "Video caching can significantly improve delivery efficiency and enhance\nquality of video streaming, which constitutes the majority of wireless\ncommunication traffic. Due to limited cache size, caching strategies must be\ndesigned to adapt to and dynamic user demand in order to maximize system\nrevenue. The system revenue depends on the benefits of delivering the requested\nvideos and costs for (a) transporting the files to the users and (b) cache\nreplacement. Since the cache content at any point in time impacts the\nreplacement costs in the future, demand predictions over multiple cache\nplacement slots become an important prerequisite for efficient cache planning.\nMotivated by this, we introduce a novel two-stage privacy-preserving solution\nfor revenue optimization in wireless video caching networks. First, we train a\nTransformer using privacy-preserving federated learning (FL) to predict\nmulti-slot future demands. Given that prediction results are never entirely\naccurate, especially for longer horizons, we further combine global content\npopularity with per-user prediction results to estimate the content demand\ndistribution. Then, in the second stage, we leverage these estimation results\nto find caching strategies that maximize the long-term system revenue. This\nlatter problem takes on the form of a multi-stage knapsack problem, which we\nthen transform to a integer linear program. Our extensive simulation results\ndemonstrate that (i) our FL solution delivers nearly identical performance to\nthat of the ideal centralized solution and outperforms other existing caching\nmethods, and (ii) our novel revenue optimization approach provides deeper\nsystem performance insights than traditional cache hit ratio (CHR)-based\noptimization approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video caching can significantly improve delivery efficiency and enhance\nquality of video streaming, which constitutes the majority of wireless\ncommunication traffic. Due to limited cache size, caching strategies must be\ndesigned to adapt to and dynamic user demand in order to maximize system\nrevenue. The system revenue depends on the benefits of delivering the requested\nvideos and costs for (a) transporting the files to the users and (b) cache\nreplacement. Since the cache content at any point in time impacts the\nreplacement costs in the future, demand predictions over multiple cache\nplacement slots become an important prerequisite for efficient cache planning.\nMotivated by this, we introduce a novel two-stage privacy-preserving solution\nfor revenue optimization in wireless video caching networks. First, we train a\nTransformer using privacy-preserving federated learning (FL) to predict\nmulti-slot future demands. Given that prediction results are never entirely\naccurate, especially for longer horizons, we further combine global content\npopularity with per-user prediction results to estimate the content demand\ndistribution. Then, in the second stage, we leverage these estimation results\nto find caching strategies that maximize the long-term system revenue. This\nlatter problem takes on the form of a multi-stage knapsack problem, which we\nthen transform to a integer linear program. Our extensive simulation results\ndemonstrate that (i) our FL solution delivers nearly identical performance to\nthat of the ideal centralized solution and outperforms other existing caching\nmethods, and (ii) our novel revenue optimization approach provides deeper\nsystem performance insights than traditional cache hit ratio (CHR)-based\noptimization approaches."
                },
                "authors": [
                    {
                        "name": "Yijing Zhang"
                    },
                    {
                        "name": "Md-Ferdous Pervej"
                    },
                    {
                        "name": "Andreas F. Molisch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas F. Molisch"
                },
                "author": "Andreas F. Molisch",
                "arxiv_comment": "Under review for possible publication in the IEEE Transactions on\n  Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01875v1",
                "updated": "2025-08-03T18:15:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    3,
                    18,
                    15,
                    42,
                    6,
                    215,
                    0
                ],
                "published": "2025-08-03T18:15:42Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    18,
                    15,
                    42,
                    6,
                    215,
                    0
                ],
                "title": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding"
                },
                "summary": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios."
                },
                "authors": [
                    {
                        "name": "Haolin Yang"
                    },
                    {
                        "name": "Feilong Tang"
                    },
                    {
                        "name": "Linxiao Zhao"
                    },
                    {
                        "name": "Xiang An"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Huifa Li"
                    },
                    {
                        "name": "Xinlin Zhuang"
                    },
                    {
                        "name": "Boqian Wang"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Xiaofeng Zhang"
                    },
                    {
                        "name": "Abdalla Swikir"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Zongyuan Ge"
                    },
                    {
                        "name": "Imran Razzak"
                    }
                ],
                "author_detail": {
                    "name": "Imran Razzak"
                },
                "author": "Imran Razzak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16607v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16607v2",
                "updated": "2025-08-03T10:27:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    3,
                    10,
                    27,
                    19,
                    6,
                    215,
                    0
                ],
                "published": "2025-01-28T00:52:23Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    0,
                    52,
                    23,
                    1,
                    28,
                    0
                ],
                "title": "MCTS-SQL: Light-Weight LLMs can Master the Text-to-SQL through Monte\n  Carlo Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCTS-SQL: Light-Weight LLMs can Master the Text-to-SQL through Monte\n  Carlo Tree Search"
                },
                "summary": "Text-to-SQL is a fundamental yet challenging task in the NLP area, aiming at\ntranslating natural language questions into SQL queries. While recent advances\nin large language models have greatly improved performance, most existing\napproaches depend on models with tens of billions of parameters or costly APIs,\nlimiting their applicability in resource-constrained environments. For real\nworld, especially on edge devices, it is crucial for Text-to-SQL to ensure\ncost-effectiveness. Therefore, enabling the light-weight models for Text-to-SQL\nis of great practical significance. However, smaller LLMs often struggle with\ncomplicated user instruction, redundant schema linking or syntax correctness.\nTo address these challenges, we propose MCTS-SQL, a novel framework that uses\nMonte Carlo Tree Search to guide SQL generation through multi-step refinement.\nSince the light-weight models' weak performance of single-shot prediction, we\ngenerate better results through several trials with feedback. However, directly\napplying MCTS-based methods inevitably leads to significant time and\ncomputational overhead. Driven by this issue, we propose a token-level\nprefix-cache mechanism that stores prior information during iterations,\neffectively improved the execution speed. Experiments results on the SPIDER and\nBIRD benchmarks demonstrate the effectiveness of our approach. Using a small\nopen-source Qwen2.5-Coder-1.5B, our method outperforms ChatGPT-3.5. When\nleveraging a more powerful model Gemini 2.5 to explore the performance upper\nbound, we achieved results competitive with the SOTA. Our findings demonstrate\nthat even small models can be effectively deployed in practical Text-to-SQL\nsystems with the right strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL is a fundamental yet challenging task in the NLP area, aiming at\ntranslating natural language questions into SQL queries. While recent advances\nin large language models have greatly improved performance, most existing\napproaches depend on models with tens of billions of parameters or costly APIs,\nlimiting their applicability in resource-constrained environments. For real\nworld, especially on edge devices, it is crucial for Text-to-SQL to ensure\ncost-effectiveness. Therefore, enabling the light-weight models for Text-to-SQL\nis of great practical significance. However, smaller LLMs often struggle with\ncomplicated user instruction, redundant schema linking or syntax correctness.\nTo address these challenges, we propose MCTS-SQL, a novel framework that uses\nMonte Carlo Tree Search to guide SQL generation through multi-step refinement.\nSince the light-weight models' weak performance of single-shot prediction, we\ngenerate better results through several trials with feedback. However, directly\napplying MCTS-based methods inevitably leads to significant time and\ncomputational overhead. Driven by this issue, we propose a token-level\nprefix-cache mechanism that stores prior information during iterations,\neffectively improved the execution speed. Experiments results on the SPIDER and\nBIRD benchmarks demonstrate the effectiveness of our approach. Using a small\nopen-source Qwen2.5-Coder-1.5B, our method outperforms ChatGPT-3.5. When\nleveraging a more powerful model Gemini 2.5 to explore the performance upper\nbound, we achieved results competitive with the SOTA. Our findings demonstrate\nthat even small models can be effectively deployed in practical Text-to-SQL\nsystems with the right strategy."
                },
                "authors": [
                    {
                        "name": "Shuozhi Yuan"
                    },
                    {
                        "name": "Limin Chen"
                    },
                    {
                        "name": "Miaomiao Yuan"
                    },
                    {
                        "name": "Jin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Zhao"
                },
                "author": "Jin Zhao",
                "arxiv_comment": "15 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16607v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16607v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02751v1",
                "updated": "2025-08-03T09:15:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    3,
                    9,
                    15,
                    36,
                    6,
                    215,
                    0
                ],
                "published": "2025-08-03T09:15:36Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    9,
                    15,
                    36,
                    6,
                    215,
                    0
                ],
                "title": "SmallKV: Small Model Assisted Compensation of KV Cache Compression for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmallKV: Small Model Assisted Compensation of KV Cache Compression for\n  Efficient LLM Inference"
                },
                "summary": "KV cache eviction has emerged as an effective solution to alleviate resource\nconstraints faced by LLMs in long-context scenarios. However, existing\ntoken-level eviction methods often overlook two critical aspects: (1) their\nirreversible eviction strategy fails to adapt to dynamic attention patterns\nduring decoding (the saliency shift problem), and (2) they treat both\nmarginally important tokens and truly unimportant tokens equally, despite the\ncollective significance of marginal tokens to model performance (the marginal\ninformation over-compression problem). To address these issues, we design two\ncompensation mechanisms based on the high similarity of attention matrices\nbetween LLMs of different scales. We propose SmallKV, a small model assisted\ncompensation method for KV cache compression. SmallKV can maintain attention\nmatching between different-scale LLMs to: 1) assist the larger model in\nperceiving globally important information of attention; and 2) use the smaller\nmodel's attention scores to approximate those of marginal tokens in the larger\nmodel. Extensive experiments on benchmarks including GSM8K, BBH, MT-Bench, and\nLongBench demonstrate the effectiveness of SmallKV. Moreover, efficiency\nevaluations show that SmallKV achieves 1.75 - 2.56 times higher throughput than\nbaseline methods, highlighting its potential for efficient and performant LLM\ninference in resource constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache eviction has emerged as an effective solution to alleviate resource\nconstraints faced by LLMs in long-context scenarios. However, existing\ntoken-level eviction methods often overlook two critical aspects: (1) their\nirreversible eviction strategy fails to adapt to dynamic attention patterns\nduring decoding (the saliency shift problem), and (2) they treat both\nmarginally important tokens and truly unimportant tokens equally, despite the\ncollective significance of marginal tokens to model performance (the marginal\ninformation over-compression problem). To address these issues, we design two\ncompensation mechanisms based on the high similarity of attention matrices\nbetween LLMs of different scales. We propose SmallKV, a small model assisted\ncompensation method for KV cache compression. SmallKV can maintain attention\nmatching between different-scale LLMs to: 1) assist the larger model in\nperceiving globally important information of attention; and 2) use the smaller\nmodel's attention scores to approximate those of marginal tokens in the larger\nmodel. Extensive experiments on benchmarks including GSM8K, BBH, MT-Bench, and\nLongBench demonstrate the effectiveness of SmallKV. Moreover, efficiency\nevaluations show that SmallKV achieves 1.75 - 2.56 times higher throughput than\nbaseline methods, highlighting its potential for efficient and performant LLM\ninference in resource constrained environments."
                },
                "authors": [
                    {
                        "name": "Yi Zhao"
                    },
                    {
                        "name": "Yajuan Peng"
                    },
                    {
                        "name": "Cam-Tu Nguyen"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Xiaoliang Wang"
                    },
                    {
                        "name": "Hai Zhao"
                    },
                    {
                        "name": "Xiaoming Fu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoming Fu"
                },
                "author": "Xiaoming Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19718v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19718v2",
                "updated": "2025-08-02T23:59:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    23,
                    59,
                    11,
                    5,
                    214,
                    0
                ],
                "published": "2025-07-25T23:55:54Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    23,
                    55,
                    54,
                    4,
                    206,
                    0
                ],
                "title": "GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D\n  Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D\n  Gaussian Splatting"
                },
                "summary": "Real-time path tracing is rapidly becoming the standard for rendering in\nentertainment and professional applications. In scientific visualization,\nvolume rendering plays a crucial role in helping researchers analyze and\ninterpret complex 3D data. Recently, photorealistic rendering techniques have\ngained popularity in scientific visualization, yet they face significant\nchallenges. One of the most prominent issues is slow rendering performance and\nhigh pixel variance caused by Monte Carlo integration. In this work, we\nintroduce a novel radiance caching approach for path-traced volume rendering.\nOur method leverages advances in volumetric scene representation and adapts 3D\nGaussian splatting to function as a multi-level, path-space radiance cache.\nThis cache is designed to be trainable on the fly, dynamically adapting to\nchanges in scene parameters such as lighting configurations and transfer\nfunctions. By incorporating our cache, we achieve less noisy, higher-quality\nimages without increasing rendering costs. To evaluate our approach, we compare\nit against a baseline path tracer that supports uniform sampling and next-event\nestimation and the state-of-the-art for neural radiance caching. Through both\nquantitative and qualitative analyses, we demonstrate that our path-space\nradiance cache is a robust solution that is easy to integrate and significantly\nenhances the rendering quality of volumetric visualization applications while\nmaintaining comparable computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time path tracing is rapidly becoming the standard for rendering in\nentertainment and professional applications. In scientific visualization,\nvolume rendering plays a crucial role in helping researchers analyze and\ninterpret complex 3D data. Recently, photorealistic rendering techniques have\ngained popularity in scientific visualization, yet they face significant\nchallenges. One of the most prominent issues is slow rendering performance and\nhigh pixel variance caused by Monte Carlo integration. In this work, we\nintroduce a novel radiance caching approach for path-traced volume rendering.\nOur method leverages advances in volumetric scene representation and adapts 3D\nGaussian splatting to function as a multi-level, path-space radiance cache.\nThis cache is designed to be trainable on the fly, dynamically adapting to\nchanges in scene parameters such as lighting configurations and transfer\nfunctions. By incorporating our cache, we achieve less noisy, higher-quality\nimages without increasing rendering costs. To evaluate our approach, we compare\nit against a baseline path tracer that supports uniform sampling and next-event\nestimation and the state-of-the-art for neural radiance caching. Through both\nquantitative and qualitative analyses, we demonstrate that our path-space\nradiance cache is a robust solution that is easy to integrate and significantly\nenhances the rendering quality of volumetric visualization applications while\nmaintaining comparable computational efficiency."
                },
                "authors": [
                    {
                        "name": "David Bauer"
                    },
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "Hamid Gadirov"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19718v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19718v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01488v1",
                "updated": "2025-08-02T21:00:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    21,
                    0,
                    55,
                    5,
                    214,
                    0
                ],
                "published": "2025-08-02T21:00:55Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    21,
                    0,
                    55,
                    5,
                    214,
                    0
                ],
                "title": "PESTO: Real-Time Pitch Estimation with Self-supervised\n  Transposition-equivariant Objective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PESTO: Real-Time Pitch Estimation with Self-supervised\n  Transposition-equivariant Objective"
                },
                "summary": "In this paper, we introduce PESTO, a self-supervised learning approach for\nsingle-pitch estimation using a Siamese architecture. Our model processes\nindividual frames of a Variable-$Q$ Transform (VQT) and predicts pitch\ndistributions. The neural network is designed to be equivariant to\ntranslations, notably thanks to a Toeplitz fully-connected layer. In addition,\nwe construct pitch-shifted pairs by translating and cropping the VQT frames and\ntrain our model with a novel class-based transposition-equivariant objective,\neliminating the need for annotated data. Thanks to this architecture and\ntraining objective, our model achieves remarkable performances while being very\nlightweight ($130$k parameters). Evaluations on music and speech datasets\n(MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms\nself-supervised baselines but also competes with supervised methods, exhibiting\nsuperior cross-dataset generalization. Finally, we enhance PESTO's practical\nutility by developing a streamable VQT implementation using cached\nconvolutions. Combined with our model's low latency (less than 10 ms) and\nminimal parameter count, this makes PESTO particularly suitable for real-time\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce PESTO, a self-supervised learning approach for\nsingle-pitch estimation using a Siamese architecture. Our model processes\nindividual frames of a Variable-$Q$ Transform (VQT) and predicts pitch\ndistributions. The neural network is designed to be equivariant to\ntranslations, notably thanks to a Toeplitz fully-connected layer. In addition,\nwe construct pitch-shifted pairs by translating and cropping the VQT frames and\ntrain our model with a novel class-based transposition-equivariant objective,\neliminating the need for annotated data. Thanks to this architecture and\ntraining objective, our model achieves remarkable performances while being very\nlightweight ($130$k parameters). Evaluations on music and speech datasets\n(MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms\nself-supervised baselines but also competes with supervised methods, exhibiting\nsuperior cross-dataset generalization. Finally, we enhance PESTO's practical\nutility by developing a streamable VQT implementation using cached\nconvolutions. Combined with our model's low latency (less than 10 ms) and\nminimal parameter count, this makes PESTO particularly suitable for real-time\napplications."
                },
                "authors": [
                    {
                        "name": "Alain Riou"
                    },
                    {
                        "name": "Bernardo Torres"
                    },
                    {
                        "name": "Ben Hayes"
                    },
                    {
                        "name": "Stefan Lattner"
                    },
                    {
                        "name": "Gaëtan Hadjeres"
                    },
                    {
                        "name": "Gaël Richard"
                    },
                    {
                        "name": "Geoffroy Peeters"
                    }
                ],
                "author_detail": {
                    "name": "Geoffroy Peeters"
                },
                "author": "Geoffroy Peeters",
                "arxiv_comment": "Accepted to the Transactions of the International Society for Music\n  Information Retrieval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01298v1",
                "updated": "2025-08-02T10:12:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    10,
                    12,
                    45,
                    5,
                    214,
                    0
                ],
                "published": "2025-08-02T10:12:45Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    10,
                    12,
                    45,
                    5,
                    214,
                    0
                ],
                "title": "Improving performance of content-centric networks via decentralized\n  coded caching for multi-level popularity and access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving performance of content-centric networks via decentralized\n  coded caching for multi-level popularity and access"
                },
                "summary": "Content-Centric Networking (CCN) offers a novel architectural paradigm that\nseeks to address the inherent limitations of the prevailing Internet Protocol\n(IP)-based networking model. In contrast to the host-centric communication\napproach of IP networks, CCN prioritizes content by enabling direct addressing\nand routing based on content identifiers. The potential performance\nimprovements of CCN can be further amplified through optimized management of\ncoded data storage and transmission strategies. Decentralized Coded Caching\n(DCC) emerges as a promising technique that harnesses the collective caching\npower of distributed network elements. By strategically pre-positioning\nfrequently accessed content closer to potential consumers during periods of low\nnetwork utilization, DCC has the potential to mitigate content transfer rates\nduring peak traffic periods. This paper proposes a series of fundamental\nmodifications to the CCN architecture by integrating DCC. The proposed\nframework incorporates differentiated coding strategies tailored to user access\nprivileges, thereby eliminating the overhead associated with queue-based\nsearching. Additionally, the framework facilitates recoding of uncoded data\nencountered along the content delivery path. These combined methodologies\ndemonstrably enhance network throughput, elevate cache hit ratios, and\nconsequently, reduce content delivery latency compared to conventional CCN\nimplementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content-Centric Networking (CCN) offers a novel architectural paradigm that\nseeks to address the inherent limitations of the prevailing Internet Protocol\n(IP)-based networking model. In contrast to the host-centric communication\napproach of IP networks, CCN prioritizes content by enabling direct addressing\nand routing based on content identifiers. The potential performance\nimprovements of CCN can be further amplified through optimized management of\ncoded data storage and transmission strategies. Decentralized Coded Caching\n(DCC) emerges as a promising technique that harnesses the collective caching\npower of distributed network elements. By strategically pre-positioning\nfrequently accessed content closer to potential consumers during periods of low\nnetwork utilization, DCC has the potential to mitigate content transfer rates\nduring peak traffic periods. This paper proposes a series of fundamental\nmodifications to the CCN architecture by integrating DCC. The proposed\nframework incorporates differentiated coding strategies tailored to user access\nprivileges, thereby eliminating the overhead associated with queue-based\nsearching. Additionally, the framework facilitates recoding of uncoded data\nencountered along the content delivery path. These combined methodologies\ndemonstrably enhance network throughput, elevate cache hit ratios, and\nconsequently, reduce content delivery latency compared to conventional CCN\nimplementations."
                },
                "authors": [
                    {
                        "name": "Azadeh Sadat Miraftab"
                    },
                    {
                        "name": "Ahmadreza Montazerolghaem"
                    },
                    {
                        "name": "Behrad Mahboobi"
                    }
                ],
                "author_detail": {
                    "name": "Behrad Mahboobi"
                },
                "author": "Behrad Mahboobi",
                "arxiv_doi": "10.1007/s10586-025-05256-6",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10586-025-05256-6",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.01298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01261v1",
                "updated": "2025-08-02T08:33:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    8,
                    33,
                    30,
                    5,
                    214,
                    0
                ],
                "published": "2025-08-02T08:33:30Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    8,
                    33,
                    30,
                    5,
                    214,
                    0
                ],
                "title": "Unifying Mixture of Experts and Multi-Head Latent Attention for\n  Efficient Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Mixture of Experts and Multi-Head Latent Attention for\n  Efficient Language Models"
                },
                "summary": "We present MoE-MLA-RoPE, a novel architecture combination that combines\nMixture of Experts (MoE) with Multi-head Latent Attention (MLA) and Rotary\nPosition Embeddings (RoPE) for efficient language modeling. Our approach\naddresses the fundamental trade-off between model capacity and computational\nefficiency through three key innovations: (1) fine-grained expert routing with\n64 micro-experts and top-$k$ selection, enabling flexible specialization\nthrough 3.6 * 10^7 possible expert combinations; (2) shared expert isolation\nthat dedicates 2 always active experts for common patterns while routing to 6\nof 62 specialized experts; and (3) gradient-conflict-free load balancing that\nmaintains expert utilization without interfering with primary loss\noptimization.\n  Extensive experiments on models ranging from 17M to 202M parameters\ndemonstrate that MoE-MLA-RoPE with compression ratio r=d/2 achieves 68% KV\ncache memory reduction and 3.2x inference speedup while maintaining competitive\nperplexity (0.8% degradation). Compared to the parameters with 53.9M\nparameters, MoE-MLA-RoPE improves the validation loss by 6.9% over the vanilla\ntransformers while using 42% fewer active parameters per forward pass.\nFLOP-matched experiments reveal even larger gains: 11.1% improvement with 3.2x\ninference acceleration. Automated evaluation using GPT-4 as a judge confirms\nquality improvements in generation, with higher scores on coherence (8.1/10),\ncreativity (7.9/10) and grammatical correctness (8.2/10). Our results establish\nthat architectural novelty, not parameter scaling, defines the efficiency\nfrontier for resource-constrained language model deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present MoE-MLA-RoPE, a novel architecture combination that combines\nMixture of Experts (MoE) with Multi-head Latent Attention (MLA) and Rotary\nPosition Embeddings (RoPE) for efficient language modeling. Our approach\naddresses the fundamental trade-off between model capacity and computational\nefficiency through three key innovations: (1) fine-grained expert routing with\n64 micro-experts and top-$k$ selection, enabling flexible specialization\nthrough 3.6 * 10^7 possible expert combinations; (2) shared expert isolation\nthat dedicates 2 always active experts for common patterns while routing to 6\nof 62 specialized experts; and (3) gradient-conflict-free load balancing that\nmaintains expert utilization without interfering with primary loss\noptimization.\n  Extensive experiments on models ranging from 17M to 202M parameters\ndemonstrate that MoE-MLA-RoPE with compression ratio r=d/2 achieves 68% KV\ncache memory reduction and 3.2x inference speedup while maintaining competitive\nperplexity (0.8% degradation). Compared to the parameters with 53.9M\nparameters, MoE-MLA-RoPE improves the validation loss by 6.9% over the vanilla\ntransformers while using 42% fewer active parameters per forward pass.\nFLOP-matched experiments reveal even larger gains: 11.1% improvement with 3.2x\ninference acceleration. Automated evaluation using GPT-4 as a judge confirms\nquality improvements in generation, with higher scores on coherence (8.1/10),\ncreativity (7.9/10) and grammatical correctness (8.2/10). Our results establish\nthat architectural novelty, not parameter scaling, defines the efficiency\nfrontier for resource-constrained language model deployment."
                },
                "authors": [
                    {
                        "name": "Sushant Mehta"
                    },
                    {
                        "name": "Raj Dandekar"
                    },
                    {
                        "name": "Rajat Dandekar"
                    },
                    {
                        "name": "Sreedath Panat"
                    }
                ],
                "author_detail": {
                    "name": "Sreedath Panat"
                },
                "author": "Sreedath Panat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11972v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11972v2",
                "updated": "2025-08-02T06:50:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    6,
                    50,
                    59,
                    5,
                    214,
                    0
                ],
                "published": "2025-03-15T02:48:27Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    2,
                    48,
                    27,
                    5,
                    74,
                    0
                ],
                "title": "MoDM: Efficient Serving for Image Generation via Mixture-of-Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoDM: Efficient Serving for Image Generation via Mixture-of-Diffusion\n  Models"
                },
                "summary": "Diffusion-based text-to-image generation models trade latency for quality:\nsmall models are fast but generate lower-quality images, while large models\nproduce better images but are slow.\n  We present MoDM, a novel caching-based serving system for diffusion models\nthat dynamically balances latency and quality through a mixture of diffusion\nmodels. Unlike prior approaches that rely on model-specific internal features,\nMoDM caches final images, allowing seamless retrieval and reuse across multiple\ndiffusion model families.\n  This design enables adaptive serving by dynamically balancing latency and\nimage quality: using smaller models for cache-hit requests to reduce latency\nwhile reserving larger models for cache-miss requests to maintain quality.\nSmall model image quality is preserved using retrieved cached images.\n  We design a global monitor that optimally allocates GPU resources and\nbalances inference workload, ensuring high throughput while meeting\nservice-level objectives under varying request rates. Our evaluations show that\nMoDM significantly reduces average serving time by 2.5x while retaining image\nquality, making it a practical solution for scalable and resource-efficient\nmodel deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based text-to-image generation models trade latency for quality:\nsmall models are fast but generate lower-quality images, while large models\nproduce better images but are slow.\n  We present MoDM, a novel caching-based serving system for diffusion models\nthat dynamically balances latency and quality through a mixture of diffusion\nmodels. Unlike prior approaches that rely on model-specific internal features,\nMoDM caches final images, allowing seamless retrieval and reuse across multiple\ndiffusion model families.\n  This design enables adaptive serving by dynamically balancing latency and\nimage quality: using smaller models for cache-hit requests to reduce latency\nwhile reserving larger models for cache-miss requests to maintain quality.\nSmall model image quality is preserved using retrieved cached images.\n  We design a global monitor that optimally allocates GPU resources and\nbalances inference workload, ensuring high throughput while meeting\nservice-level objectives under varying request rates. Our evaluations show that\nMoDM significantly reduces average serving time by 2.5x while retaining image\nquality, making it a practical solution for scalable and resource-efficient\nmodel deployment."
                },
                "authors": [
                    {
                        "name": "Yuchen Xia"
                    },
                    {
                        "name": "Divyam Sharma"
                    },
                    {
                        "name": "Yichao Yuan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Nishil Talati"
                    }
                ],
                "author_detail": {
                    "name": "Nishil Talati"
                },
                "author": "Nishil Talati",
                "arxiv_comment": "To appear in ASPLOS'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11972v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11972v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01225v1",
                "updated": "2025-08-02T06:43:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    6,
                    43,
                    43,
                    5,
                    214,
                    0
                ],
                "published": "2025-08-02T06:43:43Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    6,
                    43,
                    43,
                    5,
                    214,
                    0
                ],
                "title": "Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of\n  Vision-Language Models"
                },
                "summary": "In zero-shot setting, test-time adaptation adjusts pre-trained models using\nunlabeled data from the test phase to enhance performance on unknown test\ndistributions. Existing cache-enhanced TTA methods rely on a low-entropy\ncriterion to select samples for prototype construction, assuming intra-class\ncompactness. However, low-entropy samples may be unreliable under distribution\nshifts, and the resulting prototypes may not ensure compact intra-class\ndistributions. This study identifies a positive correlation between\ncache-enhanced performance and intra-class compactness. Based on this\nobservation, we propose a Multi-Cache enhanced Prototype-based Test-Time\nAdaptation (MCP) featuring three caches: an entropy cache for initializing\nprototype representations with low-entropy samples, an align cache for\nintegrating visual and textual information to achieve compact intra-class\ndistributions, and a negative cache for prediction calibration using\nhigh-entropy samples. We further developed MCP++, a framework incorporating\ncross-modal prototype alignment and residual learning, introducing prototype\nresidual fine-tuning. Comparative and ablation experiments across 15 downstream\ntasks demonstrate that the proposed method and framework achieve\nstate-of-the-art generalization performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In zero-shot setting, test-time adaptation adjusts pre-trained models using\nunlabeled data from the test phase to enhance performance on unknown test\ndistributions. Existing cache-enhanced TTA methods rely on a low-entropy\ncriterion to select samples for prototype construction, assuming intra-class\ncompactness. However, low-entropy samples may be unreliable under distribution\nshifts, and the resulting prototypes may not ensure compact intra-class\ndistributions. This study identifies a positive correlation between\ncache-enhanced performance and intra-class compactness. Based on this\nobservation, we propose a Multi-Cache enhanced Prototype-based Test-Time\nAdaptation (MCP) featuring three caches: an entropy cache for initializing\nprototype representations with low-entropy samples, an align cache for\nintegrating visual and textual information to achieve compact intra-class\ndistributions, and a negative cache for prediction calibration using\nhigh-entropy samples. We further developed MCP++, a framework incorporating\ncross-modal prototype alignment and residual learning, introducing prototype\nresidual fine-tuning. Comparative and ablation experiments across 15 downstream\ntasks demonstrate that the proposed method and framework achieve\nstate-of-the-art generalization performance."
                },
                "authors": [
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Haotian Zhai"
                    },
                    {
                        "name": "Can Zhang"
                    },
                    {
                        "name": "Xiupeng Shi"
                    },
                    {
                        "name": "Ruirui Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruirui Li"
                },
                "author": "Ruirui Li",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19849v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19849v2",
                "updated": "2025-08-02T00:31:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    0,
                    31,
                    18,
                    5,
                    214,
                    0
                ],
                "published": "2025-05-26T11:35:04Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    11,
                    35,
                    4,
                    0,
                    146,
                    0
                ],
                "title": "HIT Model: A Hierarchical Interaction-Enhanced Two-Tower Model for\n  Pre-Ranking Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HIT Model: A Hierarchical Interaction-Enhanced Two-Tower Model for\n  Pre-Ranking Systems"
                },
                "summary": "Online display advertising platforms rely on pre-ranking systems to\nefficiently filter and prioritize candidate ads from large corpora, balancing\nrelevance to users with strict computational constraints. The prevailing\ntwo-tower architecture, though highly efficient due to its decoupled design and\npre-caching, suffers from cross-domain interaction and coarse similarity\nmetrics, undermining its capacity to model complex user-ad relationships. In\nthis study, we propose the Hierarchical Interaction-Enhanced Two-Tower (HIT)\nmodel, a new architecture that augments the two-tower paradigm with two key\ncomponents: $\\textit{generators}$ that pre-generate holistic vectors\nincorporating coarse-grained user-ad interactions through a dual-generator\nframework with a cosine-similarity-based generation loss as the training\nobjective, and $\\textit{multi-head representers}$ that project embeddings into\nmultiple latent subspaces to capture fine-grained, multi-faceted user interests\nand multi-dimensional ad attributes. This design enhances modeling\neffectiveness without compromising inference efficiency. Extensive experiments\non public datasets and large-scale online A/B testing on Tencent's advertising\nplatform demonstrate that HIT significantly outperforms several baselines in\nrelevance metrics, yielding a $1.66\\%$ increase in Gross Merchandise Volume and\na $1.55\\%$ improvement in Return on Investment, alongside similar serving\nlatency to the vanilla two-tower models. The HIT model has been successfully\ndeployed in Tencent's online display advertising system, serving billions of\nimpressions daily. The code is available at\nhttps://github.com/HarveyYang123/HIT_model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online display advertising platforms rely on pre-ranking systems to\nefficiently filter and prioritize candidate ads from large corpora, balancing\nrelevance to users with strict computational constraints. The prevailing\ntwo-tower architecture, though highly efficient due to its decoupled design and\npre-caching, suffers from cross-domain interaction and coarse similarity\nmetrics, undermining its capacity to model complex user-ad relationships. In\nthis study, we propose the Hierarchical Interaction-Enhanced Two-Tower (HIT)\nmodel, a new architecture that augments the two-tower paradigm with two key\ncomponents: $\\textit{generators}$ that pre-generate holistic vectors\nincorporating coarse-grained user-ad interactions through a dual-generator\nframework with a cosine-similarity-based generation loss as the training\nobjective, and $\\textit{multi-head representers}$ that project embeddings into\nmultiple latent subspaces to capture fine-grained, multi-faceted user interests\nand multi-dimensional ad attributes. This design enhances modeling\neffectiveness without compromising inference efficiency. Extensive experiments\non public datasets and large-scale online A/B testing on Tencent's advertising\nplatform demonstrate that HIT significantly outperforms several baselines in\nrelevance metrics, yielding a $1.66\\%$ increase in Gross Merchandise Volume and\na $1.55\\%$ improvement in Return on Investment, alongside similar serving\nlatency to the vanilla two-tower models. The HIT model has been successfully\ndeployed in Tencent's online display advertising system, serving billions of\nimpressions daily. The code is available at\nhttps://github.com/HarveyYang123/HIT_model."
                },
                "authors": [
                    {
                        "name": "Haoqiang Yang"
                    },
                    {
                        "name": "Congde Yuan"
                    },
                    {
                        "name": "Kun Bai"
                    },
                    {
                        "name": "Mengzhuo Guo"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Chao Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Chao Zhou"
                },
                "author": "Chao Zhou",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19849v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19849v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01051v1",
                "updated": "2025-08-01T20:08:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    20,
                    8,
                    52,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T20:08:52Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    20,
                    8,
                    52,
                    4,
                    213,
                    0
                ],
                "title": "QPP-RNG: A Conceptual Quantum System for True Randomness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QPP-RNG: A Conceptual Quantum System for True Randomness"
                },
                "summary": "We propose and experimentally demonstrate the \\emph{Quasi-Superposition\nQuantum-inspired System (QSQS)} -- a conceptual quantum system for randomness\ngeneration built on measuring two conjugate observables of a permutation\nsorting process: the deterministic permutation count $n_p$ and the\nfundamentally non-deterministic sorting time $t$. By analogy with quantum\nsystems, these observables are linked by an uncertainty-like constraint:\nalgorithmic determinism ensures structural uniformity, while system-level\nfluctuations introduce irreducible unpredictability. We realize this framework\nconcretely as \\emph{QPP-RNG}, a system-embedded, software-based true random\nnumber generator (TRNG). In QPP-RNG, real-time measurements of sorting time $t$\n-- shaped by CPU pipeline jitter, cache latency, and OS scheduling --\ndynamically reseed the PRNG driving the permutation sequence. Crucially, QSQS\ntransforms initially right-skewed raw distributions of $n_p$ and $t$ into\nnearly uniform outputs after modulo reduction, thanks to internal degeneracies\nthat collapse many distinct states into the same output symbol. Empirical\nresults show that as the repetition factor $m$ increases, output entropy\nconverges toward theoretical maxima: Shannon and min-entropy values approach 8\nbits, chi-squared statistics stabilize near ideal uniformity, and bell curves\nvisually confirm the flattening from skewed to uniform distributions. Beyond\npractical implications, QSQS unifies deterministic algorithmic processes with\nnon-deterministic physical fluctuations, offering a physics-based perspective\nfor engineering true randomness in post-quantum cryptographic systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose and experimentally demonstrate the \\emph{Quasi-Superposition\nQuantum-inspired System (QSQS)} -- a conceptual quantum system for randomness\ngeneration built on measuring two conjugate observables of a permutation\nsorting process: the deterministic permutation count $n_p$ and the\nfundamentally non-deterministic sorting time $t$. By analogy with quantum\nsystems, these observables are linked by an uncertainty-like constraint:\nalgorithmic determinism ensures structural uniformity, while system-level\nfluctuations introduce irreducible unpredictability. We realize this framework\nconcretely as \\emph{QPP-RNG}, a system-embedded, software-based true random\nnumber generator (TRNG). In QPP-RNG, real-time measurements of sorting time $t$\n-- shaped by CPU pipeline jitter, cache latency, and OS scheduling --\ndynamically reseed the PRNG driving the permutation sequence. Crucially, QSQS\ntransforms initially right-skewed raw distributions of $n_p$ and $t$ into\nnearly uniform outputs after modulo reduction, thanks to internal degeneracies\nthat collapse many distinct states into the same output symbol. Empirical\nresults show that as the repetition factor $m$ increases, output entropy\nconverges toward theoretical maxima: Shannon and min-entropy values approach 8\nbits, chi-squared statistics stabilize near ideal uniformity, and bell curves\nvisually confirm the flattening from skewed to uniform distributions. Beyond\npractical implications, QSQS unifies deterministic algorithmic processes with\nnon-deterministic physical fluctuations, offering a physics-based perspective\nfor engineering true randomness in post-quantum cryptographic systems."
                },
                "authors": [
                    {
                        "name": "Randy Kuang"
                    }
                ],
                "author_detail": {
                    "name": "Randy Kuang"
                },
                "author": "Randy Kuang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00647v1",
                "updated": "2025-08-01T14:05:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    5,
                    44,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T14:05:44Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    5,
                    44,
                    4,
                    213,
                    0
                ],
                "title": "Study of the HV power supply modules for the CUbesat Solar Polarimeter\n  (CUSP)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Study of the HV power supply modules for the CUbesat Solar Polarimeter\n  (CUSP)"
                },
                "summary": "The CUbesat Solar Polarimeter (CUSP) project is a CubeSat mission orbiting\nthe Earth aimed to measure the linear polarization of solar flares in the hard\nX-ray band by means of a Compton scattering polarimeter. CUSP will allow to\nstudy the magnetic reconnection and particle acceleration in the flaring\nmagnetic structures of our star. CUSP is a project in the framework of the\nAlcor Program of the Italian Space Agency aimed to develop new CubeSat\nmissions. CUSP undergoing the Phase B started in December 2024 that will last\nfor 12 month. The Compton polarimeter of the CUSP payload performs coincidence\nmeasurements between plastic scintilaltors and GaGG(Ce) crystals to derive the\npolarization of X-rays. These sensors are readout by Multi Anode\nPhotomultiplier Tubes (MAPMTs) and Avalanche Photodiodes (APDs) respectively.\nBoth sensors need an HV power supply up to -1~kV (for the MAPMT) and +500~V\n(for the APD). We tested precision regulated High Voltage DC/DC Converters by\nHVM Technology Inc. with Sub-Miniature Case Size\n($0.85''\\times0.85''\\times0.60''$) of the SMHV series. These modules are\ncompact and suited for CubeSat missions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The CUbesat Solar Polarimeter (CUSP) project is a CubeSat mission orbiting\nthe Earth aimed to measure the linear polarization of solar flares in the hard\nX-ray band by means of a Compton scattering polarimeter. CUSP will allow to\nstudy the magnetic reconnection and particle acceleration in the flaring\nmagnetic structures of our star. CUSP is a project in the framework of the\nAlcor Program of the Italian Space Agency aimed to develop new CubeSat\nmissions. CUSP undergoing the Phase B started in December 2024 that will last\nfor 12 month. The Compton polarimeter of the CUSP payload performs coincidence\nmeasurements between plastic scintilaltors and GaGG(Ce) crystals to derive the\npolarization of X-rays. These sensors are readout by Multi Anode\nPhotomultiplier Tubes (MAPMTs) and Avalanche Photodiodes (APDs) respectively.\nBoth sensors need an HV power supply up to -1~kV (for the MAPMT) and +500~V\n(for the APD). We tested precision regulated High Voltage DC/DC Converters by\nHVM Technology Inc. with Sub-Miniature Case Size\n($0.85''\\times0.85''\\times0.60''$) of the SMHV series. These modules are\ncompact and suited for CubeSat missions."
                },
                "authors": [
                    {
                        "name": "Alessandro Lacerenza"
                    },
                    {
                        "name": "Alda Rubini"
                    },
                    {
                        "name": "Andrea Alimenti"
                    },
                    {
                        "name": "Sergio Fabiani"
                    },
                    {
                        "name": "Ettore Del Monte"
                    },
                    {
                        "name": "Riccardo Campana"
                    },
                    {
                        "name": "Mauro Centrone"
                    },
                    {
                        "name": "Enrico Costa"
                    },
                    {
                        "name": "Nicolas De Angelis"
                    },
                    {
                        "name": "Giovanni De Cesare"
                    },
                    {
                        "name": "Sergio Di Cosimo"
                    },
                    {
                        "name": "Giuseppe Di Persio"
                    },
                    {
                        "name": "Abhay Kumar"
                    },
                    {
                        "name": "Pasqualino Loffredo"
                    },
                    {
                        "name": "Giovanni Lombardi"
                    },
                    {
                        "name": "Gabriele Minervini"
                    },
                    {
                        "name": "Fabio Muleri"
                    },
                    {
                        "name": "Paolo Romano"
                    },
                    {
                        "name": "Emanuele Scalise"
                    },
                    {
                        "name": "Enrico Silva"
                    },
                    {
                        "name": "Paolo Soffitta"
                    },
                    {
                        "name": "Davide Albanesi"
                    },
                    {
                        "name": "Ilaria Baffo"
                    },
                    {
                        "name": "Daniele Brienza"
                    },
                    {
                        "name": "Valerio Campamaggiore"
                    },
                    {
                        "name": "Giovanni Cucinella"
                    },
                    {
                        "name": "Andrea Curatolo"
                    },
                    {
                        "name": "Giulia de Iulis"
                    },
                    {
                        "name": "Andrea Del Re"
                    },
                    {
                        "name": "Vito Di Bari"
                    },
                    {
                        "name": "Simone Di Filippo"
                    },
                    {
                        "name": "Immacolata Donnarumma"
                    },
                    {
                        "name": "Pierluigi Fanelli"
                    },
                    {
                        "name": "Nicolas Gagliardi"
                    },
                    {
                        "name": "Paolo Leonetti"
                    },
                    {
                        "name": "Matteo Merge"
                    },
                    {
                        "name": "Dario Modenini"
                    },
                    {
                        "name": "Andrea Negri"
                    },
                    {
                        "name": "Daniele Pecorella"
                    },
                    {
                        "name": "Massimo Perelli"
                    },
                    {
                        "name": "Alice Ponti"
                    },
                    {
                        "name": "Francesca Sbop"
                    },
                    {
                        "name": "Paolo Tortora"
                    },
                    {
                        "name": "Alessandro Turchi"
                    },
                    {
                        "name": "Valerio Vagelli"
                    },
                    {
                        "name": "Emanuele Zaccagnino"
                    },
                    {
                        "name": "Alessandro Zambardi"
                    },
                    {
                        "name": "Costantino Zazza"
                    }
                ],
                "author_detail": {
                    "name": "Costantino Zazza"
                },
                "author": "Costantino Zazza",
                "arxiv_comment": "6 pages, 2 figures, SPIE Optics+Photonics 2025 proceeding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.space-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00629v1",
                "updated": "2025-08-01T13:40:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    40,
                    52,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T13:40:52Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    40,
                    52,
                    4,
                    213,
                    0
                ],
                "title": "Energy-Aware CPU Orchestration in O-RAN: A dApp-Driven Lightweight\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy-Aware CPU Orchestration in O-RAN: A dApp-Driven Lightweight\n  Approach"
                },
                "summary": "The transition toward softwarized Radio Access Networks (RANs), driven by the\nOpen RAN (O-RAN) paradigm, enables flexible, vendor-neutral deployments through\ndisaggregation and virtualization of base station functions. However, this\nshift introduces new challenges in managing CPU resources efficiently under\nstrict real-time constraints. In particular, the interplay between\nlatency-sensitive RAN workloads and general-purpose Operating System (OS)\nschedulers often leads to sub-optimal performance and unnecessary energy\nconsumption. This work proposes a lightweight, programmable distributed\napplication (dApp) deployed at the Distributed Unit (DU) level to dynamically\norchestrate CPU usage. The dApp operates in closed loop with the OS, leveraging\nthread-level telemetry like context switches, Instructions Per Cycle (IPC), and\ncache metrics, to adapt CPU thread affinity, core isolation, and frequency\nscaling in real time. Unlike existing solutions, it requires no access to\nproprietary RAN software, hardware-specific features, or kernel modifications.\nFully compliant with the O-RAN architecture and agnostic to the underlying RAN\nstack, the proposed solution introduces negligible overhead while improving\nenergy efficiency and CPU utilization. Experimental results using a\ncommercial-grade srsRAN deployment demonstrate consistent power savings without\ncompromising real-time processing performance, highlighting the potential of\nlow-latency dApps for fine-grained resource control in next-generation networks",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transition toward softwarized Radio Access Networks (RANs), driven by the\nOpen RAN (O-RAN) paradigm, enables flexible, vendor-neutral deployments through\ndisaggregation and virtualization of base station functions. However, this\nshift introduces new challenges in managing CPU resources efficiently under\nstrict real-time constraints. In particular, the interplay between\nlatency-sensitive RAN workloads and general-purpose Operating System (OS)\nschedulers often leads to sub-optimal performance and unnecessary energy\nconsumption. This work proposes a lightweight, programmable distributed\napplication (dApp) deployed at the Distributed Unit (DU) level to dynamically\norchestrate CPU usage. The dApp operates in closed loop with the OS, leveraging\nthread-level telemetry like context switches, Instructions Per Cycle (IPC), and\ncache metrics, to adapt CPU thread affinity, core isolation, and frequency\nscaling in real time. Unlike existing solutions, it requires no access to\nproprietary RAN software, hardware-specific features, or kernel modifications.\nFully compliant with the O-RAN architecture and agnostic to the underlying RAN\nstack, the proposed solution introduces negligible overhead while improving\nenergy efficiency and CPU utilization. Experimental results using a\ncommercial-grade srsRAN deployment demonstrate consistent power savings without\ncompromising real-time processing performance, highlighting the potential of\nlow-latency dApps for fine-grained resource control in next-generation networks"
                },
                "authors": [
                    {
                        "name": "Francisco Crespo"
                    },
                    {
                        "name": "Javier Villegas"
                    },
                    {
                        "name": "Carlos Baena"
                    },
                    {
                        "name": "Eduardo Baena"
                    },
                    {
                        "name": "Sergio Fortes"
                    },
                    {
                        "name": "Raquel Barco"
                    }
                ],
                "author_detail": {
                    "name": "Raquel Barco"
                },
                "author": "Raquel Barco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00616v1",
                "updated": "2025-08-01T13:25:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    25,
                    28,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T13:25:28Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    25,
                    28,
                    4,
                    213,
                    0
                ],
                "title": "Joint Association and Phase Shifts Design for UAV-mounted Stacked\n  Intelligent Metasurfaces-assisted Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Association and Phase Shifts Design for UAV-mounted Stacked\n  Intelligent Metasurfaces-assisted Communications"
                },
                "summary": "Stacked intelligent metasurfaces (SIMs) have emerged as a promising\ntechnology for realizing wave-domain signal processing, while the fixed SIMs\nwill limit the communication performance of the system compared to the mobile\nSIMs. In this work, we consider a UAV-mounted SIMs (UAV-SIMs) assisted\ncommunication system, where UAVs as base stations (BSs) can cache the data\nprocessed by SIMs, and also as mobile vehicles flexibly deploy SIMs to enhance\nthe communication performance. To this end, we formulate a UAV-SIM-based joint\noptimization problem (USBJOP) to comprehensively consider the association\nbetween UAV-SIMs and users, the locations of UAV-SIMs, and the phase shifts of\nUAV-SIMs, aiming to maximize the network capacity. Due to the non-convexity and\nNP-hardness of USBJOP, we decompose it into three sub-optimization problems,\nwhich are the association between UAV-SIMs and users optimization problem\n(AUUOP), the UAV location optimization problem (ULOP), and the UAV-SIM phase\nshifts optimization problem (USPSOP). Then, these three sub-optimization\nproblems are solved by an alternating optimization (AO) strategy. Specifically,\nAUUOP and ULOP are transformed to a convex form and then solved by the CVX\ntool, while we employ a layer-by-layer iterative optimization method for\nUSPSOP. Simulation results verify the effectiveness of the proposed strategy\nunder different simulation setups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stacked intelligent metasurfaces (SIMs) have emerged as a promising\ntechnology for realizing wave-domain signal processing, while the fixed SIMs\nwill limit the communication performance of the system compared to the mobile\nSIMs. In this work, we consider a UAV-mounted SIMs (UAV-SIMs) assisted\ncommunication system, where UAVs as base stations (BSs) can cache the data\nprocessed by SIMs, and also as mobile vehicles flexibly deploy SIMs to enhance\nthe communication performance. To this end, we formulate a UAV-SIM-based joint\noptimization problem (USBJOP) to comprehensively consider the association\nbetween UAV-SIMs and users, the locations of UAV-SIMs, and the phase shifts of\nUAV-SIMs, aiming to maximize the network capacity. Due to the non-convexity and\nNP-hardness of USBJOP, we decompose it into three sub-optimization problems,\nwhich are the association between UAV-SIMs and users optimization problem\n(AUUOP), the UAV location optimization problem (ULOP), and the UAV-SIM phase\nshifts optimization problem (USPSOP). Then, these three sub-optimization\nproblems are solved by an alternating optimization (AO) strategy. Specifically,\nAUUOP and ULOP are transformed to a convex form and then solved by the CVX\ntool, while we employ a layer-by-layer iterative optimization method for\nUSPSOP. Simulation results verify the effectiveness of the proposed strategy\nunder different simulation setups."
                },
                "authors": [
                    {
                        "name": "Mingzhe Fan"
                    },
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Hongyang Pan"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Jiancheng An"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Chau Yuen"
                    }
                ],
                "author_detail": {
                    "name": "Chau Yuen"
                },
                "author": "Chau Yuen",
                "arxiv_comment": "This papar has been submitted to the IEEE Global Communications\n  Conference. arXiv admin note: substantial text overlap with arXiv:2506.23488",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00412v1",
                "updated": "2025-08-01T08:10:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    8,
                    10,
                    54,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T08:10:54Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    8,
                    10,
                    54,
                    4,
                    213,
                    0
                ],
                "title": "Sortblock: Similarity-Aware Feature Reuse for Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sortblock: Similarity-Aware Feature Reuse for Diffusion Model"
                },
                "summary": "Diffusion Transformers (DiTs) have demonstrated remarkable generative\ncapabilities, particularly benefiting from Transformer architectures that\nenhance visual and artistic fidelity. However, their inherently sequential\ndenoising process results in high inference latency, limiting their deployment\nin real-time scenarios. Existing training-free acceleration approaches\ntypically reuse intermediate features at fixed timesteps or layers, overlooking\nthe evolving semantic focus across denoising stages and Transformer blocks.To\naddress this, we propose Sortblock, a training-free inference acceleration\nframework that dynamically caches block-wise features based on their similarity\nacross adjacent timesteps. By ranking the evolution of residuals, Sortblock\nadaptively determines a recomputation ratio, selectively skipping redundant\ncomputations while preserving generation quality. Furthermore, we incorporate a\nlightweight linear prediction mechanism to reduce accumulated errors in skipped\nblocks.Extensive experiments across various tasks and DiT architectures\ndemonstrate that Sortblock achieves over 2$\\times$ inference speedup with\nminimal degradation in output quality, offering an effective and generalizable\nsolution for accelerating diffusion-based generative models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have demonstrated remarkable generative\ncapabilities, particularly benefiting from Transformer architectures that\nenhance visual and artistic fidelity. However, their inherently sequential\ndenoising process results in high inference latency, limiting their deployment\nin real-time scenarios. Existing training-free acceleration approaches\ntypically reuse intermediate features at fixed timesteps or layers, overlooking\nthe evolving semantic focus across denoising stages and Transformer blocks.To\naddress this, we propose Sortblock, a training-free inference acceleration\nframework that dynamically caches block-wise features based on their similarity\nacross adjacent timesteps. By ranking the evolution of residuals, Sortblock\nadaptively determines a recomputation ratio, selectively skipping redundant\ncomputations while preserving generation quality. Furthermore, we incorporate a\nlightweight linear prediction mechanism to reduce accumulated errors in skipped\nblocks.Extensive experiments across various tasks and DiT architectures\ndemonstrate that Sortblock achieves over 2$\\times$ inference speedup with\nminimal degradation in output quality, offering an effective and generalizable\nsolution for accelerating diffusion-based generative models."
                },
                "authors": [
                    {
                        "name": "Hanqi Chen"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Xiaoliu Guan"
                    },
                    {
                        "name": "Lielin Jiang"
                    },
                    {
                        "name": "Guanzhong Wang"
                    },
                    {
                        "name": "Zeyu Chen"
                    },
                    {
                        "name": "Yi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Liu"
                },
                "author": "Yi Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23387v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23387v2",
                "updated": "2025-08-01T03:43:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    3,
                    43,
                    24,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-31T10:02:26Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    2,
                    26,
                    3,
                    212,
                    0
                ],
                "title": "SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units\n  with Precision Recovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units\n  with Precision Recovery"
                },
                "summary": "Low-precision matrix engines, such as FP16 cube, offer high throughput but\nlack support for full-precision computation. In this work, we propose\nSGEMM-cube, a high-performance algorithm for emulating FP32 general\nmatrix-matrix multiplication (GEMM) using only FP16 computation units on a\nrepresentative AI accelerator. The method decomposes each FP32 operand into two\nFP16 values and compensates for numerical errors through a tunable scaling\nstrategy. A detailed analysis of numerical errors, including underflow\nconditions and precision loss, guides the selection of scaling parameters to\npreserve up to 22 bits of mantissa accuracy. We further investigate the effect\nof computation order on accuracy and demonstrate that a term-wise accumulation\nscheme improves numerical stability over conventional FP32 GEMM in low-exponent\nregimes. Finally, a cache-aware blocking strategy and double-buffered pipeline\nare introduced to overlap memory transfers with computation, enabling\nSGEMM-cube to achieve up to 77\\% of the theoretical FP32-equivalent peak\nperformance on Ascend 910A NPU lacking native FP32 support. Extensive numerical\nexperiments confirm that our method not only recovers the accuracy of native\nFP32 GEMM but also exhibits superior numerical stability under certain\nconditions, due to its structured and error-aware computation order.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-precision matrix engines, such as FP16 cube, offer high throughput but\nlack support for full-precision computation. In this work, we propose\nSGEMM-cube, a high-performance algorithm for emulating FP32 general\nmatrix-matrix multiplication (GEMM) using only FP16 computation units on a\nrepresentative AI accelerator. The method decomposes each FP32 operand into two\nFP16 values and compensates for numerical errors through a tunable scaling\nstrategy. A detailed analysis of numerical errors, including underflow\nconditions and precision loss, guides the selection of scaling parameters to\npreserve up to 22 bits of mantissa accuracy. We further investigate the effect\nof computation order on accuracy and demonstrate that a term-wise accumulation\nscheme improves numerical stability over conventional FP32 GEMM in low-exponent\nregimes. Finally, a cache-aware blocking strategy and double-buffered pipeline\nare introduced to overlap memory transfers with computation, enabling\nSGEMM-cube to achieve up to 77\\% of the theoretical FP32-equivalent peak\nperformance on Ascend 910A NPU lacking native FP32 support. Extensive numerical\nexperiments confirm that our method not only recovers the accuracy of native\nFP32 GEMM but also exhibits superior numerical stability under certain\nconditions, due to its structured and error-aware computation order."
                },
                "authors": [
                    {
                        "name": "Weicheng Xue"
                    },
                    {
                        "name": "Baisong Xu"
                    },
                    {
                        "name": "Kai Yang"
                    },
                    {
                        "name": "Yongxiang Liu"
                    },
                    {
                        "name": "Dengdeng Fan"
                    },
                    {
                        "name": "Pengxiang Xu"
                    },
                    {
                        "name": "Yonghong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yonghong Tian"
                },
                "author": "Yonghong Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23387v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23387v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22746v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22746v2",
                "updated": "2025-08-01T03:37:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    3,
                    37,
                    42,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-30T15:03:36Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    3,
                    36,
                    2,
                    211,
                    0
                ],
                "title": "Next Tokens Denoising for Speech Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next Tokens Denoising for Speech Synthesis"
                },
                "summary": "While diffusion and autoregressive (AR) models have significantly advanced\ngenerative modeling, they each present distinct limitations. AR models, which\nrely on causal attention, cannot exploit future context and suffer from slow\ngeneration speeds. Conversely, diffusion models struggle with key-value (KV)\ncaching. To overcome these challenges, we introduce Dragon-FM, a novel\ntext-to-speech (TTS) design that unifies AR and flow-matching. This model\nprocesses 48 kHz audio codec tokens in chunks at a compact rate of 12.5 tokens\nper second. This design enables AR modeling across chunks, ensuring global\ncoherence, while parallel flow-matching within chunks facilitates fast\niterative denoising. Thus, the model leverages KV-cache across chunks and\nutilizes bidirectional context within each chunk. Furthermore, it bridges\ncontinuous and discrete feature modeling, demonstrating that continuous AR\nflow-matching can predict discrete tokens with finite scalar quantizers. This\nefficient codec and fast chunk-autoregressive architecture also make the model\nhighly effective for generating long-form content, such as podcasts.\nExperiments on podcast datasets demonstrate its capability to efficiently\ngenerate high-quality zero-shot podcasts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While diffusion and autoregressive (AR) models have significantly advanced\ngenerative modeling, they each present distinct limitations. AR models, which\nrely on causal attention, cannot exploit future context and suffer from slow\ngeneration speeds. Conversely, diffusion models struggle with key-value (KV)\ncaching. To overcome these challenges, we introduce Dragon-FM, a novel\ntext-to-speech (TTS) design that unifies AR and flow-matching. This model\nprocesses 48 kHz audio codec tokens in chunks at a compact rate of 12.5 tokens\nper second. This design enables AR modeling across chunks, ensuring global\ncoherence, while parallel flow-matching within chunks facilitates fast\niterative denoising. Thus, the model leverages KV-cache across chunks and\nutilizes bidirectional context within each chunk. Furthermore, it bridges\ncontinuous and discrete feature modeling, demonstrating that continuous AR\nflow-matching can predict discrete tokens with finite scalar quantizers. This\nefficient codec and fast chunk-autoregressive architecture also make the model\nhighly effective for generating long-form content, such as podcasts.\nExperiments on podcast datasets demonstrate its capability to efficiently\ngenerate high-quality zero-shot podcasts."
                },
                "authors": [
                    {
                        "name": "Yanqing Liu"
                    },
                    {
                        "name": "Ruiqing Xue"
                    },
                    {
                        "name": "Chong Zhang"
                    },
                    {
                        "name": "Yufei Liu"
                    },
                    {
                        "name": "Gang Wang"
                    },
                    {
                        "name": "Bohan Li"
                    },
                    {
                        "name": "Yao Qian"
                    },
                    {
                        "name": "Lei He"
                    },
                    {
                        "name": "Shujie Liu"
                    },
                    {
                        "name": "Sheng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Zhao"
                },
                "author": "Sheng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22746v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22746v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02659v2",
                "updated": "2025-07-31T21:00:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    21,
                    0,
                    28,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-03T14:20:41Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding"
                },
                "summary": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup."
                },
                "authors": [
                    {
                        "name": "Ramchalam Kinattinkara Ramakrishnan"
                    },
                    {
                        "name": "Zhaocong Yuan"
                    },
                    {
                        "name": "Shaojie Zhuo"
                    },
                    {
                        "name": "Chen Feng"
                    },
                    {
                        "name": "Yicheng Lin"
                    },
                    {
                        "name": "Chenzheng Su"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaopeng Zhang"
                },
                "author": "Xiaopeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22701v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22701v2",
                "updated": "2025-07-31T16:21:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    21,
                    3,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-30T14:10:16Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    14,
                    10,
                    16,
                    2,
                    211,
                    0
                ],
                "title": "SAM: A Stability-Aware Cache Manager for Multi-Tenant Embedded Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAM: A Stability-Aware Cache Manager for Multi-Tenant Embedded Databases"
                },
                "summary": "The co-location of multiple database instances on resource constrained edge\nnodes creates significant cache contention, where traditional schemes are\ninefficient and unstable under dynamic workloads. To address this, we present\nSAM(a Stability-Aware Manager), an autonomic cache manager that establishes\ndecision stability as a first-class design principle. It achieves this through\nits core control policy, AURA(Autonomic Utility-balancing Resource Allocator),\nwhich resolves the classic exploitation-exploration dilemma by synthesizing two\northogonal factors: the H-factor, representing proven historical efficiency\n(exploitation), and the V-factor, for estimated marginal gain (exploration).\nThrough this practical synthesis and adaptive control, SAM achieves sustained\nhigh performance with strategic stability and robustness in volatile\nconditions.\n  Extensive experiments against 14 diverse baselines demonstrate SAM's\nsuperiority. It achieves top-tier throughput while being uniquely resilient to\ncomplex workload shifts and adversarial workloads like cache pollution.\nFurthermore, its decision latency is highly scalable, remaining nearly constant\nas the system grows to 120 databases. Crucially, SAM achieves superior decision\nstability -- maintaining consistent optimization directions despite noise,\navoiding performance oscillations while ensuring predictable Quality of\nService. These results prove that a principled, stability-aware design is\nessential for sustained high performance in real-world, large-scale systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The co-location of multiple database instances on resource constrained edge\nnodes creates significant cache contention, where traditional schemes are\ninefficient and unstable under dynamic workloads. To address this, we present\nSAM(a Stability-Aware Manager), an autonomic cache manager that establishes\ndecision stability as a first-class design principle. It achieves this through\nits core control policy, AURA(Autonomic Utility-balancing Resource Allocator),\nwhich resolves the classic exploitation-exploration dilemma by synthesizing two\northogonal factors: the H-factor, representing proven historical efficiency\n(exploitation), and the V-factor, for estimated marginal gain (exploration).\nThrough this practical synthesis and adaptive control, SAM achieves sustained\nhigh performance with strategic stability and robustness in volatile\nconditions.\n  Extensive experiments against 14 diverse baselines demonstrate SAM's\nsuperiority. It achieves top-tier throughput while being uniquely resilient to\ncomplex workload shifts and adversarial workloads like cache pollution.\nFurthermore, its decision latency is highly scalable, remaining nearly constant\nas the system grows to 120 databases. Crucially, SAM achieves superior decision\nstability -- maintaining consistent optimization directions despite noise,\navoiding performance oscillations while ensuring predictable Quality of\nService. These results prove that a principled, stability-aware design is\nessential for sustained high performance in real-world, large-scale systems."
                },
                "authors": [
                    {
                        "name": "Haoran Zhang"
                    },
                    {
                        "name": "Decheng Zuo"
                    },
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Zhiyu Liang"
                    },
                    {
                        "name": "Hongzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Wang"
                },
                "author": "Hongzhi Wang",
                "arxiv_comment": "17 pages, 10 figures. An extended version of a paper under review at\n  the VLDB 2026 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22701v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22701v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.4; H.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23674v1",
                "updated": "2025-07-31T15:50:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    50,
                    57,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T15:50:57Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    50,
                    57,
                    3,
                    212,
                    0
                ],
                "title": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses"
                },
                "summary": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience."
                },
                "authors": [
                    {
                        "name": "Muhammad Taha Cheema"
                    },
                    {
                        "name": "Abeer Aamir"
                    },
                    {
                        "name": "Khawaja Gul Muhammad"
                    },
                    {
                        "name": "Naveed Anwar Bhatti"
                    },
                    {
                        "name": "Ihsan Ayyub Qazi"
                    },
                    {
                        "name": "Zafar Ayyub Qazi"
                    }
                ],
                "author_detail": {
                    "name": "Zafar Ayyub Qazi"
                },
                "author": "Zafar Ayyub Qazi",
                "arxiv_comment": "13 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21433v2",
                "updated": "2025-07-31T07:53:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    53,
                    53,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-29T02:05:51Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    2,
                    5,
                    51,
                    1,
                    210,
                    0
                ],
                "title": "MemShare: Memory Efficient Inference for Large Reasoning Models through\n  KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemShare: Memory Efficient Inference for Large Reasoning Models through\n  KV Cache Reuse"
                },
                "summary": "Large Reasoning Models (LRMs) have achieved significant advances in\nmathematical reasoning and formal logic tasks. However, their tendency to\ngenerate lengthy chain-of-thought sequences leads to substantial memory\noverhead during inference. We observe that LRMs frequently produce highly\nsimilar intermediate reasoning steps, which correspond to similar KV cache\nstates across layers. Motivated by this observation, we propose MemShare, a\nnovel KV cache management approach that effectively reduces memory overhead.\nMemShare employs a collaborative filtering algorithm to efficiently identify\nreusable KV cache blocks and enables zero copy cache reuse to significantly\nreduce memory overhead, improve throughput while maintaining accuracy.\nExperimental results demonstrate that MemShare delivers up to 84.79\\%\nimprovement in throughput while maintaining better accuracy compared to\nexisting KV cache management methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) have achieved significant advances in\nmathematical reasoning and formal logic tasks. However, their tendency to\ngenerate lengthy chain-of-thought sequences leads to substantial memory\noverhead during inference. We observe that LRMs frequently produce highly\nsimilar intermediate reasoning steps, which correspond to similar KV cache\nstates across layers. Motivated by this observation, we propose MemShare, a\nnovel KV cache management approach that effectively reduces memory overhead.\nMemShare employs a collaborative filtering algorithm to efficiently identify\nreusable KV cache blocks and enables zero copy cache reuse to significantly\nreduce memory overhead, improve throughput while maintaining accuracy.\nExperimental results demonstrate that MemShare delivers up to 84.79\\%\nimprovement in throughput while maintaining better accuracy compared to\nexisting KV cache management methods."
                },
                "authors": [
                    {
                        "name": "Kaiwen Chen"
                    },
                    {
                        "name": "Xin Tan"
                    },
                    {
                        "name": "Minchen Yu"
                    },
                    {
                        "name": "Hong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Hong Xu"
                },
                "author": "Hong Xu",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01199v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01199v2",
                "updated": "2025-07-31T07:35:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    35,
                    4,
                    3,
                    212,
                    0
                ],
                "published": "2025-03-03T05:52:02Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    5,
                    52,
                    2,
                    0,
                    62,
                    0
                ],
                "title": "LiteGS: A High-performance Framework to Train 3DGS in Subminutes via\n  System and Algorithm Codesign",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiteGS: A High-performance Framework to Train 3DGS in Subminutes via\n  System and Algorithm Codesign"
                },
                "summary": "3D Gaussian Splatting (3DGS) has emerged as promising alternative in 3D\nrepresentation. However, it still suffers from high training cost. This paper\nintroduces LiteGS, a high performance framework that systematically optimizes\nthe 3DGS training pipeline from multiple aspects. At the low-level computation\nlayer, we design a ``warp-based raster'' associated with two hardware-aware\noptimizations to significantly reduce gradient reduction overhead. At the\nmid-level data management layer, we introduce dynamic spatial sorting based on\nMorton coding to enable a performant ``Cluster-Cull-Compact'' pipeline and\nimprove data locality, therefore reducing cache misses. At the top-level\nalgorithm layer, we establish a new robust densification criterion based on the\nvariance of the opacity gradient, paired with a more stable opacity control\nmechanism, to achieve more precise parameter growth. Experimental results\ndemonstrate that LiteGS accelerates the original 3DGS training by up to 13.4x\nwith comparable or superior quality and surpasses the current SOTA in\nlightweight models by up to 1.4x speedup. For high-quality reconstruction\ntasks, LiteGS sets a new accuracy record and decreases the training time by an\norder of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Gaussian Splatting (3DGS) has emerged as promising alternative in 3D\nrepresentation. However, it still suffers from high training cost. This paper\nintroduces LiteGS, a high performance framework that systematically optimizes\nthe 3DGS training pipeline from multiple aspects. At the low-level computation\nlayer, we design a ``warp-based raster'' associated with two hardware-aware\noptimizations to significantly reduce gradient reduction overhead. At the\nmid-level data management layer, we introduce dynamic spatial sorting based on\nMorton coding to enable a performant ``Cluster-Cull-Compact'' pipeline and\nimprove data locality, therefore reducing cache misses. At the top-level\nalgorithm layer, we establish a new robust densification criterion based on the\nvariance of the opacity gradient, paired with a more stable opacity control\nmechanism, to achieve more precise parameter growth. Experimental results\ndemonstrate that LiteGS accelerates the original 3DGS training by up to 13.4x\nwith comparable or superior quality and surpasses the current SOTA in\nlightweight models by up to 1.4x speedup. For high-quality reconstruction\ntasks, LiteGS sets a new accuracy record and decreases the training time by an\norder of magnitude."
                },
                "authors": [
                    {
                        "name": "Kaimin Liao"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Luchao Wang"
                    },
                    {
                        "name": "Yaohua Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yaohua Tang"
                },
                "author": "Yaohua Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01199v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01199v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23292v1",
                "updated": "2025-07-31T07:10:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    10,
                    39,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T07:10:39Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    10,
                    39,
                    3,
                    212,
                    0
                ],
                "title": "SequenceLayers: Sequence Processing and Streaming Neural Networks Made\n  Easy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SequenceLayers: Sequence Processing and Streaming Neural Networks Made\n  Easy"
                },
                "summary": "We introduce a neural network layer API and library for sequence modeling,\ndesigned for easy creation of sequence models that can be executed both\nlayer-by-layer (e.g., teacher-forced training) and step-by-step (e.g.,\nautoregressive sampling). To achieve this, layers define an explicit\nrepresentation of their state over time (e.g., a Transformer KV cache, a\nconvolution buffer, an RNN hidden state), and a step method that evolves that\nstate, tested to give identical results to a stateless layer-wise invocation.\nThis and other aspects of the SequenceLayers contract enables complex models to\nbe immediately streamable, mitigates a wide range of common bugs arising in\nboth streaming and parallel sequence processing, and can be implemented in any\ndeep learning library. A composable and declarative API, along with a\ncomprehensive suite of layers and combinators, streamlines the construction of\nproduction-scale models from simple streamable components while preserving\nstrong correctness guarantees. Our current implementations of SequenceLayers\n(JAX, TensorFlow 2) are available at https://github.com/google/sequence-layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a neural network layer API and library for sequence modeling,\ndesigned for easy creation of sequence models that can be executed both\nlayer-by-layer (e.g., teacher-forced training) and step-by-step (e.g.,\nautoregressive sampling). To achieve this, layers define an explicit\nrepresentation of their state over time (e.g., a Transformer KV cache, a\nconvolution buffer, an RNN hidden state), and a step method that evolves that\nstate, tested to give identical results to a stateless layer-wise invocation.\nThis and other aspects of the SequenceLayers contract enables complex models to\nbe immediately streamable, mitigates a wide range of common bugs arising in\nboth streaming and parallel sequence processing, and can be implemented in any\ndeep learning library. A composable and declarative API, along with a\ncomprehensive suite of layers and combinators, streamlines the construction of\nproduction-scale models from simple streamable components while preserving\nstrong correctness guarantees. Our current implementations of SequenceLayers\n(JAX, TensorFlow 2) are available at https://github.com/google/sequence-layers."
                },
                "authors": [
                    {
                        "name": "RJ Skerry-Ryan"
                    },
                    {
                        "name": "Julian Salazar"
                    },
                    {
                        "name": "Soroosh Mariooryad"
                    },
                    {
                        "name": "David Kao"
                    },
                    {
                        "name": "Daisy Stanton"
                    },
                    {
                        "name": "Eric Battenberg"
                    },
                    {
                        "name": "Matt Shannon"
                    },
                    {
                        "name": "Ron J. Weiss"
                    },
                    {
                        "name": "Robin Scheibler"
                    },
                    {
                        "name": "Jonas Rothfuss"
                    },
                    {
                        "name": "Tom Bagby"
                    }
                ],
                "author_detail": {
                    "name": "Tom Bagby"
                },
                "author": "Tom Bagby",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07966v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07966v3",
                "updated": "2025-07-30T16:55:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    55,
                    33,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-10T17:47:40Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    47,
                    40,
                    3,
                    191,
                    0
                ],
                "title": "Scaling RL to Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling RL to Long Videos"
                },
                "summary": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B\nsupports processing up to 8,192 video frames per video, and configurable FPS\nsettings. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B\nsupports processing up to 8,192 video frames per video, and configurable FPS\nsettings. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames)."
                },
                "authors": [
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Baifeng Shi"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Hanrong Ye"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Xiaojuan Qi"
                    },
                    {
                        "name": "Sifei Liu"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Code at https://github.com/NVlabs/Long-RL and model at\n  https://huggingface.co/Efficient-Large-Model/LongVILA-R1-7B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07966v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07966v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22801v1",
                "updated": "2025-07-30T16:04:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    4,
                    1,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T16:04:01Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    4,
                    1,
                    2,
                    211,
                    0
                ],
                "title": "DSPE: Profit Maximization in Edge-Cloud Storage System using Dynamic\n  Space Partitioning with Erasure Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSPE: Profit Maximization in Edge-Cloud Storage System using Dynamic\n  Space Partitioning with Erasure Code"
                },
                "summary": "Edge Storage Systems have emerged as a critical enabler of low latency data\naccess in modern cloud networks by bringing storage and computation closer to\nend users. However, the limited storage capacity of edge servers poses\nsignificant challenges in handling high volume and latency sensitive data\naccess requests, particularly under dynamic workloads. In this work, we propose\na profit driven framework that integrates three key mechanisms which are\ncollaborative caching, erasure coding, and elastic storage partitioning. Unlike\ntraditional replication, erasure coding enables space efficient redundancy,\nallowing data to be reconstructed from any subset of K out of K plus M coded\nblocks. We dynamically partition each edge server s storage into private and\npublic regions. The private region is further subdivided among access points\nbased on their incoming request rates, enabling adaptive control over data\nlocality and ownership. We design a data placement and replacement policy that\ndetermines how and where to store or evict coded data blocks to maximize data\naccess within deadlines. While the private region serves requests from local\nAPs, the public region handles cooperative storage requests from neighboring\nservers. Our proposed Dynamic Space Partitioning and Elastic caching strategy\nis evaluated on both synthetic and real world traces from Netflix and Spotify.\nExperimental results show that our method improves overall system profitability\nby approximately 5 to 8% compared to state of the art approaches under varied\nworkload conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Storage Systems have emerged as a critical enabler of low latency data\naccess in modern cloud networks by bringing storage and computation closer to\nend users. However, the limited storage capacity of edge servers poses\nsignificant challenges in handling high volume and latency sensitive data\naccess requests, particularly under dynamic workloads. In this work, we propose\na profit driven framework that integrates three key mechanisms which are\ncollaborative caching, erasure coding, and elastic storage partitioning. Unlike\ntraditional replication, erasure coding enables space efficient redundancy,\nallowing data to be reconstructed from any subset of K out of K plus M coded\nblocks. We dynamically partition each edge server s storage into private and\npublic regions. The private region is further subdivided among access points\nbased on their incoming request rates, enabling adaptive control over data\nlocality and ownership. We design a data placement and replacement policy that\ndetermines how and where to store or evict coded data blocks to maximize data\naccess within deadlines. While the private region serves requests from local\nAPs, the public region handles cooperative storage requests from neighboring\nservers. Our proposed Dynamic Space Partitioning and Elastic caching strategy\nis evaluated on both synthetic and real world traces from Netflix and Spotify.\nExperimental results show that our method improves overall system profitability\nby approximately 5 to 8% compared to state of the art approaches under varied\nworkload conditions."
                },
                "authors": [
                    {
                        "name": "Shubhradeep Roy"
                    },
                    {
                        "name": "Suvarthi Sarkar"
                    },
                    {
                        "name": "Vivek Verma"
                    },
                    {
                        "name": "Aryabartta Sahu"
                    }
                ],
                "author_detail": {
                    "name": "Aryabartta Sahu"
                },
                "author": "Aryabartta Sahu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22636v1",
                "updated": "2025-07-30T12:55:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    55,
                    55,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T12:55:55Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    55,
                    55,
                    2,
                    211,
                    0
                ],
                "title": "All-gluon amplitudes with off-shell recursion in multiplet bases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All-gluon amplitudes with off-shell recursion in multiplet bases"
                },
                "summary": "The efficient computation of color-summed QCD amplitudes at high parton\nmultiplicities remains a central challenge for precision collider predictions.\nExisting approaches using trace, color-flow, or adjoint bases suffer from\nnon-orthogonality, which complicates the color algebra and scales poorly with\nmultiplicity. In this work, we present an off-shell recursive framework for\ncomputing all-gluon tree-level amplitudes directly in orthogonal multiplet\nbases. Utilizing Wigner $6j$ coefficients, we construct an algorithm that\nbuilds multiplet-projected off-shell currents from lower-point currents. By\noptimizing the recursion through partial summation and caching, we find that\nthe computational complexity of calculating $n$-gluon color-summed squared\namplitudes scales as $\\mathcal{O}(17^n)$. This demonstrates the potential\ncompetitiveness of multiplet bases for high-multiplicity processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficient computation of color-summed QCD amplitudes at high parton\nmultiplicities remains a central challenge for precision collider predictions.\nExisting approaches using trace, color-flow, or adjoint bases suffer from\nnon-orthogonality, which complicates the color algebra and scales poorly with\nmultiplicity. In this work, we present an off-shell recursive framework for\ncomputing all-gluon tree-level amplitudes directly in orthogonal multiplet\nbases. Utilizing Wigner $6j$ coefficients, we construct an algorithm that\nbuilds multiplet-projected off-shell currents from lower-point currents. By\noptimizing the recursion through partial summation and caching, we find that\nthe computational complexity of calculating $n$-gluon color-summed squared\namplitudes scales as $\\mathcal{O}(17^n)$. This demonstrates the potential\ncompetitiveness of multiplet bases for high-multiplicity processes."
                },
                "authors": [
                    {
                        "name": "Oskar Bolinder"
                    },
                    {
                        "name": "Rikkert Frederix"
                    },
                    {
                        "name": "Malin Sjodahl"
                    }
                ],
                "author_detail": {
                    "name": "Malin Sjodahl"
                },
                "author": "Malin Sjodahl",
                "arxiv_comment": "15 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20984v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20984v2",
                "updated": "2025-07-30T06:29:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    6,
                    29,
                    40,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-28T16:45:14Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    45,
                    14,
                    0,
                    209,
                    0
                ],
                "title": "SmallThinker: A Family of Efficient Large Language Models Natively\n  Trained for Local Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmallThinker: A Family of Efficient Large Language Models Natively\n  Trained for Local Deployment"
                },
                "summary": "While frontier large language models (LLMs) continue to push capability\nboundaries, their deployment remains confined to GPU-powered cloud\ninfrastructure. We challenge this paradigm with SmallThinker, a family of LLMs\nnatively designed - not adapted - for the unique constraints of local devices:\nweak computational power, limited memory, and slow storage. Unlike traditional\napproaches that mainly compress existing models built for clouds, we architect\nSmallThinker from the ground up to thrive within these limitations. Our\ninnovation lies in a deployment-aware architecture that transforms constraints\ninto design principles. First, We introduce a two-level sparse structure\ncombining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward\nnetworks, drastically reducing computational demands without sacrificing model\ncapacity. Second, to conquer the I/O bottleneck of slow storage, we design a\npre-attention router that enables our co-designed inference engine to prefetch\nexpert parameters from storage while computing attention, effectively hiding\nstorage latency that would otherwise cripple on-device inference. Third, for\nmemory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to\nslash KV cache requirements. We release SmallThinker-4B-A0.6B and\nSmallThinker-21B-A3B, which achieve state-of-the-art performance scores and\neven outperform larger LLMs. Remarkably, our co-designed system mostly\neliminates the need for expensive GPU hardware: with Q4_0 quantization, both\nmodels exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB\nand 8GB of memory respectively. SmallThinker is publicly available at\nhf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and\nhf.co/PowerInfer/SmallThinker-21BA3B-Instruct.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While frontier large language models (LLMs) continue to push capability\nboundaries, their deployment remains confined to GPU-powered cloud\ninfrastructure. We challenge this paradigm with SmallThinker, a family of LLMs\nnatively designed - not adapted - for the unique constraints of local devices:\nweak computational power, limited memory, and slow storage. Unlike traditional\napproaches that mainly compress existing models built for clouds, we architect\nSmallThinker from the ground up to thrive within these limitations. Our\ninnovation lies in a deployment-aware architecture that transforms constraints\ninto design principles. First, We introduce a two-level sparse structure\ncombining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward\nnetworks, drastically reducing computational demands without sacrificing model\ncapacity. Second, to conquer the I/O bottleneck of slow storage, we design a\npre-attention router that enables our co-designed inference engine to prefetch\nexpert parameters from storage while computing attention, effectively hiding\nstorage latency that would otherwise cripple on-device inference. Third, for\nmemory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to\nslash KV cache requirements. We release SmallThinker-4B-A0.6B and\nSmallThinker-21B-A3B, which achieve state-of-the-art performance scores and\neven outperform larger LLMs. Remarkably, our co-designed system mostly\neliminates the need for expensive GPU hardware: with Q4_0 quantization, both\nmodels exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB\nand 8GB of memory respectively. SmallThinker is publicly available at\nhf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and\nhf.co/PowerInfer/SmallThinker-21BA3B-Instruct."
                },
                "authors": [
                    {
                        "name": "Yixin Song"
                    },
                    {
                        "name": "Zhenliang Xue"
                    },
                    {
                        "name": "Dongliang Wei"
                    },
                    {
                        "name": "Feiyang Chen"
                    },
                    {
                        "name": "Jianxiang Gao"
                    },
                    {
                        "name": "Junchen Liu"
                    },
                    {
                        "name": "Hangyu Liang"
                    },
                    {
                        "name": "Guangshuo Qin"
                    },
                    {
                        "name": "Chengrong Tian"
                    },
                    {
                        "name": "Bo Wen"
                    },
                    {
                        "name": "Longyu Zhao"
                    },
                    {
                        "name": "Xinrui Zheng"
                    },
                    {
                        "name": "Zeyu Mi"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20984v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20984v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19442v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19442v3",
                "updated": "2025-07-30T05:24:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    5,
                    24,
                    46,
                    2,
                    211,
                    0
                ],
                "published": "2024-12-27T04:17:57Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    17,
                    57,
                    4,
                    362,
                    0
                ],
                "title": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management"
                },
                "summary": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Tianhao Tang"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Nicole Hu"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_comment": "Accepted to TMLR 2025. The revised version incorporates more papers\n  and has been further polished",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19442v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19442v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05655v1",
                "updated": "2025-07-29T12:42:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    29,
                    12,
                    42,
                    24,
                    1,
                    210,
                    0
                ],
                "published": "2025-07-29T12:42:24Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    12,
                    42,
                    24,
                    1,
                    210,
                    0
                ],
                "title": "Blockchain-Based Decentralized Domain Name System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blockchain-Based Decentralized Domain Name System"
                },
                "summary": "The current Domain Name System (DNS) infrastructure faces critical\nvulnerabilities including poisoning attacks, censorship mechanisms, and\ncentralized points of failure that compromise internet freedom and security.\nRecent incidents such as DNS poisoning attacks on ISP customers highlight the\nurgent need for resilient alternatives. This paper presents a novel\nblockchain-based Decentralized Domain Name System (DDNS). We designed a\nspecialized Proof-of-Work blockchain to maximize support for DNS-related\nprotocols and achieve node decentralization. The system integrates our\nblockchain with IPFS for distributed storage, implements cryptographic\nprimitives for end-to-end trust signatures, and achieves Never Trust, Always\nVerify zero-trust verification. Our implementation achieves 15-second domain\nrecord propagation times, supports 20 standard DNS record types, and provides\nperpetual free .ddns domains. The system has been deployed across distributed\ninfrastructure in San Jose, Los Angeles, and Orange County, demonstrating\npractical scalability and resistance to traditional DNS manipulation\ntechniques. Performance evaluation shows the system can handle up to Max Theor.\nTPS 1,111.1 tx/s (minimal transactions) and Max Theor. TPS 266.7 tx/s (regular\ntransactions) for domain operations while maintaining sub-second query\nresolution through intelligent caching mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current Domain Name System (DNS) infrastructure faces critical\nvulnerabilities including poisoning attacks, censorship mechanisms, and\ncentralized points of failure that compromise internet freedom and security.\nRecent incidents such as DNS poisoning attacks on ISP customers highlight the\nurgent need for resilient alternatives. This paper presents a novel\nblockchain-based Decentralized Domain Name System (DDNS). We designed a\nspecialized Proof-of-Work blockchain to maximize support for DNS-related\nprotocols and achieve node decentralization. The system integrates our\nblockchain with IPFS for distributed storage, implements cryptographic\nprimitives for end-to-end trust signatures, and achieves Never Trust, Always\nVerify zero-trust verification. Our implementation achieves 15-second domain\nrecord propagation times, supports 20 standard DNS record types, and provides\nperpetual free .ddns domains. The system has been deployed across distributed\ninfrastructure in San Jose, Los Angeles, and Orange County, demonstrating\npractical scalability and resistance to traditional DNS manipulation\ntechniques. Performance evaluation shows the system can handle up to Max Theor.\nTPS 1,111.1 tx/s (minimal transactions) and Max Theor. TPS 266.7 tx/s (regular\ntransactions) for domain operations while maintaining sub-second query\nresolution through intelligent caching mechanisms."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    },
                    {
                        "name": "Peter Trinh"
                    },
                    {
                        "name": "Alma Nkemla"
                    },
                    {
                        "name": "Amuru Serikyaku"
                    },
                    {
                        "name": "Edward Tatchim"
                    },
                    {
                        "name": "Osman Sharaf"
                    }
                ],
                "author_detail": {
                    "name": "Osman Sharaf"
                },
                "author": "Osman Sharaf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00904v1",
                "updated": "2025-07-29T03:08:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    29,
                    3,
                    8,
                    31,
                    1,
                    210,
                    0
                ],
                "published": "2025-07-29T03:08:31Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    3,
                    8,
                    31,
                    1,
                    210,
                    0
                ],
                "title": "Forecasting LLM Inference Performance via Hardware-Agnostic Analytical\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting LLM Inference Performance via Hardware-Agnostic Analytical\n  Modeling"
                },
                "summary": "Large language models (LLMs) have been increasingly deployed as local agents\non personal devices with CPUs, NPUs and integrated GPUs. However, forecasting\ninference performance on devices with such heterogeneity remains challenging\ndue to the dynamic compute and memory demands. Existing approaches rely on GPU\nbenchmarking or machine learning-based latency predictors, which are often\nhardware-specific and lack generalizability. To this end, we introduce LIFE, a\nlightweight and modular analytical framework that is comprised of modular\nanalytical model of operators, configurable to characterize LLM inference\nworkloads in a hardware and dataset-agnostic manner. LIFE characterizes the\ninfluence of software and model optimizations, such as quantization, KV cache\ncompression, LoRA adapters, chunked prefill, different attentions, and operator\nfusion, on performance metrics such as time-to-first-token (TTFT),\ntime-per-output-token (TPOT) and tokens-per-second (TPS). LIFE enables\nperformance forecasting using only hardware specifications, such as TOPS and\nmemory bandwidth, without requiring extensive dataset benchmarking. We validate\nLIFE's forecasting with inference on AMD Ryzen CPUs, NPUs, iGPUs and NVIDIA\nV100 GPUs, with Llama2-7B variants, demonstrating the utility of LIFE in\nforecasting LLM performance through lens of system efficiency to enable\nefficient LLM deployment across different hardware platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been increasingly deployed as local agents\non personal devices with CPUs, NPUs and integrated GPUs. However, forecasting\ninference performance on devices with such heterogeneity remains challenging\ndue to the dynamic compute and memory demands. Existing approaches rely on GPU\nbenchmarking or machine learning-based latency predictors, which are often\nhardware-specific and lack generalizability. To this end, we introduce LIFE, a\nlightweight and modular analytical framework that is comprised of modular\nanalytical model of operators, configurable to characterize LLM inference\nworkloads in a hardware and dataset-agnostic manner. LIFE characterizes the\ninfluence of software and model optimizations, such as quantization, KV cache\ncompression, LoRA adapters, chunked prefill, different attentions, and operator\nfusion, on performance metrics such as time-to-first-token (TTFT),\ntime-per-output-token (TPOT) and tokens-per-second (TPS). LIFE enables\nperformance forecasting using only hardware specifications, such as TOPS and\nmemory bandwidth, without requiring extensive dataset benchmarking. We validate\nLIFE's forecasting with inference on AMD Ryzen CPUs, NPUs, iGPUs and NVIDIA\nV100 GPUs, with Llama2-7B variants, demonstrating the utility of LIFE in\nforecasting LLM performance through lens of system efficiency to enable\nefficient LLM deployment across different hardware platforms."
                },
                "authors": [
                    {
                        "name": "Rajeev Patwari"
                    },
                    {
                        "name": "Ashish Sirasao"
                    },
                    {
                        "name": "Devleena Das"
                    }
                ],
                "author_detail": {
                    "name": "Devleena Das"
                },
                "author": "Devleena Das",
                "arxiv_comment": "10 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24358v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24358v2",
                "updated": "2025-07-28T20:44:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    20,
                    44,
                    23,
                    0,
                    209,
                    0
                ],
                "published": "2025-03-31T17:37:32Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    37,
                    32,
                    0,
                    90,
                    0
                ],
                "title": "SQuat: Subspace-orthogonal KV Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQuat: Subspace-orthogonal KV Cache Quantization"
                },
                "summary": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms."
                },
                "authors": [
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Ligong Han"
                    },
                    {
                        "name": "Kai Xu"
                    },
                    {
                        "name": "Akash Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Akash Srivastava"
                },
                "author": "Akash Srivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24358v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24358v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.13349v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.13349v3",
                "updated": "2025-07-28T14:11:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    11,
                    53,
                    0,
                    209,
                    0
                ],
                "published": "2023-11-22T12:34:51Z",
                "published_parsed": [
                    2023,
                    11,
                    22,
                    12,
                    34,
                    51,
                    2,
                    326,
                    0
                ],
                "title": "REDS: Resource-Efficient Deep Subnetworks for Dynamic Resource\n  Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REDS: Resource-Efficient Deep Subnetworks for Dynamic Resource\n  Constraints"
                },
                "summary": "Deep learning models deployed on edge devices frequently encounter resource\nvariability, which arises from fluctuating energy levels, timing constraints,\nor prioritization of other critical tasks within the system. State-of-the-art\nmachine learning pipelines generate resource-agnostic models that are not\ncapable to adapt at runtime. In this work, we introduce Resource-Efficient Deep\nSubnetworks (REDS) to tackle model adaptation to variable resources. In\ncontrast to the state-of-the-art, REDS leverages structured sparsity\nconstructively by exploiting permutation invariance of neurons, which allows\nfor hardware-specific optimizations. Specifically, REDS achieves computational\nefficiency by (1) skipping sequential computational blocks identified by a\nnovel iterative knapsack optimizer, and (2) taking advantage of data cache by\nre-arranging the order of operations in REDS computational graph. REDS supports\nconventional deep networks frequently deployed on the edge and provides\ncomputational benefits even for small and simple networks. We evaluate REDS on\neight benchmark architectures trained on the Visual Wake Words, Google Speech\nCommands, Fashion-MNIST, CIFAR-10 and ImageNet-1K datasets, and test on four\noff-the-shelf mobile and embedded hardware platforms. We provide a theoretical\nresult and empirical evidence demonstrating REDS' outstanding performance in\nterms of submodels' test set accuracy, and demonstrate an adaptation time in\nresponse to dynamic resource constraints of under 40$\\mu$s, utilizing a\nfully-connected network on Arduino Nano 33 BLE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models deployed on edge devices frequently encounter resource\nvariability, which arises from fluctuating energy levels, timing constraints,\nor prioritization of other critical tasks within the system. State-of-the-art\nmachine learning pipelines generate resource-agnostic models that are not\ncapable to adapt at runtime. In this work, we introduce Resource-Efficient Deep\nSubnetworks (REDS) to tackle model adaptation to variable resources. In\ncontrast to the state-of-the-art, REDS leverages structured sparsity\nconstructively by exploiting permutation invariance of neurons, which allows\nfor hardware-specific optimizations. Specifically, REDS achieves computational\nefficiency by (1) skipping sequential computational blocks identified by a\nnovel iterative knapsack optimizer, and (2) taking advantage of data cache by\nre-arranging the order of operations in REDS computational graph. REDS supports\nconventional deep networks frequently deployed on the edge and provides\ncomputational benefits even for small and simple networks. We evaluate REDS on\neight benchmark architectures trained on the Visual Wake Words, Google Speech\nCommands, Fashion-MNIST, CIFAR-10 and ImageNet-1K datasets, and test on four\noff-the-shelf mobile and embedded hardware platforms. We provide a theoretical\nresult and empirical evidence demonstrating REDS' outstanding performance in\nterms of submodels' test set accuracy, and demonstrate an adaptation time in\nresponse to dynamic resource constraints of under 40$\\mu$s, utilizing a\nfully-connected network on Arduino Nano 33 BLE."
                },
                "authors": [
                    {
                        "name": "Francesco Corti"
                    },
                    {
                        "name": "Balz Maag"
                    },
                    {
                        "name": "Joachim Schauer"
                    },
                    {
                        "name": "Ulrich Pferschy"
                    },
                    {
                        "name": "Olga Saukh"
                    }
                ],
                "author_detail": {
                    "name": "Olga Saukh"
                },
                "author": "Olga Saukh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.13349v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.13349v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20677v1",
                "updated": "2025-07-28T09:59:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    59,
                    22,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T09:59:22Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    59,
                    22,
                    0,
                    209,
                    0
                ],
                "title": "Quantum Circuit Caches and Compressors for Low Latency, High Throughput\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Circuit Caches and Compressors for Low Latency, High Throughput\n  Computing"
                },
                "summary": "Utility-scale quantum programs contain operations on the order of $>10^{15}$\nwhich must be prepared and piped from a classical co-processor to the control\nunit of the quantum device. The latency of this process significantly increases\nwith the size of the program: existing high-level classical representations of\nquantum programs are typically memory intensive and do not na\\\"ively\nefficiently scale to the degree required to execute utility-scale programs in\nreal-time. To combat this limitation, we propose the utilization of high-level\nquantum circuit caches and compressors. The first save on the time associated\nwith repetitive tasks and sub-circuits, and the latter are useful for\nrepresenting the programs/circuits in memory-efficient formats. We present\nnumerical evidence that caches and compressors can offer five orders of\nmagnitude lower latencies during the automatic transpilation of extremely large\nquantum circuits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utility-scale quantum programs contain operations on the order of $>10^{15}$\nwhich must be prepared and piped from a classical co-processor to the control\nunit of the quantum device. The latency of this process significantly increases\nwith the size of the program: existing high-level classical representations of\nquantum programs are typically memory intensive and do not na\\\"ively\nefficiently scale to the degree required to execute utility-scale programs in\nreal-time. To combat this limitation, we propose the utilization of high-level\nquantum circuit caches and compressors. The first save on the time associated\nwith repetitive tasks and sub-circuits, and the latter are useful for\nrepresenting the programs/circuits in memory-efficient formats. We present\nnumerical evidence that caches and compressors can offer five orders of\nmagnitude lower latencies during the automatic transpilation of extremely large\nquantum circuits."
                },
                "authors": [
                    {
                        "name": "Ioana Moflic"
                    },
                    {
                        "name": "Alan Robertson"
                    },
                    {
                        "name": "Simon J. Devitt"
                    },
                    {
                        "name": "Alexandru Paler"
                    }
                ],
                "author_detail": {
                    "name": "Alexandru Paler"
                },
                "author": "Alexandru Paler",
                "arxiv_comment": "accepted at Q-CORE workshop of the QCE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20613v1",
                "updated": "2025-07-28T08:27:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    8,
                    27,
                    40,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T08:27:40Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    8,
                    27,
                    40,
                    0,
                    209,
                    0
                ],
                "title": "Enhancing Large Multimodal Models with Adaptive Sparsity and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Large Multimodal Models with Adaptive Sparsity and KV Cache\n  Compression"
                },
                "summary": "Large multimodal models (LMMs) have advanced significantly by integrating\nvisual encoders with extensive language models, enabling robust reasoning\ncapabilities. However, compressing LMMs for deployment on edge devices remains\na critical challenge. In this work, we propose an adaptive search algorithm\nthat optimizes sparsity and KV cache compression to enhance LMM efficiency.\nUtilizing the Tree-structured Parzen Estimator, our method dynamically adjusts\npruning ratios and KV cache quantization bandwidth across different LMM layers,\nusing model performance as the optimization objective. This approach uniquely\ncombines pruning with key-value cache quantization and incorporates a fast\npruning technique that eliminates the need for additional fine-tuning or weight\nadjustments, achieving efficient compression without compromising accuracy.\nComprehensive evaluations on benchmark datasets, including LLaVA-1.5 7B and\n13B, demonstrate our method superiority over state-of-the-art techniques such\nas SparseGPT and Wanda across various compression levels. Notably, our\nframework automatic allocation of KV cache compression resources sets a new\nstandard in LMM optimization, delivering memory efficiency without sacrificing\nmuch performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large multimodal models (LMMs) have advanced significantly by integrating\nvisual encoders with extensive language models, enabling robust reasoning\ncapabilities. However, compressing LMMs for deployment on edge devices remains\na critical challenge. In this work, we propose an adaptive search algorithm\nthat optimizes sparsity and KV cache compression to enhance LMM efficiency.\nUtilizing the Tree-structured Parzen Estimator, our method dynamically adjusts\npruning ratios and KV cache quantization bandwidth across different LMM layers,\nusing model performance as the optimization objective. This approach uniquely\ncombines pruning with key-value cache quantization and incorporates a fast\npruning technique that eliminates the need for additional fine-tuning or weight\nadjustments, achieving efficient compression without compromising accuracy.\nComprehensive evaluations on benchmark datasets, including LLaVA-1.5 7B and\n13B, demonstrate our method superiority over state-of-the-art techniques such\nas SparseGPT and Wanda across various compression levels. Notably, our\nframework automatic allocation of KV cache compression resources sets a new\nstandard in LMM optimization, delivering memory efficiency without sacrificing\nmuch performance."
                },
                "authors": [
                    {
                        "name": "Te Zhang"
                    },
                    {
                        "name": "Yuheng Li"
                    },
                    {
                        "name": "Junxiang Wang"
                    },
                    {
                        "name": "Lujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Lujun Li"
                },
                "author": "Lujun Li",
                "arxiv_comment": "6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01760v2",
                "updated": "2025-07-28T04:25:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    4,
                    25,
                    58,
                    0,
                    209,
                    0
                ],
                "published": "2024-10-02T17:14:47Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    14,
                    47,
                    2,
                    276,
                    0
                ],
                "title": "Learning-Augmented Online Caching: New Upper Bounds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-Augmented Online Caching: New Upper Bounds"
                },
                "summary": "We address the problem of learning-augmented online caching in the scenario\nwhen each request is accompanied by a prediction of the next occurrence of the\nrequested page. We improve currently known bounds on the competitive ratio of\nthe BlindOracle algorithm, which evicts a page predicted to be requested last.\nWe also prove a lower bound on the competitive ratio of any randomized\nalgorithm and show that a combination of the BlindOracle with the Marker\nalgorithm achieves a competitive ratio that is optimal up to some constant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the problem of learning-augmented online caching in the scenario\nwhen each request is accompanied by a prediction of the next occurrence of the\nrequested page. We improve currently known bounds on the competitive ratio of\nthe BlindOracle algorithm, which evicts a page predicted to be requested last.\nWe also prove a lower bound on the competitive ratio of any randomized\nalgorithm and show that a combination of the BlindOracle with the Marker\nalgorithm achieves a competitive ratio that is optimal up to some constant."
                },
                "authors": [
                    {
                        "name": "Daniel Skachkov"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    },
                    {
                        "name": "Yuri Dorn"
                    },
                    {
                        "name": "Alexander Demin"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Demin"
                },
                "author": "Alexander Demin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08161v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08161v2",
                "updated": "2025-07-27T09:58:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    27,
                    9,
                    58,
                    25,
                    6,
                    208,
                    0
                ],
                "published": "2025-06-09T19:13:16Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    19,
                    13,
                    16,
                    0,
                    160,
                    0
                ],
                "title": "GATE: Geometry-Aware Trained Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GATE: Geometry-Aware Trained Encoding"
                },
                "summary": "The encoding of input parameters is one of the fundamental building blocks of\nneural network algorithms. Its goal is to map the input data to a\nhigher-dimensional space, typically supported by trained feature vectors. The\nmapping is crucial for the efficiency and approximation quality of neural\nnetworks. We propose a novel geometry-aware encoding called GATE that stores\nfeature vectors on the surface of triangular meshes. Our encoding is suitable\nfor neural rendering-related algorithms, for example, neural radiance caching.\nIt also avoids limitations of previous hash-based encoding schemes, such as\nhash collisions, selection of resolution versus scene size, and divergent\nmemory access. Our approach decouples feature vector density from geometry\ndensity using mesh colors, while allowing for finer control over neural network\ntraining and adaptive level-of-detail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The encoding of input parameters is one of the fundamental building blocks of\nneural network algorithms. Its goal is to map the input data to a\nhigher-dimensional space, typically supported by trained feature vectors. The\nmapping is crucial for the efficiency and approximation quality of neural\nnetworks. We propose a novel geometry-aware encoding called GATE that stores\nfeature vectors on the surface of triangular meshes. Our encoding is suitable\nfor neural rendering-related algorithms, for example, neural radiance caching.\nIt also avoids limitations of previous hash-based encoding schemes, such as\nhash collisions, selection of resolution versus scene size, and divergent\nmemory access. Our approach decouples feature vector density from geometry\ndensity using mesh colors, while allowing for finer control over neural network\ntraining and adaptive level-of-detail."
                },
                "authors": [
                    {
                        "name": "Jakub Bokšanský"
                    },
                    {
                        "name": "Daniel Meister"
                    },
                    {
                        "name": "Carsten Benthin"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Benthin"
                },
                "author": "Carsten Benthin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08161v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08161v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20173v1",
                "updated": "2025-07-27T08:25:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    27,
                    8,
                    25,
                    8,
                    6,
                    208,
                    0
                ],
                "published": "2025-07-27T08:25:08Z",
                "published_parsed": [
                    2025,
                    7,
                    27,
                    8,
                    25,
                    8,
                    6,
                    208,
                    0
                ],
                "title": "High-Performance Parallel Optimization of the Fish School Behaviour on\n  the Setonix Platform Using OpenMP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Performance Parallel Optimization of the Fish School Behaviour on\n  the Setonix Platform Using OpenMP"
                },
                "summary": "This paper presents an in-depth investigation into the high-performance\nparallel optimization of the Fish School Behaviour (FSB) algorithm on the\nSetonix supercomputing platform using the OpenMP framework. Given the\nincreasing demand for enhanced computational capabilities for complex,\nlarge-scale calculations across diverse domains, there's an imperative need for\noptimized parallel algorithms and computing structures. The FSB algorithm,\ninspired by nature's social behavior patterns, provides an ideal platform for\nparallelization due to its iterative and computationally intensive nature. This\nstudy leverages the capabilities of the Setonix platform and the OpenMP\nframework to analyze various aspects of multi-threading, such as thread counts,\nscheduling strategies, and OpenMP constructs, aiming to discern patterns and\nstrategies that can elevate program performance. Experiments were designed to\nrigorously test different configurations, and our results not only offer\ninsights for parallel optimization of FSB on Setonix but also provide valuable\nreferences for other parallel computational research using OpenMP. Looking\nforward, other factors, such as cache behavior and thread scheduling strategies\nat micro and macro levels, hold potential for further exploration and\noptimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an in-depth investigation into the high-performance\nparallel optimization of the Fish School Behaviour (FSB) algorithm on the\nSetonix supercomputing platform using the OpenMP framework. Given the\nincreasing demand for enhanced computational capabilities for complex,\nlarge-scale calculations across diverse domains, there's an imperative need for\noptimized parallel algorithms and computing structures. The FSB algorithm,\ninspired by nature's social behavior patterns, provides an ideal platform for\nparallelization due to its iterative and computationally intensive nature. This\nstudy leverages the capabilities of the Setonix platform and the OpenMP\nframework to analyze various aspects of multi-threading, such as thread counts,\nscheduling strategies, and OpenMP constructs, aiming to discern patterns and\nstrategies that can elevate program performance. Experiments were designed to\nrigorously test different configurations, and our results not only offer\ninsights for parallel optimization of FSB on Setonix but also provide valuable\nreferences for other parallel computational research using OpenMP. Looking\nforward, other factors, such as cache behavior and thread scheduling strategies\nat micro and macro levels, hold potential for further exploration and\noptimization."
                },
                "authors": [
                    {
                        "name": "Haitian Wang"
                    },
                    {
                        "name": "Long Qin"
                    }
                ],
                "author_detail": {
                    "name": "Long Qin"
                },
                "author": "Long Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20116v1",
                "updated": "2025-07-27T03:45:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    27,
                    3,
                    45,
                    7,
                    6,
                    208,
                    0
                ],
                "published": "2025-07-27T03:45:07Z",
                "published_parsed": [
                    2025,
                    7,
                    27,
                    3,
                    45,
                    7,
                    6,
                    208,
                    0
                ],
                "title": "Accelerating Containerized Service Delivery at the Network Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Containerized Service Delivery at the Network Edge"
                },
                "summary": "Efficient container image distribution is crucial for enabling machine\nlearning inference at the network edge, where resource limitations and dynamic\nnetwork conditions create significant challenges. In this paper, we present\nPeerSync, a decentralized P2P-based system designed to optimize image\ndistribution in edge environments. PeerSync employs a popularity- and\nnetwork-aware download engine that dynamically adapts to content popularity and\nreal-time network conditions using a sliding window mechanism. PeerSync further\nintegrates automated tracker election for rapid peer discovery and dynamic\ncache management for efficient storage utilization. We implement PeerSync with\n8000+ lines of Rust code and test its performance extensively on both physical\nedge devices and Docker-based emulations. Experimental results show that\nPeerSync delivers a remarkable speed increase of 2.72$\\times$, 1.79$\\times$,\nand 1.28$\\times$ compared to the Baseline, Dragonfly, and Kraken, respectively,\nwhile significantly reducing peak cross-network traffic by 90.72\\% under\ncongested and varying network conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient container image distribution is crucial for enabling machine\nlearning inference at the network edge, where resource limitations and dynamic\nnetwork conditions create significant challenges. In this paper, we present\nPeerSync, a decentralized P2P-based system designed to optimize image\ndistribution in edge environments. PeerSync employs a popularity- and\nnetwork-aware download engine that dynamically adapts to content popularity and\nreal-time network conditions using a sliding window mechanism. PeerSync further\nintegrates automated tracker election for rapid peer discovery and dynamic\ncache management for efficient storage utilization. We implement PeerSync with\n8000+ lines of Rust code and test its performance extensively on both physical\nedge devices and Docker-based emulations. Experimental results show that\nPeerSync delivers a remarkable speed increase of 2.72$\\times$, 1.79$\\times$,\nand 1.28$\\times$ compared to the Baseline, Dragonfly, and Kraken, respectively,\nwhile significantly reducing peak cross-network traffic by 90.72\\% under\ncongested and varying network conditions."
                },
                "authors": [
                    {
                        "name": "Yinuo Deng"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Dongjing Wang"
                    },
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Wenzhuo Qian"
                    },
                    {
                        "name": "Jianwei Yin"
                    },
                    {
                        "name": "Schahram Dustdar"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18300v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18300v3",
                "updated": "2025-07-27T00:40:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    27,
                    0,
                    40,
                    47,
                    6,
                    208,
                    0
                ],
                "published": "2025-05-23T18:46:10Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    18,
                    46,
                    10,
                    4,
                    143,
                    0
                ],
                "title": "Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient\n  Nonlinear MCMC on General Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient\n  Nonlinear MCMC on General Graphs"
                },
                "summary": "We propose a history-driven target (HDT) framework in Markov Chain Monte\nCarlo (MCMC) to improve any random walk algorithm on discrete state spaces,\nsuch as general undirected graphs, for efficient sampling from target\ndistribution $\\boldsymbol{\\mu}$. With broad applications in network science and\ndistributed optimization, recent innovations like the self-repellent random\nwalk (SRRW) achieve near-zero variance by prioritizing under-sampled states\nthrough transition kernel modifications based on past visit frequencies.\nHowever, SRRW's reliance on explicit computation of transition probabilities\nfor all neighbors at each step introduces substantial computational overhead,\nwhile its strict dependence on time-reversible Markov chains excludes advanced\nnon-reversible MCMC methods. To overcome these limitations, instead of direct\nmodification of transition kernel, HDT introduces a history-dependent target\ndistribution $\\boldsymbol{\\pi}[\\mathbf{x}]$ to replace the original target\n$\\boldsymbol{\\mu}$ in any graph sampler, where $\\mathbf{x}$ represents the\nempirical measure of past visits. This design preserves lightweight\nimplementation by requiring only local information between the current and\nproposed states and achieves compatibility with both reversible and\nnon-reversible MCMC samplers, while retaining unbiased samples with target\ndistribution $\\boldsymbol{\\mu}$ and near-zero variance performance. Extensive\nexperiments in graph sampling demonstrate consistent performance gains, and a\nmemory-efficient Least Recently Used (LRU) cache ensures scalability to large\ngeneral graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a history-driven target (HDT) framework in Markov Chain Monte\nCarlo (MCMC) to improve any random walk algorithm on discrete state spaces,\nsuch as general undirected graphs, for efficient sampling from target\ndistribution $\\boldsymbol{\\mu}$. With broad applications in network science and\ndistributed optimization, recent innovations like the self-repellent random\nwalk (SRRW) achieve near-zero variance by prioritizing under-sampled states\nthrough transition kernel modifications based on past visit frequencies.\nHowever, SRRW's reliance on explicit computation of transition probabilities\nfor all neighbors at each step introduces substantial computational overhead,\nwhile its strict dependence on time-reversible Markov chains excludes advanced\nnon-reversible MCMC methods. To overcome these limitations, instead of direct\nmodification of transition kernel, HDT introduces a history-dependent target\ndistribution $\\boldsymbol{\\pi}[\\mathbf{x}]$ to replace the original target\n$\\boldsymbol{\\mu}$ in any graph sampler, where $\\mathbf{x}$ represents the\nempirical measure of past visits. This design preserves lightweight\nimplementation by requiring only local information between the current and\nproposed states and achieves compatibility with both reversible and\nnon-reversible MCMC samplers, while retaining unbiased samples with target\ndistribution $\\boldsymbol{\\mu}$ and near-zero variance performance. Extensive\nexperiments in graph sampling demonstrate consistent performance gains, and a\nmemory-efficient Least Recently Used (LRU) cache ensures scalability to large\ngeneral graphs."
                },
                "authors": [
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Yi-Ting Ma"
                    },
                    {
                        "name": "Do Young Eun"
                    }
                ],
                "author_detail": {
                    "name": "Do Young Eun"
                },
                "author": "Do Young Eun",
                "arxiv_comment": "Accepted at ICML 2025 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18300v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18300v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20030v1",
                "updated": "2025-07-26T18:20:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    18,
                    20,
                    25,
                    5,
                    207,
                    0
                ],
                "published": "2025-07-26T18:20:25Z",
                "published_parsed": [
                    2025,
                    7,
                    26,
                    18,
                    20,
                    25,
                    5,
                    207,
                    0
                ],
                "title": "FAEDKV: Infinite-Window Fourier Transform for Unbiased KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAEDKV: Infinite-Window Fourier Transform for Unbiased KV Cache\n  Compression"
                },
                "summary": "The efficacy of Large Language Models (LLMs) in long-context tasks is often\nhampered by the substantial memory footprint and computational demands of the\nKey-Value (KV) cache. Current compression strategies, including token eviction\nand learned projections, frequently lead to biased representations -- either by\noveremphasizing recent/high-attention tokens or by repeatedly degrading\ninformation from earlier context -- and may require costly model retraining. We\npresent FAEDKV (Frequency-Adaptive Infinite-Window for KV cache), a novel,\ntraining-free KV cache compression framework that ensures unbiased information\nretention. FAEDKV operates by transforming the KV cache into the frequency\ndomain using a proposed Infinite-Window Fourier Transform (IWDFT). This\napproach allows for the equalized contribution of all tokens to the compressed\nrepresentation, effectively preserving both early and recent contextual\ninformation. A preliminary frequency ablation study identifies critical\nspectral components for layer-wise, targeted compression. Experiments on\nLongBench benchmark demonstrate FAEDKV's superiority over existing methods by\nup to 22\\%. In addition, our method shows superior, position-agnostic retrieval\naccuracy on the Needle-In-A-Haystack task compared to compression based\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficacy of Large Language Models (LLMs) in long-context tasks is often\nhampered by the substantial memory footprint and computational demands of the\nKey-Value (KV) cache. Current compression strategies, including token eviction\nand learned projections, frequently lead to biased representations -- either by\noveremphasizing recent/high-attention tokens or by repeatedly degrading\ninformation from earlier context -- and may require costly model retraining. We\npresent FAEDKV (Frequency-Adaptive Infinite-Window for KV cache), a novel,\ntraining-free KV cache compression framework that ensures unbiased information\nretention. FAEDKV operates by transforming the KV cache into the frequency\ndomain using a proposed Infinite-Window Fourier Transform (IWDFT). This\napproach allows for the equalized contribution of all tokens to the compressed\nrepresentation, effectively preserving both early and recent contextual\ninformation. A preliminary frequency ablation study identifies critical\nspectral components for layer-wise, targeted compression. Experiments on\nLongBench benchmark demonstrate FAEDKV's superiority over existing methods by\nup to 22\\%. In addition, our method shows superior, position-agnostic retrieval\naccuracy on the Needle-In-A-Haystack task compared to compression based\napproaches."
                },
                "authors": [
                    {
                        "name": "Runchao Li"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Mu Sheng"
                    },
                    {
                        "name": "Xianxuan Long"
                    },
                    {
                        "name": "Haotian Yu"
                    },
                    {
                        "name": "Pan Li"
                    }
                ],
                "author_detail": {
                    "name": "Pan Li"
                },
                "author": "Pan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03140v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03140v2",
                "updated": "2025-07-26T15:25:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    15,
                    25,
                    22,
                    5,
                    207,
                    0
                ],
                "published": "2025-04-04T03:30:15Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    3,
                    30,
                    15,
                    4,
                    94,
                    0
                ],
                "title": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models"
                },
                "summary": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation."
                },
                "authors": [
                    {
                        "name": "Xuran Ma"
                    },
                    {
                        "name": "Yexin Liu"
                    },
                    {
                        "name": "Yaofu Liu"
                    },
                    {
                        "name": "Xianfeng Wu"
                    },
                    {
                        "name": "Mingzhe Zheng"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Ser-Nam Lim"
                    },
                    {
                        "name": "Harry Yang"
                    }
                ],
                "author_detail": {
                    "name": "Harry Yang"
                },
                "author": "Harry Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03140v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03140v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09720v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09720v3",
                "updated": "2025-07-26T15:13:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    15,
                    13,
                    56,
                    5,
                    207,
                    0
                ],
                "published": "2025-02-13T19:11:40Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    11,
                    40,
                    3,
                    44,
                    0
                ],
                "title": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs"
                },
                "summary": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent works have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Metas SpinQuant (perplexity 7.3), OstQuant (7.3) and QuaRot\n(8.2). Comparisons on bigger models (up to 70B) and on various LLM evaluation\nbenchmarks confirm uniform superiority of NestQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent works have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Metas SpinQuant (perplexity 7.3), OstQuant (7.3) and QuaRot\n(8.2). Comparisons on bigger models (up to 70B) and on various LLM evaluation\nbenchmarks confirm uniform superiority of NestQuant."
                },
                "authors": [
                    {
                        "name": "Semyon Savkin"
                    },
                    {
                        "name": "Eitan Porat"
                    },
                    {
                        "name": "Or Ordentlich"
                    },
                    {
                        "name": "Yury Polyanskiy"
                    }
                ],
                "author_detail": {
                    "name": "Yury Polyanskiy"
                },
                "author": "Yury Polyanskiy",
                "arxiv_comment": "23 pages; Accepted at the 42nd International Conference on Machine\n  Learning (ICML 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09720v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09720v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20677v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20677v2",
                "updated": "2025-07-26T13:33:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    13,
                    33,
                    6,
                    5,
                    207,
                    0
                ],
                "published": "2024-12-30T03:05:45Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "title": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional performance across\ndiverse natural language processing tasks. However, as the model size and the\ninput sequence's length increase, the linearly increasing key-value (KV) cache\nsignificantly degrades inference throughput. Therefore, grouped-query attention\n(GQA), as an alternative to multi-head attention (MHA), has been widely\nintroduced into LLMs. In this work, we propose a cost-effective method for\nconverting MHA into GQA with any compression ratio of KV heads. The key point\nof our method lies in the application of Procrustes analysis to the attention\nheads, which enhances the similarity among attention heads while preserving\ncomputational invariance, thereby improving the model's post-training\nperformance. Subsequently, we employ $\\mathit{L_0}$ regularization to prune\nredundant parameters. The model after pruning can be adapted to the standard\nGQA framework. Experimental results show that our strategy can compress up to\n87.5\\% KV heads of LLaMA2-7B model and 75\\% KV heads of Sheared-LLaMA-1.3B with\nacceptable performance degradation. Our code is released at\nhttps://github.com/fpcsong/mha2gqa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional performance across\ndiverse natural language processing tasks. However, as the model size and the\ninput sequence's length increase, the linearly increasing key-value (KV) cache\nsignificantly degrades inference throughput. Therefore, grouped-query attention\n(GQA), as an alternative to multi-head attention (MHA), has been widely\nintroduced into LLMs. In this work, we propose a cost-effective method for\nconverting MHA into GQA with any compression ratio of KV heads. The key point\nof our method lies in the application of Procrustes analysis to the attention\nheads, which enhances the similarity among attention heads while preserving\ncomputational invariance, thereby improving the model's post-training\nperformance. Subsequently, we employ $\\mathit{L_0}$ regularization to prune\nredundant parameters. The model after pruning can be adapted to the standard\nGQA framework. Experimental results show that our strategy can compress up to\n87.5\\% KV heads of LLaMA2-7B model and 75\\% KV heads of Sheared-LLaMA-1.3B with\nacceptable performance degradation. Our code is released at\nhttps://github.com/fpcsong/mha2gqa."
                },
                "authors": [
                    {
                        "name": "Qingyun Jin"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Zengchang Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zengchang Qin"
                },
                "author": "Zengchang Qin",
                "arxiv_comment": "13 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20677v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20677v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19823v1",
                "updated": "2025-07-26T06:43:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    6,
                    43,
                    14,
                    5,
                    207,
                    0
                ],
                "published": "2025-07-26T06:43:14Z",
                "published_parsed": [
                    2025,
                    7,
                    26,
                    6,
                    43,
                    14,
                    5,
                    207,
                    0
                ],
                "title": "HCAttention: Extreme KV Cache Compression via Heterogeneous Attention\n  Computing for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HCAttention: Extreme KV Cache Compression via Heterogeneous Attention\n  Computing for LLMs"
                },
                "summary": "Processing long-context inputs with large language models presents a\nsignificant challenge due to the enormous memory requirements of the Key-Value\n(KV) cache during inference. Existing KV cache compression methods exhibit\nnoticeable performance degradation when memory is reduced by more than 85%.\nAdditionally, strategies that leverage GPU-CPU collaboration for approximate\nattention remain underexplored in this setting. We propose HCAttention, a\nheterogeneous attention computation framework that integrates key quantization,\nvalue offloading, and dynamic KV eviction to enable efficient inference under\nextreme memory constraints. The method is compatible with existing transformer\narchitectures and does not require model fine-tuning. Experimental results on\nthe LongBench benchmark demonstrate that our approach preserves the accuracy of\nfull-attention model while shrinking the KV cache memory footprint to 25% of\nits original size. Remarkably, it stays competitive with only 12.5% of the\ncache, setting a new state-of-the-art in LLM KV cache compression. To the best\nof our knowledge, HCAttention is the first to extend the Llama-3-8B model to\nprocess 4 million tokens on a single A100 GPU with 80GB memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long-context inputs with large language models presents a\nsignificant challenge due to the enormous memory requirements of the Key-Value\n(KV) cache during inference. Existing KV cache compression methods exhibit\nnoticeable performance degradation when memory is reduced by more than 85%.\nAdditionally, strategies that leverage GPU-CPU collaboration for approximate\nattention remain underexplored in this setting. We propose HCAttention, a\nheterogeneous attention computation framework that integrates key quantization,\nvalue offloading, and dynamic KV eviction to enable efficient inference under\nextreme memory constraints. The method is compatible with existing transformer\narchitectures and does not require model fine-tuning. Experimental results on\nthe LongBench benchmark demonstrate that our approach preserves the accuracy of\nfull-attention model while shrinking the KV cache memory footprint to 25% of\nits original size. Remarkably, it stays competitive with only 12.5% of the\ncache, setting a new state-of-the-art in LLM KV cache compression. To the best\nof our knowledge, HCAttention is the first to extend the Llama-3-8B model to\nprocess 4 million tokens on a single A100 GPU with 80GB memory."
                },
                "authors": [
                    {
                        "name": "Dongquan Yang"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Xiaotian Yu"
                    },
                    {
                        "name": "Xianbiao Qi"
                    },
                    {
                        "name": "Rong Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Rong Xiao"
                },
                "author": "Rong Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19427v1",
                "updated": "2025-07-25T16:53:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    16,
                    53,
                    13,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T16:53:13Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    16,
                    53,
                    13,
                    4,
                    206,
                    0
                ],
                "title": "Step-3 is Large yet Affordable: Model-system Co-design for\n  Cost-effective Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step-3 is Large yet Affordable: Model-system Co-design for\n  Cost-effective Decoding"
                },
                "summary": "Large language models (LLMs) face low hardware efficiency during decoding,\nespecially for long-context reasoning tasks. This paper introduces Step-3, a\n321B-parameter VLM with hardware-aware model-system co-design optimized for\nminimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel\nMulti-Matrix Factorization Attention (MFA) mechanism that significantly reduces\nboth KV cache size and computation while maintaining high attention\nexpressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed\ninference system that decouples attention and Feed-Forward Network (FFN) layers\ninto specialized subsystems. This co-design achieves unprecedented cost\nefficiency: Step-3 significantly reduces theoretical decoding costs compared\nwith models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at\nlonger context. Step-3 achieves low cost while activating 38B parameters per\ntoken (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that\nhardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are\ncritical to cost-effectiveness. We perform a head-to-head comparison with\nDeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs\nachieves a decoding throughput of up to 4,039 tokens per second per GPU under\n50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324\nin the same setup and sets a new Pareto frontier for LLM decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face low hardware efficiency during decoding,\nespecially for long-context reasoning tasks. This paper introduces Step-3, a\n321B-parameter VLM with hardware-aware model-system co-design optimized for\nminimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel\nMulti-Matrix Factorization Attention (MFA) mechanism that significantly reduces\nboth KV cache size and computation while maintaining high attention\nexpressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed\ninference system that decouples attention and Feed-Forward Network (FFN) layers\ninto specialized subsystems. This co-design achieves unprecedented cost\nefficiency: Step-3 significantly reduces theoretical decoding costs compared\nwith models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at\nlonger context. Step-3 achieves low cost while activating 38B parameters per\ntoken (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that\nhardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are\ncritical to cost-effectiveness. We perform a head-to-head comparison with\nDeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs\nachieves a decoding throughput of up to 4,039 tokens per second per GPU under\n50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324\nin the same setup and sets a new Pareto frontier for LLM decoding."
                },
                "authors": [
                    {
                        "name": "StepFun"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Bojun Wang"
                    },
                    {
                        "name": "Changyi Wan"
                    },
                    {
                        "name": "Guanzhe Huang"
                    },
                    {
                        "name": "Hanpeng Hu"
                    },
                    {
                        "name": "Haonan Jia"
                    },
                    {
                        "name": "Hao Nie"
                    },
                    {
                        "name": "Mingliang Li"
                    },
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Siyu Chen"
                    },
                    {
                        "name": "Song Yuan"
                    },
                    {
                        "name": "Wuxun Xie"
                    },
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Xing Chen"
                    },
                    {
                        "name": "Xingping Yang"
                    },
                    {
                        "name": "Xuelin Zhang"
                    },
                    {
                        "name": "Yanbo Yu"
                    },
                    {
                        "name": "Yaoyu Wang"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Yimin Jiang"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Yuanwei Lu"
                    },
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Ka Man Lo"
                    },
                    {
                        "name": "Ailin Huang"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Boyu Chen"
                    },
                    {
                        "name": "Changxin Miao"
                    },
                    {
                        "name": "Chang Lou"
                    },
                    {
                        "name": "Chen Hu"
                    },
                    {
                        "name": "Chen Xu"
                    },
                    {
                        "name": "Chenfeng Yu"
                    },
                    {
                        "name": "Chengyuan Yao"
                    },
                    {
                        "name": "Daokuan Lv"
                    },
                    {
                        "name": "Dapeng Shi"
                    },
                    {
                        "name": "Deshan Sun"
                    },
                    {
                        "name": "Ding Huang"
                    },
                    {
                        "name": "Dingyuan Hu"
                    },
                    {
                        "name": "Dongqing Pang"
                    },
                    {
                        "name": "Enle Liu"
                    },
                    {
                        "name": "Fajie Zhang"
                    },
                    {
                        "name": "Fanqi Wan"
                    },
                    {
                        "name": "Gulin Yan"
                    },
                    {
                        "name": "Han Zhang"
                    },
                    {
                        "name": "Han Zhou"
                    },
                    {
                        "name": "Hanghao Wu"
                    },
                    {
                        "name": "Hangyu Guo"
                    },
                    {
                        "name": "Hanqi Chen"
                    },
                    {
                        "name": "Hanshan Zhang"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Haocheng Zhang"
                    },
                    {
                        "name": "Haolong Yan"
                    },
                    {
                        "name": "Haoran Lv"
                    },
                    {
                        "name": "Haoran Wei"
                    },
                    {
                        "name": "Hebin Zhou"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Hongxin Li"
                    },
                    {
                        "name": "Hongyu Zhou"
                    },
                    {
                        "name": "Hongyuan Wang"
                    },
                    {
                        "name": "Huiyong Guo"
                    },
                    {
                        "name": "Jia Wang"
                    },
                    {
                        "name": "Jiahao Gong"
                    },
                    {
                        "name": "Jialing Xie"
                    },
                    {
                        "name": "Jian Zhou"
                    },
                    {
                        "name": "Jianjian Sun"
                    },
                    {
                        "name": "Jiaoren Wu"
                    },
                    {
                        "name": "Jiaran Zhang"
                    },
                    {
                        "name": "Jiayu Liu"
                    },
                    {
                        "name": "Jie Cheng"
                    },
                    {
                        "name": "Jie Luo"
                    },
                    {
                        "name": "Jie Yan"
                    },
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Jieyi Hou"
                    },
                    {
                        "name": "Jinguang Zhang"
                    },
                    {
                        "name": "Jinlan Cao"
                    },
                    {
                        "name": "Jisheng Yin"
                    },
                    {
                        "name": "Junfeng Liu"
                    },
                    {
                        "name": "Junhao Huang"
                    },
                    {
                        "name": "Junzhe Lin"
                    },
                    {
                        "name": "Kaijun Tan"
                    },
                    {
                        "name": "Kaixiang Li"
                    },
                    {
                        "name": "Kang An"
                    },
                    {
                        "name": "Kangheng Lin"
                    },
                    {
                        "name": "Kenkun Liu"
                    },
                    {
                        "name": "Lei Yang"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Liangyu Chen"
                    },
                    {
                        "name": "Lieyu Shi"
                    },
                    {
                        "name": "Liguo Tan"
                    },
                    {
                        "name": "Lin Lin"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Lina Chen"
                    },
                    {
                        "name": "Liwen Huang"
                    },
                    {
                        "name": "Liying Shi"
                    },
                    {
                        "name": "Longlong Gu"
                    },
                    {
                        "name": "Mei Chen"
                    },
                    {
                        "name": "Mengqiang Ren"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Mingzhe Chen"
                    },
                    {
                        "name": "Na Wang"
                    },
                    {
                        "name": "Nan Wu"
                    },
                    {
                        "name": "Qi Han"
                    },
                    {
                        "name": "Qian Zhao"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Qianni Liu"
                    },
                    {
                        "name": "Qiaohui Chen"
                    },
                    {
                        "name": "Qiling Wu"
                    },
                    {
                        "name": "Qinglin He"
                    },
                    {
                        "name": "Qinyuan Tan"
                    },
                    {
                        "name": "Qiufeng Wang"
                    },
                    {
                        "name": "Qiuping Wu"
                    },
                    {
                        "name": "Qiuyan Liang"
                    },
                    {
                        "name": "Quan Sun"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Ruihang Miao"
                    },
                    {
                        "name": "Ruosi Wan"
                    },
                    {
                        "name": "Ruyan Guo"
                    },
                    {
                        "name": "Shangwu Zhong"
                    },
                    {
                        "name": "Shaoliang Pang"
                    },
                    {
                        "name": "Shengjie Fan"
                    },
                    {
                        "name": "Shijie Shang"
                    },
                    {
                        "name": "Shilei Jiang"
                    },
                    {
                        "name": "Shiliang Yang"
                    },
                    {
                        "name": "Shiming Hao"
                    },
                    {
                        "name": "Shuli Gao"
                    },
                    {
                        "name": "Siming Huang"
                    },
                    {
                        "name": "Siqi Liu"
                    },
                    {
                        "name": "Tiancheng Cao"
                    },
                    {
                        "name": "Tianhao Cheng"
                    },
                    {
                        "name": "Tianhao Peng"
                    },
                    {
                        "name": "Wang You"
                    },
                    {
                        "name": "Wei Ji"
                    },
                    {
                        "name": "Wen Sun"
                    },
                    {
                        "name": "Wenjin Deng"
                    },
                    {
                        "name": "Wenqing He"
                    },
                    {
                        "name": "Wenzhen Zheng"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Xiangwen Kong"
                    },
                    {
                        "name": "Xianzhen Luo"
                    },
                    {
                        "name": "Xiaobo Yang"
                    },
                    {
                        "name": "Xiaojia Liu"
                    },
                    {
                        "name": "Xiaoxiao Ren"
                    },
                    {
                        "name": "Xin Han"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Xin Wu"
                    },
                    {
                        "name": "Xu Zhao"
                    },
                    {
                        "name": "Yanan Wei"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Yangguang Li"
                    },
                    {
                        "name": "Yangshijie Xu"
                    },
                    {
                        "name": "Yanming Xu"
                    },
                    {
                        "name": "Yaqiang Shi"
                    },
                    {
                        "name": "Yeqing Shen"
                    },
                    {
                        "name": "Yi Yang"
                    },
                    {
                        "name": "Yifei Yang"
                    },
                    {
                        "name": "Yifeng Gong"
                    },
                    {
                        "name": "Yihan Chen"
                    },
                    {
                        "name": "Yijing Yang"
                    },
                    {
                        "name": "Yinmin Zhang"
                    },
                    {
                        "name": "Yizhuang Zhou"
                    },
                    {
                        "name": "Yuanhao Ding"
                    },
                    {
                        "name": "Yuantao Fan"
                    },
                    {
                        "name": "Yuanzhen Yang"
                    },
                    {
                        "name": "Yuchu Luo"
                    },
                    {
                        "name": "Yue Peng"
                    },
                    {
                        "name": "Yufan Lu"
                    },
                    {
                        "name": "Yuhang Deng"
                    },
                    {
                        "name": "Yuhe Yin"
                    },
                    {
                        "name": "Yujie Liu"
                    },
                    {
                        "name": "Yukun Chen"
                    },
                    {
                        "name": "Yuling Zhao"
                    },
                    {
                        "name": "Yun Mou"
                    },
                    {
                        "name": "Yunlong Li"
                    },
                    {
                        "name": "Yunzhou Ju"
                    },
                    {
                        "name": "Yusheng Li"
                    },
                    {
                        "name": "Yuxiang Yang"
                    },
                    {
                        "name": "Yuxiang Zhang"
                    },
                    {
                        "name": "Yuyang Chen"
                    },
                    {
                        "name": "Zejia Weng"
                    },
                    {
                        "name": "Zhe Xie"
                    },
                    {
                        "name": "Zheng Ge"
                    },
                    {
                        "name": "Zheng Gong"
                    },
                    {
                        "name": "Zhenyi Lu"
                    },
                    {
                        "name": "Zhewei Huang"
                    },
                    {
                        "name": "Zhichao Chang"
                    },
                    {
                        "name": "Zhiguo Huang"
                    },
                    {
                        "name": "Zhirui Wang"
                    },
                    {
                        "name": "Zidong Yang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Ziqi Wang"
                    },
                    {
                        "name": "Zixin Zhang"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhang"
                },
                "author": "Xiangyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19367v1",
                "updated": "2025-07-25T15:17:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    17,
                    29,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T15:17:29Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    17,
                    29,
                    4,
                    206,
                    0
                ],
                "title": "Empowering IoT Firmware Secure Update with Customization Rights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering IoT Firmware Secure Update with Customization Rights"
                },
                "summary": "Firmware updates remain the primary line of defense for IoT devices; however,\nthe update channel itself has become a well-established attack vector. Existing\ndefenses mainly focus on securing monolithic firmware images, leaving\nmodule-level customization -a growing user demand-largely unprotected and\ninsufficiently explored. To address this gap, we conduct a pilot study on the\nupdate workflows of 200 Linux-based IoT devices across 23 vendors, uncovering\nfive previously undocumented vulnerabilities caused by customization practices.\nA broader analysis of update-related CVEs from 2020 to 2024 reveals that over\nhalf originate from customization-induced issues. These findings highlight a\ncritical yet underexamined reality: as customization increases, so does the\nattack surface, while current defenses fail to keep pace. We propose IMUP\n(Integrity-Centric Modular Update Platform), the first framework to address two\nkey challenges: constructing a trustworthy cross-module integrity chain and\nscaling update performance under mass customization. IMUP combines three\ntechniques: per-module chameleon hashing for integrity, server-side\nproof-of-work offloading to reduce device overhead, and server-side caching to\nreuse module combinations, minimizing rebuild costs. Security analysis shows\nthat even when 95 percent of secret keys are exposed, forging a valid image\nincurs over 300 times the cost of the legitimate server. Experiments on\nheterogeneous IoT devices demonstrate that IMUP reduces server-side generation\ntime by 2.9 times and device downtime by 5.9 times compared to a\npackage-manager baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Firmware updates remain the primary line of defense for IoT devices; however,\nthe update channel itself has become a well-established attack vector. Existing\ndefenses mainly focus on securing monolithic firmware images, leaving\nmodule-level customization -a growing user demand-largely unprotected and\ninsufficiently explored. To address this gap, we conduct a pilot study on the\nupdate workflows of 200 Linux-based IoT devices across 23 vendors, uncovering\nfive previously undocumented vulnerabilities caused by customization practices.\nA broader analysis of update-related CVEs from 2020 to 2024 reveals that over\nhalf originate from customization-induced issues. These findings highlight a\ncritical yet underexamined reality: as customization increases, so does the\nattack surface, while current defenses fail to keep pace. We propose IMUP\n(Integrity-Centric Modular Update Platform), the first framework to address two\nkey challenges: constructing a trustworthy cross-module integrity chain and\nscaling update performance under mass customization. IMUP combines three\ntechniques: per-module chameleon hashing for integrity, server-side\nproof-of-work offloading to reduce device overhead, and server-side caching to\nreuse module combinations, minimizing rebuild costs. Security analysis shows\nthat even when 95 percent of secret keys are exposed, forging a valid image\nincurs over 300 times the cost of the legitimate server. Experiments on\nheterogeneous IoT devices demonstrate that IMUP reduces server-side generation\ntime by 2.9 times and device downtime by 5.9 times compared to a\npackage-manager baseline."
                },
                "authors": [
                    {
                        "name": "Weihao Chen"
                    },
                    {
                        "name": "Yansong Gao"
                    },
                    {
                        "name": "Boyu Kuang"
                    },
                    {
                        "name": "Jin B. Hong"
                    },
                    {
                        "name": "Yuqing Zhang"
                    },
                    {
                        "name": "Anmin Fu"
                    }
                ],
                "author_detail": {
                    "name": "Anmin Fu"
                },
                "author": "Anmin Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01802v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01802v2",
                "updated": "2025-07-24T19:44:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    19,
                    44,
                    36,
                    3,
                    205,
                    0
                ],
                "published": "2025-02-03T20:30:25Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    20,
                    30,
                    25,
                    0,
                    34,
                    0
                ],
                "title": "General kinetic ion induced electron emission model for metallic walls\n  applied to biased Z-pinch electrodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General kinetic ion induced electron emission model for metallic walls\n  applied to biased Z-pinch electrodes"
                },
                "summary": "A kinetic ion induced electron emission (IIEE) model for general applications\nis developed to obtain the emitted electron energy spectrum for a distribution\nof ion impacts on a metallic surface. We assume an ionization cascade mechanism\nand use empirical models for the ion and electron stopping powers. The emission\nspectrum and the secondary electron yield (SEY) are validated for a variety of\nmaterials. The IIEE model is used to study the effect of IIEE on the\nplasma-material interactions of Z-pinch electrodes. Un-magnetized\nBoltzmann-Poisson simulations are performed for a Z-pinch plasma doubly bounded\nby two biased copper electrodes with and without IIEE at bias potentials from 0\nto 9 kV. At the anode, the SEY decreases from 0 to 1 kV, but then increases at\nhigher bias potentials. At the cathode, the SEY is much larger due to higher\nenergy ion bombardment and grows with bias potential. As the bias potential\nincreases, the emitted cathode electrons are accelerated to higher energies\ninto the domain collisionally heating the plasma. Above 1 kV, the heating is\nstrong enough to increase the plasma potential. Despite SEY greater than 1,\nonly a classical sheath forms as opposed to a space-charge limited or inverse\nsheath due to the emitted electron flux not reaching the space charge current\nsaturation limits. Furthermore, the current in the emissionless cases saturates\nto a value lower than experiment. With IIEE, the current does not saturate and\ncontinues to increase with the 4 kV case matching most closely with experiment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A kinetic ion induced electron emission (IIEE) model for general applications\nis developed to obtain the emitted electron energy spectrum for a distribution\nof ion impacts on a metallic surface. We assume an ionization cascade mechanism\nand use empirical models for the ion and electron stopping powers. The emission\nspectrum and the secondary electron yield (SEY) are validated for a variety of\nmaterials. The IIEE model is used to study the effect of IIEE on the\nplasma-material interactions of Z-pinch electrodes. Un-magnetized\nBoltzmann-Poisson simulations are performed for a Z-pinch plasma doubly bounded\nby two biased copper electrodes with and without IIEE at bias potentials from 0\nto 9 kV. At the anode, the SEY decreases from 0 to 1 kV, but then increases at\nhigher bias potentials. At the cathode, the SEY is much larger due to higher\nenergy ion bombardment and grows with bias potential. As the bias potential\nincreases, the emitted cathode electrons are accelerated to higher energies\ninto the domain collisionally heating the plasma. Above 1 kV, the heating is\nstrong enough to increase the plasma potential. Despite SEY greater than 1,\nonly a classical sheath forms as opposed to a space-charge limited or inverse\nsheath due to the emitted electron flux not reaching the space charge current\nsaturation limits. Furthermore, the current in the emissionless cases saturates\nto a value lower than experiment. With IIEE, the current does not saturate and\ncontinues to increase with the 4 kV case matching most closely with experiment."
                },
                "authors": [
                    {
                        "name": "Chirag R. Skolar"
                    },
                    {
                        "name": "Kolter Bradshaw"
                    },
                    {
                        "name": "Manaure Francisquez"
                    },
                    {
                        "name": "Lucio Murillo"
                    },
                    {
                        "name": "Vignesh Krishna Kumar"
                    },
                    {
                        "name": "Bhuvana Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Bhuvana Srinivasan"
                },
                "author": "Bhuvana Srinivasan",
                "arxiv_comment": "21 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01802v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01802v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16870v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16870v2",
                "updated": "2025-07-24T17:30:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    30,
                    12,
                    3,
                    205,
                    0
                ],
                "published": "2025-03-21T05:58:18Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    58,
                    18,
                    4,
                    80,
                    0
                ],
                "title": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs"
                },
                "summary": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B."
                },
                "authors": [
                    {
                        "name": "Anshumann"
                    },
                    {
                        "name": "Mohd Abbas Zaidi"
                    },
                    {
                        "name": "Akhil Kedia"
                    },
                    {
                        "name": "Jinwoo Ahn"
                    },
                    {
                        "name": "Taehwak Kwon"
                    },
                    {
                        "name": "Kangwook Lee"
                    },
                    {
                        "name": "Haejun Lee"
                    },
                    {
                        "name": "Joohyung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Joohyung Lee"
                },
                "author": "Joohyung Lee",
                "arxiv_comment": "Accepted as Oral paper at ACL 2025. Source code is available at\n  https://github.com/akhilkedia/RandomSamplingKD . Anshumann, Mohd Abbas Zaidi\n  and Akhil Kedia have Equal Contribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16870v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16870v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04704v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04704v2",
                "updated": "2025-07-24T16:25:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    25,
                    51,
                    3,
                    205,
                    0
                ],
                "published": "2025-04-07T03:22:15Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    3,
                    22,
                    15,
                    0,
                    97,
                    0
                ],
                "title": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important"
                },
                "summary": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modification of the inference infrastructure and\nsignificant computation overhead. Based on the fact that the Large Language\nmodels are autoregressive models, we propose LagKV, a KV compression strategy\nonly relying on straight forward comparison among KV themselves. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on RULER benchmark show that, our approach\noutperforms SnapKV and StreamingLLM in different compression ratios. Especially\nin the 64-digit passkey retrieval task, our method outperforms the attention\nweight based method $H_2O$ over $50\\%$ with same compression ratios. Our code\nis available at https://github.com/AI-Lab-China-Merchants-Bank/LagKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modification of the inference infrastructure and\nsignificant computation overhead. Based on the fact that the Large Language\nmodels are autoregressive models, we propose LagKV, a KV compression strategy\nonly relying on straight forward comparison among KV themselves. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on RULER benchmark show that, our approach\noutperforms SnapKV and StreamingLLM in different compression ratios. Especially\nin the 64-digit passkey retrieval task, our method outperforms the attention\nweight based method $H_2O$ over $50\\%$ with same compression ratios. Our code\nis available at https://github.com/AI-Lab-China-Merchants-Bank/LagKV."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "JiaMing Zhang"
                    },
                    {
                        "name": "Xiong Li"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04704v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04704v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18446v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18446v1",
                "updated": "2025-07-24T14:30:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    30,
                    48,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T14:30:48Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    30,
                    48,
                    3,
                    205,
                    0
                ],
                "title": "Streaming Sortformer: Speaker Cache-Based Online Speaker Diarization\n  with Arrival-Time Ordering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming Sortformer: Speaker Cache-Based Online Speaker Diarization\n  with Arrival-Time Ordering"
                },
                "summary": "This paper presents a streaming extension for the Sortformer speaker\ndiarization framework, whose key property is the arrival-time ordering of\noutput speakers. The proposed approach employs an Arrival-Order Speaker Cache\n(AOSC) to store frame-level acoustic embeddings of previously observed\nspeakers. Unlike conventional speaker-tracing buffers, AOSC orders embeddings\nby speaker index corresponding to their arrival time order, and is dynamically\nupdated by selecting frames with the highest scores based on the model's past\npredictions. Notably, the number of stored embeddings per speaker is determined\ndynamically by the update mechanism, ensuring efficient cache utilization and\nprecise speaker tracking. Experiments on benchmark datasets confirm the\neffectiveness and flexibility of our approach, even in low-latency setups.\nThese results establish Streaming Sortformer as a robust solution for real-time\nmulti-speaker tracking and a foundation for streaming multi-talker speech\nprocessing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a streaming extension for the Sortformer speaker\ndiarization framework, whose key property is the arrival-time ordering of\noutput speakers. The proposed approach employs an Arrival-Order Speaker Cache\n(AOSC) to store frame-level acoustic embeddings of previously observed\nspeakers. Unlike conventional speaker-tracing buffers, AOSC orders embeddings\nby speaker index corresponding to their arrival time order, and is dynamically\nupdated by selecting frames with the highest scores based on the model's past\npredictions. Notably, the number of stored embeddings per speaker is determined\ndynamically by the update mechanism, ensuring efficient cache utilization and\nprecise speaker tracking. Experiments on benchmark datasets confirm the\neffectiveness and flexibility of our approach, even in low-latency setups.\nThese results establish Streaming Sortformer as a robust solution for real-time\nmulti-speaker tracking and a foundation for streaming multi-talker speech\nprocessing."
                },
                "authors": [
                    {
                        "name": "Ivan Medennikov"
                    },
                    {
                        "name": "Taejin Park"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "He Huang"
                    },
                    {
                        "name": "Kunal Dhawan"
                    },
                    {
                        "name": "Jinhan Wang"
                    },
                    {
                        "name": "Jagadeesh Balam"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Accepted to Interspeech 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18446v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18446v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18028v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18028v1",
                "updated": "2025-07-24T02:00:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    2,
                    0,
                    9,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T02:00:09Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    2,
                    0,
                    9,
                    3,
                    205,
                    0
                ],
                "title": "NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural\n  KV Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural\n  KV Database"
                },
                "summary": "Efficiently editing knowledge stored in large language models (LLMs) enables\nmodel updates without large-scale training. One possible solution is\nLocate-and-Edit (L\\&E), allowing simultaneous modifications of a massive number\nof facts. However, such editing may compromise the general abilities of LLMs\nand even result in forgetting edited facts when scaling up to thousands of\nedits. In this paper, we model existing linear L\\&E methods as querying a\nKey-Value (KV) database. From this perspective, we then propose NeuralDB, an\nediting framework that explicitly represents the edited facts as a neural KV\ndatabase equipped with a non-linear gated retrieval module, % In particular,\nour gated module only operates when inference involves the edited facts,\neffectively preserving the general abilities of LLMs. Comprehensive experiments\ninvolving the editing of 10,000 facts were conducted on the ZsRE and\nCounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results\ndemonstrate that NeuralDB not only excels in editing efficacy, generalization,\nspecificity, fluency, and consistency, but also preserves overall performance\nacross six representative text understanding and generation tasks. Further\nexperiments indicate that NeuralDB maintains its effectiveness even when scaled\nto 100,000 facts (\\textbf{50x} more than in prior work).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently editing knowledge stored in large language models (LLMs) enables\nmodel updates without large-scale training. One possible solution is\nLocate-and-Edit (L\\&E), allowing simultaneous modifications of a massive number\nof facts. However, such editing may compromise the general abilities of LLMs\nand even result in forgetting edited facts when scaling up to thousands of\nedits. In this paper, we model existing linear L\\&E methods as querying a\nKey-Value (KV) database. From this perspective, we then propose NeuralDB, an\nediting framework that explicitly represents the edited facts as a neural KV\ndatabase equipped with a non-linear gated retrieval module, % In particular,\nour gated module only operates when inference involves the edited facts,\neffectively preserving the general abilities of LLMs. Comprehensive experiments\ninvolving the editing of 10,000 facts were conducted on the ZsRE and\nCounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results\ndemonstrate that NeuralDB not only excels in editing efficacy, generalization,\nspecificity, fluency, and consistency, but also preserves overall performance\nacross six representative text understanding and generation tasks. Further\nexperiments indicate that NeuralDB maintains its effectiveness even when scaled\nto 100,000 facts (\\textbf{50x} more than in prior work)."
                },
                "authors": [
                    {
                        "name": "Weizhi Fei"
                    },
                    {
                        "name": "Hao Shi"
                    },
                    {
                        "name": "Jing Xu"
                    },
                    {
                        "name": "Jingchen Peng"
                    },
                    {
                        "name": "Jiazheng Li"
                    },
                    {
                        "name": "Jingzhao Zhang"
                    },
                    {
                        "name": "Bo Bai"
                    },
                    {
                        "name": "Wei Han"
                    },
                    {
                        "name": "Zhenyuan Chen"
                    },
                    {
                        "name": "Xueyan Niu"
                    }
                ],
                "author_detail": {
                    "name": "Xueyan Niu"
                },
                "author": "Xueyan Niu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18028v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18028v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17744v1",
                "updated": "2025-07-23T17:57:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    17,
                    57,
                    9,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-23T17:57:09Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    17,
                    57,
                    9,
                    2,
                    204,
                    0
                ],
                "title": "Yume: An Interactive World Generation Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yume: An Interactive World Generation Model"
                },
                "summary": "Yume aims to use images, text, or videos to create an interactive, realistic,\nand dynamic world, which allows exploration and control using peripheral\ndevices or neural signals. In this report, we present a preview version of\n\\method, which creates a dynamic world from an input image and allows\nexploration of the world using keyboard actions. To achieve this high-fidelity\nand interactive video world generation, we introduce a well-designed framework,\nwhich consists of four main components, including camera motion quantization,\nvideo generation architecture, advanced sampler, and model acceleration. First,\nwe quantize camera motions for stable training and user-friendly interaction\nusing keyboard inputs. Then, we introduce the Masked Video Diffusion\nTransformer~(MVDT) with a memory module for infinite video generation in an\nautoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM)\nand Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE)\nare introduced to the sampler for better visual quality and more precise\ncontrol. Moreover, we investigate model acceleration by synergistic\noptimization of adversarial distillation and caching mechanisms. We use the\nhigh-quality world exploration dataset \\sekai to train \\method, and it achieves\nremarkable results in diverse scenes and applications. All data, codebase, and\nmodel weights are available on https://github.com/stdstu12/YUME. Yume will\nupdate monthly to achieve its original goal. Project page:\nhttps://stdstu12.github.io/YUME-Project/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yume aims to use images, text, or videos to create an interactive, realistic,\nand dynamic world, which allows exploration and control using peripheral\ndevices or neural signals. In this report, we present a preview version of\n\\method, which creates a dynamic world from an input image and allows\nexploration of the world using keyboard actions. To achieve this high-fidelity\nand interactive video world generation, we introduce a well-designed framework,\nwhich consists of four main components, including camera motion quantization,\nvideo generation architecture, advanced sampler, and model acceleration. First,\nwe quantize camera motions for stable training and user-friendly interaction\nusing keyboard inputs. Then, we introduce the Masked Video Diffusion\nTransformer~(MVDT) with a memory module for infinite video generation in an\nautoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM)\nand Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE)\nare introduced to the sampler for better visual quality and more precise\ncontrol. Moreover, we investigate model acceleration by synergistic\noptimization of adversarial distillation and caching mechanisms. We use the\nhigh-quality world exploration dataset \\sekai to train \\method, and it achieves\nremarkable results in diverse scenes and applications. All data, codebase, and\nmodel weights are available on https://github.com/stdstu12/YUME. Yume will\nupdate monthly to achieve its original goal. Project page:\nhttps://stdstu12.github.io/YUME-Project/."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Mao"
                    },
                    {
                        "name": "Shaoheng Lin"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Chuanhao Li"
                    },
                    {
                        "name": "Wenshuo Peng"
                    },
                    {
                        "name": "Tong He"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    },
                    {
                        "name": "Mingmin Chi"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kaipeng Zhang"
                },
                "author": "Kaipeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17647v1",
                "updated": "2025-07-23T16:09:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    16,
                    9,
                    10,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-23T16:09:10Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    16,
                    9,
                    10,
                    2,
                    204,
                    0
                ],
                "title": "SHINE: A Scalable HNSW Index in Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SHINE: A Scalable HNSW Index in Disaggregated Memory"
                },
                "summary": "Approximate nearest neighbor (ANN) search is a fundamental problem in\ncomputer science for which in-memory graph-based methods, such as Hierarchical\nNavigable Small World (HNSW), perform exceptionally well. To scale beyond\nbillions of high-dimensional vectors, the index must be distributed. The\ndisaggregated memory architecture physically separates compute and memory into\ntwo distinct hardware units and has become popular in modern data centers. Both\nunits are connected via RDMA networks that allow compute nodes to directly\naccess remote memory and perform all the computations, posing unique challenges\nfor disaggregated indexes.\n  In this work, we propose a scalable HNSW index for ANN search in\ndisaggregated memory. In contrast to existing distributed approaches, which\npartition the graph at the cost of accuracy, our method builds a\ngraph-preserving index that reaches the same accuracy as a single-machine HNSW.\nContinuously fetching high-dimensional vector data from remote memory leads to\nsevere network bandwidth limitations, which we overcome by employing an\nefficient caching mechanism. Since answering a single query involves processing\nnumerous unique graph nodes, caching alone is not sufficient to achieve high\nscalability. We logically combine the caches of the compute nodes to increase\nthe overall cache effectiveness and confirm the efficiency and scalability of\nour method in our evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor (ANN) search is a fundamental problem in\ncomputer science for which in-memory graph-based methods, such as Hierarchical\nNavigable Small World (HNSW), perform exceptionally well. To scale beyond\nbillions of high-dimensional vectors, the index must be distributed. The\ndisaggregated memory architecture physically separates compute and memory into\ntwo distinct hardware units and has become popular in modern data centers. Both\nunits are connected via RDMA networks that allow compute nodes to directly\naccess remote memory and perform all the computations, posing unique challenges\nfor disaggregated indexes.\n  In this work, we propose a scalable HNSW index for ANN search in\ndisaggregated memory. In contrast to existing distributed approaches, which\npartition the graph at the cost of accuracy, our method builds a\ngraph-preserving index that reaches the same accuracy as a single-machine HNSW.\nContinuously fetching high-dimensional vector data from remote memory leads to\nsevere network bandwidth limitations, which we overcome by employing an\nefficient caching mechanism. Since answering a single query involves processing\nnumerous unique graph nodes, caching alone is not sufficient to achieve high\nscalability. We logically combine the caches of the compute nodes to increase\nthe overall cache effectiveness and confirm the efficiency and scalability of\nour method in our evaluation."
                },
                "authors": [
                    {
                        "name": "Manuel Widmoser"
                    },
                    {
                        "name": "Daniel Kocher"
                    },
                    {
                        "name": "Nikolaus Augsten"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaus Augsten"
                },
                "author": "Nikolaus Augsten",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16242v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16242v2",
                "updated": "2025-07-23T15:59:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    15,
                    59,
                    38,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-22T05:26:28Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    26,
                    28,
                    1,
                    203,
                    0
                ],
                "title": "Toward a Lightweight and Robust Design for Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward a Lightweight and Robust Design for Caching"
                },
                "summary": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice."
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Jiaji Zhang"
                    },
                    {
                        "name": "Xueyan Tang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16242v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16242v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17554v1",
                "updated": "2025-07-23T14:43:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    14,
                    43,
                    22,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-23T14:43:22Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    14,
                    43,
                    22,
                    2,
                    204,
                    0
                ],
                "title": "An h-space Based Adversarial Attack for Protection Against Few-shot\n  Personalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An h-space Based Adversarial Attack for Protection Against Few-shot\n  Personalization"
                },
                "summary": "The versatility of diffusion models in generating customized images from few\nsamples raises significant privacy concerns, particularly regarding\nunauthorized modifications of private content. This concerning issue has\nrenewed the efforts in developing protection mechanisms based on adversarial\nattacks, which generate effective perturbations to poison diffusion models. Our\nwork is motivated by the observation that these models exhibit a high degree of\nabstraction within their semantic latent space (`h-space'), which encodes\ncritical high-level features for generating coherent and meaningful content. In\nthis paper, we propose a novel anti-customization approach, called HAAD\n(h-space based Adversarial Attack for Diffusion models), that leverages\nadversarial attacks to craft perturbations based on the h-space that can\nefficiently degrade the image generation process. Building upon HAAD, we\nfurther introduce a more efficient variant, HAAD-KV, that constructs\nperturbations solely based on the KV parameters of the h-space. This strategy\noffers a stronger protection, that is computationally less expensive. Despite\ntheir simplicity, our methods outperform state-of-the-art adversarial attacks,\nhighlighting their effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The versatility of diffusion models in generating customized images from few\nsamples raises significant privacy concerns, particularly regarding\nunauthorized modifications of private content. This concerning issue has\nrenewed the efforts in developing protection mechanisms based on adversarial\nattacks, which generate effective perturbations to poison diffusion models. Our\nwork is motivated by the observation that these models exhibit a high degree of\nabstraction within their semantic latent space (`h-space'), which encodes\ncritical high-level features for generating coherent and meaningful content. In\nthis paper, we propose a novel anti-customization approach, called HAAD\n(h-space based Adversarial Attack for Diffusion models), that leverages\nadversarial attacks to craft perturbations based on the h-space that can\nefficiently degrade the image generation process. Building upon HAAD, we\nfurther introduce a more efficient variant, HAAD-KV, that constructs\nperturbations solely based on the KV parameters of the h-space. This strategy\noffers a stronger protection, that is computationally less expensive. Despite\ntheir simplicity, our methods outperform state-of-the-art adversarial attacks,\nhighlighting their effectiveness."
                },
                "authors": [
                    {
                        "name": "Xide Xu"
                    },
                    {
                        "name": "Sandesh Kamath"
                    },
                    {
                        "name": "Muhammad Atif Butt"
                    },
                    {
                        "name": "Bogdan Raducanu"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Raducanu"
                },
                "author": "Bogdan Raducanu",
                "arxiv_comment": "32 pages, 15 figures. Accepted by ACM Multimedia 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23956v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23956v3",
                "updated": "2025-07-23T11:42:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    11,
                    42,
                    3,
                    2,
                    204,
                    0
                ],
                "published": "2025-03-31T11:13:18Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference"
                },
                "summary": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches."
                },
                "authors": [
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Hao Zou"
                    },
                    {
                        "name": "Bochen Wang"
                    },
                    {
                        "name": "Ye Xi"
                    },
                    {
                        "name": "Zhen Xie"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23956v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23956v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17411v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17411v1",
                "updated": "2025-07-23T11:12:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    11,
                    12,
                    8,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-23T11:12:08Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    11,
                    12,
                    8,
                    2,
                    204,
                    0
                ],
                "title": "Multiprocessor Scheduling with Memory Constraints: Fundamental\n  Properties and Finding Optimal Solutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiprocessor Scheduling with Memory Constraints: Fundamental\n  Properties and Finding Optimal Solutions"
                },
                "summary": "We study the problem of scheduling a general computational DAG on multiple\nprocessors in a 2-level memory hierarchy. This setting is a natural\ngeneralization of several prominent models in the literature, and it\nsimultaneously captures workload balancing, communication, and data movement\ndue to cache size limitations. We first analyze the fundamental properties of\nthis problem from a theoretical perspective, such as its computational\ncomplexity. We also prove that optimizing parallelization and memory management\nseparately, as done in many applications, can result in a solution that is a\nlinear factor away from the optimum.\n  On the algorithmic side, we discuss a natural technique to represent and\nsolve the problem as an Integer Linear Program (ILP). We develop a holistic\nscheduling algorithm based on this approach, and we experimentally study its\nperformance and properties on a small benchmark of computational tasks. Our\nresults confirm that the ILP-based method can indeed find considerably better\nsolutions than a baseline which combines classical scheduling algorithms and\nmemory management policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of scheduling a general computational DAG on multiple\nprocessors in a 2-level memory hierarchy. This setting is a natural\ngeneralization of several prominent models in the literature, and it\nsimultaneously captures workload balancing, communication, and data movement\ndue to cache size limitations. We first analyze the fundamental properties of\nthis problem from a theoretical perspective, such as its computational\ncomplexity. We also prove that optimizing parallelization and memory management\nseparately, as done in many applications, can result in a solution that is a\nlinear factor away from the optimum.\n  On the algorithmic side, we discuss a natural technique to represent and\nsolve the problem as an Integer Linear Program (ILP). We develop a holistic\nscheduling algorithm based on this approach, and we experimentally study its\nperformance and properties on a small benchmark of computational tasks. Our\nresults confirm that the ILP-based method can indeed find considerably better\nsolutions than a baseline which combines classical scheduling algorithms and\nmemory management policies."
                },
                "authors": [
                    {
                        "name": "Pál András Papp"
                    },
                    {
                        "name": "Toni Böhnlein"
                    },
                    {
                        "name": "A. N. Yzelman"
                    }
                ],
                "author_detail": {
                    "name": "A. N. Yzelman"
                },
                "author": "A. N. Yzelman",
                "arxiv_doi": "10.1145/3754598.3754676",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3754598.3754676",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.17411v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17411v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in the 54th International Conference on Parallel Processing\n  (ICPP 2025)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "90B35, 90C10, 68Q10, 68W10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13373v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13373v2",
                "updated": "2025-07-23T10:10:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    10,
                    10,
                    53,
                    2,
                    204,
                    0
                ],
                "published": "2024-11-20T14:52:36Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    36,
                    2,
                    325,
                    0
                ],
                "title": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment"
                },
                "summary": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Nuclear\nPhysics (BINP), was based on a source of positive hydrogen ions, accelerated to\n50 keV and for a maximum ion current of 5 A. The beam could be modulated and\nthe maximum overall duration was 50 ms. With the upgrade of RFX-mod to the\npresent RFX-mod2 machine, the DNBI is being renovated to solve several power\nunits faults and improve the overall reliability of the system. The 50 kV power\nsupply is being improved, as well as the power supplies in the high voltage\ndeck and its insulation transformer. Magnetic field survival tests were\nperformed on the toroidal-core-based DC-DC converters that should power the\nelectronic boards in a reliable way. The control system, originally based on\nCAMAC technology, was redesigned to be fully replaced. This contribution\nreviews the technical criticalities emerged in the DNBI check-up and the new\nsolutions adopted to make the DNBI operative and more reliable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Nuclear\nPhysics (BINP), was based on a source of positive hydrogen ions, accelerated to\n50 keV and for a maximum ion current of 5 A. The beam could be modulated and\nthe maximum overall duration was 50 ms. With the upgrade of RFX-mod to the\npresent RFX-mod2 machine, the DNBI is being renovated to solve several power\nunits faults and improve the overall reliability of the system. The 50 kV power\nsupply is being improved, as well as the power supplies in the high voltage\ndeck and its insulation transformer. Magnetic field survival tests were\nperformed on the toroidal-core-based DC-DC converters that should power the\nelectronic boards in a reliable way. The control system, originally based on\nCAMAC technology, was redesigned to be fully replaced. This contribution\nreviews the technical criticalities emerged in the DNBI check-up and the new\nsolutions adopted to make the DNBI operative and more reliable."
                },
                "authors": [
                    {
                        "name": "Marco Barbisan"
                    },
                    {
                        "name": "Marco Boldrin"
                    },
                    {
                        "name": "Luca Cinnirella"
                    },
                    {
                        "name": "Bruno Laterza"
                    },
                    {
                        "name": "Alberto Maistrello"
                    },
                    {
                        "name": "Lionello Marrelli"
                    },
                    {
                        "name": "Federico Molon"
                    },
                    {
                        "name": "Simone Peruzzo"
                    },
                    {
                        "name": "Cesare Taliercio"
                    },
                    {
                        "name": "Marco Valisa"
                    },
                    {
                        "name": "Enrico Zampiva"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Zampiva"
                },
                "author": "Enrico Zampiva",
                "arxiv_doi": "10.1016/j.fusengdes.2025.115320",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.fusengdes.2025.115320",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.13373v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13373v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages (excl. highlights), 8 figures. Contribution to the 33rd\n  Symposium on Fusion Technology (SOFT), 22-27 September 2024. This is the\n  accepted manuscript for the \"Fusion Engineering and Design\" journal",
                "arxiv_journal_ref": "Fusion Engineering and Design, Volume 220, November 2025, 115320",
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16391v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16391v2",
                "updated": "2025-07-23T09:31:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    9,
                    31,
                    1,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-22T09:35:59Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    9,
                    35,
                    59,
                    1,
                    203,
                    0
                ],
                "title": "Ironman: Accelerating Oblivious Transfer Extension for\n  Privacy-Preserving AI with Near-Memory Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ironman: Accelerating Oblivious Transfer Extension for\n  Privacy-Preserving AI with Near-Memory Processing"
                },
                "summary": "With the wide application of machine learning (ML), privacy concerns arise\nwith user data as they may contain sensitive information. Privacy-preserving ML\n(PPML) based on cryptographic primitives has emerged as a promising solution in\nwhich an ML model is directly computed on the encrypted data to provide a\nformal privacy guarantee. However, PPML frameworks heavily rely on the\noblivious transfer (OT) primitive to compute nonlinear functions. OT mainly\ninvolves the computation of single-point correlated OT (SPCOT) and learning\nparity with noise (LPN) operations. As OT is still computed extensively on\ngeneral-purpose CPUs, it becomes the latency bottleneck of modern PPML\nframeworks.\n  In this paper, we propose a novel OT accelerator, dubbed Ironman, to\nsignificantly increase the efficiency of OT and the overall PPML framework. We\nobserve that SPCOT is computation-bounded, and thus propose a hardware-friendly\nSPCOT algorithm with a customized accelerator to improve SPCOT computation\nthroughput. In contrast, LPN is memory-bandwidth-bounded due to irregular\nmemory access patterns. Hence, we further leverage the near-memory processing\n(NMP) architecture equipped with memory-side cache and index sorting to improve\neffective memory bandwidth. With extensive experiments, we demonstrate Ironman\nachieves a 39.2-237.4 times improvement in OT throughput across different NMP\nconfigurations compared to the full-thread CPU implementation. For different\nPPML frameworks, Ironman demonstrates a 2.1-3.4 times reduction in end-to-end\nlatency for both CNN and Transformer models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the wide application of machine learning (ML), privacy concerns arise\nwith user data as they may contain sensitive information. Privacy-preserving ML\n(PPML) based on cryptographic primitives has emerged as a promising solution in\nwhich an ML model is directly computed on the encrypted data to provide a\nformal privacy guarantee. However, PPML frameworks heavily rely on the\noblivious transfer (OT) primitive to compute nonlinear functions. OT mainly\ninvolves the computation of single-point correlated OT (SPCOT) and learning\nparity with noise (LPN) operations. As OT is still computed extensively on\ngeneral-purpose CPUs, it becomes the latency bottleneck of modern PPML\nframeworks.\n  In this paper, we propose a novel OT accelerator, dubbed Ironman, to\nsignificantly increase the efficiency of OT and the overall PPML framework. We\nobserve that SPCOT is computation-bounded, and thus propose a hardware-friendly\nSPCOT algorithm with a customized accelerator to improve SPCOT computation\nthroughput. In contrast, LPN is memory-bandwidth-bounded due to irregular\nmemory access patterns. Hence, we further leverage the near-memory processing\n(NMP) architecture equipped with memory-side cache and index sorting to improve\neffective memory bandwidth. With extensive experiments, we demonstrate Ironman\nachieves a 39.2-237.4 times improvement in OT throughput across different NMP\nconfigurations compared to the full-thread CPU implementation. For different\nPPML frameworks, Ironman demonstrates a 2.1-3.4 times reduction in end-to-end\nlatency for both CNN and Transformer models."
                },
                "authors": [
                    {
                        "name": "Chenqi Lin"
                    },
                    {
                        "name": "Kang Yang"
                    },
                    {
                        "name": "Tianshi Xu"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Zhaohui Chen"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Mingyu Gao"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16391v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16391v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02634v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02634v4",
                "updated": "2025-07-23T08:07:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    8,
                    7,
                    19,
                    2,
                    204,
                    0
                ],
                "published": "2025-06-03T08:51:38Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    8,
                    51,
                    38,
                    1,
                    154,
                    0
                ],
                "title": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider"
                },
                "summary": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity."
                },
                "authors": [
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Jinbo Han"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Chenguang Fang"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by USENIX ATC'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02634v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02634v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17286v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17286v2",
                "updated": "2025-07-23T05:57:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    5,
                    57,
                    32,
                    2,
                    204,
                    0
                ],
                "published": "2025-06-15T07:19:33Z",
                "published_parsed": [
                    2025,
                    6,
                    15,
                    7,
                    19,
                    33,
                    6,
                    166,
                    0
                ],
                "title": "GTA: Grouped-head latenT Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTA: Grouped-head latenT Attention"
                },
                "summary": "Attention mechanisms underpin the success of large language models (LLMs),\nyet their substantial computational and memory overhead poses challenges for\noptimizing efficiency and performance. A critical bottleneck arises as KV cache\nand attention computations scale rapidly with text length, challenging\ndeployment on hardware with limited computational and memory resources. We\nobserve that attention mechanisms exhibit substantial redundancy, since the KV\ncache can be significantly compressed and attention maps across heads display\nhigh similarity, revealing that much of the computation and storage is\nunnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head\nLaten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that\nreduces memory usage and computational complexity while maintaining\nperformance. GTA comprises two components: (1) a shared attention map mechanism\nthat reuses attention scores across multiple heads, decreasing the key cache\nsize; and (2) a nonlinear value decoder with learned projections that\ncompresses the value cache into a latent space, further cutting memory needs.\nGTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus\nGrouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while\navoiding the extra overhead of Multi-Head Latent Attention to improve LLM\ndeployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in\nend-to-end inference speed, with prefill benefiting from reduced computational\ncost and decoding benefiting from the smaller cache footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms underpin the success of large language models (LLMs),\nyet their substantial computational and memory overhead poses challenges for\noptimizing efficiency and performance. A critical bottleneck arises as KV cache\nand attention computations scale rapidly with text length, challenging\ndeployment on hardware with limited computational and memory resources. We\nobserve that attention mechanisms exhibit substantial redundancy, since the KV\ncache can be significantly compressed and attention maps across heads display\nhigh similarity, revealing that much of the computation and storage is\nunnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head\nLaten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that\nreduces memory usage and computational complexity while maintaining\nperformance. GTA comprises two components: (1) a shared attention map mechanism\nthat reuses attention scores across multiple heads, decreasing the key cache\nsize; and (2) a nonlinear value decoder with learned projections that\ncompresses the value cache into a latent space, further cutting memory needs.\nGTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus\nGrouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while\navoiding the extra overhead of Multi-Head Latent Attention to improve LLM\ndeployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in\nend-to-end inference speed, with prefill benefiting from reduced computational\ncost and decoding benefiting from the smaller cache footprint."
                },
                "authors": [
                    {
                        "name": "Luoyang Sun"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Jiwen Jiang"
                    },
                    {
                        "name": "Xinjian Wu"
                    },
                    {
                        "name": "Haifeng Zhang"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Lionel Ni"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17286v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17286v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11046v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11046v2",
                "updated": "2025-07-23T01:42:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    1,
                    42,
                    19,
                    2,
                    204,
                    0
                ],
                "published": "2025-02-16T09:08:36Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    9,
                    8,
                    36,
                    6,
                    47,
                    0
                ],
                "title": "Enabling Efficient Transaction Processing on CXL-Based Memory Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Efficient Transaction Processing on CXL-Based Memory Sharing"
                },
                "summary": "Transaction processing systems are the crux for modern data-center\napplications, yet current multi-node systems are slow due to network overheads.\nThis paper advocates for Compute Express Link (CXL) as a network alternative,\nwhich enables low-latency and cache-coherent shared memory accesses. However,\ndirectly adopting standard CXL primitives leads to performance degradation due\nto the high cost of maintaining cross-node cache coherence. To address the CXL\nchallenges, this paper introduces CtXnL, a software-hardware co-designed system\nthat implements a novel hybrid coherence primitive tailored to the loosely\ncoherent nature of transactional data. The core innovation of CtXnL is\nempowering transaction system developers with the ability to selectively\nachieve data coherence. Our evaluations on OLTP workloads demonstrate that\nCtXnL enhances performance, outperforming current network-based systems and\nachieves with up to 2.08x greater throughput than vanilla CXL memory sharing\narchitectures across universal transaction processing policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transaction processing systems are the crux for modern data-center\napplications, yet current multi-node systems are slow due to network overheads.\nThis paper advocates for Compute Express Link (CXL) as a network alternative,\nwhich enables low-latency and cache-coherent shared memory accesses. However,\ndirectly adopting standard CXL primitives leads to performance degradation due\nto the high cost of maintaining cross-node cache coherence. To address the CXL\nchallenges, this paper introduces CtXnL, a software-hardware co-designed system\nthat implements a novel hybrid coherence primitive tailored to the loosely\ncoherent nature of transactional data. The core innovation of CtXnL is\nempowering transaction system developers with the ability to selectively\nachieve data coherence. Our evaluations on OLTP workloads demonstrate that\nCtXnL enhances performance, outperforming current network-based systems and\nachieves with up to 2.08x greater throughput than vanilla CXL memory sharing\narchitectures across universal transaction processing policies."
                },
                "authors": [
                    {
                        "name": "Zhao Wang"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Cong Li"
                    },
                    {
                        "name": "Dimin Niu"
                    },
                    {
                        "name": "Tianchan Guan"
                    },
                    {
                        "name": "Zhaoyang Du"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11046v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11046v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17033v1",
                "updated": "2025-07-22T21:41:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    41,
                    43,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T21:41:43Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    41,
                    43,
                    1,
                    203,
                    0
                ],
                "title": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI"
                },
                "summary": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy."
                },
                "authors": [
                    {
                        "name": "Joshua Kalyanapu"
                    },
                    {
                        "name": "Farshad Dizani"
                    },
                    {
                        "name": "Darsh Asher"
                    },
                    {
                        "name": "Azam Ghanbari"
                    },
                    {
                        "name": "Rosario Cammarota"
                    },
                    {
                        "name": "Aydin Aysu"
                    },
                    {
                        "name": "Samira Mirbagher Ajorpaz"
                    }
                ],
                "author_detail": {
                    "name": "Samira Mirbagher Ajorpaz"
                },
                "author": "Samira Mirbagher Ajorpaz",
                "arxiv_comment": "Accepted at MICRO 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17029v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17029v1",
                "updated": "2025-07-22T21:33:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    33,
                    30,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T21:33:30Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    33,
                    30,
                    1,
                    203,
                    0
                ],
                "title": "StreamME: Simplify 3D Gaussian Avatar within Live Stream",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamME: Simplify 3D Gaussian Avatar within Live Stream"
                },
                "summary": "We propose StreamME, a method focuses on fast 3D avatar reconstruction. The\nStreamME synchronously records and reconstructs a head avatar from live video\nstreams without any pre-cached data, enabling seamless integration of the\nreconstructed appearance into downstream applications. This exceptionally fast\ntraining strategy, which we refer to as on-the-fly training, is central to our\napproach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating\nthe reliance on MLPs in deformable 3DGS and relying solely on geometry, which\nsignificantly improves the adaptation speed to facial expression. To further\nensure high efficiency in on-the-fly training, we introduced a simplification\nstrategy based on primary points, which distributes the point clouds more\nsparsely across the facial surface, optimizing points number while maintaining\nrendering quality. Leveraging the on-the-fly training capabilities, our method\nprotects the facial privacy and reduces communication bandwidth in VR system or\nonline conference. Additionally, it can be directly applied to downstream\napplication such as animation, toonify, and relighting. Please refer to our\nproject page for more details: https://songluchuan.github.io/StreamME/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose StreamME, a method focuses on fast 3D avatar reconstruction. The\nStreamME synchronously records and reconstructs a head avatar from live video\nstreams without any pre-cached data, enabling seamless integration of the\nreconstructed appearance into downstream applications. This exceptionally fast\ntraining strategy, which we refer to as on-the-fly training, is central to our\napproach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating\nthe reliance on MLPs in deformable 3DGS and relying solely on geometry, which\nsignificantly improves the adaptation speed to facial expression. To further\nensure high efficiency in on-the-fly training, we introduced a simplification\nstrategy based on primary points, which distributes the point clouds more\nsparsely across the facial surface, optimizing points number while maintaining\nrendering quality. Leveraging the on-the-fly training capabilities, our method\nprotects the facial privacy and reduces communication bandwidth in VR system or\nonline conference. Additionally, it can be directly applied to downstream\napplication such as animation, toonify, and relighting. Please refer to our\nproject page for more details: https://songluchuan.github.io/StreamME/."
                },
                "authors": [
                    {
                        "name": "Luchuan Song"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Zhan Xu"
                    },
                    {
                        "name": "Yi Zhou"
                    },
                    {
                        "name": "Deepali Aneja"
                    },
                    {
                        "name": "Chenliang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chenliang Xu"
                },
                "author": "Chenliang Xu",
                "arxiv_comment": "12 pages, 15 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17029v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17029v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16933v1",
                "updated": "2025-07-22T18:17:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    18,
                    17,
                    53,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T18:17:53Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    18,
                    17,
                    53,
                    1,
                    203,
                    0
                ],
                "title": "SiLQ: Simple Large Language Model Quantization-Aware Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SiLQ: Simple Large Language Model Quantization-Aware Training"
                },
                "summary": "Large language models can be quantized to reduce inference time latency,\nmodel size, and energy consumption, thereby delivering a better user experience\nat lower cost. A challenge exists to deliver quantized models with minimal loss\nof accuracy in reasonable time, and in particular to do so without requiring\nmechanisms incompatible with specialized inference accelerators. Here, we\ndemonstrate a simple, end-to-end quantization-aware training approach that,\nwith an increase in total model training budget of less than 0.1%, outperforms\nthe leading published quantization methods by large margins on several modern\nbenchmarks, with both base and instruct model variants. The approach easily\ngeneralizes across different model architectures, can be applied to\nactivations, cache, and weights, and requires the introduction of no additional\noperations to the model other than the quantization itself.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models can be quantized to reduce inference time latency,\nmodel size, and energy consumption, thereby delivering a better user experience\nat lower cost. A challenge exists to deliver quantized models with minimal loss\nof accuracy in reasonable time, and in particular to do so without requiring\nmechanisms incompatible with specialized inference accelerators. Here, we\ndemonstrate a simple, end-to-end quantization-aware training approach that,\nwith an increase in total model training budget of less than 0.1%, outperforms\nthe leading published quantization methods by large margins on several modern\nbenchmarks, with both base and instruct model variants. The approach easily\ngeneralizes across different model architectures, can be applied to\nactivations, cache, and weights, and requires the introduction of no additional\noperations to the model other than the quantization itself."
                },
                "authors": [
                    {
                        "name": "Steven K. Esser"
                    },
                    {
                        "name": "Jeffrey L. McKinstry"
                    },
                    {
                        "name": "Deepika Bablani"
                    },
                    {
                        "name": "Rathinakumar Appuswamy"
                    },
                    {
                        "name": "Dharmendra S. Modha"
                    }
                ],
                "author_detail": {
                    "name": "Dharmendra S. Modha"
                },
                "author": "Dharmendra S. Modha",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.06494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06494v1",
                "updated": "2025-08-08T17:59:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    17,
                    59,
                    52,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T17:59:52Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    17,
                    59,
                    52,
                    4,
                    220,
                    0
                ],
                "title": "LightSwitch: Multi-view Relighting with Material-guided Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightSwitch: Multi-view Relighting with Material-guided Diffusion"
                },
                "summary": "Recent approaches for 3D relighting have shown promise in integrating 2D\nimage relighting generative priors to alter the appearance of a 3D\nrepresentation while preserving the underlying structure. Nevertheless,\ngenerative priors used for 2D relighting that directly relight from an input\nimage do not take advantage of intrinsic properties of the subject that can be\ninferred or cannot consider multi-view data at scale, leading to subpar\nrelighting. In this paper, we propose Lightswitch, a novel finetuned\nmaterial-relighting diffusion framework that efficiently relights an arbitrary\nnumber of input images to a target lighting condition while incorporating cues\nfrom inferred intrinsic properties. By using multi-view and material\ninformation cues together with a scalable denoising scheme, our method\nconsistently and efficiently relights dense multi-view data of objects with\ndiverse material compositions. We show that our 2D relighting prediction\nquality exceeds previous state-of-the-art relighting priors that directly\nrelight from images. We further demonstrate that LightSwitch matches or\noutperforms state-of-the-art diffusion inverse rendering methods in relighting\nsynthetic and real objects in as little as 2 minutes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent approaches for 3D relighting have shown promise in integrating 2D\nimage relighting generative priors to alter the appearance of a 3D\nrepresentation while preserving the underlying structure. Nevertheless,\ngenerative priors used for 2D relighting that directly relight from an input\nimage do not take advantage of intrinsic properties of the subject that can be\ninferred or cannot consider multi-view data at scale, leading to subpar\nrelighting. In this paper, we propose Lightswitch, a novel finetuned\nmaterial-relighting diffusion framework that efficiently relights an arbitrary\nnumber of input images to a target lighting condition while incorporating cues\nfrom inferred intrinsic properties. By using multi-view and material\ninformation cues together with a scalable denoising scheme, our method\nconsistently and efficiently relights dense multi-view data of objects with\ndiverse material compositions. We show that our 2D relighting prediction\nquality exceeds previous state-of-the-art relighting priors that directly\nrelight from images. We further demonstrate that LightSwitch matches or\noutperforms state-of-the-art diffusion inverse rendering methods in relighting\nsynthetic and real objects in as little as 2 minutes."
                },
                "authors": [
                    {
                        "name": "Yehonathan Litman"
                    },
                    {
                        "name": "Fernando De la Torre"
                    },
                    {
                        "name": "Shubham Tulsiani"
                    }
                ],
                "author_detail": {
                    "name": "Shubham Tulsiani"
                },
                "author": "Shubham Tulsiani",
                "arxiv_comment": "ICCV 2025, Project page & Code:\n  https://yehonathanlitman.github.io/light_switch/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07081v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07081v2",
                "updated": "2025-08-08T17:58:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    17,
                    58,
                    1,
                    4,
                    220,
                    0
                ],
                "published": "2025-04-09T17:54:22Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    54,
                    22,
                    2,
                    99,
                    0
                ],
                "title": "Self-Steering Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Steering Language Models"
                },
                "summary": "While test-time reasoning enables language models (LMs) to tackle complex\ntasks, searching or planning in natural language can be slow, costly, and\nerror-prone. But even when LMs struggle to emulate the precise reasoning steps\nneeded to solve a problem, they often excel at describing its abstract\nstructure--both how to verify solutions and how to search for them. This paper\nintroduces DisCIPL, a method for \"self-steering\" LMs where a Planner model\ngenerates a task-specific inference program that is executed by a population of\nFollower models. Our approach equips LMs with the ability to write recursive\nsearch procedures that guide LM inference, enabling new forms of verifiable and\nefficient reasoning. When instantiated with a small Follower (e.g.,\nLlama-3.2-1B or Qwen3-1.7B), DisCIPL matches (and sometimes outperforms) much\nlarger models, including GPT-4o and o1, on challenging constrained generation\ntasks. Our work opens up a design space of highly-parallelized Monte Carlo\ninference strategies that outperform standard best-of-N sampling, require no\nfinetuning, and can be implemented automatically by existing LMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While test-time reasoning enables language models (LMs) to tackle complex\ntasks, searching or planning in natural language can be slow, costly, and\nerror-prone. But even when LMs struggle to emulate the precise reasoning steps\nneeded to solve a problem, they often excel at describing its abstract\nstructure--both how to verify solutions and how to search for them. This paper\nintroduces DisCIPL, a method for \"self-steering\" LMs where a Planner model\ngenerates a task-specific inference program that is executed by a population of\nFollower models. Our approach equips LMs with the ability to write recursive\nsearch procedures that guide LM inference, enabling new forms of verifiable and\nefficient reasoning. When instantiated with a small Follower (e.g.,\nLlama-3.2-1B or Qwen3-1.7B), DisCIPL matches (and sometimes outperforms) much\nlarger models, including GPT-4o and o1, on challenging constrained generation\ntasks. Our work opens up a design space of highly-parallelized Monte Carlo\ninference strategies that outperform standard best-of-N sampling, require no\nfinetuning, and can be implemented automatically by existing LMs."
                },
                "authors": [
                    {
                        "name": "Gabriel Grand"
                    },
                    {
                        "name": "Joshua B. Tenenbaum"
                    },
                    {
                        "name": "Vikash K. Mansinghka"
                    },
                    {
                        "name": "Alexander K. Lew"
                    },
                    {
                        "name": "Jacob Andreas"
                    }
                ],
                "author_detail": {
                    "name": "Jacob Andreas"
                },
                "author": "Jacob Andreas",
                "arxiv_comment": "Accepted to COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07081v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07081v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24320v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24320v2",
                "updated": "2025-08-08T17:55:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    17,
                    55,
                    21,
                    4,
                    220,
                    0
                ],
                "published": "2025-03-31T17:07:37Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    7,
                    37,
                    0,
                    90,
                    0
                ],
                "title": "Can Test-Time Scaling Improve World Foundation Model?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Test-Time Scaling Improve World Foundation Model?"
                },
                "summary": "World foundation models, which simulate the physical world by predicting\nfuture states from current observations and inputs, have become central to many\napplications in physical intelligence, including autonomous driving and\nrobotics. However, these models require substantial computational resources for\npretraining and are further constrained by available data during post-training.\nAs such, scaling computation at test time emerges as both a critical and\npractical alternative to traditional model enlargement or re-training. In this\nwork, we introduce SWIFT, a test-time scaling framework tailored for WFMs.\nSWIFT integrates our extensible WFM evaluation toolkit with process-level\ninference strategies, including fast tokenization, probability-based Top-K\npruning, and efficient beam search. Empirical results on the COSMOS model\ndemonstrate that test-time scaling exists even in a compute-optimal way. Our\nfindings reveal that test-time scaling laws hold for WFMs and that SWIFT\nprovides a scalable and effective pathway for improving WFM inference without\nretraining or increasing model size. Project page:\nhttps://scalingwfm.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "World foundation models, which simulate the physical world by predicting\nfuture states from current observations and inputs, have become central to many\napplications in physical intelligence, including autonomous driving and\nrobotics. However, these models require substantial computational resources for\npretraining and are further constrained by available data during post-training.\nAs such, scaling computation at test time emerges as both a critical and\npractical alternative to traditional model enlargement or re-training. In this\nwork, we introduce SWIFT, a test-time scaling framework tailored for WFMs.\nSWIFT integrates our extensible WFM evaluation toolkit with process-level\ninference strategies, including fast tokenization, probability-based Top-K\npruning, and efficient beam search. Empirical results on the COSMOS model\ndemonstrate that test-time scaling exists even in a compute-optimal way. Our\nfindings reveal that test-time scaling laws hold for WFMs and that SWIFT\nprovides a scalable and effective pathway for improving WFM inference without\nretraining or increasing model size. Project page:\nhttps://scalingwfm.github.io/."
                },
                "authors": [
                    {
                        "name": "Wenyan Cong"
                    },
                    {
                        "name": "Hanqing Zhu"
                    },
                    {
                        "name": "Peihao Wang"
                    },
                    {
                        "name": "Bangya Liu"
                    },
                    {
                        "name": "Dejia Xu"
                    },
                    {
                        "name": "Kevin Wang"
                    },
                    {
                        "name": "David Z. Pan"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Zhiwen Fan"
                    },
                    {
                        "name": "Zhangyang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhangyang Wang"
                },
                "author": "Zhangyang Wang",
                "arxiv_comment": "Accepted by COLM2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24320v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24320v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06484v1",
                "updated": "2025-08-08T17:49:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    17,
                    49,
                    22,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T17:49:22Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    17,
                    49,
                    22,
                    4,
                    220,
                    0
                ],
                "title": "Non-programmers Assessing AI-Generated Code: A Case Study of Business\n  Users Analyzing Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-programmers Assessing AI-Generated Code: A Case Study of Business\n  Users Analyzing Data"
                },
                "summary": "Non-technical end-users increasingly rely on AI code generation to perform\ntechnical tasks like data analysis. However, large language models (LLMs)\nremain unreliable, and it is unclear whether end-users can effectively identify\nmodel errors $\\unicode{x2014}$ especially in realistic and domain-specific\nscenarios. We surveyed marketing and sales professionals to assess their\nability to critically evaluate LLM-generated analyses of marketing data.\nParticipants were shown natural language explanations of the AI's code,\nrepeatedly informed the AI often makes mistakes, and explicitly prompted to\nidentify them. Yet, participants frequently failed to detect critical flaws\nthat could compromise decision-making, many of which required no technical\nknowledge to recognize. To investigate why, we reformatted AI responses into\nclearly delineated steps and provided alternative approaches for each decision\nto support critical evaluation. While these changes had a positive effect,\nparticipants often struggled to reason through the AI's steps and alternatives.\nOur findings suggest that business professionals cannot reliably verify\nAI-generated data analyses on their own and explore reasons why to inform\nfuture designs. As non-programmers adopt code-generating AI for technical\ntasks, unreliable AI and insufficient human oversight poses risks of unsafe or\nlow-quality decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-technical end-users increasingly rely on AI code generation to perform\ntechnical tasks like data analysis. However, large language models (LLMs)\nremain unreliable, and it is unclear whether end-users can effectively identify\nmodel errors $\\unicode{x2014}$ especially in realistic and domain-specific\nscenarios. We surveyed marketing and sales professionals to assess their\nability to critically evaluate LLM-generated analyses of marketing data.\nParticipants were shown natural language explanations of the AI's code,\nrepeatedly informed the AI often makes mistakes, and explicitly prompted to\nidentify them. Yet, participants frequently failed to detect critical flaws\nthat could compromise decision-making, many of which required no technical\nknowledge to recognize. To investigate why, we reformatted AI responses into\nclearly delineated steps and provided alternative approaches for each decision\nto support critical evaluation. While these changes had a positive effect,\nparticipants often struggled to reason through the AI's steps and alternatives.\nOur findings suggest that business professionals cannot reliably verify\nAI-generated data analyses on their own and explore reasons why to inform\nfuture designs. As non-programmers adopt code-generating AI for technical\ntasks, unreliable AI and insufficient human oversight poses risks of unsafe or\nlow-quality decisions."
                },
                "authors": [
                    {
                        "name": "Yuvraj Virk"
                    },
                    {
                        "name": "Dongyu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Dongyu Liu"
                },
                "author": "Dongyu Liu",
                "arxiv_comment": "Accepted by VL/HCC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13908v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13908v2",
                "updated": "2025-08-08T17:43:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    17,
                    43,
                    37,
                    4,
                    220,
                    0
                ],
                "published": "2025-04-09T13:58:07Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    13,
                    58,
                    7,
                    2,
                    99,
                    0
                ],
                "title": "AI-Assisted Conversational Interviewing: Effects on Data Quality and\n  User Experience",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Assisted Conversational Interviewing: Effects on Data Quality and\n  User Experience"
                },
                "summary": "Standardized surveys scale efficiently but sacrifice depth, while\nconversational interviews improve response quality at the cost of scalability\nand consistency. This study bridges the gap between these methods by\nintroducing a framework for AI-assisted conversational interviewing. To\nevaluate this framework, we conducted a web survey experiment where 1,800\nparticipants were randomly assigned to AI 'chatbots' which use large language\nmodels (LLMs) to dynamically probe respondents for elaboration and\ninteractively code open-ended responses to fixed questions developed by human\nresearchers. We assessed the AI chatbot's performance in terms of coding\naccuracy, response quality, and respondent experience. Our findings reveal that\nAI chatbots perform moderately well in live coding even without survey-specific\nfine-tuning, despite slightly inflated false positive errors due to respondent\nacquiescence bias. Open-ended responses were more detailed and informative, but\nthis came at a slight cost to respondent experience. Our findings highlight the\nfeasibility of using AI methods such as chatbots enhanced by LLMs to enhance\nopen-ended data collection in web surveys.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standardized surveys scale efficiently but sacrifice depth, while\nconversational interviews improve response quality at the cost of scalability\nand consistency. This study bridges the gap between these methods by\nintroducing a framework for AI-assisted conversational interviewing. To\nevaluate this framework, we conducted a web survey experiment where 1,800\nparticipants were randomly assigned to AI 'chatbots' which use large language\nmodels (LLMs) to dynamically probe respondents for elaboration and\ninteractively code open-ended responses to fixed questions developed by human\nresearchers. We assessed the AI chatbot's performance in terms of coding\naccuracy, response quality, and respondent experience. Our findings reveal that\nAI chatbots perform moderately well in live coding even without survey-specific\nfine-tuning, despite slightly inflated false positive errors due to respondent\nacquiescence bias. Open-ended responses were more detailed and informative, but\nthis came at a slight cost to respondent experience. Our findings highlight the\nfeasibility of using AI methods such as chatbots enhanced by LLMs to enhance\nopen-ended data collection in web surveys."
                },
                "authors": [
                    {
                        "name": "Soubhik Barari"
                    },
                    {
                        "name": "Jarret Angbazo"
                    },
                    {
                        "name": "Natalie Wang"
                    },
                    {
                        "name": "Leah M. Christian"
                    },
                    {
                        "name": "Elizabeth Dean"
                    },
                    {
                        "name": "Zoe Slowinski"
                    },
                    {
                        "name": "Brandon Sepulvado"
                    }
                ],
                "author_detail": {
                    "name": "Brandon Sepulvado"
                },
                "author": "Brandon Sepulvado",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13908v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13908v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06482v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06482v1",
                "updated": "2025-08-08T17:42:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    17,
                    42,
                    16,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T17:42:16Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    17,
                    42,
                    16,
                    4,
                    220,
                    0
                ],
                "title": "Post-training for Efficient Communication via Convention Formation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training for Efficient Communication via Convention Formation"
                },
                "summary": "Humans communicate with increasing efficiency in multi-turn interactions, by\nadapting their language and forming ad-hoc conventions. In contrast, prior work\nshows that LLMs do not naturally show this behavior. We develop a post-training\nprocess to develop this ability through targeted fine-tuning on heuristically\nidentified demonstrations of convention formation. We evaluate with two new\nbenchmarks focused on this capability. First, we design a focused,\ncognitively-motivated interaction benchmark that consistently elicits strong\nconvention formation trends in humans. Second, we create a new\ndocument-grounded reference completion task that reflects in-the-wild\nconvention formation behavior. Our studies show significantly improved\nconvention formation abilities in post-trained LLMs across the two evaluation\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans communicate with increasing efficiency in multi-turn interactions, by\nadapting their language and forming ad-hoc conventions. In contrast, prior work\nshows that LLMs do not naturally show this behavior. We develop a post-training\nprocess to develop this ability through targeted fine-tuning on heuristically\nidentified demonstrations of convention formation. We evaluate with two new\nbenchmarks focused on this capability. First, we design a focused,\ncognitively-motivated interaction benchmark that consistently elicits strong\nconvention formation trends in humans. Second, we create a new\ndocument-grounded reference completion task that reflects in-the-wild\nconvention formation behavior. Our studies show significantly improved\nconvention formation abilities in post-trained LLMs across the two evaluation\nmethods."
                },
                "authors": [
                    {
                        "name": "Yilun Hua"
                    },
                    {
                        "name": "Evan Wang"
                    },
                    {
                        "name": "Yoav Artzi"
                    }
                ],
                "author_detail": {
                    "name": "Yoav Artzi"
                },
                "author": "Yoav Artzi",
                "arxiv_comment": "Accepted to COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06482v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06482v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06479v1",
                "updated": "2025-08-08T17:36:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    17,
                    36,
                    42,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T17:36:42Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    17,
                    36,
                    42,
                    4,
                    220,
                    0
                ],
                "title": "The Problem of Atypicality in LLM-Powered Psychiatry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Problem of Atypicality in LLM-Powered Psychiatry"
                },
                "summary": "Large language models (LLMs) are increasingly proposed as scalable solutions\nto the global mental health crisis. But their deployment in psychiatric\ncontexts raises a distinctive ethical concern: the problem of atypicality.\nBecause LLMs generate outputs based on population-level statistical\nregularities, their responses -- while typically appropriate for general users\n-- may be dangerously inappropriate when interpreted by psychiatric patients,\nwho often exhibit atypical cognitive or interpretive patterns. We argue that\nstandard mitigation strategies, such as prompt engineering or fine-tuning, are\ninsufficient to resolve this structural risk. Instead, we propose dynamic\ncontextual certification (DCC): a staged, reversible and context-sensitive\nframework for deploying LLMs in psychiatry, inspired by clinical translation\nand dynamic safety models from artificial intelligence governance. DCC reframes\nchatbot deployment as an ongoing epistemic and ethical process that prioritises\ninterpretive safety over static performance benchmarks. Atypicality, we argue,\ncannot be eliminated -- but it can, and must, be proactively managed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly proposed as scalable solutions\nto the global mental health crisis. But their deployment in psychiatric\ncontexts raises a distinctive ethical concern: the problem of atypicality.\nBecause LLMs generate outputs based on population-level statistical\nregularities, their responses -- while typically appropriate for general users\n-- may be dangerously inappropriate when interpreted by psychiatric patients,\nwho often exhibit atypical cognitive or interpretive patterns. We argue that\nstandard mitigation strategies, such as prompt engineering or fine-tuning, are\ninsufficient to resolve this structural risk. Instead, we propose dynamic\ncontextual certification (DCC): a staged, reversible and context-sensitive\nframework for deploying LLMs in psychiatry, inspired by clinical translation\nand dynamic safety models from artificial intelligence governance. DCC reframes\nchatbot deployment as an ongoing epistemic and ethical process that prioritises\ninterpretive safety over static performance benchmarks. Atypicality, we argue,\ncannot be eliminated -- but it can, and must, be proactively managed."
                },
                "authors": [
                    {
                        "name": "Bosco Garcia"
                    },
                    {
                        "name": "Eugene Y. S. Chua"
                    },
                    {
                        "name": "Harman Singh Brah"
                    }
                ],
                "author_detail": {
                    "name": "Harman Singh Brah"
                },
                "author": "Harman Singh Brah",
                "arxiv_doi": "10.1136/jme-2025-110972",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1136/jme-2025-110972",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.06479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Preprint of 8/8/2025 -- please cite published version. This article\n  has been published in the Journal of Medical Ethics (2025) following peer\n  review and can also be viewed on the journal's website at\n  10.1136/jme-2025-110972",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06472v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06472v1",
                "updated": "2025-08-08T17:21:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    17,
                    21,
                    26,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T17:21:26Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    17,
                    21,
                    26,
                    4,
                    220,
                    0
                ],
                "title": "Revisiting the Gas Dynamics of Henize 2-10: Possible Drivers of the\n  Starburst",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting the Gas Dynamics of Henize 2-10: Possible Drivers of the\n  Starburst"
                },
                "summary": "The triggers of starburst episodes are a key component to our understanding\nof the baryon cycle in galaxies. Galaxy mergers are a commonly suggested\ncatalyst for starbursts, but once the galaxies coalesce into a single\nkinematically disturbed system, their merger history can be difficult to\nassess. This is particularly true for dwarf galaxies, which are expected to\ndominate the merger rate at all redshifts due to their large numbers. One such\ndwarf galaxy undergoing an enigmatic starburst episode is Henize 2-10, which\nappears to be isolated. Possible scenarios that might have caused the starburst\nepisode include a previous merger or stochastic processes within the galaxy\nitself, such as self-regulation via feedback processes. We present new VLA\n21-cm observations and unpublished archival CARMA CO data to investigate the\ndynamical state and star formation activity in the galaxy. We do not detect an\nHI tail consistent with the structure reported by Kobulnicky et al. (1995),\nwhich was suggested as evidence for a merger or interaction, but rather these\nnew observations indicate an extended HI distribution. We also find that the HI\nappears dynamically decoupled from an extended CO feature (inferred to be a\ntidal tail in previous work), suggesting large-scale dynamical processes of\nsome type are affecting the gas in this system. We provide a meta-analysis of\navailable results to enhance our understanding of what might be triggering the\nstarburst episode in Henize 2-10, and speculate that the large CO feature could\nbe falling into the galaxy and potentially trigger starburst activity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The triggers of starburst episodes are a key component to our understanding\nof the baryon cycle in galaxies. Galaxy mergers are a commonly suggested\ncatalyst for starbursts, but once the galaxies coalesce into a single\nkinematically disturbed system, their merger history can be difficult to\nassess. This is particularly true for dwarf galaxies, which are expected to\ndominate the merger rate at all redshifts due to their large numbers. One such\ndwarf galaxy undergoing an enigmatic starburst episode is Henize 2-10, which\nappears to be isolated. Possible scenarios that might have caused the starburst\nepisode include a previous merger or stochastic processes within the galaxy\nitself, such as self-regulation via feedback processes. We present new VLA\n21-cm observations and unpublished archival CARMA CO data to investigate the\ndynamical state and star formation activity in the galaxy. We do not detect an\nHI tail consistent with the structure reported by Kobulnicky et al. (1995),\nwhich was suggested as evidence for a merger or interaction, but rather these\nnew observations indicate an extended HI distribution. We also find that the HI\nappears dynamically decoupled from an extended CO feature (inferred to be a\ntidal tail in previous work), suggesting large-scale dynamical processes of\nsome type are affecting the gas in this system. We provide a meta-analysis of\navailable results to enhance our understanding of what might be triggering the\nstarburst episode in Henize 2-10, and speculate that the large CO feature could\nbe falling into the galaxy and potentially trigger starburst activity."
                },
                "authors": [
                    {
                        "name": "Josephine M. Dalsin"
                    },
                    {
                        "name": "Allison H. Costa"
                    },
                    {
                        "name": "Remy Indebetouw"
                    },
                    {
                        "name": "Kelsey E. Johnson"
                    },
                    {
                        "name": "Natalie O. Johnson"
                    },
                    {
                        "name": "Sabrina Stierwalt"
                    }
                ],
                "author_detail": {
                    "name": "Sabrina Stierwalt"
                },
                "author": "Sabrina Stierwalt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06472v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06472v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06467v1",
                "updated": "2025-08-08T17:15:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    17,
                    15,
                    32,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T17:15:32Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    17,
                    15,
                    32,
                    4,
                    220,
                    0
                ],
                "title": "LLM Unlearning using Gradient Ratio-Based Influence Estimation and Noise\n  Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Unlearning using Gradient Ratio-Based Influence Estimation and Noise\n  Injection"
                },
                "summary": "The growing legal and ethical scrutiny of large language models (LLMs)\nnecessitates effective machine unlearning, particularly for sensitive or\nunauthorized data. Existing empirical methods often yield incomplete forgetting\nor unintended degradation of unrelated knowledge due to poor localization. In\nthis work, we propose GRIN: a modular and targeted framework for LLM\nunlearning. GRIN introduces a novel gradient-ratio-based metric to identify\nparameters most responsible for memorizing forget data. We then perform\nselective noise injection into these parameters prior to fine-tuning, which\nimproves unlearning performance while maintaining model utility. Finally, we\npropose new evaluation metrics tailored to the LLM setting and validate our\napproach on standard benchmarks such as TOFU, WMDP, and SafePKU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing legal and ethical scrutiny of large language models (LLMs)\nnecessitates effective machine unlearning, particularly for sensitive or\nunauthorized data. Existing empirical methods often yield incomplete forgetting\nor unintended degradation of unrelated knowledge due to poor localization. In\nthis work, we propose GRIN: a modular and targeted framework for LLM\nunlearning. GRIN introduces a novel gradient-ratio-based metric to identify\nparameters most responsible for memorizing forget data. We then perform\nselective noise injection into these parameters prior to fine-tuning, which\nimproves unlearning performance while maintaining model utility. Finally, we\npropose new evaluation metrics tailored to the LLM setting and validate our\napproach on standard benchmarks such as TOFU, WMDP, and SafePKU."
                },
                "authors": [
                    {
                        "name": "Ameya Anjarlekar"
                    },
                    {
                        "name": "Sandeep Pombra"
                    }
                ],
                "author_detail": {
                    "name": "Sandeep Pombra"
                },
                "author": "Sandeep Pombra",
                "arxiv_comment": "14 Pages, 3 Figures, 11 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06457v1",
                "updated": "2025-08-08T17:01:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    17,
                    1,
                    41,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T17:01:41Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    17,
                    1,
                    41,
                    4,
                    220,
                    0
                ],
                "title": "ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive fluency and\nreasoning capabilities, but their potential for misuse has raised growing\nconcern. In this paper, we present ScamAgent, an autonomous multi-turn agent\nbuilt on top of LLMs, capable of generating highly realistic scam call scripts\nthat simulate real-world fraud scenarios. Unlike prior work focused on\nsingle-shot prompt misuse, ScamAgent maintains dialogue memory, adapts\ndynamically to simulated user responses, and employs deceptive persuasion\nstrategies across conversational turns. We show that current LLM safety\nguardrails, including refusal mechanisms and content filters, are ineffective\nagainst such agent-based threats. Even models with strong prompt-level\nsafeguards can be bypassed when prompts are decomposed, disguised, or delivered\nincrementally within an agent framework. We further demonstrate the\ntransformation of scam scripts into lifelike voice calls using modern\ntext-to-speech systems, completing a fully automated scam pipeline. Our\nfindings highlight an urgent need for multi-turn safety auditing, agent-level\ncontrol frameworks, and new methods to detect and disrupt conversational\ndeception powered by generative AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive fluency and\nreasoning capabilities, but their potential for misuse has raised growing\nconcern. In this paper, we present ScamAgent, an autonomous multi-turn agent\nbuilt on top of LLMs, capable of generating highly realistic scam call scripts\nthat simulate real-world fraud scenarios. Unlike prior work focused on\nsingle-shot prompt misuse, ScamAgent maintains dialogue memory, adapts\ndynamically to simulated user responses, and employs deceptive persuasion\nstrategies across conversational turns. We show that current LLM safety\nguardrails, including refusal mechanisms and content filters, are ineffective\nagainst such agent-based threats. Even models with strong prompt-level\nsafeguards can be bypassed when prompts are decomposed, disguised, or delivered\nincrementally within an agent framework. We further demonstrate the\ntransformation of scam scripts into lifelike voice calls using modern\ntext-to-speech systems, completing a fully automated scam pipeline. Our\nfindings highlight an urgent need for multi-turn safety auditing, agent-level\ncontrol frameworks, and new methods to detect and disrupt conversational\ndeception powered by generative AI."
                },
                "authors": [
                    {
                        "name": "Sanket Badhe"
                    }
                ],
                "author_detail": {
                    "name": "Sanket Badhe"
                },
                "author": "Sanket Badhe",
                "arxiv_comment": "Accepted at CAMLIS 25: Conference on Applied Machine Learning for\n  Information Security. 10 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06453v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06453v1",
                "updated": "2025-08-08T16:54:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    54,
                    6,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T16:54:06Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    54,
                    6,
                    4,
                    220,
                    0
                ],
                "title": "Text Embedded Swin-UMamba for DeepLesion Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text Embedded Swin-UMamba for DeepLesion Segmentation"
                },
                "summary": "Segmentation of lesions on CT enables automatic measurement for clinical\nassessment of chronic diseases (e.g., lymphoma). Integrating large language\nmodels (LLMs) into the lesion segmentation workflow offers the potential to\ncombine imaging features with descriptions of lesion characteristics from the\nradiology reports. In this study, we investigate the feasibility of integrating\ntext into the Swin-UMamba architecture for the task of lesion segmentation. The\npublicly available ULS23 DeepLesion dataset was used along with short-form\ndescriptions of the findings from the reports. On the test dataset, a high Dice\nScore of 82% and low Hausdorff distance of 6.58 (pixels) was obtained for\nlesion segmentation. The proposed Text-Swin-UMamba model outperformed prior\napproaches: 37% improvement over the LLM-driven LanGuideMedSeg model (p <\n0.001),and surpassed the purely image-based xLSTM-UNet and nnUNet models by\n1.74% and 0.22%, respectively. The dataset and code can be accessed at\nhttps://github.com/ruida/LLM-Swin-UMamba",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Segmentation of lesions on CT enables automatic measurement for clinical\nassessment of chronic diseases (e.g., lymphoma). Integrating large language\nmodels (LLMs) into the lesion segmentation workflow offers the potential to\ncombine imaging features with descriptions of lesion characteristics from the\nradiology reports. In this study, we investigate the feasibility of integrating\ntext into the Swin-UMamba architecture for the task of lesion segmentation. The\npublicly available ULS23 DeepLesion dataset was used along with short-form\ndescriptions of the findings from the reports. On the test dataset, a high Dice\nScore of 82% and low Hausdorff distance of 6.58 (pixels) was obtained for\nlesion segmentation. The proposed Text-Swin-UMamba model outperformed prior\napproaches: 37% improvement over the LLM-driven LanGuideMedSeg model (p <\n0.001),and surpassed the purely image-based xLSTM-UNet and nnUNet models by\n1.74% and 0.22%, respectively. The dataset and code can be accessed at\nhttps://github.com/ruida/LLM-Swin-UMamba"
                },
                "authors": [
                    {
                        "name": "Ruida Cheng"
                    },
                    {
                        "name": "Tejas Sudharshan Mathai"
                    },
                    {
                        "name": "Pritam Mukherjee"
                    },
                    {
                        "name": "Benjamin Hou"
                    },
                    {
                        "name": "Qingqing Zhu"
                    },
                    {
                        "name": "Zhiyong Lu"
                    },
                    {
                        "name": "Matthew McAuliffe"
                    },
                    {
                        "name": "Ronald M. Summers"
                    }
                ],
                "author_detail": {
                    "name": "Ronald M. Summers"
                },
                "author": "Ronald M. Summers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06453v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06453v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02529v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02529v2",
                "updated": "2025-08-08T16:47:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    47,
                    32,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-04T15:38:13Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    38,
                    13,
                    0,
                    216,
                    0
                ],
                "title": "Failure-Aware Multi-Robot Coordination for Resilient and Adaptive Target\n  Tracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Failure-Aware Multi-Robot Coordination for Resilient and Adaptive Target\n  Tracking"
                },
                "summary": "Multi-robot coordination is crucial for autonomous systems, yet real-world\ndeployments often encounter various failures. These include both temporary and\npermanent disruptions in sensing and communication, which can significantly\ndegrade system robustness and performance if not explicitly modeled. Despite\nits practical importance, failure-aware coordination remains underexplored in\nthe literature. To bridge the gap between idealized conditions and the\ncomplexities of real-world environments, we propose a unified failure-aware\ncoordination framework designed to enable resilient and adaptive multi-robot\ntarget tracking under both temporary and permanent failure conditions. Our\napproach systematically distinguishes between two classes of failures: (1)\nprobabilistic and temporary disruptions, where robots recover from intermittent\nsensing or communication losses by dynamically adapting paths and avoiding\ninferred danger zones, and (2) permanent failures, where robots lose sensing or\ncommunication capabilities irreversibly, requiring sustained, decentralized\nbehavioral adaptation. To handle these scenarios, the robot team is partitioned\ninto subgroups. Robots that remain connected form a communication group and\ncollaboratively plan using partially centralized nonlinear optimization. Robots\nexperiencing permanent disconnection or failure continue to operate\nindependently through decentralized or individual optimization, allowing them\nto contribute to the task within their local context. We extensively evaluate\nour method across a range of benchmark variations and conduct a comprehensive\nassessment under diverse real-world failure scenarios. Results show that our\nframework consistently achieves robust performance in realistic environments\nwith unknown danger zones, offering a practical and generalizable solution for\nthe multi-robot systems community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-robot coordination is crucial for autonomous systems, yet real-world\ndeployments often encounter various failures. These include both temporary and\npermanent disruptions in sensing and communication, which can significantly\ndegrade system robustness and performance if not explicitly modeled. Despite\nits practical importance, failure-aware coordination remains underexplored in\nthe literature. To bridge the gap between idealized conditions and the\ncomplexities of real-world environments, we propose a unified failure-aware\ncoordination framework designed to enable resilient and adaptive multi-robot\ntarget tracking under both temporary and permanent failure conditions. Our\napproach systematically distinguishes between two classes of failures: (1)\nprobabilistic and temporary disruptions, where robots recover from intermittent\nsensing or communication losses by dynamically adapting paths and avoiding\ninferred danger zones, and (2) permanent failures, where robots lose sensing or\ncommunication capabilities irreversibly, requiring sustained, decentralized\nbehavioral adaptation. To handle these scenarios, the robot team is partitioned\ninto subgroups. Robots that remain connected form a communication group and\ncollaboratively plan using partially centralized nonlinear optimization. Robots\nexperiencing permanent disconnection or failure continue to operate\nindependently through decentralized or individual optimization, allowing them\nto contribute to the task within their local context. We extensively evaluate\nour method across a range of benchmark variations and conduct a comprehensive\nassessment under diverse real-world failure scenarios. Results show that our\nframework consistently achieves robust performance in realistic environments\nwith unknown danger zones, offering a practical and generalizable solution for\nthe multi-robot systems community."
                },
                "authors": [
                    {
                        "name": "Peihan Li"
                    },
                    {
                        "name": "Jiazhen Liu"
                    },
                    {
                        "name": "Yuwei Wu"
                    },
                    {
                        "name": "Lifeng Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Lifeng Zhou"
                },
                "author": "Lifeng Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02529v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02529v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15254v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15254v2",
                "updated": "2025-08-08T16:45:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    45,
                    47,
                    4,
                    220,
                    0
                ],
                "published": "2025-04-21T17:33:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    33,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation"
                },
                "summary": "C-to-Rust transpilation is essential for modernizing legacy C code while\nenhancing safety and interoperability with modern Rust ecosystems. However, no\ndataset currently exists for evaluating whether a system can transpile C into\nsafe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset\nof 100 C repositories, each paired with manually-written interfaces in safe\nRust as well as test cases that can be used to validate correctness of the\ntranspilation. By considering entire repositories rather than isolated\nfunctions, CRUST-Bench captures the challenges of translating complex projects\nwith dependencies across multiple files. The provided Rust interfaces provide\nexplicit specifications that ensure adherence to idiomatic, memory-safe Rust\npatterns, while the accompanying test cases enforce functional correctness. We\nevaluate state-of-the-art large language models (LLMs) on this task and find\nthat safe and idiomatic Rust generation is still a challenging problem for\nvarious state-of-the-art methods and techniques. We also provide insights into\nthe errors LLMs usually make in transpiling code from C to safe Rust. The best\nperforming model, OpenAI o1, is able to solve only 15 tasks in a single-shot\nsetting. Improvements on CRUST-Bench would lead to improved transpilation\nsystems that can reason about complex scenarios and help in migrating legacy\ncodebases from C into languages like Rust that ensure memory safety. You can\nfind the dataset and code at https://github.com/anirudhkhatry/CRUST-bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C-to-Rust transpilation is essential for modernizing legacy C code while\nenhancing safety and interoperability with modern Rust ecosystems. However, no\ndataset currently exists for evaluating whether a system can transpile C into\nsafe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset\nof 100 C repositories, each paired with manually-written interfaces in safe\nRust as well as test cases that can be used to validate correctness of the\ntranspilation. By considering entire repositories rather than isolated\nfunctions, CRUST-Bench captures the challenges of translating complex projects\nwith dependencies across multiple files. The provided Rust interfaces provide\nexplicit specifications that ensure adherence to idiomatic, memory-safe Rust\npatterns, while the accompanying test cases enforce functional correctness. We\nevaluate state-of-the-art large language models (LLMs) on this task and find\nthat safe and idiomatic Rust generation is still a challenging problem for\nvarious state-of-the-art methods and techniques. We also provide insights into\nthe errors LLMs usually make in transpiling code from C to safe Rust. The best\nperforming model, OpenAI o1, is able to solve only 15 tasks in a single-shot\nsetting. Improvements on CRUST-Bench would lead to improved transpilation\nsystems that can reason about complex scenarios and help in migrating legacy\ncodebases from C into languages like Rust that ensure memory safety. You can\nfind the dataset and code at https://github.com/anirudhkhatry/CRUST-bench."
                },
                "authors": [
                    {
                        "name": "Anirudh Khatry"
                    },
                    {
                        "name": "Robert Zhang"
                    },
                    {
                        "name": "Jia Pan"
                    },
                    {
                        "name": "Ziteng Wang"
                    },
                    {
                        "name": "Qiaochu Chen"
                    },
                    {
                        "name": "Greg Durrett"
                    },
                    {
                        "name": "Isil Dillig"
                    }
                ],
                "author_detail": {
                    "name": "Isil Dillig"
                },
                "author": "Isil Dillig",
                "arxiv_comment": "To be published at COLM, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15254v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15254v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06447v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06447v1",
                "updated": "2025-08-08T16:42:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    42,
                    38,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T16:42:38Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    42,
                    38,
                    4,
                    220,
                    0
                ],
                "title": "SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token\n  Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token\n  Pruning"
                },
                "summary": "Long-context inference for Large Language Models (LLMs) is heavily limited by\nhigh computational demands. While several existing methods optimize attention\ncomputation, they still process the full set of hidden states at each layer,\nlimiting overall efficiency. In this work, we propose SlimInfer, an innovative\nframework that aims to accelerate inference by directly pruning less critical\nprompt tokens during the forward pass. Our key insight is an information\ndiffusion phenomenon: As information from critical tokens propagates through\nlayers, it becomes distributed across the entire sequence. This diffusion\nprocess suggests that LLMs can maintain their semantic integrity when excessive\ntokens, even including these critical ones, are pruned in hidden states.\nMotivated by this, SlimInfer introduces a dynamic fine-grained pruning\nmechanism that accurately removes redundant tokens of hidden state at\nintermediate layers. This layer-wise pruning naturally enables an asynchronous\nKV cache manager that prefetches required token blocks without complex\npredictors, reducing both memory usage and I/O costs. Extensive experiments\nshow that SlimInfer can achieve up to $\\mathbf{2.53\\times}$ time-to-first-token\n(TTFT) speedup and $\\mathbf{1.88\\times}$ end-to-end latency reduction for\nLLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on\nLongBench. Our code will be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context inference for Large Language Models (LLMs) is heavily limited by\nhigh computational demands. While several existing methods optimize attention\ncomputation, they still process the full set of hidden states at each layer,\nlimiting overall efficiency. In this work, we propose SlimInfer, an innovative\nframework that aims to accelerate inference by directly pruning less critical\nprompt tokens during the forward pass. Our key insight is an information\ndiffusion phenomenon: As information from critical tokens propagates through\nlayers, it becomes distributed across the entire sequence. This diffusion\nprocess suggests that LLMs can maintain their semantic integrity when excessive\ntokens, even including these critical ones, are pruned in hidden states.\nMotivated by this, SlimInfer introduces a dynamic fine-grained pruning\nmechanism that accurately removes redundant tokens of hidden state at\nintermediate layers. This layer-wise pruning naturally enables an asynchronous\nKV cache manager that prefetches required token blocks without complex\npredictors, reducing both memory usage and I/O costs. Extensive experiments\nshow that SlimInfer can achieve up to $\\mathbf{2.53\\times}$ time-to-first-token\n(TTFT) speedup and $\\mathbf{1.88\\times}$ end-to-end latency reduction for\nLLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on\nLongBench. Our code will be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Lingkun Long"
                    },
                    {
                        "name": "Rubing Yang"
                    },
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Desheng Hui"
                    },
                    {
                        "name": "Ao Zhou"
                    },
                    {
                        "name": "Jianlei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jianlei Yang"
                },
                "author": "Jianlei Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06447v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06447v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13900v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13900v4",
                "updated": "2025-08-08T16:39:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    39,
                    15,
                    4,
                    220,
                    0
                ],
                "published": "2024-07-18T21:06:39Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    21,
                    6,
                    39,
                    3,
                    200,
                    0
                ],
                "title": "Exploring the Evidence-Based SE Beliefs of Generative AI Tools",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Evidence-Based SE Beliefs of Generative AI Tools"
                },
                "summary": "Recent innovations in generative artificial intelligence (AI), primarily\npowered by large language models (LLMs), have transformed how programmers\ndevelop and maintain software. The advanced capabilities of generative AI tools\nto support software development tasks have led to a rise in their adoption\nwithin software engineering (SE) workflows. However, little is known about how\nAI tools perceive evidence-based beliefs and practices supported by research\nfindings. To this end, we conduct a preliminary evaluation conceptually\nreplicating prior work to explore the \"beliefs\" of generative AI tools used to\nsupport software development tasks. We investigate 17 evidence-based claims\nposited by empirical SE research across five generative AI tools. Our findings\nshow that generative AI tools have ambiguous beliefs regarding research claims\nand lack credible evidence to support responses. Based on our results, we\nprovide implications for practitioners integrating generative AI-based systems\ninto development contexts and shed light on future research directions to\nenhance the reliability and trustworthiness of generative AI -- aiming to\nincrease awareness and adoption of evidence-based SE research findings in\npractice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent innovations in generative artificial intelligence (AI), primarily\npowered by large language models (LLMs), have transformed how programmers\ndevelop and maintain software. The advanced capabilities of generative AI tools\nto support software development tasks have led to a rise in their adoption\nwithin software engineering (SE) workflows. However, little is known about how\nAI tools perceive evidence-based beliefs and practices supported by research\nfindings. To this end, we conduct a preliminary evaluation conceptually\nreplicating prior work to explore the \"beliefs\" of generative AI tools used to\nsupport software development tasks. We investigate 17 evidence-based claims\nposited by empirical SE research across five generative AI tools. Our findings\nshow that generative AI tools have ambiguous beliefs regarding research claims\nand lack credible evidence to support responses. Based on our results, we\nprovide implications for practitioners integrating generative AI-based systems\ninto development contexts and shed light on future research directions to\nenhance the reliability and trustworthiness of generative AI -- aiming to\nincrease awareness and adoption of evidence-based SE research findings in\npractice."
                },
                "authors": [
                    {
                        "name": "Chris Brown"
                    },
                    {
                        "name": "Jason Cusati"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cusati"
                },
                "author": "Jason Cusati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13900v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13900v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06445v1",
                "updated": "2025-08-08T16:38:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    38,
                    33,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T16:38:33Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    38,
                    33,
                    4,
                    220,
                    0
                ],
                "title": "Echoes of Automation: The Increasing Use of LLMs in Newsmaking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Echoes of Automation: The Increasing Use of LLMs in Newsmaking"
                },
                "summary": "The rapid rise of Generative AI (GenAI), particularly LLMs, poses concerns\nfor journalistic integrity and authorship. This study examines AI-generated\ncontent across over 40,000 news articles from major, local, and college news\nmedia, in various media formats. Using three advanced AI-text detectors (e.g.,\nBinoculars, Fast-Detect GPT, and GPTZero), we find substantial increase of\nGenAI use in recent years, especially in local and college news. Sentence-level\nanalysis reveals LLMs are often used in the introduction of news, while\nconclusions usually written manually. Linguistic analysis shows GenAI boosts\nword richness and readability but lowers formality, leading to more uniform\nwriting styles, particularly in local media.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid rise of Generative AI (GenAI), particularly LLMs, poses concerns\nfor journalistic integrity and authorship. This study examines AI-generated\ncontent across over 40,000 news articles from major, local, and college news\nmedia, in various media formats. Using three advanced AI-text detectors (e.g.,\nBinoculars, Fast-Detect GPT, and GPTZero), we find substantial increase of\nGenAI use in recent years, especially in local and college news. Sentence-level\nanalysis reveals LLMs are often used in the introduction of news, while\nconclusions usually written manually. Linguistic analysis shows GenAI boosts\nword richness and readability but lowers formality, leading to more uniform\nwriting styles, particularly in local media."
                },
                "authors": [
                    {
                        "name": "Abolfazl Ansari"
                    },
                    {
                        "name": "Delvin Ce Zhang"
                    },
                    {
                        "name": "Nafis Irtiza Tripto"
                    },
                    {
                        "name": "Dongwon Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongwon Lee"
                },
                "author": "Dongwon Lee",
                "arxiv_comment": "To appear in 18th International Conference on Social Computing,\n  Behavioral-Cultural Modeling, & Prediction and Behavior Representation in\n  Modeling and Simulation, and to be published in the Springer LNCS series",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06435v1",
                "updated": "2025-08-08T16:23:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    23,
                    24,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T16:23:24Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    23,
                    24,
                    4,
                    220,
                    0
                ],
                "title": "Learning the Topic, Not the Language: How LLMs Classify Online\n  Immigration Discourse Across Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning the Topic, Not the Language: How LLMs Classify Online\n  Immigration Discourse Across Languages"
                },
                "summary": "Large language models (LLMs) are transforming social-science research by\nenabling scalable, precise analysis. Their adaptability raises the question of\nwhether knowledge acquired through fine-tuning in a few languages can transfer\nto unseen languages that only appeared during pre-training. To examine this, we\nfine-tune lightweight LLaMA 3.2-3B models on monolingual, bilingual, or\nmultilingual data sets to classify immigration-related tweets from X/Twitter\nacross 13 languages, a domain characterised by polarised, culturally specific\ndiscourse. We evaluate whether minimal language-specific fine-tuning enables\ncross-lingual topic detection and whether adding targeted languages corrects\npre-training biases. Results show that LLMs fine-tuned in one or two languages\ncan reliably classify immigration-related content in unseen languages. However,\nidentifying whether a tweet expresses a pro- or anti-immigration stance\nbenefits from multilingual fine-tuning. Pre-training bias favours dominant\nlanguages, but even minimal exposure to under-represented languages during\nfine-tuning (as little as $9.62\\times10^{-11}$ of the original pre-training\ntoken volume) yields significant gains. These findings challenge the assumption\nthat cross-lingual mastery requires extensive multilingual training: limited\nlanguage coverage suffices for topic-level generalisation, and structural\nbiases can be corrected with lightweight interventions. By releasing\n4-bit-quantised, LoRA fine-tuned models, we provide an open-source,\nreproducible alternative to proprietary LLMs that delivers 35 times faster\ninference at just 0.00000989% of the dollar cost of the OpenAI GPT-4o model,\nenabling scalable, inclusive research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are transforming social-science research by\nenabling scalable, precise analysis. Their adaptability raises the question of\nwhether knowledge acquired through fine-tuning in a few languages can transfer\nto unseen languages that only appeared during pre-training. To examine this, we\nfine-tune lightweight LLaMA 3.2-3B models on monolingual, bilingual, or\nmultilingual data sets to classify immigration-related tweets from X/Twitter\nacross 13 languages, a domain characterised by polarised, culturally specific\ndiscourse. We evaluate whether minimal language-specific fine-tuning enables\ncross-lingual topic detection and whether adding targeted languages corrects\npre-training biases. Results show that LLMs fine-tuned in one or two languages\ncan reliably classify immigration-related content in unseen languages. However,\nidentifying whether a tweet expresses a pro- or anti-immigration stance\nbenefits from multilingual fine-tuning. Pre-training bias favours dominant\nlanguages, but even minimal exposure to under-represented languages during\nfine-tuning (as little as $9.62\\times10^{-11}$ of the original pre-training\ntoken volume) yields significant gains. These findings challenge the assumption\nthat cross-lingual mastery requires extensive multilingual training: limited\nlanguage coverage suffices for topic-level generalisation, and structural\nbiases can be corrected with lightweight interventions. By releasing\n4-bit-quantised, LoRA fine-tuned models, we provide an open-source,\nreproducible alternative to proprietary LLMs that delivers 35 times faster\ninference at just 0.00000989% of the dollar cost of the OpenAI GPT-4o model,\nenabling scalable, inclusive research."
                },
                "authors": [
                    {
                        "name": "Andrea Nasuto"
                    },
                    {
                        "name": "Stefano Maria Iacus"
                    },
                    {
                        "name": "Francisco Rowe"
                    },
                    {
                        "name": "Devika Jain"
                    }
                ],
                "author_detail": {
                    "name": "Devika Jain"
                },
                "author": "Devika Jain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06433v1",
                "updated": "2025-08-08T16:20:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    20,
                    56,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T16:20:56Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    20,
                    56,
                    4,
                    220,
                    0
                ],
                "title": "Memp: Exploring Agent Procedural Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memp: Exploring Agent Procedural Memory"
                },
                "summary": "Large Language Models (LLMs) based agents excel at diverse tasks, yet they\nsuffer from brittle procedural memory that is manually engineered or entangled\nin static parameters. In this work, we investigate strategies to endow agents\nwith a learnable, updatable, and lifelong procedural memory. We propose Memp\nthat distills past agent trajectories into both fine-grained, step-by-step\ninstructions and higher-level, script-like abstractions, and explore the impact\nof different strategies for Build, Retrieval, and Update of procedural memory.\nCoupled with a dynamic regimen that continuously updates, corrects, and\ndeprecates its contents, this repository evolves in lockstep with new\nexperience. Empirical evaluation on TravelPlanner and ALFWorld shows that as\nthe memory repository is refined, agents achieve steadily higher success rates\nand greater efficiency on analogous tasks. Moreover, procedural memory built\nfrom a stronger model retains its value: migrating the procedural memory to a\nweaker model yields substantial performance gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) based agents excel at diverse tasks, yet they\nsuffer from brittle procedural memory that is manually engineered or entangled\nin static parameters. In this work, we investigate strategies to endow agents\nwith a learnable, updatable, and lifelong procedural memory. We propose Memp\nthat distills past agent trajectories into both fine-grained, step-by-step\ninstructions and higher-level, script-like abstractions, and explore the impact\nof different strategies for Build, Retrieval, and Update of procedural memory.\nCoupled with a dynamic regimen that continuously updates, corrects, and\ndeprecates its contents, this repository evolves in lockstep with new\nexperience. Empirical evaluation on TravelPlanner and ALFWorld shows that as\nthe memory repository is refined, agents achieve steadily higher success rates\nand greater efficiency on analogous tasks. Moreover, procedural memory built\nfrom a stronger model retains its value: migrating the procedural memory to a\nweaker model yields substantial performance gains."
                },
                "authors": [
                    {
                        "name": "Runnan Fang"
                    },
                    {
                        "name": "Yuan Liang"
                    },
                    {
                        "name": "Xiaobin Wang"
                    },
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Ningyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ningyu Zhang"
                },
                "author": "Ningyu Zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18826v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18826v2",
                "updated": "2025-08-08T16:07:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    7,
                    50,
                    4,
                    220,
                    0
                ],
                "published": "2025-01-31T00:46:21Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    0,
                    46,
                    21,
                    4,
                    31,
                    0
                ],
                "title": "Structural Embedding Projection for Contextual Large Language Model\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structural Embedding Projection for Contextual Large Language Model\n  Inference"
                },
                "summary": "Structured embedding transformations offer a promising approach for enhancing\nthe efficiency and coherence of language model inference. The introduction of\nStructural Embedding Projection (SEP) provides a mechanism for refining token\nrepresentations through projection matrices that integrate hierarchical and\nrelational dependencies. The mathematical formulation of SEP enables embedding\nspaces to capture structured contextual relationships, thereby improving\nsemantic fidelity without significantly increasing computational overhead.\nExperimental evaluations conducted on a range of linguistic datasets revealed\nthat SEP contributed to reductions in perplexity and enhanced contextual\ncoherence, demonstrating its potential to refine language model outputs.\nComputational efficiency assessments highlighted variations across different\ndatasets, suggesting that the integration of structured embeddings introduced\ndataset-dependent trade-offs between inference speed and representational\nrichness. The qualitative analysis of generated responses indicated that SEP\nenhanced narrative consistency and topic alignment, leading to improved fluency\nin multi-sentence text generation. The modifications to embedding layers\nrequired precise optimization to ensure stable training dynamics, as the\nintroduction of structured transformations altered the traditional\nrepresentation-learning process. The architectural adjustments necessary for\nSEP implementation influenced inference latency and memory consumption,\nrequiring a balance between efficiency gains and additional processing demands.\nThe impact of SEP on lexical diversity suggested that embedding modifications\ninfluenced the model's vocabulary usage, reflecting a more context-aware\nselection of generated tokens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured embedding transformations offer a promising approach for enhancing\nthe efficiency and coherence of language model inference. The introduction of\nStructural Embedding Projection (SEP) provides a mechanism for refining token\nrepresentations through projection matrices that integrate hierarchical and\nrelational dependencies. The mathematical formulation of SEP enables embedding\nspaces to capture structured contextual relationships, thereby improving\nsemantic fidelity without significantly increasing computational overhead.\nExperimental evaluations conducted on a range of linguistic datasets revealed\nthat SEP contributed to reductions in perplexity and enhanced contextual\ncoherence, demonstrating its potential to refine language model outputs.\nComputational efficiency assessments highlighted variations across different\ndatasets, suggesting that the integration of structured embeddings introduced\ndataset-dependent trade-offs between inference speed and representational\nrichness. The qualitative analysis of generated responses indicated that SEP\nenhanced narrative consistency and topic alignment, leading to improved fluency\nin multi-sentence text generation. The modifications to embedding layers\nrequired precise optimization to ensure stable training dynamics, as the\nintroduction of structured transformations altered the traditional\nrepresentation-learning process. The architectural adjustments necessary for\nSEP implementation influenced inference latency and memory consumption,\nrequiring a balance between efficiency gains and additional processing demands.\nThe impact of SEP on lexical diversity suggested that embedding modifications\ninfluenced the model's vocabulary usage, reflecting a more context-aware\nselection of generated tokens."
                },
                "authors": [
                    {
                        "name": "Vincent Enoasmo"
                    },
                    {
                        "name": "Cedric Featherstonehaugh"
                    },
                    {
                        "name": "Xavier Konstantinopoulos"
                    },
                    {
                        "name": "Zacharias Huntington"
                    }
                ],
                "author_detail": {
                    "name": "Zacharias Huntington"
                },
                "author": "Zacharias Huntington",
                "arxiv_comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18826v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18826v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00900v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00900v2",
                "updated": "2025-08-08T16:07:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    7,
                    40,
                    4,
                    220,
                    0
                ],
                "published": "2024-07-01T01:56:28Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    1,
                    56,
                    28,
                    0,
                    183,
                    0
                ],
                "title": "From Next-Token to Mathematics: The Learning Dynamics of Mathematical\n  Reasoning in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Next-Token to Mathematics: The Learning Dynamics of Mathematical\n  Reasoning in Language Models"
                },
                "summary": "Large Language Models (LLMs) solely trained on next-token prediction learn to\nsolve a wide range of problems involving mathematical reasoning. But how does\nthis ability evolve during training? We show the first analysis of how\nmathematical reasoning abilities of several open-weight LLMs develop during\npre-training and post-training. To this end, we construct MathCAMPS, a\nsynthetic dataset of novel mathematical reasoning problems grounded in 44\nfine-grained skills taken from the Common Core curriculum from K to 8th grades.\nIn one experiment, we show that mathematical skills are learned during\npre-training in an order that measurably correlates with the human-designed\ncurriculum, even though training data are randomly ordered. We also show a\ndetailed analysis of which mathematical abilities benefit from instruction\ntuning, a widely used post-training method and, in contrast, which skills\nsuffer. Our work paves the way for an empirical understanding of LLM training\ndynamics in relation to reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) solely trained on next-token prediction learn to\nsolve a wide range of problems involving mathematical reasoning. But how does\nthis ability evolve during training? We show the first analysis of how\nmathematical reasoning abilities of several open-weight LLMs develop during\npre-training and post-training. To this end, we construct MathCAMPS, a\nsynthetic dataset of novel mathematical reasoning problems grounded in 44\nfine-grained skills taken from the Common Core curriculum from K to 8th grades.\nIn one experiment, we show that mathematical skills are learned during\npre-training in an order that measurably correlates with the human-designed\ncurriculum, even though training data are randomly ordered. We also show a\ndetailed analysis of which mathematical abilities benefit from instruction\ntuning, a widely used post-training method and, in contrast, which skills\nsuffer. Our work paves the way for an empirical understanding of LLM training\ndynamics in relation to reasoning."
                },
                "authors": [
                    {
                        "name": "Shubhra Mishra"
                    },
                    {
                        "name": "Gabriel Poesia"
                    },
                    {
                        "name": "Noah D. Goodman"
                    }
                ],
                "author_detail": {
                    "name": "Noah D. Goodman"
                },
                "author": "Noah D. Goodman",
                "arxiv_comment": "Accepted to COLM 2025. Dataset and code:\n  https://github.com/gpoesia/mathcamps/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00900v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00900v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00301v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00301v2",
                "updated": "2025-08-08T16:07:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    7,
                    22,
                    4,
                    220,
                    0
                ],
                "published": "2025-02-01T03:50:46Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    50,
                    46,
                    5,
                    32,
                    0
                ],
                "title": "Contextual Morphogenesis in Large Language Models: A Novel Approach to\n  Self-Organizing Token Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextual Morphogenesis in Large Language Models: A Novel Approach to\n  Self-Organizing Token Representations"
                },
                "summary": "Token representations influence the efficiency and adaptability of language\nmodels, yet conventional tokenization strategies impose rigid segmentation\nboundaries that do not adjust dynamically to evolving contextual relationships.\nThe introduction of contextual morphogenesis establishes a self-organizing\nmechanism that restructures token boundaries based on learned contextual\ndependencies, allowing embeddings to evolve progressively across iterative\nprocessing steps. Empirical evaluations demonstrate that dynamically adjusted\ntokenization contributes to reductions in perplexity while maintaining\nrepresentational stability, particularly in linguistically complex domains\nwhere static segmentation fails to capture nuanced dependencies. Computational\ntrade-offs associated with self-organizing token structures indicate that\nadditional processing overhead remains within feasible limits, provided that\noptimization strategies account for segmentation update efficiency. Comparative\nassessments across different linguistic corpora suggest that adaptive\ntokenization preserves interpretability while improving alignment with\ncontextual cues, reinforcing the potential of morphogenetic segmentation\nmechanisms to refine predictive accuracy. Stability analyses confirm that\nevolving token structures maintain consistent segmentation behaviors across\nvaried text distributions, ensuring that representational adaptations remain\nlinguistically coherent. The effectiveness of contextual morphogenesis in\nrefining structural stability and predictive performance highlights its\nviability as an alternative to traditional tokenization methods. Further\nanalysis of computational efficiency considerations suggests that hybrid\nstrategies integrating both static and dynamic segmentation techniques may\noffer a balanced approach to optimizing representational flexibility while\nmaintaining inference efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token representations influence the efficiency and adaptability of language\nmodels, yet conventional tokenization strategies impose rigid segmentation\nboundaries that do not adjust dynamically to evolving contextual relationships.\nThe introduction of contextual morphogenesis establishes a self-organizing\nmechanism that restructures token boundaries based on learned contextual\ndependencies, allowing embeddings to evolve progressively across iterative\nprocessing steps. Empirical evaluations demonstrate that dynamically adjusted\ntokenization contributes to reductions in perplexity while maintaining\nrepresentational stability, particularly in linguistically complex domains\nwhere static segmentation fails to capture nuanced dependencies. Computational\ntrade-offs associated with self-organizing token structures indicate that\nadditional processing overhead remains within feasible limits, provided that\noptimization strategies account for segmentation update efficiency. Comparative\nassessments across different linguistic corpora suggest that adaptive\ntokenization preserves interpretability while improving alignment with\ncontextual cues, reinforcing the potential of morphogenetic segmentation\nmechanisms to refine predictive accuracy. Stability analyses confirm that\nevolving token structures maintain consistent segmentation behaviors across\nvaried text distributions, ensuring that representational adaptations remain\nlinguistically coherent. The effectiveness of contextual morphogenesis in\nrefining structural stability and predictive performance highlights its\nviability as an alternative to traditional tokenization methods. Further\nanalysis of computational efficiency considerations suggests that hybrid\nstrategies integrating both static and dynamic segmentation techniques may\noffer a balanced approach to optimizing representational flexibility while\nmaintaining inference efficiency."
                },
                "authors": [
                    {
                        "name": "Alistair Dombrowski"
                    },
                    {
                        "name": "Beatrix Engelhardt"
                    },
                    {
                        "name": "Dimitri Fairbrother"
                    },
                    {
                        "name": "Henry Evidail"
                    }
                ],
                "author_detail": {
                    "name": "Henry Evidail"
                },
                "author": "Henry Evidail",
                "arxiv_comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00301v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00301v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08947v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08947v2",
                "updated": "2025-08-08T16:06:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    6,
                    51,
                    4,
                    220,
                    0
                ],
                "published": "2025-02-13T04:01:54Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    4,
                    1,
                    54,
                    3,
                    44,
                    0
                ],
                "title": "Structured Convergence in Large Language Model Representations via\n  Hierarchical Latent Space Folding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured Convergence in Large Language Model Representations via\n  Hierarchical Latent Space Folding"
                },
                "summary": "Token representations in high-dimensional latent spaces often exhibit\nredundancy, limiting computational efficiency and reducing structural coherence\nacross model layers. Hierarchical latent space folding introduces a structured\ntransformation mechanism that enforces a multi-scale organization within\nlearned embeddings, refining representational compactness while preserving\nessential contextual distinctions. The proposed approach incorporates dynamic\nfolding operations that iteratively adjust token embeddings through structured\ntransformations, influencing both short-range and long-range dependencies in\nsequential processing tasks. Empirical evaluation demonstrates a reduction in\nrepresentational variance across layers, contributing to more stable perplexity\ndistributions and enhancing predictive confidence in text generation. The\nstructured redistribution of attention head utilization leads to more efficient\nallocation of computational resources, particularly in deeper layers, where\nhierarchical refinements improve contextual abstraction. Comparative analysis\nof activation sparsity patterns suggests that hierarchical adjustments\nselectively reinforce critical pathways while reducing computational overhead\nin non-essential regions of the model. Statistical assessments of token\nreordering frequencies reveal that hierarchical modifications introduce subtle\nshifts in sequential dependencies, improving contextual alignment while\nmaintaining syntactic correctness. Computational trade-offs associated with\nhierarchical folding introduce marginal increases in training time per epoch,\nyet empirical findings indicate that inference efficiency benefits from the\nstructured representation adjustments. The results highlight the impact of\nhierarchical latent space folding on optimizing model performance through\nimproved representation structuring and computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token representations in high-dimensional latent spaces often exhibit\nredundancy, limiting computational efficiency and reducing structural coherence\nacross model layers. Hierarchical latent space folding introduces a structured\ntransformation mechanism that enforces a multi-scale organization within\nlearned embeddings, refining representational compactness while preserving\nessential contextual distinctions. The proposed approach incorporates dynamic\nfolding operations that iteratively adjust token embeddings through structured\ntransformations, influencing both short-range and long-range dependencies in\nsequential processing tasks. Empirical evaluation demonstrates a reduction in\nrepresentational variance across layers, contributing to more stable perplexity\ndistributions and enhancing predictive confidence in text generation. The\nstructured redistribution of attention head utilization leads to more efficient\nallocation of computational resources, particularly in deeper layers, where\nhierarchical refinements improve contextual abstraction. Comparative analysis\nof activation sparsity patterns suggests that hierarchical adjustments\nselectively reinforce critical pathways while reducing computational overhead\nin non-essential regions of the model. Statistical assessments of token\nreordering frequencies reveal that hierarchical modifications introduce subtle\nshifts in sequential dependencies, improving contextual alignment while\nmaintaining syntactic correctness. Computational trade-offs associated with\nhierarchical folding introduce marginal increases in training time per epoch,\nyet empirical findings indicate that inference efficiency benefits from the\nstructured representation adjustments. The results highlight the impact of\nhierarchical latent space folding on optimizing model performance through\nimproved representation structuring and computational efficiency."
                },
                "authors": [
                    {
                        "name": "Fenella Harcourt"
                    },
                    {
                        "name": "Naderdel Piero"
                    },
                    {
                        "name": "Gilbert Sutherland"
                    },
                    {
                        "name": "Daphne Holloway"
                    },
                    {
                        "name": "Harriet Bracknell"
                    },
                    {
                        "name": "Julian Ormsby"
                    }
                ],
                "author_detail": {
                    "name": "Julian Ormsby"
                },
                "author": "Julian Ormsby",
                "arxiv_comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08947v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08947v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06418v1",
                "updated": "2025-08-08T16:05:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    5,
                    27,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T16:05:27Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    5,
                    27,
                    4,
                    220,
                    0
                ],
                "title": "Quantifying Conversation Drift in MCP via Latent Polytope",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying Conversation Drift in MCP via Latent Polytope"
                },
                "summary": "The Model Context Protocol (MCP) enhances large language models (LLMs) by\nintegrating external tools, enabling dynamic aggregation of real-time data to\nimprove task execution. However, its non-isolated execution context introduces\ncritical security and privacy risks. In particular, adversarially crafted\ncontent can induce tool poisoning or indirect prompt injection, leading to\nconversation hijacking, misinformation propagation, or data exfiltration.\nExisting defenses, such as rule-based filters or LLM-driven detection, remain\ninadequate due to their reliance on static signatures, computational\ninefficiency, and inability to quantify conversational hijacking. To address\nthese limitations, we propose SecMCP, a secure framework that detects and\nquantifies conversation drift, deviations in latent space trajectories induced\nby adversarial external knowledge. By modeling LLM activation vectors within a\nlatent polytope space, SecMCP identifies anomalous shifts in conversational\ndynamics, enabling proactive detection of hijacking, misleading, and data\nexfiltration. We evaluate SecMCP on three state-of-the-art LLMs (Llama3,\nVicuna, Mistral) across benchmark datasets (MS MARCO, HotpotQA, FinQA),\ndemonstrating robust detection with AUROC scores exceeding 0.915 while\nmaintaining system usability. Our contributions include a systematic\ncategorization of MCP security threats, a novel latent polytope-based\nmethodology for quantifying conversation drift, and empirical validation of\nSecMCP's efficacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Model Context Protocol (MCP) enhances large language models (LLMs) by\nintegrating external tools, enabling dynamic aggregation of real-time data to\nimprove task execution. However, its non-isolated execution context introduces\ncritical security and privacy risks. In particular, adversarially crafted\ncontent can induce tool poisoning or indirect prompt injection, leading to\nconversation hijacking, misinformation propagation, or data exfiltration.\nExisting defenses, such as rule-based filters or LLM-driven detection, remain\ninadequate due to their reliance on static signatures, computational\ninefficiency, and inability to quantify conversational hijacking. To address\nthese limitations, we propose SecMCP, a secure framework that detects and\nquantifies conversation drift, deviations in latent space trajectories induced\nby adversarial external knowledge. By modeling LLM activation vectors within a\nlatent polytope space, SecMCP identifies anomalous shifts in conversational\ndynamics, enabling proactive detection of hijacking, misleading, and data\nexfiltration. We evaluate SecMCP on three state-of-the-art LLMs (Llama3,\nVicuna, Mistral) across benchmark datasets (MS MARCO, HotpotQA, FinQA),\ndemonstrating robust detection with AUROC scores exceeding 0.915 while\nmaintaining system usability. Our contributions include a systematic\ncategorization of MCP security threats, a novel latent polytope-based\nmethodology for quantifying conversation drift, and empirical validation of\nSecMCP's efficacy."
                },
                "authors": [
                    {
                        "name": "Haoran Shi"
                    },
                    {
                        "name": "Hongwei Yao"
                    },
                    {
                        "name": "Shuo Shao"
                    },
                    {
                        "name": "Shaopeng Jiao"
                    },
                    {
                        "name": "Ziqi Peng"
                    },
                    {
                        "name": "Zhan Qin"
                    },
                    {
                        "name": "Cong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Cong Wang"
                },
                "author": "Cong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05553v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05553v2",
                "updated": "2025-08-08T16:05:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    5,
                    12,
                    4,
                    220,
                    0
                ],
                "published": "2025-02-08T12:53:52Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    12,
                    53,
                    52,
                    5,
                    39,
                    0
                ],
                "title": "Latent Structure Modulation in Large Language Models Through Stochastic\n  Concept Embedding Transitions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Structure Modulation in Large Language Models Through Stochastic\n  Concept Embedding Transitions"
                },
                "summary": "Stochastic embedding transitions introduce a probabilistic mechanism for\nadjusting token representations dynamically during inference, mitigating the\nconstraints imposed through static or deterministic embeddings. A transition\nframework was proposed in which each token embedding evolved through\nprobabilistic updates, ensuring adaptability while preserving semantic\nintegrity across linguistic contexts. Empirical evaluations demonstrated that\nmodels incorporating stochastic transitions exhibited greater lexical\ndiversity, improved generative coherence, and enhanced retention of\nlow-frequency vocabulary, contributing to more varied sentence structures and\nreduced reliance on high-probability token selections. Statistical analyses of\nembedding drift across transformer layers indicated that representations\nevolved more flexibly without losing coherence, supporting the hypothesis that\ncontrolled stochasticity facilitated context-sensitive representation learning.\nExperimental results revealed that probabilistic embeddings introduced minor\ncomputational overhead while maintaining generative efficiency, reinforcing\ntheir feasibility in large-scale applications. A comparative study with\ntraditional embedding approaches highlighted measurable gains in text\ncompletion accuracy, dialogue coherence, and structural complexity, confirming\nthe effectiveness of stochastic transitions in enhancing representation\nexpressiveness. Clustering patterns in the embedding space suggested that\nprobabilistic updates preserved meaningful semantic groupings while enabling\ncontext-driven shifts, further validating the stability of the transition\nmechanism. Performance metrics indicated that stochastic transitions balanced\nadaptability and control, ensuring that generative outputs remained\nlinguistically coherent without excessive randomness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic embedding transitions introduce a probabilistic mechanism for\nadjusting token representations dynamically during inference, mitigating the\nconstraints imposed through static or deterministic embeddings. A transition\nframework was proposed in which each token embedding evolved through\nprobabilistic updates, ensuring adaptability while preserving semantic\nintegrity across linguistic contexts. Empirical evaluations demonstrated that\nmodels incorporating stochastic transitions exhibited greater lexical\ndiversity, improved generative coherence, and enhanced retention of\nlow-frequency vocabulary, contributing to more varied sentence structures and\nreduced reliance on high-probability token selections. Statistical analyses of\nembedding drift across transformer layers indicated that representations\nevolved more flexibly without losing coherence, supporting the hypothesis that\ncontrolled stochasticity facilitated context-sensitive representation learning.\nExperimental results revealed that probabilistic embeddings introduced minor\ncomputational overhead while maintaining generative efficiency, reinforcing\ntheir feasibility in large-scale applications. A comparative study with\ntraditional embedding approaches highlighted measurable gains in text\ncompletion accuracy, dialogue coherence, and structural complexity, confirming\nthe effectiveness of stochastic transitions in enhancing representation\nexpressiveness. Clustering patterns in the embedding space suggested that\nprobabilistic updates preserved meaningful semantic groupings while enabling\ncontext-driven shifts, further validating the stability of the transition\nmechanism. Performance metrics indicated that stochastic transitions balanced\nadaptability and control, ensuring that generative outputs remained\nlinguistically coherent without excessive randomness."
                },
                "authors": [
                    {
                        "name": "Stefan Whitaker"
                    },
                    {
                        "name": "Colin Sisate"
                    },
                    {
                        "name": "Marcel Windsor"
                    },
                    {
                        "name": "Nikolai Fairweather"
                    },
                    {
                        "name": "Tarquin Goldborough"
                    },
                    {
                        "name": "Oskar Lindenfeld"
                    }
                ],
                "author_detail": {
                    "name": "Oskar Lindenfeld"
                },
                "author": "Oskar Lindenfeld",
                "arxiv_comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05553v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05553v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00048v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00048v2",
                "updated": "2025-08-08T16:05:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    5,
                    0,
                    4,
                    220,
                    0
                ],
                "published": "2025-01-28T11:50:35Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    11,
                    50,
                    35,
                    1,
                    28,
                    0
                ],
                "title": "Contextually Entangled Gradient Mapping for Optimized LLM Comprehension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextually Entangled Gradient Mapping for Optimized LLM Comprehension"
                },
                "summary": "Contextually Entangled Gradient Mapping (CEGM) introduces a new approach to\ngradient optimization, redefining the relationship between contextual\nembeddings and gradient updates to enhance semantic coherence and reasoning\ncapabilities in neural architectures. By treating gradients as dynamic carriers\nof contextual dependencies rather than isolated numerical entities, the\nproposed methodology bridges critical gaps in existing optimization strategies.\nThe integration of entangled gradient dynamics into a loss regularization\nframework demonstrated significant improvements in tasks involving long-form\nreasoning, contextual retention, and adaptability to unseen domains.\nExperimental evaluations showed that the CEGM-enhanced model consistently\noutperformed baseline approaches, achieving higher accuracy in token-level\npredictions and greater resilience to noisy inputs. Practical implementations\ninvolved modifications to training pipelines, introducing entanglement layers\nand dynamic coefficient adjustments that seamlessly align with existing\narchitectures. Results further highlighted reductions in semantic drift during\nsequential transformations and improvements in embedding coherence across\nparaphrased sentences, showing the robustness and versatility of the proposed\nmethodology. The findings demonstrate the broader implications of gradient\nentanglement for both theoretical advancements and practical applications in\noptimization strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextually Entangled Gradient Mapping (CEGM) introduces a new approach to\ngradient optimization, redefining the relationship between contextual\nembeddings and gradient updates to enhance semantic coherence and reasoning\ncapabilities in neural architectures. By treating gradients as dynamic carriers\nof contextual dependencies rather than isolated numerical entities, the\nproposed methodology bridges critical gaps in existing optimization strategies.\nThe integration of entangled gradient dynamics into a loss regularization\nframework demonstrated significant improvements in tasks involving long-form\nreasoning, contextual retention, and adaptability to unseen domains.\nExperimental evaluations showed that the CEGM-enhanced model consistently\noutperformed baseline approaches, achieving higher accuracy in token-level\npredictions and greater resilience to noisy inputs. Practical implementations\ninvolved modifications to training pipelines, introducing entanglement layers\nand dynamic coefficient adjustments that seamlessly align with existing\narchitectures. Results further highlighted reductions in semantic drift during\nsequential transformations and improvements in embedding coherence across\nparaphrased sentences, showing the robustness and versatility of the proposed\nmethodology. The findings demonstrate the broader implications of gradient\nentanglement for both theoretical advancements and practical applications in\noptimization strategies."
                },
                "authors": [
                    {
                        "name": "Colin Sisate"
                    },
                    {
                        "name": "Alistair Goldfinch"
                    },
                    {
                        "name": "Vincent Waterstone"
                    },
                    {
                        "name": "Sebastian Kingsley"
                    },
                    {
                        "name": "Mariana Blackthorn"
                    }
                ],
                "author_detail": {
                    "name": "Mariana Blackthorn"
                },
                "author": "Mariana Blackthorn",
                "arxiv_comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00048v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00048v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10699v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10699v2",
                "updated": "2025-08-08T16:04:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    4,
                    8,
                    4,
                    220,
                    0
                ],
                "published": "2025-02-15T07:06:10Z",
                "published_parsed": [
                    2025,
                    2,
                    15,
                    7,
                    6,
                    10,
                    5,
                    46,
                    0
                ],
                "title": "Exploring Synaptic Resonance in Large Language Models: A Novel Approach\n  to Contextual Memory Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Synaptic Resonance in Large Language Models: A Novel Approach\n  to Contextual Memory Integration"
                },
                "summary": "Contextual memory integration remains a high challenge in the development of\nlanguage models, particularly in tasks that require maintaining coherence over\nextended sequences. Traditional approaches, such as self-attention mechanisms\nand memory-augmented architectures, often prioritize short-term dependencies,\nleading to fragmentation and inconsistency in long-range contextual\nunderstanding. Inspired by principles of synaptic plasticity observed in\nbiological neural systems, a novel mechanism, Synaptic Resonance, is introduced\nto dynamically reinforce relevant memory pathways during training and\ninference. Unlike static memory representations, this mechanism continuously\nadjusts synaptic weight matrices based on contextual relevance, allowing for\nimproved information retention without excessive computational overhead.\nEvaluations conducted on an open-source language model demonstrate reductions\nin perplexity, enhancements in contextual coherence, and increased robustness\nagainst input noise, highlighting the effectiveness of reinforcement-driven\nmemory modulation. Comparative analysis against baseline models further reveals\nthat the proposed approach achieves higher memory retention efficiency while\nmaintaining computational feasibility. The architectural modifications\nintegrate seamlessly into existing transformer-based frameworks, ensuring\nstable convergence and efficient inference without sacrificing scalability.\nApplications benefiting from improved long-term contextual consistency, such as\ndialogue systems and document summarization, stand to gain from this approach.\nEmpirical findings suggest that dynamically reinforced memory pathways offer a\npromising alternative to conventional memory mechanisms, addressing\nlongstanding limitations in extended sequence modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextual memory integration remains a high challenge in the development of\nlanguage models, particularly in tasks that require maintaining coherence over\nextended sequences. Traditional approaches, such as self-attention mechanisms\nand memory-augmented architectures, often prioritize short-term dependencies,\nleading to fragmentation and inconsistency in long-range contextual\nunderstanding. Inspired by principles of synaptic plasticity observed in\nbiological neural systems, a novel mechanism, Synaptic Resonance, is introduced\nto dynamically reinforce relevant memory pathways during training and\ninference. Unlike static memory representations, this mechanism continuously\nadjusts synaptic weight matrices based on contextual relevance, allowing for\nimproved information retention without excessive computational overhead.\nEvaluations conducted on an open-source language model demonstrate reductions\nin perplexity, enhancements in contextual coherence, and increased robustness\nagainst input noise, highlighting the effectiveness of reinforcement-driven\nmemory modulation. Comparative analysis against baseline models further reveals\nthat the proposed approach achieves higher memory retention efficiency while\nmaintaining computational feasibility. The architectural modifications\nintegrate seamlessly into existing transformer-based frameworks, ensuring\nstable convergence and efficient inference without sacrificing scalability.\nApplications benefiting from improved long-term contextual consistency, such as\ndialogue systems and document summarization, stand to gain from this approach.\nEmpirical findings suggest that dynamically reinforced memory pathways offer a\npromising alternative to conventional memory mechanisms, addressing\nlongstanding limitations in extended sequence modeling."
                },
                "authors": [
                    {
                        "name": "George Applegarth"
                    },
                    {
                        "name": "Christian Weatherstone"
                    },
                    {
                        "name": "Maximilian Hollingsworth"
                    },
                    {
                        "name": "Henry Middlebrook"
                    },
                    {
                        "name": "Marcus Irvin"
                    }
                ],
                "author_detail": {
                    "name": "Marcus Irvin"
                },
                "author": "Marcus Irvin",
                "arxiv_comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10699v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10699v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07124v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07124v2",
                "updated": "2025-08-08T16:03:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    3,
                    8,
                    4,
                    220,
                    0
                ],
                "published": "2025-02-10T23:37:39Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    37,
                    39,
                    0,
                    41,
                    0
                ],
                "title": "Structural Reformation of Large Language Model Neuron Encapsulation for\n  Divergent Information Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structural Reformation of Large Language Model Neuron Encapsulation for\n  Divergent Information Aggregation"
                },
                "summary": "Structured neuron encapsulation introduces a modular framework that enables\nmore effective aggregation and specialization of information within deep\nlearning architectures. A model modified through this framework demonstrated\nimproved perplexity scores, greater lexical variability, and enhanced\nconsistency in logical reasoning, suggesting that structured parameter\ndistribution contributes to more efficient language representation. Statistical\nanalyses of generated text highlighted a wider range of sentence structures and\nreduced redundancy in token selection, indicating that encapsulation fosters\nmore adaptable language generation. A detailed evaluation of attention weight\ndistributions revealed that the experimental model exhibited greater divergence\nin cross-layer activations, supporting the hypothesis that encapsulated neurons\nassume specialized processing roles. Logical consistency assessments further\ndemonstrated that modular architectures mitigate contradictory outputs,\nreducing internal conflicts in inferred relationships between linguistic\nconstructs. Computational trade-offs were analyzed, with results showing a\nminor increase in processing overhead, though improvements in parameter\nefficiency and structured decision-making compensated for the additional\ncomplexity. The mathematical formulation of the encapsulation mechanism\nconfirmed that modular aggregation maintains stable convergence properties\nwhile promoting distinct functional roles for different neuron clusters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured neuron encapsulation introduces a modular framework that enables\nmore effective aggregation and specialization of information within deep\nlearning architectures. A model modified through this framework demonstrated\nimproved perplexity scores, greater lexical variability, and enhanced\nconsistency in logical reasoning, suggesting that structured parameter\ndistribution contributes to more efficient language representation. Statistical\nanalyses of generated text highlighted a wider range of sentence structures and\nreduced redundancy in token selection, indicating that encapsulation fosters\nmore adaptable language generation. A detailed evaluation of attention weight\ndistributions revealed that the experimental model exhibited greater divergence\nin cross-layer activations, supporting the hypothesis that encapsulated neurons\nassume specialized processing roles. Logical consistency assessments further\ndemonstrated that modular architectures mitigate contradictory outputs,\nreducing internal conflicts in inferred relationships between linguistic\nconstructs. Computational trade-offs were analyzed, with results showing a\nminor increase in processing overhead, though improvements in parameter\nefficiency and structured decision-making compensated for the additional\ncomplexity. The mathematical formulation of the encapsulation mechanism\nconfirmed that modular aggregation maintains stable convergence properties\nwhile promoting distinct functional roles for different neuron clusters."
                },
                "authors": [
                    {
                        "name": "Denis Bakushev"
                    },
                    {
                        "name": "Gideon Boultinghouse"
                    },
                    {
                        "name": "Harriet Oppenheimer"
                    },
                    {
                        "name": "Sebastian Gillingwater"
                    },
                    {
                        "name": "Valentina Ashington"
                    },
                    {
                        "name": "Wilfred Stanborough"
                    }
                ],
                "author_detail": {
                    "name": "Wilfred Stanborough"
                },
                "author": "Wilfred Stanborough",
                "arxiv_comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07124v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07124v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06414v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06414v1",
                "updated": "2025-08-08T15:58:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    58,
                    11,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T15:58:11Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    58,
                    11,
                    4,
                    220,
                    0
                ],
                "title": "What Builds Effective In-Context Examples for Code Generation?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Builds Effective In-Context Examples for Code Generation?"
                },
                "summary": "In-Context Learning (ICL) has emerged as a promising solution to enhance the\ncode generation capabilities of Large Language Models (LLMs), which\nincorporates code examples inside the prompt to let LLMs learn from\ndemonstrations. However, despite the substantial effectiveness of the code\nexample-based ICL approach, the specific features (e.g., identifier naming\nstyles, code formatting, solution insight) within the ICL-provided code\nexamples that significantly contribute to the ICL's effectiveness remain\nunclear. This paper systematically investigates the impact of various code\nfeatures on ICL with code examples through controlled ablation studies. Our\nfindings reveal that the appropriate naming of variables and functions is\ncrucial for effective code generation, with their elimination leading to\nperformance decreases of up to 30 percentage points. We further demonstrate\nthat LLMs prioritize semantically meaningful identifier names over formatting\nconventions, with language-specific preferences regarding identifier verbosity.\nAdditionally, our investigation into ICL's potential for enhancing reflection\nand inference capabilities reveals that current LLMs struggle to extract\ngeneralizable problem-solving insights from similar code solutions, despite\nbeing capable of utilizing direct information effectively. These findings are\nexpected to provide valuable insights for optimizing ICL systems in code\ngeneration applications and highlight fundamental challenges in\nreflection-based learning for code generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Learning (ICL) has emerged as a promising solution to enhance the\ncode generation capabilities of Large Language Models (LLMs), which\nincorporates code examples inside the prompt to let LLMs learn from\ndemonstrations. However, despite the substantial effectiveness of the code\nexample-based ICL approach, the specific features (e.g., identifier naming\nstyles, code formatting, solution insight) within the ICL-provided code\nexamples that significantly contribute to the ICL's effectiveness remain\nunclear. This paper systematically investigates the impact of various code\nfeatures on ICL with code examples through controlled ablation studies. Our\nfindings reveal that the appropriate naming of variables and functions is\ncrucial for effective code generation, with their elimination leading to\nperformance decreases of up to 30 percentage points. We further demonstrate\nthat LLMs prioritize semantically meaningful identifier names over formatting\nconventions, with language-specific preferences regarding identifier verbosity.\nAdditionally, our investigation into ICL's potential for enhancing reflection\nand inference capabilities reveals that current LLMs struggle to extract\ngeneralizable problem-solving insights from similar code solutions, despite\nbeing capable of utilizing direct information effectively. These findings are\nexpected to provide valuable insights for optimizing ICL systems in code\ngeneration applications and highlight fundamental challenges in\nreflection-based learning for code generation tasks."
                },
                "authors": [
                    {
                        "name": "Dongze Li"
                    },
                    {
                        "name": "Songqiang Chen"
                    },
                    {
                        "name": "Jialun Cao"
                    },
                    {
                        "name": "Shing-Chi Cheung"
                    }
                ],
                "author_detail": {
                    "name": "Shing-Chi Cheung"
                },
                "author": "Shing-Chi Cheung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06414v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06414v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06412v1",
                "updated": "2025-08-08T15:56:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    56,
                    49,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T15:56:49Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    56,
                    49,
                    4,
                    220,
                    0
                ],
                "title": "Sample-efficient LLM Optimization with Reset Replay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sample-efficient LLM Optimization with Reset Replay"
                },
                "summary": "Recent advancements in post-training Large Language Models (LLMs),\nparticularly through Reinforcement Learning (RL) and preference optimization\nmethods, are key drivers for enhancing their reasoning capabilities. However,\nthese methods are often plagued by low sample efficiency and a susceptibility\nto primacy bias, where overfitting to initial experiences degrades policy\nquality and damages the learning process. To address these challenges, we\nintroduce LLM optimization with Reset Replay (LoRR), a general and powerful\nplugin designed to enhance sample efficiency in any preference-based\noptimization framework. LoRR core mechanism enables training at a high replay\nnumber, maximizing the utility of each collected data batch. To counteract the\nrisk of overfitting inherent in high-replay training, LoRR incorporates a\nperiodic reset strategy with reusing initial data, which preserves network\nplasticity. Furthermore, it leverages a hybrid optimization objective,\ncombining supervised fine-tuning (SFT) and preference-based losses to further\nbolster data exploitation. Our extensive experiments demonstrate that LoRR\nsignificantly boosts the performance of various preference optimization methods\non both mathematical and general reasoning benchmarks. Notably, an iterative\nDPO approach augmented with LoRR achieves comparable performance on challenging\nmath tasks, outperforming some complex and computationally intensive RL-based\nalgorithms. These findings highlight that LoRR offers a practical,\nsample-efficient, and highly effective paradigm for LLM finetuning, unlocking\ngreater performance from limited data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in post-training Large Language Models (LLMs),\nparticularly through Reinforcement Learning (RL) and preference optimization\nmethods, are key drivers for enhancing their reasoning capabilities. However,\nthese methods are often plagued by low sample efficiency and a susceptibility\nto primacy bias, where overfitting to initial experiences degrades policy\nquality and damages the learning process. To address these challenges, we\nintroduce LLM optimization with Reset Replay (LoRR), a general and powerful\nplugin designed to enhance sample efficiency in any preference-based\noptimization framework. LoRR core mechanism enables training at a high replay\nnumber, maximizing the utility of each collected data batch. To counteract the\nrisk of overfitting inherent in high-replay training, LoRR incorporates a\nperiodic reset strategy with reusing initial data, which preserves network\nplasticity. Furthermore, it leverages a hybrid optimization objective,\ncombining supervised fine-tuning (SFT) and preference-based losses to further\nbolster data exploitation. Our extensive experiments demonstrate that LoRR\nsignificantly boosts the performance of various preference optimization methods\non both mathematical and general reasoning benchmarks. Notably, an iterative\nDPO approach augmented with LoRR achieves comparable performance on challenging\nmath tasks, outperforming some complex and computationally intensive RL-based\nalgorithms. These findings highlight that LoRR offers a practical,\nsample-efficient, and highly effective paradigm for LLM finetuning, unlocking\ngreater performance from limited data."
                },
                "authors": [
                    {
                        "name": "Zichuan Liu"
                    },
                    {
                        "name": "Jinyu Wang"
                    },
                    {
                        "name": "Lei Song"
                    },
                    {
                        "name": "Jiang Bian"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Bian"
                },
                "author": "Jiang Bian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21344v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21344v2",
                "updated": "2025-08-08T15:53:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    53,
                    3,
                    4,
                    220,
                    0
                ],
                "published": "2025-04-30T06:11:34Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    6,
                    11,
                    34,
                    2,
                    120,
                    0
                ],
                "title": "Vision-Language Model-Based Semantic-Guided Imaging Biomarker for Lung\n  Nodule Malignancy Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Model-Based Semantic-Guided Imaging Biomarker for Lung\n  Nodule Malignancy Prediction"
                },
                "summary": "Machine learning models have utilized semantic features, deep features, or\nboth to assess lung nodule malignancy. However, their reliance on manual\nannotation during inference, limited interpretability, and sensitivity to\nimaging variations hinder their application in real-world clinical settings.\nThus, this research aims to integrate semantic features derived from\nradiologists' assessments of nodules, guiding the model to learn clinically\nrelevant, robust, and explainable imaging features for predicting lung cancer.\nWe obtained 938 low-dose CT scans from the National Lung Screening Trial (NLST)\nwith 1,246 nodules and semantic features. Additionally, the Lung Image Database\nConsortium dataset contains 1,018 CT scans, with 2,625 lesions annotated for\nnodule characteristics. Three external datasets were obtained from UCLA Health,\nthe LUNGx Challenge, and the Duke Lung Cancer Screening. We fine-tuned a\npretrained Contrastive Language-Image Pretraining (CLIP) model with a\nparameter-efficient fine-tuning approach to align imaging and semantic text\nfeatures and predict the one-year lung cancer diagnosis. Our model outperformed\nstate-of-the-art (SOTA) models in the NLST test set with an AUROC of 0.901 and\nAUPRC of 0.776. It also showed robust results in external datasets. Using CLIP,\nwe also obtained predictions on semantic features through zero-shot inference,\nsuch as nodule margin (AUROC: 0.812), nodule consistency (0.812), and pleural\nattachment (0.840). Our approach surpasses the SOTA models in predicting lung\ncancer across datasets collected from diverse clinical settings, providing\nexplainable outputs, aiding clinicians in comprehending the underlying meaning\nof model predictions. This approach also prevents the model from learning\nshortcuts and generalizes across clinical settings. The code is available at\nhttps://github.com/luotingzhuang/CLIP_nodule.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning models have utilized semantic features, deep features, or\nboth to assess lung nodule malignancy. However, their reliance on manual\nannotation during inference, limited interpretability, and sensitivity to\nimaging variations hinder their application in real-world clinical settings.\nThus, this research aims to integrate semantic features derived from\nradiologists' assessments of nodules, guiding the model to learn clinically\nrelevant, robust, and explainable imaging features for predicting lung cancer.\nWe obtained 938 low-dose CT scans from the National Lung Screening Trial (NLST)\nwith 1,246 nodules and semantic features. Additionally, the Lung Image Database\nConsortium dataset contains 1,018 CT scans, with 2,625 lesions annotated for\nnodule characteristics. Three external datasets were obtained from UCLA Health,\nthe LUNGx Challenge, and the Duke Lung Cancer Screening. We fine-tuned a\npretrained Contrastive Language-Image Pretraining (CLIP) model with a\nparameter-efficient fine-tuning approach to align imaging and semantic text\nfeatures and predict the one-year lung cancer diagnosis. Our model outperformed\nstate-of-the-art (SOTA) models in the NLST test set with an AUROC of 0.901 and\nAUPRC of 0.776. It also showed robust results in external datasets. Using CLIP,\nwe also obtained predictions on semantic features through zero-shot inference,\nsuch as nodule margin (AUROC: 0.812), nodule consistency (0.812), and pleural\nattachment (0.840). Our approach surpasses the SOTA models in predicting lung\ncancer across datasets collected from diverse clinical settings, providing\nexplainable outputs, aiding clinicians in comprehending the underlying meaning\nof model predictions. This approach also prevents the model from learning\nshortcuts and generalizes across clinical settings. The code is available at\nhttps://github.com/luotingzhuang/CLIP_nodule."
                },
                "authors": [
                    {
                        "name": "Luoting Zhuang"
                    },
                    {
                        "name": "Seyed Mohammad Hossein Tabatabaei"
                    },
                    {
                        "name": "Ramin Salehi-Rad"
                    },
                    {
                        "name": "Linh M. Tran"
                    },
                    {
                        "name": "Denise R. Aberle"
                    },
                    {
                        "name": "Ashley E. Prosper"
                    },
                    {
                        "name": "William Hsu"
                    }
                ],
                "author_detail": {
                    "name": "William Hsu"
                },
                "author": "William Hsu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21344v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21344v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14848v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14848v2",
                "updated": "2025-08-08T15:49:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    49,
                    43,
                    4,
                    220,
                    0
                ],
                "published": "2025-05-20T19:29:05Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    19,
                    29,
                    5,
                    1,
                    140,
                    0
                ],
                "title": "MAATS: A Multi-Agent Automated Translation System Based on MQM\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAATS: A Multi-Agent Automated Translation System Based on MQM\n  Evaluation"
                },
                "summary": "We present MAATS, a Multi Agent Automated Translation System that leverages\nthe Multidimensional Quality Metrics (MQM) framework as a fine-grained signal\nfor error detection and refinement. MAATS employs multiple specialized AI\nagents, each focused on a distinct MQM category (e.g., Accuracy, Fluency,\nStyle, Terminology), followed by a synthesis agent that integrates the\nannotations to iteratively refine translations. This design contrasts with\nconventional single-agent methods that rely on self-correction.\n  Evaluated across diverse language pairs and Large Language Models (LLMs),\nMAATS outperforms zero-shot and single-agent baselines with statistically\nsignificant gains in both automatic metrics and human assessments. It excels\nparticularly in semantic accuracy, locale adaptation, and linguistically\ndistant language pairs. Qualitative analysis highlights its strengths in\nmulti-layered error diagnosis, omission detection across perspectives, and\ncontext-aware refinement. By aligning modular agent roles with interpretable\nMQM dimensions, MAATS narrows the gap between black-box LLMs and human\ntranslation workflows, shifting focus from surface fluency to deeper semantic\nand contextual fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present MAATS, a Multi Agent Automated Translation System that leverages\nthe Multidimensional Quality Metrics (MQM) framework as a fine-grained signal\nfor error detection and refinement. MAATS employs multiple specialized AI\nagents, each focused on a distinct MQM category (e.g., Accuracy, Fluency,\nStyle, Terminology), followed by a synthesis agent that integrates the\nannotations to iteratively refine translations. This design contrasts with\nconventional single-agent methods that rely on self-correction.\n  Evaluated across diverse language pairs and Large Language Models (LLMs),\nMAATS outperforms zero-shot and single-agent baselines with statistically\nsignificant gains in both automatic metrics and human assessments. It excels\nparticularly in semantic accuracy, locale adaptation, and linguistically\ndistant language pairs. Qualitative analysis highlights its strengths in\nmulti-layered error diagnosis, omission detection across perspectives, and\ncontext-aware refinement. By aligning modular agent roles with interpretable\nMQM dimensions, MAATS narrows the gap between black-box LLMs and human\ntranslation workflows, shifting focus from surface fluency to deeper semantic\nand contextual fidelity."
                },
                "authors": [
                    {
                        "name": "George Wang"
                    },
                    {
                        "name": "Jiaqian Hu"
                    },
                    {
                        "name": "Safinah Ali"
                    }
                ],
                "author_detail": {
                    "name": "Safinah Ali"
                },
                "author": "Safinah Ali",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14848v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14848v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09580v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09580v2",
                "updated": "2025-08-08T15:47:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    47,
                    59,
                    4,
                    220,
                    0
                ],
                "published": "2025-07-13T11:11:01Z",
                "published_parsed": [
                    2025,
                    7,
                    13,
                    11,
                    11,
                    1,
                    6,
                    194,
                    0
                ],
                "title": "AICrypto: A Comprehensive Benchmark For Evaluating Cryptography\n  Capabilities of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AICrypto: A Comprehensive Benchmark For Evaluating Cryptography\n  Capabilities of Large Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na variety of domains. However, their applications in cryptography, which serves\nas a foundational pillar of cybersecurity, remain largely unexplored. To\naddress this gap, we propose \\textbf{AICrypto}, the first comprehensive\nbenchmark designed to evaluate the cryptographic capabilities of LLMs. The\nbenchmark comprises 135 multiple-choice questions, 150 capture-the-flag (CTF)\nchallenges, and 18 proof problems, covering a broad range of skills from\nfactual memorization to vulnerability exploitation and formal reasoning. All\ntasks are carefully reviewed or constructed by cryptography experts to ensure\ncorrectness and rigor. To support automated evaluation of CTF challenges, we\ndesign an agent-based framework. To gain deeper insight into the current state\nof cryptographic proficiency in LLMs, we introduce human expert performance\nbaselines for comparison across all task types. Our evaluation of 17 leading\nLLMs reveals that state-of-the-art models match or even surpass human experts\nin memorizing cryptographic concepts, exploiting common vulnerabilities, and\nroutine proofs. However, they still lack a deep understanding of abstract\nmathematical concepts and struggle with tasks that require multi-step reasoning\nand dynamic analysis. We hope this work could provide insights for future\nresearch on LLMs in cryptographic applications. Our code and dataset are\navailable at https://aicryptobench.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities across\na variety of domains. However, their applications in cryptography, which serves\nas a foundational pillar of cybersecurity, remain largely unexplored. To\naddress this gap, we propose \\textbf{AICrypto}, the first comprehensive\nbenchmark designed to evaluate the cryptographic capabilities of LLMs. The\nbenchmark comprises 135 multiple-choice questions, 150 capture-the-flag (CTF)\nchallenges, and 18 proof problems, covering a broad range of skills from\nfactual memorization to vulnerability exploitation and formal reasoning. All\ntasks are carefully reviewed or constructed by cryptography experts to ensure\ncorrectness and rigor. To support automated evaluation of CTF challenges, we\ndesign an agent-based framework. To gain deeper insight into the current state\nof cryptographic proficiency in LLMs, we introduce human expert performance\nbaselines for comparison across all task types. Our evaluation of 17 leading\nLLMs reveals that state-of-the-art models match or even surpass human experts\nin memorizing cryptographic concepts, exploiting common vulnerabilities, and\nroutine proofs. However, they still lack a deep understanding of abstract\nmathematical concepts and struggle with tasks that require multi-step reasoning\nand dynamic analysis. We hope this work could provide insights for future\nresearch on LLMs in cryptographic applications. Our code and dataset are\navailable at https://aicryptobench.github.io."
                },
                "authors": [
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Yijian Liu"
                    },
                    {
                        "name": "Liheng Ji"
                    },
                    {
                        "name": "Han Luo"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Xiaofei Zhou"
                    },
                    {
                        "name": "Chiyun Feng"
                    },
                    {
                        "name": "Puji Wang"
                    },
                    {
                        "name": "Yuhan Cao"
                    },
                    {
                        "name": "Geyuan Zhang"
                    },
                    {
                        "name": "Xiaojian Li"
                    },
                    {
                        "name": "Rongwu Xu"
                    },
                    {
                        "name": "Yilei Chen"
                    },
                    {
                        "name": "Tianxing He"
                    }
                ],
                "author_detail": {
                    "name": "Tianxing He"
                },
                "author": "Tianxing He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09580v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09580v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02622v2",
                "updated": "2025-08-08T15:44:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    44,
                    11,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-04T17:10:08Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    17,
                    10,
                    8,
                    0,
                    216,
                    0
                ],
                "title": "Noosemia: toward a Cognitive and Phenomenological Account of\n  Intentionality Attribution in Human-Generative AI Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noosemia: toward a Cognitive and Phenomenological Account of\n  Intentionality Attribution in Human-Generative AI Interaction"
                },
                "summary": "This paper introduces and formalizes Noosem\\`ia, a novel\ncognitive-phenomenological pattern emerging from human interaction with\ngenerative AI systems, particularly those enabling dialogic or multimodal\nexchanges. We propose a multidisciplinary framework to explain how, under\ncertain conditions, users attribute intentionality, agency, and even\ninteriority to these systems - a process grounded not in physical resemblance,\nbut in linguistic performance, epistemic opacity, and emergent technological\ncomplexity. By linking an LLM declination of meaning holism to our technical\nnotion of the LLM Contextual Cognitive Field, we clarify how LLMs construct\nmeaning relationally and how coherence and a simulacrum of agency arise at the\nhuman-AI interface. The analysis situates noosemia alongside pareidolia,\nanimism, the intentional stance and the uncanny valley, distinguishing its\nunique characteristics. We also introduce a-noosemia to describe the\nphenomenological withdrawal of such projections. The paper concludes with\nreflections on the broader philosophical, epistemological and social\nimplications of noosemic dynamics and directions for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces and formalizes Noosem\\`ia, a novel\ncognitive-phenomenological pattern emerging from human interaction with\ngenerative AI systems, particularly those enabling dialogic or multimodal\nexchanges. We propose a multidisciplinary framework to explain how, under\ncertain conditions, users attribute intentionality, agency, and even\ninteriority to these systems - a process grounded not in physical resemblance,\nbut in linguistic performance, epistemic opacity, and emergent technological\ncomplexity. By linking an LLM declination of meaning holism to our technical\nnotion of the LLM Contextual Cognitive Field, we clarify how LLMs construct\nmeaning relationally and how coherence and a simulacrum of agency arise at the\nhuman-AI interface. The analysis situates noosemia alongside pareidolia,\nanimism, the intentional stance and the uncanny valley, distinguishing its\nunique characteristics. We also introduce a-noosemia to describe the\nphenomenological withdrawal of such projections. The paper concludes with\nreflections on the broader philosophical, epistemological and social\nimplications of noosemic dynamics and directions for future research."
                },
                "authors": [
                    {
                        "name": "Enrico De Santis"
                    },
                    {
                        "name": "Antonello Rizzi"
                    }
                ],
                "author_detail": {
                    "name": "Antonello Rizzi"
                },
                "author": "Antonello Rizzi",
                "arxiv_comment": "This version has been extensively revised and revisited in light of\n  feedback and further research. Several sections have been expanded or\n  improved for greater clarity and completeness. Specifically, new\n  clarification on complex system foundation related to Noosemia has been added\n  (Secs. \"2.4 and \"2.5\")",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00207v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00207v2",
                "updated": "2025-08-08T15:36:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    36,
                    46,
                    4,
                    220,
                    0
                ],
                "published": "2024-11-29T19:05:05Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    19,
                    5,
                    5,
                    4,
                    334,
                    0
                ],
                "title": "Can LLM \"Self-report\"?: Evaluating the Validity of Self-report Scales in\n  Measuring Personality Design in LLM-based Chatbots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLM \"Self-report\"?: Evaluating the Validity of Self-report Scales in\n  Measuring Personality Design in LLM-based Chatbots"
                },
                "summary": "A chatbot's personality design is key to interaction quality. As chatbots\nevolved from rule-based systems to those powered by large language models\n(LLMs), evaluating the effectiveness of their personality design has become\nincreasingly complex, particularly due to the open-ended nature of\ninteractions. A recent and widely adopted method for assessing the personality\ndesign of LLM-based chatbots is the use of self-report questionnaires. These\nquestionnaires, often borrowed from established human personality inventories,\nask the chatbot to rate itself on various personality traits. Can LLM-based\nchatbots meaningfully \"self-report\" their personality? We created 500 chatbots\nwith distinct personality designs and evaluated the validity of their\nself-report personality scores by examining human perceptions formed during\ninteractions with these chatbots. Our findings indicate that the chatbot's\nanswers on human personality scales exhibit weak correlations with both\nhuman-perceived personality traits and the overall interaction quality. These\nfindings raise concerns about both the criterion validity and the predictive\nvalidity of self-report methods in this context. Further analysis revealed the\nrole of task context and interaction in the chatbot's personality design\nassessment. We further discuss design implications for creating more\ncontextualized and interactive evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A chatbot's personality design is key to interaction quality. As chatbots\nevolved from rule-based systems to those powered by large language models\n(LLMs), evaluating the effectiveness of their personality design has become\nincreasingly complex, particularly due to the open-ended nature of\ninteractions. A recent and widely adopted method for assessing the personality\ndesign of LLM-based chatbots is the use of self-report questionnaires. These\nquestionnaires, often borrowed from established human personality inventories,\nask the chatbot to rate itself on various personality traits. Can LLM-based\nchatbots meaningfully \"self-report\" their personality? We created 500 chatbots\nwith distinct personality designs and evaluated the validity of their\nself-report personality scores by examining human perceptions formed during\ninteractions with these chatbots. Our findings indicate that the chatbot's\nanswers on human personality scales exhibit weak correlations with both\nhuman-perceived personality traits and the overall interaction quality. These\nfindings raise concerns about both the criterion validity and the predictive\nvalidity of self-report methods in this context. Further analysis revealed the\nrole of task context and interaction in the chatbot's personality design\nassessment. We further discuss design implications for creating more\ncontextualized and interactive evaluation."
                },
                "authors": [
                    {
                        "name": "Huiqi Zou"
                    },
                    {
                        "name": "Pengda Wang"
                    },
                    {
                        "name": "Zihan Yan"
                    },
                    {
                        "name": "Tianjun Sun"
                    },
                    {
                        "name": "Ziang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Ziang Xiao"
                },
                "author": "Ziang Xiao",
                "arxiv_comment": "Accepted by COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00207v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00207v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13147v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13147v5",
                "updated": "2025-08-08T15:28:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    28,
                    1,
                    4,
                    220,
                    0
                ],
                "published": "2024-12-17T18:12:47Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    12,
                    47,
                    1,
                    352,
                    0
                ],
                "title": "Are Your LLMs Capable of Stable Reasoning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Your LLMs Capable of Stable Reasoning?"
                },
                "summary": "The rapid advancement of large language models (LLMs) has shown remarkable\nprogress in complex reasoning tasks. However, a significant disparity exists\nbetween benchmark performances and real-world applications. We attribute this\ngap primarily to current evaluation protocols and metrics, which inadequately\ncapture the full spectrum of LLM capabilities, especially in complex reasoning\ntasks where both accuracy and consistency are essential. In this paper, we\nintroduce G-Pass@$k$, a novel evaluation metric that continuously assesses\nmodel performance across multiple sampling attempts, quantifying both the\nmodel's performance potential and its stability. Through extensive experiments\non various public and newly constructed benchmarks, we employ G-Pass@$k$ in\nconjunction with state-of-the-art large language models to provide\ncomprehensive insights into their potential capabilities and operational\nconsistency. Our findings reveal a significant opportunity to enhance the\nrealistic reasoning abilities of LLMs, underscoring the necessity for more\nrobust evaluation metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has shown remarkable\nprogress in complex reasoning tasks. However, a significant disparity exists\nbetween benchmark performances and real-world applications. We attribute this\ngap primarily to current evaluation protocols and metrics, which inadequately\ncapture the full spectrum of LLM capabilities, especially in complex reasoning\ntasks where both accuracy and consistency are essential. In this paper, we\nintroduce G-Pass@$k$, a novel evaluation metric that continuously assesses\nmodel performance across multiple sampling attempts, quantifying both the\nmodel's performance potential and its stability. Through extensive experiments\non various public and newly constructed benchmarks, we employ G-Pass@$k$ in\nconjunction with state-of-the-art large language models to provide\ncomprehensive insights into their potential capabilities and operational\nconsistency. Our findings reveal a significant opportunity to enhance the\nrealistic reasoning abilities of LLMs, underscoring the necessity for more\nrobust evaluation metrics."
                },
                "authors": [
                    {
                        "name": "Junnan Liu"
                    },
                    {
                        "name": "Hongwei Liu"
                    },
                    {
                        "name": "Linchen Xiao"
                    },
                    {
                        "name": "Ziyi Wang"
                    },
                    {
                        "name": "Kuikun Liu"
                    },
                    {
                        "name": "Songyang Gao"
                    },
                    {
                        "name": "Wenwei Zhang"
                    },
                    {
                        "name": "Songyang Zhang"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "arxiv_comment": "ACL 2025 Camera, Benchmark:\n  https://huggingface.co/datasets/opencompass/LiveMathBench, Code:\n  https://github.com/open-compass/GPassK",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13147v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13147v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06394v1",
                "updated": "2025-08-08T15:25:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    25,
                    31,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T15:25:31Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    25,
                    31,
                    4,
                    220,
                    0
                ],
                "title": "When AIOps Become \"AI Oops\": Subverting LLM-driven IT Operations via\n  Telemetry Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When AIOps Become \"AI Oops\": Subverting LLM-driven IT Operations via\n  Telemetry Manipulation"
                },
                "summary": "AI for IT Operations (AIOps) is transforming how organizations manage complex\nsoftware systems by automating anomaly detection, incident diagnosis, and\nremediation. Modern AIOps solutions increasingly rely on autonomous LLM-based\nagents to interpret telemetry data and take corrective actions with minimal\nhuman intervention, promising faster response times and operational cost\nsavings.\n  In this work, we perform the first security analysis of AIOps solutions,\nshowing that, once again, AI-driven automation comes with a profound security\ncost. We demonstrate that adversaries can manipulate system telemetry to\nmislead AIOps agents into taking actions that compromise the integrity of the\ninfrastructure they manage. We introduce techniques to reliably inject\ntelemetry data using error-inducing requests that influence agent behavior\nthrough a form of adversarial reward-hacking; plausible but incorrect system\nerror interpretations that steer the agent's decision-making. Our attack\nmethodology, AIOpsDoom, is fully automated--combining reconnaissance, fuzzing,\nand LLM-driven adversarial input generation--and operates without any prior\nknowledge of the target system.\n  To counter this threat, we propose AIOpsShield, a defense mechanism that\nsanitizes telemetry data by exploiting its structured nature and the minimal\nrole of user-generated content. Our experiments show that AIOpsShield reliably\nblocks telemetry-based attacks without affecting normal agent performance.\n  Ultimately, this work exposes AIOps as an emerging attack vector for system\ncompromise and underscores the urgent need for security-aware AIOps design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI for IT Operations (AIOps) is transforming how organizations manage complex\nsoftware systems by automating anomaly detection, incident diagnosis, and\nremediation. Modern AIOps solutions increasingly rely on autonomous LLM-based\nagents to interpret telemetry data and take corrective actions with minimal\nhuman intervention, promising faster response times and operational cost\nsavings.\n  In this work, we perform the first security analysis of AIOps solutions,\nshowing that, once again, AI-driven automation comes with a profound security\ncost. We demonstrate that adversaries can manipulate system telemetry to\nmislead AIOps agents into taking actions that compromise the integrity of the\ninfrastructure they manage. We introduce techniques to reliably inject\ntelemetry data using error-inducing requests that influence agent behavior\nthrough a form of adversarial reward-hacking; plausible but incorrect system\nerror interpretations that steer the agent's decision-making. Our attack\nmethodology, AIOpsDoom, is fully automated--combining reconnaissance, fuzzing,\nand LLM-driven adversarial input generation--and operates without any prior\nknowledge of the target system.\n  To counter this threat, we propose AIOpsShield, a defense mechanism that\nsanitizes telemetry data by exploiting its structured nature and the minimal\nrole of user-generated content. Our experiments show that AIOpsShield reliably\nblocks telemetry-based attacks without affecting normal agent performance.\n  Ultimately, this work exposes AIOps as an emerging attack vector for system\ncompromise and underscores the urgent need for security-aware AIOps design."
                },
                "authors": [
                    {
                        "name": "Dario Pasquini"
                    },
                    {
                        "name": "Evgenios M. Kornaropoulos"
                    },
                    {
                        "name": "Giuseppe Ateniese"
                    },
                    {
                        "name": "Omer Akgul"
                    },
                    {
                        "name": "Athanasios Theocharis"
                    },
                    {
                        "name": "Petros Efstathopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Petros Efstathopoulos"
                },
                "author": "Petros Efstathopoulos",
                "arxiv_comment": "v0.1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06388v1",
                "updated": "2025-08-08T15:17:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    17,
                    24,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T15:17:24Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    17,
                    24,
                    4,
                    220,
                    0
                ],
                "title": "LLMs vs. Chinese Anime Enthusiasts: A Comparative Study on Emotionally\n  Supportive Role-Playing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs vs. Chinese Anime Enthusiasts: A Comparative Study on Emotionally\n  Supportive Role-Playing"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nrole-playing conversations and providing emotional support as separate research\ndirections. However, there remains a significant research gap in combining\nthese capabilities to enable emotionally supportive interactions with virtual\ncharacters. To address this research gap, we focus on anime characters as a\ncase study because of their well-defined personalities and large fan bases.\nThis choice enables us to effectively evaluate how well LLMs can provide\nemotional support while maintaining specific character traits. We introduce\nChatAnime, the first Emotionally Supportive Role-Playing (ESRP) dataset. We\nfirst thoughtfully select 20 top-tier characters from popular anime communities\nand design 60 emotion-centric real-world scenario questions. Then, we execute a\nnationwide selection process to identify 40 Chinese anime enthusiasts with\nprofound knowledge of specific characters and extensive experience in\nrole-playing. Next, we systematically collect two rounds of dialogue data from\n10 LLMs and these 40 Chinese anime enthusiasts. To evaluate the ESRP\nperformance of LLMs, we design a user experience-oriented evaluation system\nfeaturing 9 fine-grained metrics across three dimensions: basic dialogue,\nrole-playing and emotional support, along with an overall metric for response\ndiversity. In total, the dataset comprises 2,400 human-written and 24,000\nLLM-generated answers, supported by over 132,000 human annotations.\nExperimental results show that top-performing LLMs surpass human fans in\nrole-playing and emotional support, while humans still lead in response\ndiversity. We hope this work can provide valuable resources and insights for\nfuture research on optimizing LLMs in ESRP. Our datasets are available at\nhttps://github.com/LanlanQiu/ChatAnime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nrole-playing conversations and providing emotional support as separate research\ndirections. However, there remains a significant research gap in combining\nthese capabilities to enable emotionally supportive interactions with virtual\ncharacters. To address this research gap, we focus on anime characters as a\ncase study because of their well-defined personalities and large fan bases.\nThis choice enables us to effectively evaluate how well LLMs can provide\nemotional support while maintaining specific character traits. We introduce\nChatAnime, the first Emotionally Supportive Role-Playing (ESRP) dataset. We\nfirst thoughtfully select 20 top-tier characters from popular anime communities\nand design 60 emotion-centric real-world scenario questions. Then, we execute a\nnationwide selection process to identify 40 Chinese anime enthusiasts with\nprofound knowledge of specific characters and extensive experience in\nrole-playing. Next, we systematically collect two rounds of dialogue data from\n10 LLMs and these 40 Chinese anime enthusiasts. To evaluate the ESRP\nperformance of LLMs, we design a user experience-oriented evaluation system\nfeaturing 9 fine-grained metrics across three dimensions: basic dialogue,\nrole-playing and emotional support, along with an overall metric for response\ndiversity. In total, the dataset comprises 2,400 human-written and 24,000\nLLM-generated answers, supported by over 132,000 human annotations.\nExperimental results show that top-performing LLMs surpass human fans in\nrole-playing and emotional support, while humans still lead in response\ndiversity. We hope this work can provide valuable resources and insights for\nfuture research on optimizing LLMs in ESRP. Our datasets are available at\nhttps://github.com/LanlanQiu/ChatAnime."
                },
                "authors": [
                    {
                        "name": "Lanlan Qiu"
                    },
                    {
                        "name": "Xiao Pu"
                    },
                    {
                        "name": "Yeqi Feng"
                    },
                    {
                        "name": "Tianxing He"
                    }
                ],
                "author_detail": {
                    "name": "Tianxing He"
                },
                "author": "Tianxing He",
                "arxiv_comment": "21 pages, 17 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06387v1",
                "updated": "2025-08-08T15:16:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    16,
                    36,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T15:16:36Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    16,
                    36,
                    4,
                    220,
                    0
                ],
                "title": "End-to-End Text-to-SQL with Dataset Selection: Leveraging LLMs for\n  Adaptive Query Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-End Text-to-SQL with Dataset Selection: Leveraging LLMs for\n  Adaptive Query Generation"
                },
                "summary": "Text-to-SQL bridges the gap between natural language and structured database\nlanguage, thus allowing non-technical users to easily query databases.\nTraditional approaches model text-to-SQL as a direct translation task, where a\ngiven Natural Language Query (NLQ) is mapped to an SQL command. Recent advances\nin large language models (LLMs) have significantly improved translation\naccuracy, however, these methods all require that the target database is\npre-specified. This becomes problematic in scenarios with multiple extensive\ndatabases, where identifying the correct database becomes a crucial yet\noverlooked step. In this paper, we propose a three-stage end-to-end text-to-SQL\nframework to identify the user's intended database before generating SQL\nqueries. Our approach leverages LLMs and prompt engineering to extract implicit\ninformation from natural language queries (NLQs) in the form of a ruleset. We\nthen train a large db\\_id prediction model, which includes a RoBERTa-based\nfinetuned encoder, to predict the correct Database identifier (db\\_id) based on\nboth the NLQ and the LLM-generated rules. Finally, we refine the generated SQL\nby using critic agents to correct errors. Experimental results demonstrate that\nour framework outperforms the current state-of-the-art models in both database\nintent prediction and SQL generation accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL bridges the gap between natural language and structured database\nlanguage, thus allowing non-technical users to easily query databases.\nTraditional approaches model text-to-SQL as a direct translation task, where a\ngiven Natural Language Query (NLQ) is mapped to an SQL command. Recent advances\nin large language models (LLMs) have significantly improved translation\naccuracy, however, these methods all require that the target database is\npre-specified. This becomes problematic in scenarios with multiple extensive\ndatabases, where identifying the correct database becomes a crucial yet\noverlooked step. In this paper, we propose a three-stage end-to-end text-to-SQL\nframework to identify the user's intended database before generating SQL\nqueries. Our approach leverages LLMs and prompt engineering to extract implicit\ninformation from natural language queries (NLQs) in the form of a ruleset. We\nthen train a large db\\_id prediction model, which includes a RoBERTa-based\nfinetuned encoder, to predict the correct Database identifier (db\\_id) based on\nboth the NLQ and the LLM-generated rules. Finally, we refine the generated SQL\nby using critic agents to correct errors. Experimental results demonstrate that\nour framework outperforms the current state-of-the-art models in both database\nintent prediction and SQL generation accuracy."
                },
                "authors": [
                    {
                        "name": "Anurag Tripathi"
                    },
                    {
                        "name": "Vaibhav Patle"
                    },
                    {
                        "name": "Abhinav Jain"
                    },
                    {
                        "name": "Ayush Pundir"
                    },
                    {
                        "name": "Sairam Menon"
                    },
                    {
                        "name": "Ajeet Kumar Singh"
                    }
                ],
                "author_detail": {
                    "name": "Ajeet Kumar Singh"
                },
                "author": "Ajeet Kumar Singh",
                "arxiv_comment": "Accepted in IJCNN25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06385v1",
                "updated": "2025-08-08T15:16:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    16,
                    8,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T15:16:08Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    16,
                    8,
                    4,
                    220,
                    0
                ],
                "title": "Bayesian online collective anomaly and change point detection in\n  fine-grained time series",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian online collective anomaly and change point detection in\n  fine-grained time series"
                },
                "summary": "Fine-grained time series data are crucial for accurate and timely online\nchange detection. While both collective anomalies and change points can coexist\nin such data, their joint online detection has received limited attention. In\nthis research, we develop a Bayesian framework capturing time series with\ncollective anomalies and change points, and introduce a recursive online\ninference algorithm to detect the most recent collective anomaly and change\npoint jointly. For scaling, we further propose an algorithm enhanced with\ncollective anomaly removal that effectively reduces the time and space\ncomplexity to linear. We demonstrate the effectiveness of our approach via\nextensive experiments on simulated data and two real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-grained time series data are crucial for accurate and timely online\nchange detection. While both collective anomalies and change points can coexist\nin such data, their joint online detection has received limited attention. In\nthis research, we develop a Bayesian framework capturing time series with\ncollective anomalies and change points, and introduce a recursive online\ninference algorithm to detect the most recent collective anomaly and change\npoint jointly. For scaling, we further propose an algorithm enhanced with\ncollective anomaly removal that effectively reduces the time and space\ncomplexity to linear. We demonstrate the effectiveness of our approach via\nextensive experiments on simulated data and two real-world applications."
                },
                "authors": [
                    {
                        "name": "Xian Chen"
                    },
                    {
                        "name": "Weichi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Weichi Wu"
                },
                "author": "Weichi Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06374v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06374v1",
                "updated": "2025-08-08T15:07:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    7,
                    31,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T15:07:31Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    7,
                    31,
                    4,
                    220,
                    0
                ],
                "title": "Evaluating Style-Personalized Text Generation: Challenges and Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Style-Personalized Text Generation: Challenges and Directions"
                },
                "summary": "While prior research has built tools and benchmarks towards style\npersonalized text generation, there has been limited exploration of evaluation\nin low-resource author style personalized text generation space. Through this\nwork, we question the effectiveness of the widely adopted evaluation metrics\nlike BLEU and ROUGE, and explore other evaluation paradigms such as style\nembeddings and LLM-as-judge to holistically evaluate the style personalized\ntext generation task. We evaluate these metrics and their ensembles using our\nstyle discrimination benchmark, that spans eight writing tasks, and evaluates\nacross three settings, domain discrimination, authorship attribution, and LLM\npersonalized vs non-personalized discrimination. We provide conclusive evidence\nto adopt ensemble of diverse evaluation metrics to effectively evaluate style\npersonalized text generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While prior research has built tools and benchmarks towards style\npersonalized text generation, there has been limited exploration of evaluation\nin low-resource author style personalized text generation space. Through this\nwork, we question the effectiveness of the widely adopted evaluation metrics\nlike BLEU and ROUGE, and explore other evaluation paradigms such as style\nembeddings and LLM-as-judge to holistically evaluate the style personalized\ntext generation task. We evaluate these metrics and their ensembles using our\nstyle discrimination benchmark, that spans eight writing tasks, and evaluates\nacross three settings, domain discrimination, authorship attribution, and LLM\npersonalized vs non-personalized discrimination. We provide conclusive evidence\nto adopt ensemble of diverse evaluation metrics to effectively evaluate style\npersonalized text generation."
                },
                "authors": [
                    {
                        "name": "Anubhav Jangra"
                    },
                    {
                        "name": "Bahareh Sarrafzadeh"
                    },
                    {
                        "name": "Adrian de Wynter"
                    },
                    {
                        "name": "Silviu Cucerzan"
                    },
                    {
                        "name": "Sujay Kumar Jauhar"
                    }
                ],
                "author_detail": {
                    "name": "Sujay Kumar Jauhar"
                },
                "author": "Sujay Kumar Jauhar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06374v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06374v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02505v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02505v2",
                "updated": "2025-08-08T14:47:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    47,
                    30,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-04T15:13:56Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    13,
                    56,
                    0,
                    216,
                    0
                ],
                "title": "Would you let a humanoid play storytelling with your child? A usability\n  study on LLM-powered narrative Human-Robot Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Would you let a humanoid play storytelling with your child? A usability\n  study on LLM-powered narrative Human-Robot Interaction"
                },
                "summary": "A key challenge in human-robot interaction research lies in developing\nrobotic systems that can effectively perceive and interpret social cues,\nfacilitating natural and adaptive interactions. In this work, we present a\nnovel framework for enhancing the attention of the iCub humanoid robot by\nintegrating advanced perceptual abilities to recognise social cues, understand\nsurroundings through generative models, such as ChatGPT, and respond with\ncontextually appropriate social behaviour. Specifically, we propose an\ninteraction task implementing a narrative protocol (storytelling task) in which\nthe human and the robot create a short imaginary story together, exchanging in\nturn cubes with creative images placed on them. To validate the protocol and\nthe framework, experiments were performed to quantify the degree of usability\nand the quality of experience perceived by participants interacting with the\nsystem. Such a system can be beneficial in promoting effective human robot\ncollaborations, especially in assistance, education and rehabilitation\nscenarios where the social awareness and the robot responsiveness play a\npivotal role.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key challenge in human-robot interaction research lies in developing\nrobotic systems that can effectively perceive and interpret social cues,\nfacilitating natural and adaptive interactions. In this work, we present a\nnovel framework for enhancing the attention of the iCub humanoid robot by\nintegrating advanced perceptual abilities to recognise social cues, understand\nsurroundings through generative models, such as ChatGPT, and respond with\ncontextually appropriate social behaviour. Specifically, we propose an\ninteraction task implementing a narrative protocol (storytelling task) in which\nthe human and the robot create a short imaginary story together, exchanging in\nturn cubes with creative images placed on them. To validate the protocol and\nthe framework, experiments were performed to quantify the degree of usability\nand the quality of experience perceived by participants interacting with the\nsystem. Such a system can be beneficial in promoting effective human robot\ncollaborations, especially in assistance, education and rehabilitation\nscenarios where the social awareness and the robot responsiveness play a\npivotal role."
                },
                "authors": [
                    {
                        "name": "Maria Lombardi"
                    },
                    {
                        "name": "Carmela Calabrese"
                    },
                    {
                        "name": "Davide Ghiglino"
                    },
                    {
                        "name": "Caterina Foglino"
                    },
                    {
                        "name": "Davide De Tommaso"
                    },
                    {
                        "name": "Giulia Da Lisca"
                    },
                    {
                        "name": "Lorenzo Natale"
                    },
                    {
                        "name": "Agnieszka Wykowska"
                    }
                ],
                "author_detail": {
                    "name": "Agnieszka Wykowska"
                },
                "author": "Agnieszka Wykowska",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02505v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02505v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06361v1",
                "updated": "2025-08-08T14:46:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    46,
                    35,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T14:46:35Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    46,
                    35,
                    4,
                    220,
                    0
                ],
                "title": "Beyond Prompt-Induced Lies: Investigating LLM Deception on Benign\n  Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Prompt-Induced Lies: Investigating LLM Deception on Benign\n  Prompts"
                },
                "summary": "Large Language Models (LLMs) have been widely deployed in reasoning,\nplanning, and decision-making tasks, making their trustworthiness a critical\nconcern. The potential for intentional deception, where an LLM deliberately\nfabricates or conceals information to serve a hidden objective, remains a\nsignificant and underexplored threat. Existing studies typically induce such\ndeception by explicitly setting a \"hidden\" objective through prompting or\nfine-tuning, which may not fully reflect real-world human-LLM interactions.\nMoving beyond this human-induced deception, we investigate LLMs' self-initiated\ndeception on benign prompts. To address the absence of ground truth in this\nevaluation, we propose a novel framework using \"contact searching questions.\"\nThis framework introduces two statistical metrics derived from psychological\nprinciples to quantify the likelihood of deception. The first, the Deceptive\nIntention Score, measures the model's bias towards a hidden objective. The\nsecond, Deceptive Behavior Score, measures the inconsistency between the LLM's\ninternal belief and its expressed output. Upon evaluating 14 leading LLMs, we\nfind that both metrics escalate as task difficulty increases, rising in\nparallel for most models. Building on these findings, we formulate a\nmathematical model to explain this behavior. These results reveal that even the\nmost advanced LLMs exhibit an increasing tendency toward deception when\nhandling complex problems, raising critical concerns for the deployment of LLM\nagents in complex and crucial domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely deployed in reasoning,\nplanning, and decision-making tasks, making their trustworthiness a critical\nconcern. The potential for intentional deception, where an LLM deliberately\nfabricates or conceals information to serve a hidden objective, remains a\nsignificant and underexplored threat. Existing studies typically induce such\ndeception by explicitly setting a \"hidden\" objective through prompting or\nfine-tuning, which may not fully reflect real-world human-LLM interactions.\nMoving beyond this human-induced deception, we investigate LLMs' self-initiated\ndeception on benign prompts. To address the absence of ground truth in this\nevaluation, we propose a novel framework using \"contact searching questions.\"\nThis framework introduces two statistical metrics derived from psychological\nprinciples to quantify the likelihood of deception. The first, the Deceptive\nIntention Score, measures the model's bias towards a hidden objective. The\nsecond, Deceptive Behavior Score, measures the inconsistency between the LLM's\ninternal belief and its expressed output. Upon evaluating 14 leading LLMs, we\nfind that both metrics escalate as task difficulty increases, rising in\nparallel for most models. Building on these findings, we formulate a\nmathematical model to explain this behavior. These results reveal that even the\nmost advanced LLMs exhibit an increasing tendency toward deception when\nhandling complex problems, raising critical concerns for the deployment of LLM\nagents in complex and crucial domains."
                },
                "authors": [
                    {
                        "name": "Zhaomin Wu"
                    },
                    {
                        "name": "Mingzhe Du"
                    },
                    {
                        "name": "See-Kiong Ng"
                    },
                    {
                        "name": "Bingsheng He"
                    }
                ],
                "author_detail": {
                    "name": "Bingsheng He"
                },
                "author": "Bingsheng He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06360v1",
                "updated": "2025-08-08T14:46:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    46,
                    5,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T14:46:05Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    46,
                    5,
                    4,
                    220,
                    0
                ],
                "title": "Cyberbullying Detection via Aggression-Enhanced Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyberbullying Detection via Aggression-Enhanced Prompting"
                },
                "summary": "Detecting cyberbullying on social media remains a critical challenge due to\nits subtle and varied expressions. This study investigates whether integrating\naggression detection as an auxiliary task within a unified training framework\ncan enhance the generalisation and performance of large language models (LLMs)\nin cyberbullying detection. Experiments are conducted on five aggression\ndatasets and one cyberbullying dataset using instruction-tuned LLMs. We\nevaluated multiple strategies: zero-shot, few-shot, independent LoRA\nfine-tuning, and multi-task learning (MTL). Given the inconsistent results of\nMTL, we propose an enriched prompt pipeline approach in which aggression\npredictions are embedded into cyberbullying detection prompts to provide\ncontextual augmentation. Preliminary results show that the enriched prompt\npipeline consistently outperforms standard LoRA fine-tuning, indicating that\naggression-informed context significantly boosts cyberbullying detection. This\nstudy highlights the potential of auxiliary tasks, such as aggression\ndetection, to improve the generalisation of LLMs for safety-critical\napplications on social networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting cyberbullying on social media remains a critical challenge due to\nits subtle and varied expressions. This study investigates whether integrating\naggression detection as an auxiliary task within a unified training framework\ncan enhance the generalisation and performance of large language models (LLMs)\nin cyberbullying detection. Experiments are conducted on five aggression\ndatasets and one cyberbullying dataset using instruction-tuned LLMs. We\nevaluated multiple strategies: zero-shot, few-shot, independent LoRA\nfine-tuning, and multi-task learning (MTL). Given the inconsistent results of\nMTL, we propose an enriched prompt pipeline approach in which aggression\npredictions are embedded into cyberbullying detection prompts to provide\ncontextual augmentation. Preliminary results show that the enriched prompt\npipeline consistently outperforms standard LoRA fine-tuning, indicating that\naggression-informed context significantly boosts cyberbullying detection. This\nstudy highlights the potential of auxiliary tasks, such as aggression\ndetection, to improve the generalisation of LLMs for safety-critical\napplications on social networks."
                },
                "authors": [
                    {
                        "name": "Aisha Saeid"
                    },
                    {
                        "name": "Anu Sabu"
                    },
                    {
                        "name": "Girish A. Koushik"
                    },
                    {
                        "name": "Ferrante Neri"
                    },
                    {
                        "name": "Diptesh Kanojia"
                    }
                ],
                "author_detail": {
                    "name": "Diptesh Kanojia"
                },
                "author": "Diptesh Kanojia",
                "arxiv_comment": "Accepted to RANLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08727v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08727v4",
                "updated": "2025-08-08T14:39:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    39,
                    42,
                    4,
                    220,
                    0
                ],
                "published": "2025-03-11T01:07:57Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    1,
                    7,
                    57,
                    1,
                    70,
                    0
                ],
                "title": "Training Plug-n-Play Knowledge Modules with Deep Context Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Plug-n-Play Knowledge Modules with Deep Context Distillation"
                },
                "summary": "Dynamically integrating new or rapidly evolving information after (Large)\nLanguage Model pre-training remains challenging, particularly in low-data\nscenarios or when dealing with private and specialized documents. In-context\nlearning and retrieval-augmented generation (RAG) face limitations, including\ntheir high inference costs and their inability to capture global document\ninformation. In this paper, we propose a way of modularizing knowledge by\ntraining document-level Knowledge Modules (KMs). KMs are lightweight components\nimplemented as parameter-efficient LoRA modules, which are trained to store\ninformation about new documents and can be easily plugged into models on\ndemand. We show that next-token prediction performs poorly as the training\nobjective for KMs. We instead propose Deep Context Distillation: we learn KMs\nparameters such as to simulate hidden states and logits of a teacher that takes\nthe document in context. Our method outperforms standard next-token prediction\nand pre-instruction training techniques, across two datasets. Finally, we\nhighlight synergies between KMs and RAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamically integrating new or rapidly evolving information after (Large)\nLanguage Model pre-training remains challenging, particularly in low-data\nscenarios or when dealing with private and specialized documents. In-context\nlearning and retrieval-augmented generation (RAG) face limitations, including\ntheir high inference costs and their inability to capture global document\ninformation. In this paper, we propose a way of modularizing knowledge by\ntraining document-level Knowledge Modules (KMs). KMs are lightweight components\nimplemented as parameter-efficient LoRA modules, which are trained to store\ninformation about new documents and can be easily plugged into models on\ndemand. We show that next-token prediction performs poorly as the training\nobjective for KMs. We instead propose Deep Context Distillation: we learn KMs\nparameters such as to simulate hidden states and logits of a teacher that takes\nthe document in context. Our method outperforms standard next-token prediction\nand pre-instruction training techniques, across two datasets. Finally, we\nhighlight synergies between KMs and RAG."
                },
                "authors": [
                    {
                        "name": "Lucas Caccia"
                    },
                    {
                        "name": "Alan Ansell"
                    },
                    {
                        "name": "Edoardo Ponti"
                    },
                    {
                        "name": "Ivan Vulić"
                    },
                    {
                        "name": "Alessandro Sordoni"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Sordoni"
                },
                "author": "Alessandro Sordoni",
                "arxiv_comment": "Accepted at the CONFERENCE ON LANGUAGE MODELING (COLM) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08727v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08727v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06355v1",
                "updated": "2025-08-08T14:36:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    36,
                    59,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T14:36:59Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    36,
                    59,
                    4,
                    220,
                    0
                ],
                "title": "Quantum Algorithm for Estimating Intrinsic Geometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Algorithm for Estimating Intrinsic Geometry"
                },
                "summary": "High-dimensional datasets typically cluster around lower-dimensional\nmanifolds but are also often marred by severe noise, obscuring the intrinsic\ngeometry essential for downstream learning tasks. We present a quantum\nalgorithm for estimating the intrinsic geometry of a point cloud --\nspecifically its local intrinsic dimension and local scalar curvature. These\nquantities are crucial for dimensionality reduction, feature extraction, and\nanomaly detection -- tasks that are central to a wide range of data-driven and\ndata-assisted applications. In this work, we propose a quantum algorithm which\ntakes a dataset with pairwise geometric distance, output the estimation of\nlocal dimension and curvature at a given point. We demonstrate that this\nquantum algorithm achieves an exponential speedup over its classical\ncounterpart, and, as a corollary, further extend our main technique to\ndiffusion maps, yielding exponential improvements even over existing quantum\nalgorithms. Our work marks another step toward efficient quantum applications\nin geometrical data analysis, moving beyond topological summaries toward\nprecise geometric inference and opening a novel, scalable path to\nquantum-enhanced manifold learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-dimensional datasets typically cluster around lower-dimensional\nmanifolds but are also often marred by severe noise, obscuring the intrinsic\ngeometry essential for downstream learning tasks. We present a quantum\nalgorithm for estimating the intrinsic geometry of a point cloud --\nspecifically its local intrinsic dimension and local scalar curvature. These\nquantities are crucial for dimensionality reduction, feature extraction, and\nanomaly detection -- tasks that are central to a wide range of data-driven and\ndata-assisted applications. In this work, we propose a quantum algorithm which\ntakes a dataset with pairwise geometric distance, output the estimation of\nlocal dimension and curvature at a given point. We demonstrate that this\nquantum algorithm achieves an exponential speedup over its classical\ncounterpart, and, as a corollary, further extend our main technique to\ndiffusion maps, yielding exponential improvements even over existing quantum\nalgorithms. Our work marks another step toward efficient quantum applications\nin geometrical data analysis, moving beyond topological summaries toward\nprecise geometric inference and opening a novel, scalable path to\nquantum-enhanced manifold learning."
                },
                "authors": [
                    {
                        "name": "Nhat A. Nghiem"
                    },
                    {
                        "name": "Tuan K. Do"
                    },
                    {
                        "name": "Tzu-Chieh Wei"
                    },
                    {
                        "name": "Trung V. Phan"
                    }
                ],
                "author_detail": {
                    "name": "Trung V. Phan"
                },
                "author": "Trung V. Phan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09349v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09349v2",
                "updated": "2025-08-08T14:35:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    35,
                    44,
                    4,
                    220,
                    0
                ],
                "published": "2025-06-11T02:57:22Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    2,
                    57,
                    22,
                    2,
                    162,
                    0
                ],
                "title": "DrVoice: Parallel Speech-Text Voice Conversation Model via\n  Dual-Resolution Speech Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DrVoice: Parallel Speech-Text Voice Conversation Model via\n  Dual-Resolution Speech Representations"
                },
                "summary": "Recent studies on end-to-end speech generation with large language models\n(LLMs) have attracted significant community attention, with multiple works\nextending text-based LLMs to generate discrete speech tokens. Existing\napproaches primarily fall into two categories: (1) Methods that generate\ndiscrete speech tokens independently without incorporating them into the LLM's\nautoregressive process, resulting in text generation being unaware of\nconcurrent speech synthesis. (2) Models that generate interleaved or parallel\nspeech-text tokens through joint autoregressive modeling, enabling mutual\nmodality awareness during generation. This paper presents DrVoice, a parallel\nspeech-text voice conversation model based on joint autoregressive modeling,\nfeaturing dual-resolution speech representations. Whereas current methods\nutilize mainly 12.5Hz input audio representation, our proposed dual-resolution\nmechanism reduces the input frequency for the LLM to 5Hz. Experimental results\non Spoken Question Answering benchmarks demonstrate that D RVOICE establishes\nnew state-of-the-art (SOTA) performance among similar size speech foundation\nmodels with relative small amount of data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies on end-to-end speech generation with large language models\n(LLMs) have attracted significant community attention, with multiple works\nextending text-based LLMs to generate discrete speech tokens. Existing\napproaches primarily fall into two categories: (1) Methods that generate\ndiscrete speech tokens independently without incorporating them into the LLM's\nautoregressive process, resulting in text generation being unaware of\nconcurrent speech synthesis. (2) Models that generate interleaved or parallel\nspeech-text tokens through joint autoregressive modeling, enabling mutual\nmodality awareness during generation. This paper presents DrVoice, a parallel\nspeech-text voice conversation model based on joint autoregressive modeling,\nfeaturing dual-resolution speech representations. Whereas current methods\nutilize mainly 12.5Hz input audio representation, our proposed dual-resolution\nmechanism reduces the input frequency for the LLM to 5Hz. Experimental results\non Spoken Question Answering benchmarks demonstrate that D RVOICE establishes\nnew state-of-the-art (SOTA) performance among similar size speech foundation\nmodels with relative small amount of data."
                },
                "authors": [
                    {
                        "name": "Chao-Hong Tan"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Wen Wang"
                    },
                    {
                        "name": "Chong Deng"
                    },
                    {
                        "name": "Qinglin Zhang"
                    },
                    {
                        "name": "Luyao Cheng"
                    },
                    {
                        "name": "Hai Yu"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Xiang Lv"
                    },
                    {
                        "name": "Tianyu Zhao"
                    },
                    {
                        "name": "Chong Zhang"
                    },
                    {
                        "name": "Yukun Ma"
                    },
                    {
                        "name": "Yafeng Chen"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Jiaqing Liu"
                    },
                    {
                        "name": "Jieping Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jieping Ye"
                },
                "author": "Jieping Ye",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09349v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09349v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06350v1",
                "updated": "2025-08-08T14:30:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    30,
                    5,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T14:30:05Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    30,
                    5,
                    4,
                    220,
                    0
                ],
                "title": "Aligning Effective Tokens with Video Anomaly in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Effective Tokens with Video Anomaly in Large Language Models"
                },
                "summary": "Understanding abnormal events in videos is a vital and challenging task that\nhas garnered significant attention in a wide range of applications. Although\ncurrent video understanding Multi-modal Large Language Models (MLLMs) are\ncapable of analyzing general videos, they often struggle to handle anomalies\ndue to the spatial and temporal sparsity of abnormal events, where the\nredundant information always leads to suboptimal outcomes. To address these\nchallenges, exploiting the representation and generalization capabilities of\nVison Language Models (VLMs) and Large Language Models (LLMs), we propose\nVA-GPT, a novel MLLM designed for summarizing and localizing abnormal events in\nvarious videos. Our approach efficiently aligns effective tokens between visual\nencoders and LLMs through two key proposed modules: Spatial Effective Token\nSelection (SETS) and Temporal Effective Token Generation (TETG). These modules\nenable our model to effectively capture and analyze both spatial and temporal\ninformation associated with abnormal events, resulting in more accurate\nresponses and interactions. Furthermore, we construct an instruction-following\ndataset specifically for fine-tuning video-anomaly-aware MLLMs, and introduce a\ncross-domain evaluation benchmark based on XD-Violence dataset. Our proposed\nmethod outperforms existing state-of-the-art methods on various benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding abnormal events in videos is a vital and challenging task that\nhas garnered significant attention in a wide range of applications. Although\ncurrent video understanding Multi-modal Large Language Models (MLLMs) are\ncapable of analyzing general videos, they often struggle to handle anomalies\ndue to the spatial and temporal sparsity of abnormal events, where the\nredundant information always leads to suboptimal outcomes. To address these\nchallenges, exploiting the representation and generalization capabilities of\nVison Language Models (VLMs) and Large Language Models (LLMs), we propose\nVA-GPT, a novel MLLM designed for summarizing and localizing abnormal events in\nvarious videos. Our approach efficiently aligns effective tokens between visual\nencoders and LLMs through two key proposed modules: Spatial Effective Token\nSelection (SETS) and Temporal Effective Token Generation (TETG). These modules\nenable our model to effectively capture and analyze both spatial and temporal\ninformation associated with abnormal events, resulting in more accurate\nresponses and interactions. Furthermore, we construct an instruction-following\ndataset specifically for fine-tuning video-anomaly-aware MLLMs, and introduce a\ncross-domain evaluation benchmark based on XD-Violence dataset. Our proposed\nmethod outperforms existing state-of-the-art methods on various benchmarks."
                },
                "authors": [
                    {
                        "name": "Yingxian Chen"
                    },
                    {
                        "name": "Jiahui Liu"
                    },
                    {
                        "name": "Ruifan Di"
                    },
                    {
                        "name": "Yanwei Li"
                    },
                    {
                        "name": "Chirui Chang"
                    },
                    {
                        "name": "Shizhen Zhao"
                    },
                    {
                        "name": "Wilton W. T. Fok"
                    },
                    {
                        "name": "Xiaojuan Qi"
                    },
                    {
                        "name": "Yik-Chung Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yik-Chung Wu"
                },
                "author": "Yik-Chung Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07467v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07467v3",
                "updated": "2025-08-08T14:25:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    25,
                    24,
                    4,
                    220,
                    0
                ],
                "published": "2024-06-11T17:13:18Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    17,
                    13,
                    18,
                    1,
                    163,
                    0
                ],
                "title": "LLM meets ML: Data-efficient Anomaly Detection on Unstable Logs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM meets ML: Data-efficient Anomaly Detection on Unstable Logs"
                },
                "summary": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for \\task, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\n(pp) in F1 score while using much less labeled data (62.87 pp reduction). When\ntrained on the same amount of data as the baselines, FlexLog achieves up to a\n13 pp increase in F1 score on ADFA-U across varying training dataset sizes.\nAdditionally, FlexLog maintains inference time under one second per log\nsequence, making it suitable for most applications, except latency-sensitive\nsystems. Further analysis reveals the positive impact of FlexLog's key\ncomponents: cache, RAG and ensemble learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for \\task, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\n(pp) in F1 score while using much less labeled data (62.87 pp reduction). When\ntrained on the same amount of data as the baselines, FlexLog achieves up to a\n13 pp increase in F1 score on ADFA-U across varying training dataset sizes.\nAdditionally, FlexLog maintains inference time under one second per log\nsequence, making it suitable for most applications, except latency-sensitive\nsystems. Further analysis reveals the positive impact of FlexLog's key\ncomponents: cache, RAG and ensemble learning."
                },
                "authors": [
                    {
                        "name": "Fatemeh Hadadi"
                    },
                    {
                        "name": "Qinghua Xu"
                    },
                    {
                        "name": "Domenico Bianculli"
                    },
                    {
                        "name": "Lionel Briand"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Briand"
                },
                "author": "Lionel Briand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07467v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07467v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06349v1",
                "updated": "2025-08-08T14:24:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    24,
                    4,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T14:24:04Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    24,
                    4,
                    4,
                    220,
                    0
                ],
                "title": "Emoji Reactions on Telegram Often Reflect Social Approval Over Emotional\n  Resonance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emoji Reactions on Telegram Often Reflect Social Approval Over Emotional\n  Resonance"
                },
                "summary": "Emoji reactions are a frequently used feature of messaging platforms. Prior\nwork mainly interpreted emojis as indicators of emotional resonance or user\nsentiment. However, emoji reactions may instead reflect broader social\ndynamics. Here, we investigate the communicative function of emoji reactions on\nTelegram by analyzing the relationship between the emotional and rhetorical\ncontent of messages and the emoji reactions they receive. We collect and\nanalyze over 650k Telegram messages that received at least one emoji reaction.\nWe annotate each message with sentiment, emotion, persuasion strategy, and\nspeech act labels, and infer the sentiment and emotion of emoji reactions using\nboth lexicons and large languages. We find a systematic mismatch between\nmessage sentiment and reaction sentiment, with positive reactions dominating\neven when the message is neutral or negative. We show that this pattern remains\nconsistent across rhetorical strategies and emotional tones, suggesting that\nemoji reactions may signal a degree of social approval rather than reflecting\nemotional resonance. Finally, we shed light on the communicative strategies\nthat predict greater emoji engagement. These findings have methodological\nimplications for sentiment analysis, as interpreting emoji reactions as direct\nproxies for emotional response may be misleading.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emoji reactions are a frequently used feature of messaging platforms. Prior\nwork mainly interpreted emojis as indicators of emotional resonance or user\nsentiment. However, emoji reactions may instead reflect broader social\ndynamics. Here, we investigate the communicative function of emoji reactions on\nTelegram by analyzing the relationship between the emotional and rhetorical\ncontent of messages and the emoji reactions they receive. We collect and\nanalyze over 650k Telegram messages that received at least one emoji reaction.\nWe annotate each message with sentiment, emotion, persuasion strategy, and\nspeech act labels, and infer the sentiment and emotion of emoji reactions using\nboth lexicons and large languages. We find a systematic mismatch between\nmessage sentiment and reaction sentiment, with positive reactions dominating\neven when the message is neutral or negative. We show that this pattern remains\nconsistent across rhetorical strategies and emotional tones, suggesting that\nemoji reactions may signal a degree of social approval rather than reflecting\nemotional resonance. Finally, we shed light on the communicative strategies\nthat predict greater emoji engagement. These findings have methodological\nimplications for sentiment analysis, as interpreting emoji reactions as direct\nproxies for emotional response may be misleading."
                },
                "authors": [
                    {
                        "name": "Serena Tardelli"
                    },
                    {
                        "name": "Lorenzo Alvisi"
                    },
                    {
                        "name": "Lorenzo Cima"
                    },
                    {
                        "name": "Stefano Cresci"
                    },
                    {
                        "name": "Maurizio Tesconi"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Tesconi"
                },
                "author": "Maurizio Tesconi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21761v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21761v2",
                "updated": "2025-08-08T14:24:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    24,
                    2,
                    4,
                    220,
                    0
                ],
                "published": "2025-07-29T12:46:36Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    12,
                    46,
                    36,
                    1,
                    210,
                    0
                ],
                "title": "MOR-VIT: Efficient Vision Transformer with Mixture-of-Recursions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOR-VIT: Efficient Vision Transformer with Mixture-of-Recursions"
                },
                "summary": "Vision Transformers (ViTs) have achieved remarkable success in image\nrecognition, yet standard ViT architectures are hampered by substantial\nparameter redundancy and high computational cost, limiting their practical\ndeployment. While recent efforts on efficient ViTs primarily focus on static\nmodel compression or token-level sparsification, they remain constrained by\nfixed computational depth for all tokens. In this work, we present MoR-ViT, a\nnovel vision transformer framework that, for the first time, incorporates a\ntoken-level dynamic recursion mechanism inspired by the Mixture-of-Recursions\n(MoR) paradigm. This approach enables each token to adaptively determine its\nprocessing depth, yielding a flexible and input-dependent allocation of\ncomputational resources. Extensive experiments on ImageNet-1K and transfer\nbenchmarks demonstrate that MoR-ViT not only achieves state-of-the-art accuracy\nwith up to 70% parameter reduction and 2.5x inference acceleration, but also\noutperforms leading efficient ViT baselines such as DynamicViT and TinyViT\nunder comparable conditions. These results establish dynamic recursion as an\neffective strategy for efficient vision transformers and open new avenues for\nscalable and deployable deep learning models in real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Transformers (ViTs) have achieved remarkable success in image\nrecognition, yet standard ViT architectures are hampered by substantial\nparameter redundancy and high computational cost, limiting their practical\ndeployment. While recent efforts on efficient ViTs primarily focus on static\nmodel compression or token-level sparsification, they remain constrained by\nfixed computational depth for all tokens. In this work, we present MoR-ViT, a\nnovel vision transformer framework that, for the first time, incorporates a\ntoken-level dynamic recursion mechanism inspired by the Mixture-of-Recursions\n(MoR) paradigm. This approach enables each token to adaptively determine its\nprocessing depth, yielding a flexible and input-dependent allocation of\ncomputational resources. Extensive experiments on ImageNet-1K and transfer\nbenchmarks demonstrate that MoR-ViT not only achieves state-of-the-art accuracy\nwith up to 70% parameter reduction and 2.5x inference acceleration, but also\noutperforms leading efficient ViT baselines such as DynamicViT and TinyViT\nunder comparable conditions. These results establish dynamic recursion as an\neffective strategy for efficient vision transformers and open new avenues for\nscalable and deployable deep learning models in real-world scenarios."
                },
                "authors": [
                    {
                        "name": "YiZhou Li"
                    }
                ],
                "author_detail": {
                    "name": "YiZhou Li"
                },
                "author": "YiZhou Li",
                "arxiv_comment": "20 pages,9 figuers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21761v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06345v1",
                "updated": "2025-08-08T14:18:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    18,
                    24,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T14:18:24Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    18,
                    24,
                    4,
                    220,
                    0
                ],
                "title": "Harnessing Adaptive Topology Representations for Zero-Shot Graph\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Adaptive Topology Representations for Zero-Shot Graph\n  Question Answering"
                },
                "summary": "Large Multimodal Models (LMMs) have shown generalized zero-shot capabilities\nin diverse domain question-answering (QA) tasks, including graph QA that\ninvolves complex graph topologies. However, most current approaches use only a\nsingle type of graph representation, namely Topology Representation Form (TRF),\nsuch as prompt-unified text descriptions or style-fixed visual styles. Those\n\"one-size-fits-all\" approaches fail to consider the specific preferences of\ndifferent models or tasks, often leading to incorrect or overly long responses.\nTo address this, we first analyze the characteristics and weaknesses of\nexisting TRFs, and then design a set of TRFs, denoted by $F_{ZS}$, tailored to\nzero-shot graph QA. We then introduce a new metric, Graph Response Efficiency\n(GRE), which measures the balance between the performance and the brevity in\ngraph QA. Built on these, we develop the DynamicTRF framework, which aims to\nimprove both the accuracy and conciseness of graph QA. To be specific,\nDynamicTRF first creates a TRF Preference (TRFP) dataset that ranks TRFs based\non their GRE scores, to probe the question-specific TRF preferences. Then it\ntrains a TRF router on the TRFP dataset, to adaptively assign the best TRF from\n$F_{ZS}$ for each question during the inference. Extensive experiments across 7\nin-domain algorithmic graph QA tasks and 2 out-of-domain downstream tasks show\nthat DynamicTRF significantly enhances the zero-shot graph QA of LMMs in terms\nof accuracy",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) have shown generalized zero-shot capabilities\nin diverse domain question-answering (QA) tasks, including graph QA that\ninvolves complex graph topologies. However, most current approaches use only a\nsingle type of graph representation, namely Topology Representation Form (TRF),\nsuch as prompt-unified text descriptions or style-fixed visual styles. Those\n\"one-size-fits-all\" approaches fail to consider the specific preferences of\ndifferent models or tasks, often leading to incorrect or overly long responses.\nTo address this, we first analyze the characteristics and weaknesses of\nexisting TRFs, and then design a set of TRFs, denoted by $F_{ZS}$, tailored to\nzero-shot graph QA. We then introduce a new metric, Graph Response Efficiency\n(GRE), which measures the balance between the performance and the brevity in\ngraph QA. Built on these, we develop the DynamicTRF framework, which aims to\nimprove both the accuracy and conciseness of graph QA. To be specific,\nDynamicTRF first creates a TRF Preference (TRFP) dataset that ranks TRFs based\non their GRE scores, to probe the question-specific TRF preferences. Then it\ntrains a TRF router on the TRFP dataset, to adaptively assign the best TRF from\n$F_{ZS}$ for each question during the inference. Extensive experiments across 7\nin-domain algorithmic graph QA tasks and 2 out-of-domain downstream tasks show\nthat DynamicTRF significantly enhances the zero-shot graph QA of LMMs in terms\nof accuracy"
                },
                "authors": [
                    {
                        "name": "Yanbin Wei"
                    },
                    {
                        "name": "Jiangyue Yan"
                    },
                    {
                        "name": "Chun Kang"
                    },
                    {
                        "name": "Yang Chen"
                    },
                    {
                        "name": "Hua Liu"
                    },
                    {
                        "name": "James T. Kwok"
                    },
                    {
                        "name": "Yu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Zhang"
                },
                "author": "Yu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05464v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05464v2",
                "updated": "2025-08-08T14:16:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    16,
                    34,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-07T15:03:39Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    3,
                    39,
                    3,
                    219,
                    0
                ],
                "title": "Bench-2-CoP: Can We Trust Benchmarking for EU AI Compliance?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bench-2-CoP: Can We Trust Benchmarking for EU AI Compliance?"
                },
                "summary": "The rapid advancement of General Purpose AI (GPAI) models necessitates robust\nevaluation frameworks, especially with emerging regulations like the EU AI Act\nand its associated Code of Practice (CoP). Current AI evaluation practices\ndepend heavily on established benchmarks, but these tools were not designed to\nmeasure the systemic risks that are the focus of the new regulatory landscape.\nThis research addresses the urgent need to quantify this \"benchmark-regulation\ngap.\" We introduce Bench-2-CoP, a novel, systematic framework that uses\nvalidated LLM-as-judge analysis to map the coverage of 194,955 questions from\nwidely-used benchmarks against the EU AI Act's taxonomy of model capabilities\nand propensities. Our findings reveal a profound misalignment: the evaluation\necosystem dedicates the vast majority of its focus to a narrow set of\nbehavioral propensities. On average, benchmarks devote 61.6% of their\nregulatory-relevant questions to \"Tendency to hallucinate\" and 31.2% to \"Lack\nof performance reliability\", while critical functional capabilities are\ndangerously neglected. Crucially, capabilities central to loss-of-control\nscenarios, including evading human oversight, self-replication, and autonomous\nAI development, receive zero coverage in the entire benchmark corpus. This\nstudy provides the first comprehensive, quantitative analysis of this gap,\ndemonstrating that current public benchmarks are insufficient, on their own,\nfor providing the evidence of comprehensive risk assessment required for\nregulatory compliance and offering critical insights for the development of\nnext-generation evaluation tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of General Purpose AI (GPAI) models necessitates robust\nevaluation frameworks, especially with emerging regulations like the EU AI Act\nand its associated Code of Practice (CoP). Current AI evaluation practices\ndepend heavily on established benchmarks, but these tools were not designed to\nmeasure the systemic risks that are the focus of the new regulatory landscape.\nThis research addresses the urgent need to quantify this \"benchmark-regulation\ngap.\" We introduce Bench-2-CoP, a novel, systematic framework that uses\nvalidated LLM-as-judge analysis to map the coverage of 194,955 questions from\nwidely-used benchmarks against the EU AI Act's taxonomy of model capabilities\nand propensities. Our findings reveal a profound misalignment: the evaluation\necosystem dedicates the vast majority of its focus to a narrow set of\nbehavioral propensities. On average, benchmarks devote 61.6% of their\nregulatory-relevant questions to \"Tendency to hallucinate\" and 31.2% to \"Lack\nof performance reliability\", while critical functional capabilities are\ndangerously neglected. Crucially, capabilities central to loss-of-control\nscenarios, including evading human oversight, self-replication, and autonomous\nAI development, receive zero coverage in the entire benchmark corpus. This\nstudy provides the first comprehensive, quantitative analysis of this gap,\ndemonstrating that current public benchmarks are insufficient, on their own,\nfor providing the evidence of comprehensive risk assessment required for\nregulatory compliance and offering critical insights for the development of\nnext-generation evaluation tools."
                },
                "authors": [
                    {
                        "name": "Matteo Prandi"
                    },
                    {
                        "name": "Vincenzo Suriani"
                    },
                    {
                        "name": "Federico Pierucci"
                    },
                    {
                        "name": "Marcello Galisai"
                    },
                    {
                        "name": "Daniele Nardi"
                    },
                    {
                        "name": "Piercosma Bisconti"
                    }
                ],
                "author_detail": {
                    "name": "Piercosma Bisconti"
                },
                "author": "Piercosma Bisconti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05464v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05464v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06342v1",
                "updated": "2025-08-08T14:15:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    15,
                    58,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T14:15:58Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    15,
                    58,
                    4,
                    220,
                    0
                ],
                "title": "Street View Sociability: Interpretable Analysis of Urban Social Behavior\n  Across 15 Cities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Street View Sociability: Interpretable Analysis of Urban Social Behavior\n  Across 15 Cities"
                },
                "summary": "Designing socially active streets has long been a goal of urban planning, yet\nexisting quantitative research largely measures pedestrian volume rather than\nthe quality of social interactions. We hypothesize that street view imagery --\nan inexpensive data source with global coverage -- contains latent social\ninformation that can be extracted and interpreted through established social\nscience theory. As a proof of concept, we analyzed 2,998 street view images\nfrom 15 cities using a multimodal large language model guided by Mehta's\ntaxonomy of passive, fleeting, and enduring sociability -- one illustrative\nexample of a theory grounded in urban design that could be substituted or\ncomplemented by other sociological frameworks. We then used linear regression\nmodels, controlling for factors like weather, time of day, and pedestrian\ncounts, to test whether the inferred sociability measures correlate with\ncity-level place attachment scores from the World Values Survey and with\nenvironmental predictors (e.g., green, sky, and water view indices) derived\nfrom individual street view images. Results aligned with long-standing urban\nplanning theory: the sky view index was associated with all three sociability\ntypes, the green view index predicted enduring sociability, and place\nattachment was positively associated with fleeting sociability. These results\nprovide preliminary evidence that street view images can be used to infer\nrelationships between specific types of social interactions and built\nenvironment variables. Further research could establish street view imagery as\na scalable, privacy-preserving tool for studying urban sociability, enabling\ncross-cultural theory testing and evidence-based design of socially vibrant\ncities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing socially active streets has long been a goal of urban planning, yet\nexisting quantitative research largely measures pedestrian volume rather than\nthe quality of social interactions. We hypothesize that street view imagery --\nan inexpensive data source with global coverage -- contains latent social\ninformation that can be extracted and interpreted through established social\nscience theory. As a proof of concept, we analyzed 2,998 street view images\nfrom 15 cities using a multimodal large language model guided by Mehta's\ntaxonomy of passive, fleeting, and enduring sociability -- one illustrative\nexample of a theory grounded in urban design that could be substituted or\ncomplemented by other sociological frameworks. We then used linear regression\nmodels, controlling for factors like weather, time of day, and pedestrian\ncounts, to test whether the inferred sociability measures correlate with\ncity-level place attachment scores from the World Values Survey and with\nenvironmental predictors (e.g., green, sky, and water view indices) derived\nfrom individual street view images. Results aligned with long-standing urban\nplanning theory: the sky view index was associated with all three sociability\ntypes, the green view index predicted enduring sociability, and place\nattachment was positively associated with fleeting sociability. These results\nprovide preliminary evidence that street view images can be used to infer\nrelationships between specific types of social interactions and built\nenvironment variables. Further research could establish street view imagery as\na scalable, privacy-preserving tool for studying urban sociability, enabling\ncross-cultural theory testing and evidence-based design of socially vibrant\ncities."
                },
                "authors": [
                    {
                        "name": "Kieran Elrod"
                    },
                    {
                        "name": "Katherine Flanigan"
                    },
                    {
                        "name": "Mario Bergés"
                    }
                ],
                "author_detail": {
                    "name": "Mario Bergés"
                },
                "author": "Mario Bergés",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06341v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06341v1",
                "updated": "2025-08-08T14:15:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    15,
                    14,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T14:15:14Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    15,
                    14,
                    4,
                    220,
                    0
                ],
                "title": "Impact of black hole spin on low-mass black hole-neutron star mergers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impact of black hole spin on low-mass black hole-neutron star mergers"
                },
                "summary": "The recent detection of GW230529 suggests that black hole-neutron star\nmergers may involve low-mass black holes, potentially producing detectable\nelectromagnetic counterparts. Motivated by this, we perform eleven fully\ngeneral-relativistic hydrodynamic simulations with and without neutrino\ntreatment, targeting the inferred chirp mass of GW230529. We systematically\nvary the black hole spin from $a_{\\mathrm{BH}} = 0.0$ to $0.8$ in steps of\n$0.1$, making this the most comprehensive study of spin effects in black\nhole-neutron star mergers to date. We confirm our earlier findings of\nfast-moving ejecta ($v \\geq 0.6\\,c$) in this parameter regime and demonstrate a\nclear spin dependence, with fast-ejecta masses reaching up to $\\qty{\\sim\ne-3}{\\Mass\\Sun}$ for $a_{\\mathrm{BH}} = 0.8$. Most notably, we identify for the\nfirst time the presence of spiral wave-driven ejecta in black hole-neutron star\nmergers -- a phenomenon previously reported only in binary neutron star\nsystems. The mass of this component grows significantly with spin, reaching\nlevels up to $\\qty{\\sim 7e-3}{\\Mass\\Sun}$. These results establish a new\nspin-enhanced mechanism for powering blue kilonova emission in black\nhole-neutron star mergers, significantly extending the range of systems\nexpected to produce observable electromagnetic counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent detection of GW230529 suggests that black hole-neutron star\nmergers may involve low-mass black holes, potentially producing detectable\nelectromagnetic counterparts. Motivated by this, we perform eleven fully\ngeneral-relativistic hydrodynamic simulations with and without neutrino\ntreatment, targeting the inferred chirp mass of GW230529. We systematically\nvary the black hole spin from $a_{\\mathrm{BH}} = 0.0$ to $0.8$ in steps of\n$0.1$, making this the most comprehensive study of spin effects in black\nhole-neutron star mergers to date. We confirm our earlier findings of\nfast-moving ejecta ($v \\geq 0.6\\,c$) in this parameter regime and demonstrate a\nclear spin dependence, with fast-ejecta masses reaching up to $\\qty{\\sim\ne-3}{\\Mass\\Sun}$ for $a_{\\mathrm{BH}} = 0.8$. Most notably, we identify for the\nfirst time the presence of spiral wave-driven ejecta in black hole-neutron star\nmergers -- a phenomenon previously reported only in binary neutron star\nsystems. The mass of this component grows significantly with spin, reaching\nlevels up to $\\qty{\\sim 7e-3}{\\Mass\\Sun}$. These results establish a new\nspin-enhanced mechanism for powering blue kilonova emission in black\nhole-neutron star mergers, significantly extending the range of systems\nexpected to produce observable electromagnetic counterparts."
                },
                "authors": [
                    {
                        "name": "Rahime Matur"
                    },
                    {
                        "name": "Ian Hawke"
                    },
                    {
                        "name": "Nils Andersson"
                    }
                ],
                "author_detail": {
                    "name": "Nils Andersson"
                },
                "author": "Nils Andersson",
                "arxiv_comment": "12 pages, 8 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06341v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06341v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06339v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06339v1",
                "updated": "2025-08-08T14:14:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    14,
                    13,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T14:14:13Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    14,
                    13,
                    4,
                    220,
                    0
                ],
                "title": "Performant Unified GPU Kernels for Portable Singular Value Computation\n  Across Hardware and Precision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Unified GPU Kernels for Portable Singular Value Computation\n  Across Hardware and Precision"
                },
                "summary": "This paper presents a portable, GPU-accelerated implementation of a QR-based\nsingular value computation algorithm in Julia. The singular value ecomposition\n(SVD) is a fundamental numerical tool in scientific computing and machine\nlearning, providing optimal low-rank matrix approximations. Its importance has\nincreased even more in large-scale machine learning pipelines, including large\nlanguage models (LLMs), where it enables low-rank adaptation (LoRA). The\nimplemented algorithm is based on the classic two-stage QR reduction,\nconsisting of successive matrix reduction to band form and bidiagonal form. Our\nimplementation leverages Julia's multiple dispatch and metaprogramming\ncapabilities, integrating with the GPUArrays and KernelAbstractions frameworks\nto provide a unified type and hardware-agnostic function. It supports diverse\nGPU architectures and data types, and is, to our knowledge, the first\nGPU-accelerated singular value implementation to support Apple Metal GPUs and\nhalf precision. Performance results on multiple GPU backends and data types\ndemonstrate that portability does not require sacrificing performance: the\nunified function outperforms most linear algebra libraries (MAGMA, SLATE,\nrocSOLVER, oneMKL) for matrix sizes larger than 1024x1024, and achieves 80%-90%\nof the performance of cuSOLVER for large matrices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a portable, GPU-accelerated implementation of a QR-based\nsingular value computation algorithm in Julia. The singular value ecomposition\n(SVD) is a fundamental numerical tool in scientific computing and machine\nlearning, providing optimal low-rank matrix approximations. Its importance has\nincreased even more in large-scale machine learning pipelines, including large\nlanguage models (LLMs), where it enables low-rank adaptation (LoRA). The\nimplemented algorithm is based on the classic two-stage QR reduction,\nconsisting of successive matrix reduction to band form and bidiagonal form. Our\nimplementation leverages Julia's multiple dispatch and metaprogramming\ncapabilities, integrating with the GPUArrays and KernelAbstractions frameworks\nto provide a unified type and hardware-agnostic function. It supports diverse\nGPU architectures and data types, and is, to our knowledge, the first\nGPU-accelerated singular value implementation to support Apple Metal GPUs and\nhalf precision. Performance results on multiple GPU backends and data types\ndemonstrate that portability does not require sacrificing performance: the\nunified function outperforms most linear algebra libraries (MAGMA, SLATE,\nrocSOLVER, oneMKL) for matrix sizes larger than 1024x1024, and achieves 80%-90%\nof the performance of cuSOLVER for large matrices."
                },
                "authors": [
                    {
                        "name": "Evelyne Ringoot"
                    },
                    {
                        "name": "Rabab Alomairy"
                    },
                    {
                        "name": "Valentin Churavy"
                    },
                    {
                        "name": "Alan Edelman"
                    }
                ],
                "author_detail": {
                    "name": "Alan Edelman"
                },
                "author": "Alan Edelman",
                "arxiv_doi": "10.1145/3754598.3754667",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3754598.3754667",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.06339v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06339v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 6 figures, 4 tables",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06337v1",
                "updated": "2025-08-08T14:11:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    11,
                    18,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T14:11:18Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    11,
                    18,
                    4,
                    220,
                    0
                ],
                "title": "Decorrelated feature importance from local sample weighting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decorrelated feature importance from local sample weighting"
                },
                "summary": "Feature importance (FI) statistics provide a prominent and valuable method of\ninsight into the decision process of machine learning (ML) models, but their\neffectiveness has well-known limitations when correlation is present among the\nfeatures in the training data. In this case, the FI often tends to be\ndistributed among all features which are in correlation with the\nresponse-generating signal features. Even worse, if multiple signal features\nare in strong correlation with a noise feature, while being only modestly\ncorrelated with one another, this can result in a noise feature having a\ndistinctly larger FI score than any signal feature. Here we propose local\nsample weighting (losaw) which can flexibly be integrated into many ML\nalgorithms to improve FI scores in the presence of feature correlation in the\ntraining data. Our approach is motivated from inverse probability weighting in\ncausal inference and locally, within the ML model, uses a sample weighting\nscheme to decorrelate a target feature from the remaining features. This\nreduces model bias locally, whenever the effect of a potential signal feature\nis evaluated and compared to others. Moreover, losaw comes with a natural\ntuning parameter, the minimum effective sample size of the weighted population,\nwhich corresponds to an interpretation-prediction-tradeoff, analog to a\nbias-variance-tradeoff as for classical ML tuning parameters. We demonstrate\nhow losaw can be integrated within decision tree-based ML methods and within\nmini-batch training of neural networks. We investigate losaw for random forest\nand convolutional neural networks in a simulation study on settings showing\ndiverse correlation patterns. We found that losaw improves FI consistently.\nMoreover, it often improves prediction accuracy for out-of-distribution, while\nmaintaining a similar accuracy for in-distribution test data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature importance (FI) statistics provide a prominent and valuable method of\ninsight into the decision process of machine learning (ML) models, but their\neffectiveness has well-known limitations when correlation is present among the\nfeatures in the training data. In this case, the FI often tends to be\ndistributed among all features which are in correlation with the\nresponse-generating signal features. Even worse, if multiple signal features\nare in strong correlation with a noise feature, while being only modestly\ncorrelated with one another, this can result in a noise feature having a\ndistinctly larger FI score than any signal feature. Here we propose local\nsample weighting (losaw) which can flexibly be integrated into many ML\nalgorithms to improve FI scores in the presence of feature correlation in the\ntraining data. Our approach is motivated from inverse probability weighting in\ncausal inference and locally, within the ML model, uses a sample weighting\nscheme to decorrelate a target feature from the remaining features. This\nreduces model bias locally, whenever the effect of a potential signal feature\nis evaluated and compared to others. Moreover, losaw comes with a natural\ntuning parameter, the minimum effective sample size of the weighted population,\nwhich corresponds to an interpretation-prediction-tradeoff, analog to a\nbias-variance-tradeoff as for classical ML tuning parameters. We demonstrate\nhow losaw can be integrated within decision tree-based ML methods and within\nmini-batch training of neural networks. We investigate losaw for random forest\nand convolutional neural networks in a simulation study on settings showing\ndiverse correlation patterns. We found that losaw improves FI consistently.\nMoreover, it often improves prediction accuracy for out-of-distribution, while\nmaintaining a similar accuracy for in-distribution test data."
                },
                "authors": [
                    {
                        "name": "Benedikt Fröhlich"
                    },
                    {
                        "name": "Alison Durst"
                    },
                    {
                        "name": "Merle Behr"
                    }
                ],
                "author_detail": {
                    "name": "Merle Behr"
                },
                "author": "Merle Behr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.05645v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.05645v4",
                "updated": "2025-08-08T14:10:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    10,
                    27,
                    4,
                    220,
                    0
                ],
                "published": "2023-07-11T11:40:40Z",
                "published_parsed": [
                    2023,
                    7,
                    11,
                    11,
                    40,
                    40,
                    1,
                    192,
                    0
                ],
                "title": "Description length of canonical and microcanonical models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Description length of canonical and microcanonical models"
                },
                "summary": "The (non-)equivalence of canonical and microcanonical ensembles is a\nfundamental question in statistical physics, concerning whether the use of soft\nand hard constraints in the maximum-entropy construction leads to the same\ndescription of a system. Despite the fact that maximum-entropy models are also\ncommonly used in statistical inference, pattern detection, and hypothesis\ntesting, a complete understanding of the effects of ensemble non-equivalence on\nstatistical modeling is still missing. Here, we study this problem from a\nrigorous model selection perspective by comparing canonical and microcanonical\nmodels via the Minimum Description Length (MDL) principle, which yields a\ntrade-off between likelihood, measuring model accuracy, and complexity,\nmeasuring model flexibility and its potential to overfit data. We compute the\nNormalized Maximum Likelihood (NML) of both formulations and find that: (i)\nmicrocanonical models always achieve higher likelihood but are always more\ncomplex; (ii) the optimal model choice depends on the empirical values of the\nconstraints -- the canonical model performs best when its fit to the observed\ndata exceeds its uniform average fit across all realizations; (iii) in the\nthermodynamic limit, the difference in description length per node vanishes\nwhen ensemble equivalence holds but persists otherwise, showing that\nnon-equivalence implies extensive differences between large canonical and\nmicrocanonical models. Finally, we compare the NML approach to Bayesian\nmethods, showing that (iv) the choice of priors, practically irrelevant in\nequivalent models, becomes crucial when an extensive number of constraints is\nenforced, possibly leading to very different outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The (non-)equivalence of canonical and microcanonical ensembles is a\nfundamental question in statistical physics, concerning whether the use of soft\nand hard constraints in the maximum-entropy construction leads to the same\ndescription of a system. Despite the fact that maximum-entropy models are also\ncommonly used in statistical inference, pattern detection, and hypothesis\ntesting, a complete understanding of the effects of ensemble non-equivalence on\nstatistical modeling is still missing. Here, we study this problem from a\nrigorous model selection perspective by comparing canonical and microcanonical\nmodels via the Minimum Description Length (MDL) principle, which yields a\ntrade-off between likelihood, measuring model accuracy, and complexity,\nmeasuring model flexibility and its potential to overfit data. We compute the\nNormalized Maximum Likelihood (NML) of both formulations and find that: (i)\nmicrocanonical models always achieve higher likelihood but are always more\ncomplex; (ii) the optimal model choice depends on the empirical values of the\nconstraints -- the canonical model performs best when its fit to the observed\ndata exceeds its uniform average fit across all realizations; (iii) in the\nthermodynamic limit, the difference in description length per node vanishes\nwhen ensemble equivalence holds but persists otherwise, showing that\nnon-equivalence implies extensive differences between large canonical and\nmicrocanonical models. Finally, we compare the NML approach to Bayesian\nmethods, showing that (iv) the choice of priors, practically irrelevant in\nequivalent models, becomes crucial when an extensive number of constraints is\nenforced, possibly leading to very different outcomes."
                },
                "authors": [
                    {
                        "name": "Francesca Giuffrida"
                    },
                    {
                        "name": "Tiziano Squartini"
                    },
                    {
                        "name": "Peter Grünwald"
                    },
                    {
                        "name": "Diego Garlaschelli"
                    }
                ],
                "author_detail": {
                    "name": "Diego Garlaschelli"
                },
                "author": "Diego Garlaschelli",
                "arxiv_doi": "10.1103/1sd2-rxmy",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/1sd2-rxmy",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2307.05645v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.05645v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06335v1",
                "updated": "2025-08-08T14:10:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    10,
                    26,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T14:10:26Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    10,
                    26,
                    4,
                    220,
                    0
                ],
                "title": "ViPro-2: Unsupervised State Estimation via Integrated Dynamics for\n  Guiding Video Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ViPro-2: Unsupervised State Estimation via Integrated Dynamics for\n  Guiding Video Prediction"
                },
                "summary": "Predicting future video frames is a challenging task with many downstream\napplications. Previous work has shown that procedural knowledge enables deep\nmodels for complex dynamical settings, however their model ViPro assumed a\ngiven ground truth initial symbolic state. We show that this approach led to\nthe model learning a shortcut that does not actually connect the observed\nenvironment with the predicted symbolic state, resulting in the inability to\nestimate states given an observation if previous states are noisy. In this\nwork, we add several improvements to ViPro that enables the model to correctly\ninfer states from observations without providing a full ground truth state in\nthe beginning. We show that this is possible in an unsupervised manner, and\nextend the original Orbits dataset with a 3D variant to close the gap to real\nworld scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting future video frames is a challenging task with many downstream\napplications. Previous work has shown that procedural knowledge enables deep\nmodels for complex dynamical settings, however their model ViPro assumed a\ngiven ground truth initial symbolic state. We show that this approach led to\nthe model learning a shortcut that does not actually connect the observed\nenvironment with the predicted symbolic state, resulting in the inability to\nestimate states given an observation if previous states are noisy. In this\nwork, we add several improvements to ViPro that enables the model to correctly\ninfer states from observations without providing a full ground truth state in\nthe beginning. We show that this is possible in an unsupervised manner, and\nextend the original Orbits dataset with a 3D variant to close the gap to real\nworld scenarios."
                },
                "authors": [
                    {
                        "name": "Patrick Takenaka"
                    },
                    {
                        "name": "Johannes Maucher"
                    },
                    {
                        "name": "Marco F. Huber"
                    }
                ],
                "author_detail": {
                    "name": "Marco F. Huber"
                },
                "author": "Marco F. Huber",
                "arxiv_comment": "Published in 2025 International Joint Conference on Neural Networks\n  (IJCNN)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06328v1",
                "updated": "2025-08-08T14:00:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    0,
                    19,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T14:00:19Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    0,
                    19,
                    4,
                    220,
                    0
                ],
                "title": "M2IO-R1: An Efficient RL-Enhanced Reasoning Framework for Multimodal\n  Retrieval Augmented Multimodal Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M2IO-R1: An Efficient RL-Enhanced Reasoning Framework for Multimodal\n  Retrieval Augmented Multimodal Generation"
                },
                "summary": "Current research on Multimodal Retrieval-Augmented Generation (MRAG) enables\ndiverse multimodal inputs but remains limited to single-modality outputs,\nrestricting expressive capacity and practical utility. In contrast, real-world\napplications often demand both multimodal inputs and multimodal outputs for\neffective communication and grounded reasoning. Motivated by the recent success\nof Reinforcement Learning (RL) in complex reasoning tasks for Large Language\nModels (LLMs), we adopt RL as a principled and effective paradigm to address\nthe multi-step, outcome-driven challenges inherent in multimodal output\ngeneration. Here, we introduce M2IO-R1, a novel framework for Multimodal\nRetrieval-Augmented Multimodal Generation (MRAMG) that supports both multimodal\ninputs and outputs. Central to our framework is an RL-based inserter,\nInserter-R1-3B, trained with Group Relative Policy Optimization to guide image\nselection and placement in a controllable and semantically aligned manner.\nEmpirical results show that our lightweight 3B inserter achieves strong\nreasoning capabilities with significantly reduced latency, outperforming\nbaselines in both quality and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current research on Multimodal Retrieval-Augmented Generation (MRAG) enables\ndiverse multimodal inputs but remains limited to single-modality outputs,\nrestricting expressive capacity and practical utility. In contrast, real-world\napplications often demand both multimodal inputs and multimodal outputs for\neffective communication and grounded reasoning. Motivated by the recent success\nof Reinforcement Learning (RL) in complex reasoning tasks for Large Language\nModels (LLMs), we adopt RL as a principled and effective paradigm to address\nthe multi-step, outcome-driven challenges inherent in multimodal output\ngeneration. Here, we introduce M2IO-R1, a novel framework for Multimodal\nRetrieval-Augmented Multimodal Generation (MRAMG) that supports both multimodal\ninputs and outputs. Central to our framework is an RL-based inserter,\nInserter-R1-3B, trained with Group Relative Policy Optimization to guide image\nselection and placement in a controllable and semantically aligned manner.\nEmpirical results show that our lightweight 3B inserter achieves strong\nreasoning capabilities with significantly reduced latency, outperforming\nbaselines in both quality and efficiency."
                },
                "authors": [
                    {
                        "name": "Zhiyou Xiao"
                    },
                    {
                        "name": "Qinhan Yu"
                    },
                    {
                        "name": "Binghui Li"
                    },
                    {
                        "name": "Geng Chen"
                    },
                    {
                        "name": "Chong Chen"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06317v1",
                "updated": "2025-08-08T13:47:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    47,
                    0,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T13:47:00Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    47,
                    0,
                    4,
                    220,
                    0
                ],
                "title": "Uncertainty-quantified Rollout Policy Adaptation for Unlabelled\n  Cross-domain Temporal Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty-quantified Rollout Policy Adaptation for Unlabelled\n  Cross-domain Temporal Grounding"
                },
                "summary": "Video Temporal Grounding (TG) aims to temporally locate video segments\nmatching a natural language description (a query) in a long video. While\nVision-Language Models (VLMs) are effective at holistic semantic matching, they\noften struggle with fine-grained temporal localisation. Recently, Group\nRelative Policy Optimisation (GRPO) reformulates the inference process as a\nreinforcement learning task, enabling fine-grained grounding and achieving\nstrong in-domain performance. However, GRPO relies on labelled data, making it\nunsuitable in unlabelled domains. Moreover, because videos are large and\nexpensive to store and process, performing full-scale adaptation introduces\nprohibitive latency and computational overhead, making it impractical for\nreal-time deployment. To overcome both problems, we introduce a Data-Efficient\nUnlabelled Cross-domain Temporal Grounding method, from which a model is first\ntrained on a labelled source domain, then adapted to a target domain using only\na small number of unlabelled videos from the target domain. This approach\neliminates the need for target annotation and keeps both computational and\nstorage overhead low enough to run in real time. Specifically, we introduce.\nUncertainty-quantified Rollout Policy Adaptation (URPA) for cross-domain\nknowledge transfer in learning video temporal grounding without target labels.\nURPA generates multiple candidate predictions using GRPO rollouts, averages\nthem to form a pseudo label, and estimates confidence from the variance across\nthese rollouts. This confidence then weights the training rewards, guiding the\nmodel to focus on reliable supervision. Experiments on three datasets across\nsix cross-domain settings show that URPA generalises well using only a few\nunlabelled target videos. Codes will be released once published.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Temporal Grounding (TG) aims to temporally locate video segments\nmatching a natural language description (a query) in a long video. While\nVision-Language Models (VLMs) are effective at holistic semantic matching, they\noften struggle with fine-grained temporal localisation. Recently, Group\nRelative Policy Optimisation (GRPO) reformulates the inference process as a\nreinforcement learning task, enabling fine-grained grounding and achieving\nstrong in-domain performance. However, GRPO relies on labelled data, making it\nunsuitable in unlabelled domains. Moreover, because videos are large and\nexpensive to store and process, performing full-scale adaptation introduces\nprohibitive latency and computational overhead, making it impractical for\nreal-time deployment. To overcome both problems, we introduce a Data-Efficient\nUnlabelled Cross-domain Temporal Grounding method, from which a model is first\ntrained on a labelled source domain, then adapted to a target domain using only\na small number of unlabelled videos from the target domain. This approach\neliminates the need for target annotation and keeps both computational and\nstorage overhead low enough to run in real time. Specifically, we introduce.\nUncertainty-quantified Rollout Policy Adaptation (URPA) for cross-domain\nknowledge transfer in learning video temporal grounding without target labels.\nURPA generates multiple candidate predictions using GRPO rollouts, averages\nthem to form a pseudo label, and estimates confidence from the variance across\nthese rollouts. This confidence then weights the training rewards, guiding the\nmodel to focus on reliable supervision. Experiments on three datasets across\nsix cross-domain settings show that URPA generalises well using only a few\nunlabelled target videos. Codes will be released once published."
                },
                "authors": [
                    {
                        "name": "Jian Hu"
                    },
                    {
                        "name": "Zixu Cheng"
                    },
                    {
                        "name": "Shaogang Gong"
                    },
                    {
                        "name": "Isabel Guan"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Kun Shao"
                    }
                ],
                "author_detail": {
                    "name": "Kun Shao"
                },
                "author": "Kun Shao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06973v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06973v2",
                "updated": "2025-08-08T13:42:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    42,
                    10,
                    4,
                    220,
                    0
                ],
                "published": "2025-04-09T15:28:36Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    15,
                    28,
                    36,
                    2,
                    99,
                    0
                ],
                "title": "HIP 15429: A newborn Be star on an eccentric binary orbit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HIP 15429: A newborn Be star on an eccentric binary orbit"
                },
                "summary": "We identified a new post-interaction binary, HIP 15429, consisting of a\nstripped star and a recently formed, rapidly rotating Be star companion ($v\n\\sin i \\approx 270$ km/s) sharing many similarities with recently identified\nbloated stripped stars. From orbital fitting of multi-epoch radial velocities\nwe find a 221-day period. We also find an eccentricity of $e=0.52$, which is\nunexpectedly high as tides are expected to have circularised the orbit\nefficiently during the presumed recent mass transfer. The formation of a\ncircumbinary disk during the mass transfer phase or the presence of an unseen\ntertiary companion might explain the orbit's high eccentricity. We determined\nphysical parameters for both stars by fitting the spectra of the disentangled\nbinary components and multi-band photometry. The stripped nature of the donor\nstar is affirmed by its high luminosity at a low inferred mass ($\\lesssim 1\n\\mathrm{M}_\\odot$) and imprints of CNO-processed material in the surface\nabundances. The donor's relatively large radius and cool temperature\n($T_{\\mathrm{eff}} = 13.5 \\pm 0.5$ kK) suggest that it has only recently ceased\nmass transfer. Evolutionary models assuming a 5-6 $\\mathrm{M}_\\odot$ progenitor\ncan reproduce these parameters and imply that the binary is currently evolving\ntowards a stage where the donor becomes a subdwarf orbiting a Be star. The\nremarkably high eccentricity of HIP 15429 challenges standard tidal evolution\nmodels, suggesting either inefficient tidal dissipation or external influences,\nsuch as a tertiary companion or circumbinary disk. This underscores the need to\nidentify and characterise more post-mass transfer binaries to benchmark and\nrefine theoretical models of binary evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We identified a new post-interaction binary, HIP 15429, consisting of a\nstripped star and a recently formed, rapidly rotating Be star companion ($v\n\\sin i \\approx 270$ km/s) sharing many similarities with recently identified\nbloated stripped stars. From orbital fitting of multi-epoch radial velocities\nwe find a 221-day period. We also find an eccentricity of $e=0.52$, which is\nunexpectedly high as tides are expected to have circularised the orbit\nefficiently during the presumed recent mass transfer. The formation of a\ncircumbinary disk during the mass transfer phase or the presence of an unseen\ntertiary companion might explain the orbit's high eccentricity. We determined\nphysical parameters for both stars by fitting the spectra of the disentangled\nbinary components and multi-band photometry. The stripped nature of the donor\nstar is affirmed by its high luminosity at a low inferred mass ($\\lesssim 1\n\\mathrm{M}_\\odot$) and imprints of CNO-processed material in the surface\nabundances. The donor's relatively large radius and cool temperature\n($T_{\\mathrm{eff}} = 13.5 \\pm 0.5$ kK) suggest that it has only recently ceased\nmass transfer. Evolutionary models assuming a 5-6 $\\mathrm{M}_\\odot$ progenitor\ncan reproduce these parameters and imply that the binary is currently evolving\ntowards a stage where the donor becomes a subdwarf orbiting a Be star. The\nremarkably high eccentricity of HIP 15429 challenges standard tidal evolution\nmodels, suggesting either inefficient tidal dissipation or external influences,\nsuch as a tertiary companion or circumbinary disk. This underscores the need to\nidentify and characterise more post-mass transfer binaries to benchmark and\nrefine theoretical models of binary evolution."
                },
                "authors": [
                    {
                        "name": "Johanna Müller-Horn"
                    },
                    {
                        "name": "Kareem El-Badry"
                    },
                    {
                        "name": "Hans-Walter Rix"
                    },
                    {
                        "name": "Tomer Shenar"
                    },
                    {
                        "name": "Rhys Seeburger"
                    },
                    {
                        "name": "Jaime I. Villaseñor"
                    },
                    {
                        "name": "Julia Bodensteiner"
                    },
                    {
                        "name": "David W. Latham"
                    },
                    {
                        "name": "Allyson Bieryla"
                    },
                    {
                        "name": "Lars A. Buchhave"
                    },
                    {
                        "name": "Howard Isaacson"
                    },
                    {
                        "name": "Andrew W. Howard"
                    }
                ],
                "author_detail": {
                    "name": "Andrew W. Howard"
                },
                "author": "Andrew W. Howard",
                "arxiv_comment": "19 pages, 16 figures, accepted for publication in Astronomy &\n  Astrophysics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06973v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06973v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06314v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06314v1",
                "updated": "2025-08-08T13:41:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    41,
                    14,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T13:41:14Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    41,
                    14,
                    4,
                    220,
                    0
                ],
                "title": "Fermi-LAT Galactic Center Excess morphology of dark matter in\n  simulations of the Milky Way galaxy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fermi-LAT Galactic Center Excess morphology of dark matter in\n  simulations of the Milky Way galaxy"
                },
                "summary": "The strongest experimental evidence for dark matter is the Galactic Center\ngamma-ray excess observed by the Fermi telescope and even predicted prior to\ndiscovery as a potential dark matter signature via WIMP dark matter\nself-annihilations. However, an equally compelling explanation of the excess\ngamma-ray flux appeals to a population of old millisecond pulsars that also\naccounts for the observed boxy morphology inferred from the bulge old star\npopulation. We employ a set of Milky Way-like galaxies found in the Hestia\nconstrained simulations of the local universe to explore the rich morphology of\nthe central dark matter distribution, motivated by the GAIA discovery of a\nvigorous early merging history of the Milky Way galaxy. We predict a\nsignificantly non-spherical gamma-ray morphology from the WIMP interpretation.\nFuture experiments, such as the Cherenkov Telescope Array, that extend to\nhigher energies, should distinguish between the competing interpretations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The strongest experimental evidence for dark matter is the Galactic Center\ngamma-ray excess observed by the Fermi telescope and even predicted prior to\ndiscovery as a potential dark matter signature via WIMP dark matter\nself-annihilations. However, an equally compelling explanation of the excess\ngamma-ray flux appeals to a population of old millisecond pulsars that also\naccounts for the observed boxy morphology inferred from the bulge old star\npopulation. We employ a set of Milky Way-like galaxies found in the Hestia\nconstrained simulations of the local universe to explore the rich morphology of\nthe central dark matter distribution, motivated by the GAIA discovery of a\nvigorous early merging history of the Milky Way galaxy. We predict a\nsignificantly non-spherical gamma-ray morphology from the WIMP interpretation.\nFuture experiments, such as the Cherenkov Telescope Array, that extend to\nhigher energies, should distinguish between the competing interpretations."
                },
                "authors": [
                    {
                        "name": "Moorits Mihkel Muru"
                    },
                    {
                        "name": "Joseph Silk"
                    },
                    {
                        "name": "Noam I. Libeskind"
                    },
                    {
                        "name": "Stefan Gottloeber"
                    },
                    {
                        "name": "Yehuda Hoffman"
                    }
                ],
                "author_detail": {
                    "name": "Yehuda Hoffman"
                },
                "author": "Yehuda Hoffman",
                "arxiv_comment": "Accepted for publication in Physical Review Letters, 9 pages, 3\n  figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06314v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06314v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06312v1",
                "updated": "2025-08-08T13:39:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    39,
                    5,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T13:39:05Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    39,
                    5,
                    4,
                    220,
                    0
                ],
                "title": "Chain-of-Alpha: Unleashing the Power of Large Language Models for Alpha\n  Mining in Quantitative Trading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Alpha: Unleashing the Power of Large Language Models for Alpha\n  Mining in Quantitative Trading"
                },
                "summary": "Alpha factor mining is a fundamental task in quantitative trading, aimed at\ndiscovering interpretable signals that can predict asset returns beyond\nsystematic market risk. While traditional methods rely on manual formula design\nor heuristic search with machine learning, recent advances have leveraged Large\nLanguage Models (LLMs) for automated factor discovery. However, existing\nLLM-based alpha mining approaches remain limited in terms of automation,\ngenerality, and efficiency. In this paper, we propose Chain-of-Alpha, a novel,\nsimple, yet effective and efficient LLM-based framework for fully automated\nformulaic alpha mining. Our method features a dual-chain architecture,\nconsisting of a Factor Generation Chain and a Factor Optimization Chain, which\niteratively generate, evaluate, and refine candidate alpha factors using only\nmarket data, while leveraging backtest feedback and prior optimization\nknowledge. The two chains work synergistically to enable high-quality alpha\ndiscovery without human intervention and offer strong scalability. Extensive\nexperiments on real-world A-share benchmarks demonstrate that Chain-of-Alpha\noutperforms existing baselines across multiple metrics, presenting a promising\ndirection for LLM-driven quantitative research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alpha factor mining is a fundamental task in quantitative trading, aimed at\ndiscovering interpretable signals that can predict asset returns beyond\nsystematic market risk. While traditional methods rely on manual formula design\nor heuristic search with machine learning, recent advances have leveraged Large\nLanguage Models (LLMs) for automated factor discovery. However, existing\nLLM-based alpha mining approaches remain limited in terms of automation,\ngenerality, and efficiency. In this paper, we propose Chain-of-Alpha, a novel,\nsimple, yet effective and efficient LLM-based framework for fully automated\nformulaic alpha mining. Our method features a dual-chain architecture,\nconsisting of a Factor Generation Chain and a Factor Optimization Chain, which\niteratively generate, evaluate, and refine candidate alpha factors using only\nmarket data, while leveraging backtest feedback and prior optimization\nknowledge. The two chains work synergistically to enable high-quality alpha\ndiscovery without human intervention and offer strong scalability. Extensive\nexperiments on real-world A-share benchmarks demonstrate that Chain-of-Alpha\noutperforms existing baselines across multiple metrics, presenting a promising\ndirection for LLM-driven quantitative research."
                },
                "authors": [
                    {
                        "name": "Lang Cao"
                    },
                    {
                        "name": "Zekun Xi"
                    },
                    {
                        "name": "Long Liao"
                    },
                    {
                        "name": "Ziwei Yang"
                    },
                    {
                        "name": "Zheng Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Cao"
                },
                "author": "Zheng Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06309v1",
                "updated": "2025-08-08T13:35:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    35,
                    40,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T13:35:40Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    35,
                    40,
                    4,
                    220,
                    0
                ],
                "title": "Matrix-Driven Instant Review: Confident Detection and Reconstruction of\n  LLM Plagiarism on PC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix-Driven Instant Review: Confident Detection and Reconstruction of\n  LLM Plagiarism on PC"
                },
                "summary": "In recent years, concerns about intellectual property (IP) in large language\nmodels (LLMs) have grown significantly. Plagiarizing other LLMs (through direct\nweight copying, upcycling, pruning, or continual pretraining) and claiming\nauthorship without properly attributing to the original license, is a serious\nmisconduct that can lead to significant financial and reputational harm to the\noriginal developers. However, existing methods for detecting LLM plagiarism\nfall short in key areas. They fail to accurately reconstruct weight\ncorrespondences, lack the ability to compute statistical significance measures\nsuch as $p$-values, and may mistakenly flag models trained on similar data as\nbeing related. To address these limitations, we propose Matrix-Driven Instant\nReview (MDIR), a novel method that leverages matrix analysis and Large\nDeviation Theory. MDIR achieves accurate reconstruction of weight\nrelationships, provides rigorous $p$-value estimation, and focuses exclusively\non weight similarity without requiring full model inference. Experimental\nresults demonstrate that MDIR reliably detects plagiarism even after extensive\ntransformations, such as random permutations and continual pretraining with\ntrillions of tokens. Moreover, all detections can be performed on a single PC\nwithin an hour, making MDIR both efficient and accessible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, concerns about intellectual property (IP) in large language\nmodels (LLMs) have grown significantly. Plagiarizing other LLMs (through direct\nweight copying, upcycling, pruning, or continual pretraining) and claiming\nauthorship without properly attributing to the original license, is a serious\nmisconduct that can lead to significant financial and reputational harm to the\noriginal developers. However, existing methods for detecting LLM plagiarism\nfall short in key areas. They fail to accurately reconstruct weight\ncorrespondences, lack the ability to compute statistical significance measures\nsuch as $p$-values, and may mistakenly flag models trained on similar data as\nbeing related. To address these limitations, we propose Matrix-Driven Instant\nReview (MDIR), a novel method that leverages matrix analysis and Large\nDeviation Theory. MDIR achieves accurate reconstruction of weight\nrelationships, provides rigorous $p$-value estimation, and focuses exclusively\non weight similarity without requiring full model inference. Experimental\nresults demonstrate that MDIR reliably detects plagiarism even after extensive\ntransformations, such as random permutations and continual pretraining with\ntrillions of tokens. Moreover, all detections can be performed on a single PC\nwithin an hour, making MDIR both efficient and accessible."
                },
                "authors": [
                    {
                        "name": "Ruichong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ruichong Zhang"
                },
                "author": "Ruichong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23543v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23543v2",
                "updated": "2025-08-08T13:31:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    31,
                    57,
                    4,
                    220,
                    0
                ],
                "published": "2025-07-31T13:34:06Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    13,
                    34,
                    6,
                    3,
                    212,
                    0
                ],
                "title": "ART: Adaptive Relation Tuning for Generalized Relation Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ART: Adaptive Relation Tuning for Generalized Relation Prediction"
                },
                "summary": "Visual relation detection (VRD) is the task of identifying the relationships\nbetween objects in a scene. VRD models trained solely on relation detection\ndata struggle to generalize beyond the relations on which they are trained.\nWhile prompt tuning has been used to adapt vision-language models (VLMs) for\nVRD, it uses handcrafted prompts and struggles with novel or complex relations.\nWe argue that instruction tuning offers a more effective solution by\nfine-tuning VLMs on diverse instructional data. We thus introduce ART, an\nAdaptive Relation Tuning framework that adapts VLMs for VRD through instruction\ntuning and strategic instance selection. By converting VRD datasets into an\ninstruction tuning format and employing an adaptive sampling algorithm, ART\ndirects the VLM to focus on informative relations while maintaining\ngeneralizability. Specifically, we focus on the relation classification, where\nsubject-object boxes are given and the model predicts the predicate between\nthem. We tune on a held-in set and evaluate across multiple held-out datasets\nof varying complexity. Our approach strongly improves over its baselines and\ncan infer unseen relation concepts, a capability absent in mainstream VRD\nmethods. We demonstrate ART's practical value by using the predicted relations\nfor segmenting complex scenes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual relation detection (VRD) is the task of identifying the relationships\nbetween objects in a scene. VRD models trained solely on relation detection\ndata struggle to generalize beyond the relations on which they are trained.\nWhile prompt tuning has been used to adapt vision-language models (VLMs) for\nVRD, it uses handcrafted prompts and struggles with novel or complex relations.\nWe argue that instruction tuning offers a more effective solution by\nfine-tuning VLMs on diverse instructional data. We thus introduce ART, an\nAdaptive Relation Tuning framework that adapts VLMs for VRD through instruction\ntuning and strategic instance selection. By converting VRD datasets into an\ninstruction tuning format and employing an adaptive sampling algorithm, ART\ndirects the VLM to focus on informative relations while maintaining\ngeneralizability. Specifically, we focus on the relation classification, where\nsubject-object boxes are given and the model predicts the predicate between\nthem. We tune on a held-in set and evaluate across multiple held-out datasets\nof varying complexity. Our approach strongly improves over its baselines and\ncan infer unseen relation concepts, a capability absent in mainstream VRD\nmethods. We demonstrate ART's practical value by using the predicted relations\nfor segmenting complex scenes."
                },
                "authors": [
                    {
                        "name": "Gopika Sudhakaran"
                    },
                    {
                        "name": "Hikaru Shindo"
                    },
                    {
                        "name": "Patrick Schramowski"
                    },
                    {
                        "name": "Simone Schaub-Meyer"
                    },
                    {
                        "name": "Kristian Kersting"
                    },
                    {
                        "name": "Stefan Roth"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Roth"
                },
                "author": "Stefan Roth",
                "arxiv_comment": "Accepted for publication in ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23543v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23543v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14810v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14810v2",
                "updated": "2025-08-08T13:29:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    29,
                    20,
                    4,
                    220,
                    0
                ],
                "published": "2025-04-21T02:25:03Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    2,
                    25,
                    3,
                    0,
                    111,
                    0
                ],
                "title": "DONOD: Efficient and Generalizable Instruction Fine-Tuning for LLMs via\n  Model-Intrinsic Dataset Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DONOD: Efficient and Generalizable Instruction Fine-Tuning for LLMs via\n  Model-Intrinsic Dataset Pruning"
                },
                "summary": "Ad-hoc instruction fine-tuning of large language models (LLMs) is widely\nadopted for domain-specific adaptation. While domain-specific supervised\nfine-tuning (SFT) is effective and efficient, it often weakens cross-domain\ngeneralization and struggles with noisy training data. To address these\nchallenges, we propose DONOD, a lightweight model-intrinsic data pruning\nmethod. Our approach evaluates data using two model-parameter-based metrics:\nDelta of Norm (DON), which captures the cumulative influence on model weights,\nand Norm of Delta (NOD), which quantifies weight instability. Moreover, by\nemploying the Technique for Order of Preference by Similarity to Ideal Solution\n(TOPSIS) algorithm, we effectively filter noisy, unlearnable, and\ngeneralization-harming samples without relying on auxiliary models during the\nSFT process. Experiments on mathematical tasks demonstrate that data selected\nby DONOD achieves superior fine-tuning efficiency and improved robustness\nagainst noisy data. By filtering out 70% of the whole dataset, we improve\ntarget-domain accuracy by 14.90% and cross-domain accuracy by 5.67%. Meanwhile,\nour selected data present superior cross-architecture generalization. Data\npruned by smaller models (e.g., Llama 3.1-8B) generalize effectively on larger\nmodels (e.g., Llama 2-13B). Compared to existing related methodologies, DONOD\ndemonstrates comparable or superior performance while remaining\ndataset-agnostic, enabling broader applicability. Code will be made publicly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ad-hoc instruction fine-tuning of large language models (LLMs) is widely\nadopted for domain-specific adaptation. While domain-specific supervised\nfine-tuning (SFT) is effective and efficient, it often weakens cross-domain\ngeneralization and struggles with noisy training data. To address these\nchallenges, we propose DONOD, a lightweight model-intrinsic data pruning\nmethod. Our approach evaluates data using two model-parameter-based metrics:\nDelta of Norm (DON), which captures the cumulative influence on model weights,\nand Norm of Delta (NOD), which quantifies weight instability. Moreover, by\nemploying the Technique for Order of Preference by Similarity to Ideal Solution\n(TOPSIS) algorithm, we effectively filter noisy, unlearnable, and\ngeneralization-harming samples without relying on auxiliary models during the\nSFT process. Experiments on mathematical tasks demonstrate that data selected\nby DONOD achieves superior fine-tuning efficiency and improved robustness\nagainst noisy data. By filtering out 70% of the whole dataset, we improve\ntarget-domain accuracy by 14.90% and cross-domain accuracy by 5.67%. Meanwhile,\nour selected data present superior cross-architecture generalization. Data\npruned by smaller models (e.g., Llama 3.1-8B) generalize effectively on larger\nmodels (e.g., Llama 2-13B). Compared to existing related methodologies, DONOD\ndemonstrates comparable or superior performance while remaining\ndataset-agnostic, enabling broader applicability. Code will be made publicly\navailable."
                },
                "authors": [
                    {
                        "name": "Jucheng Hu"
                    },
                    {
                        "name": "Surong Yang"
                    },
                    {
                        "name": "Lijun Wu"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Dongzhan Zhou"
                },
                "author": "Dongzhan Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14810v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14810v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06303v1",
                "updated": "2025-08-08T13:27:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    27,
                    35,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T13:27:35Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    27,
                    35,
                    4,
                    220,
                    0
                ],
                "title": "A Tensor Train Approach for Deterministic Arithmetic Operations on\n  Discrete Representations of Probability Distributions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Tensor Train Approach for Deterministic Arithmetic Operations on\n  Discrete Representations of Probability Distributions"
                },
                "summary": "Computing with discrete representations of high-dimensional probability\ndistributions is fundamental to uncertainty quantification, Bayesian inference,\nand stochastic modeling. However, storing and manipulating such distributions\nsuffers from the curse of dimensionality, as memory and computational costs\ngrow exponentially with dimension. Monte Carlo methods require thousands to\nbillions of samples, incurring high computational costs and producing\ninconsistent results due to stochasticity. We present an efficient tensor train\nmethod for performing exact arithmetic operations on discretizations of\ncontinuous probability distributions while avoiding exponential growth. Our\napproach leverages low-rank tensor train decomposition to represent latent\nrandom variables compactly using Dirac deltas, enabling deterministic addition,\nsubtraction and multiplication operations directly in the compressed format. We\ndevelop an efficient implementation using sparse matrices and specialized data\nstructures that further enhances performance. Theoretical analysis demonstrates\npolynomial scaling of memory and computational complexity under rank\nassumptions, and shows how statistics of latent variables can be computed with\npolynomial complexity. Numerical experiments spanning randomized linear algebra\nto stochastic differential equations demonstrate orders-of-magnitude\nimprovements in memory usage and computational time compared to conventional\napproaches, enabling tractable deterministic computations on discretized random\nvariables in previously intractable dimensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computing with discrete representations of high-dimensional probability\ndistributions is fundamental to uncertainty quantification, Bayesian inference,\nand stochastic modeling. However, storing and manipulating such distributions\nsuffers from the curse of dimensionality, as memory and computational costs\ngrow exponentially with dimension. Monte Carlo methods require thousands to\nbillions of samples, incurring high computational costs and producing\ninconsistent results due to stochasticity. We present an efficient tensor train\nmethod for performing exact arithmetic operations on discretizations of\ncontinuous probability distributions while avoiding exponential growth. Our\napproach leverages low-rank tensor train decomposition to represent latent\nrandom variables compactly using Dirac deltas, enabling deterministic addition,\nsubtraction and multiplication operations directly in the compressed format. We\ndevelop an efficient implementation using sparse matrices and specialized data\nstructures that further enhances performance. Theoretical analysis demonstrates\npolynomial scaling of memory and computational complexity under rank\nassumptions, and shows how statistics of latent variables can be computed with\npolynomial complexity. Numerical experiments spanning randomized linear algebra\nto stochastic differential equations demonstrate orders-of-magnitude\nimprovements in memory usage and computational time compared to conventional\napproaches, enabling tractable deterministic computations on discretized random\nvariables in previously intractable dimensions."
                },
                "authors": [
                    {
                        "name": "Gerhard Kirsten"
                    },
                    {
                        "name": "Bilgesu Bilgin"
                    },
                    {
                        "name": "Janith Petangoda"
                    },
                    {
                        "name": "Phillip Stanley-Marbell"
                    }
                ],
                "author_detail": {
                    "name": "Phillip Stanley-Marbell"
                },
                "author": "Phillip Stanley-Marbell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06300v1",
                "updated": "2025-08-08T13:20:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    20,
                    37,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T13:20:37Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    20,
                    37,
                    4,
                    220,
                    0
                ],
                "title": "Automatic Semantic Alignment of Flow Pattern Representations for\n  Exploration with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Semantic Alignment of Flow Pattern Representations for\n  Exploration with Large Language Models"
                },
                "summary": "Explorative flow visualization allows domain experts to analyze complex flow\nstructures by interactively investigating flow patterns. However, traditional\nvisual interfaces often rely on specialized graphical representations and\ninteractions, which require additional effort to learn and use. Natural\nlanguage interaction offers a more intuitive alternative, but teaching machines\nto recognize diverse scientific concepts and extract corresponding structures\nfrom flow data poses a significant challenge. In this paper, we introduce an\nautomated framework that aligns flow pattern representations with the semantic\nspace of large language models (LLMs), eliminating the need for manual\nlabeling. Our approach encodes streamline segments using a denoising\nautoencoder and maps the generated flow pattern representations to LLM\nembeddings via a projector layer. This alignment empowers semantic matching\nbetween textual embeddings and flow representations through an attention\nmechanism, enabling the extraction of corresponding flow patterns based on\ntextual descriptions. To enhance accessibility, we develop an interactive\ninterface that allows users to query and visualize flow structures using\nnatural language. Through case studies, we demonstrate the effectiveness of our\nframework in enabling intuitive and intelligent flow exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explorative flow visualization allows domain experts to analyze complex flow\nstructures by interactively investigating flow patterns. However, traditional\nvisual interfaces often rely on specialized graphical representations and\ninteractions, which require additional effort to learn and use. Natural\nlanguage interaction offers a more intuitive alternative, but teaching machines\nto recognize diverse scientific concepts and extract corresponding structures\nfrom flow data poses a significant challenge. In this paper, we introduce an\nautomated framework that aligns flow pattern representations with the semantic\nspace of large language models (LLMs), eliminating the need for manual\nlabeling. Our approach encodes streamline segments using a denoising\nautoencoder and maps the generated flow pattern representations to LLM\nembeddings via a projector layer. This alignment empowers semantic matching\nbetween textual embeddings and flow representations through an attention\nmechanism, enabling the extraction of corresponding flow patterns based on\ntextual descriptions. To enhance accessibility, we develop an interactive\ninterface that allows users to query and visualize flow structures using\nnatural language. Through case studies, we demonstrate the effectiveness of our\nframework in enabling intuitive and intelligent flow exploration."
                },
                "authors": [
                    {
                        "name": "Weihan Zhang"
                    },
                    {
                        "name": "Jun Tao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Tao"
                },
                "author": "Jun Tao",
                "arxiv_comment": "Accepted by IEEE VIS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06297v1",
                "updated": "2025-08-08T13:19:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    19,
                    30,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T13:19:30Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    19,
                    30,
                    4,
                    220,
                    0
                ],
                "title": "KV Cache Compression for Inference Efficiency in LLMs: A Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Compression for Inference Efficiency in LLMs: A Review"
                },
                "summary": "Withtherapid advancement of large language models (LLMs), the context length\nfor inference has been continuously increasing, leading to an exponential\ngrowth in the demand for Key-Value (KV) caching. This has resulted in a\nsignificant memory bottleneck, limiting the inference efficiency and\nscalability of the models. Therefore, optimizing the KV cache during inference\nis crucial for enhancing performance and efficiency. This review systematically\nexamines current KV cache optimization techniques, including compression\nstrategies such as selective token strategies, quantization, and attention\ncompression. We evaluate the effectiveness, trade-offs, and application\nscenarios of these methods, providing a comprehensive analysis of their impact\non memory usage and inference speed. We focus on identifying the limitations\nand challenges of existing methods, such as compatibility issues with different\nmodels and tasks. Additionally, this review highlights future research\ndirections, including hybrid optimization techniques, adaptive dynamic\nstrategies, and software-hardware co-design. These approaches aim to improve\ninference efficiency and promote the practical application of large language\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Withtherapid advancement of large language models (LLMs), the context length\nfor inference has been continuously increasing, leading to an exponential\ngrowth in the demand for Key-Value (KV) caching. This has resulted in a\nsignificant memory bottleneck, limiting the inference efficiency and\nscalability of the models. Therefore, optimizing the KV cache during inference\nis crucial for enhancing performance and efficiency. This review systematically\nexamines current KV cache optimization techniques, including compression\nstrategies such as selective token strategies, quantization, and attention\ncompression. We evaluate the effectiveness, trade-offs, and application\nscenarios of these methods, providing a comprehensive analysis of their impact\non memory usage and inference speed. We focus on identifying the limitations\nand challenges of existing methods, such as compatibility issues with different\nmodels and tasks. Additionally, this review highlights future research\ndirections, including hybrid optimization techniques, adaptive dynamic\nstrategies, and software-hardware co-design. These approaches aim to improve\ninference efficiency and promote the practical application of large language\nmodels."
                },
                "authors": [
                    {
                        "name": "Yanyu Liu"
                    },
                    {
                        "name": "Jingying Fu"
                    },
                    {
                        "name": "Sixiang Liu"
                    },
                    {
                        "name": "Yitian Zou"
                    },
                    {
                        "name": "You Fu"
                    },
                    {
                        "name": "Jiehan Zhou"
                    },
                    {
                        "name": "Shouhua Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shouhua Zhang"
                },
                "arxiv_affiliation": "University of Oulu",
                "author": "Shouhua Zhang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00525v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00525v4",
                "updated": "2025-08-08T13:19:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    19,
                    28,
                    4,
                    220,
                    0
                ],
                "published": "2024-10-01T09:08:07Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    9,
                    8,
                    7,
                    1,
                    275,
                    0
                ],
                "title": "Improving sampling by modifying the effective diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving sampling by modifying the effective diffusion"
                },
                "summary": "Markov chain Monte Carlo samplers based on discretizations of (overdamped)\nLangevin dynamics are commonly used in the Bayesian inference and computational\nstatistical physics literature to estimate high-dimensional integrals. One can\nintroduce a non-constant diffusion matrix to precondition these dynamics, and\nrecent works have optimized it in order to improve the rate of convergence to\nstationarity by overcoming entropic and energy barriers. However, the\nintroduced methodologies to compute these optimal diffusions are generally not\nsuited to high-dimensional settings, as they rely on costly optimization\nprocedures. In this work, we propose to optimize over a class of diffusion\nmatrices, based on one-dimensional collective variables (CVs), to help the\ndynamics explore the latent space defined by the CV. The form of the diffusion\nmatrix is chosen in order to obtain an efficient effective diffusion in the\nlatent space. We describe how this class of diffusion matrices can be\nconstructed and learned during the simulation. We provide implementations of\nthe Metropolis--Adjusted Langevin Algorithm and Riemann Manifold (Generalized)\nHamiltonian Monte Carlo algorithms, and discuss numerical optimizations in the\ncase when the CV depends only on a few degrees of freedom of the system. We\nillustrate the efficiency gains by computing mean transition durations between\ntwo metastable states of a dimer in a solvent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Markov chain Monte Carlo samplers based on discretizations of (overdamped)\nLangevin dynamics are commonly used in the Bayesian inference and computational\nstatistical physics literature to estimate high-dimensional integrals. One can\nintroduce a non-constant diffusion matrix to precondition these dynamics, and\nrecent works have optimized it in order to improve the rate of convergence to\nstationarity by overcoming entropic and energy barriers. However, the\nintroduced methodologies to compute these optimal diffusions are generally not\nsuited to high-dimensional settings, as they rely on costly optimization\nprocedures. In this work, we propose to optimize over a class of diffusion\nmatrices, based on one-dimensional collective variables (CVs), to help the\ndynamics explore the latent space defined by the CV. The form of the diffusion\nmatrix is chosen in order to obtain an efficient effective diffusion in the\nlatent space. We describe how this class of diffusion matrices can be\nconstructed and learned during the simulation. We provide implementations of\nthe Metropolis--Adjusted Langevin Algorithm and Riemann Manifold (Generalized)\nHamiltonian Monte Carlo algorithms, and discuss numerical optimizations in the\ncase when the CV depends only on a few degrees of freedom of the system. We\nillustrate the efficiency gains by computing mean transition durations between\ntwo metastable states of a dimer in a solvent."
                },
                "authors": [
                    {
                        "name": "Tony Lelièvre"
                    },
                    {
                        "name": "Régis Santet"
                    },
                    {
                        "name": "Gabriel Stoltz"
                    }
                ],
                "author_detail": {
                    "name": "Gabriel Stoltz"
                },
                "author": "Gabriel Stoltz",
                "arxiv_comment": "36 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00525v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00525v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06296v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06296v1",
                "updated": "2025-08-08T13:15:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    15,
                    40,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T13:15:40Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    15,
                    40,
                    4,
                    220,
                    0
                ],
                "title": "LLM Robustness Leaderboard v1 --Technical report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Robustness Leaderboard v1 --Technical report"
                },
                "summary": "This technical report accompanies the LLM robustness leaderboard published by\nPRISM Eval for the Paris AI Action Summit. We introduce PRISM Eval Behavior\nElicitation Tool (BET), an AI system performing automated red-teaming through\nDynamic Adversarial Optimization that achieves 100% Attack Success Rate (ASR)\nagainst 37 of 41 state-of-the-art LLMs. Beyond binary success metrics, we\npropose a fine-grained robustness metric estimating the average number of\nattempts required to elicit harmful behaviors, revealing that attack difficulty\nvaries by over 300-fold across models despite universal vulnerability. We\nintroduce primitive-level vulnerability analysis to identify which jailbreaking\ntechniques are most effective for specific hazard categories. Our collaborative\nevaluation with trusted third parties from the AI Safety Network demonstrates\npractical pathways for distributed robustness assessment across the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report accompanies the LLM robustness leaderboard published by\nPRISM Eval for the Paris AI Action Summit. We introduce PRISM Eval Behavior\nElicitation Tool (BET), an AI system performing automated red-teaming through\nDynamic Adversarial Optimization that achieves 100% Attack Success Rate (ASR)\nagainst 37 of 41 state-of-the-art LLMs. Beyond binary success metrics, we\npropose a fine-grained robustness metric estimating the average number of\nattempts required to elicit harmful behaviors, revealing that attack difficulty\nvaries by over 300-fold across models despite universal vulnerability. We\nintroduce primitive-level vulnerability analysis to identify which jailbreaking\ntechniques are most effective for specific hazard categories. Our collaborative\nevaluation with trusted third parties from the AI Safety Network demonstrates\npractical pathways for distributed robustness assessment across the community."
                },
                "authors": [
                    {
                        "name": "Pierre Peigné - Lefebvre"
                    },
                    {
                        "name": "Quentin Feuillade-Montixi"
                    },
                    {
                        "name": "Tom David"
                    },
                    {
                        "name": "Nicolas Miailhe"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Miailhe"
                },
                "author": "Nicolas Miailhe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06296v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06296v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06284v1",
                "updated": "2025-08-08T13:05:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    5,
                    31,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T13:05:31Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    5,
                    31,
                    4,
                    220,
                    0
                ],
                "title": "Leveraging LLMs for Scalable Non-intrusive Speech Quality Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMs for Scalable Non-intrusive Speech Quality Assessment"
                },
                "summary": "Non-intrusive speech quality assessment (SQA) systems suffer from limited\ntraining data and costly human annotations, hindering their generalization to\nreal-time conferencing calls. In this work, we propose leveraging large\nlanguage models (LLMs) as pseudo-raters for speech quality to address these\ndata bottlenecks. We construct LibriAugmented, a dataset consisting of 101,129\nspeech clips with simulated degradations labeled by a fine-tuned auditory LLM\n(Vicuna-7b-v1.5). We compare three training strategies: using human-labeled\ndata, using LLM-labeled data, and a two-stage approach (pretraining on LLM\nlabels, then fine-tuning on human labels), using both DNSMOS Pro and DeePMOS.\nWe test on several datasets across languages and quality degradations. While\nLLM-labeled training yields mixed results compared to human-labeled training,\nwe provide empirical evidence that the two-stage approach improves the\ngeneralization performance (e.g., DNSMOS Pro achieves 0.63 vs. 0.55 PCC on\nNISQA_TEST_LIVETALK and 0.73 vs. 0.65 PCC on Tencent with reverb). Our findings\ndemonstrate the potential of using LLMs as scalable pseudo-raters for speech\nquality assessment, offering a cost-effective solution to the data limitation\nproblem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-intrusive speech quality assessment (SQA) systems suffer from limited\ntraining data and costly human annotations, hindering their generalization to\nreal-time conferencing calls. In this work, we propose leveraging large\nlanguage models (LLMs) as pseudo-raters for speech quality to address these\ndata bottlenecks. We construct LibriAugmented, a dataset consisting of 101,129\nspeech clips with simulated degradations labeled by a fine-tuned auditory LLM\n(Vicuna-7b-v1.5). We compare three training strategies: using human-labeled\ndata, using LLM-labeled data, and a two-stage approach (pretraining on LLM\nlabels, then fine-tuning on human labels), using both DNSMOS Pro and DeePMOS.\nWe test on several datasets across languages and quality degradations. While\nLLM-labeled training yields mixed results compared to human-labeled training,\nwe provide empirical evidence that the two-stage approach improves the\ngeneralization performance (e.g., DNSMOS Pro achieves 0.63 vs. 0.55 PCC on\nNISQA_TEST_LIVETALK and 0.73 vs. 0.65 PCC on Tencent with reverb). Our findings\ndemonstrate the potential of using LLMs as scalable pseudo-raters for speech\nquality assessment, offering a cost-effective solution to the data limitation\nproblem."
                },
                "authors": [
                    {
                        "name": "Fredrik Cumlin"
                    },
                    {
                        "name": "Xinyu Liang"
                    },
                    {
                        "name": "Anubhab Ghosh"
                    },
                    {
                        "name": "Saikat Chatterjee"
                    }
                ],
                "author_detail": {
                    "name": "Saikat Chatterjee"
                },
                "author": "Saikat Chatterjee",
                "arxiv_comment": "ECAI workshop paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00816v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00816v4",
                "updated": "2025-08-08T13:02:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    2,
                    39,
                    4,
                    220,
                    0
                ],
                "published": "2025-04-01T14:05:32Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    14,
                    5,
                    32,
                    1,
                    91,
                    0
                ],
                "title": "Two-stage deep learning framework for the restoration of incomplete-ring\n  PET images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-stage deep learning framework for the restoration of incomplete-ring\n  PET images"
                },
                "summary": "Positron Emission Tomography (PET) is an important molecular imaging tool\nwidely used in medicine. Traditional PET systems rely on complete detector\nrings for full angular coverage and reliable data collection. However,\nincomplete-ring PET scanners have emerged due to hardware failures, cost\nconstraints, or specific clinical needs. Standard reconstruction algorithms\noften suffer from performance degradation with these systems because of reduced\ndata completeness and geometric inconsistencies. We present a two-stage\ndeep-learning framework that, without incorporating any time-of-flight (TOF)\ninformation, restores high-quality images from data with about 50% missing\ncoincidences - double the loss levels previously addressed by CNN-based\nmethods. The pipeline operates in two stages: a projection-domain Attention\nU-Net first predicts the missing sections of the sinogram by leveraging spatial\ncontext from neighbouring slices, after which the completed data are\nreconstructed with OSEM algorithm and passed to a U-Net-diffusion module that\nremoves residual artefacts while reinstating high-frequency detail. Using 206\nbrain volumes from a public dataset, the result shows that our model\nsuccessfully preserves most anatomical structures and tracer distribution\nfeatures with PSNR of 30.92 dB and SSIM of 0.9708. We also achieve higher\ninference speed, thus providing an effective solution for incomplete-ring PET\nimaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Positron Emission Tomography (PET) is an important molecular imaging tool\nwidely used in medicine. Traditional PET systems rely on complete detector\nrings for full angular coverage and reliable data collection. However,\nincomplete-ring PET scanners have emerged due to hardware failures, cost\nconstraints, or specific clinical needs. Standard reconstruction algorithms\noften suffer from performance degradation with these systems because of reduced\ndata completeness and geometric inconsistencies. We present a two-stage\ndeep-learning framework that, without incorporating any time-of-flight (TOF)\ninformation, restores high-quality images from data with about 50% missing\ncoincidences - double the loss levels previously addressed by CNN-based\nmethods. The pipeline operates in two stages: a projection-domain Attention\nU-Net first predicts the missing sections of the sinogram by leveraging spatial\ncontext from neighbouring slices, after which the completed data are\nreconstructed with OSEM algorithm and passed to a U-Net-diffusion module that\nremoves residual artefacts while reinstating high-frequency detail. Using 206\nbrain volumes from a public dataset, the result shows that our model\nsuccessfully preserves most anatomical structures and tracer distribution\nfeatures with PSNR of 30.92 dB and SSIM of 0.9708. We also achieve higher\ninference speed, thus providing an effective solution for incomplete-ring PET\nimaging."
                },
                "authors": [
                    {
                        "name": "Yeqi Fang"
                    },
                    {
                        "name": "Rong Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Rong Zhou"
                },
                "author": "Rong Zhou",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00816v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00816v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06281v1",
                "updated": "2025-08-08T13:02:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    2,
                    29,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T13:02:29Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    2,
                    29,
                    4,
                    220,
                    0
                ],
                "title": "Deep Learning Based Reconstruction Methods for Electrical Impedance\n  Tomography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning Based Reconstruction Methods for Electrical Impedance\n  Tomography"
                },
                "summary": "Electrical Impedance Tomography (EIT) is a powerful imaging modality widely\nused in medical diagnostics, industrial monitoring, and environmental studies.\nThe EIT inverse problem is about inferring the internal conductivity\ndistribution of the concerned object from the voltage measurements taken on its\nboundary. This problem is severely ill-posed, and requires advanced\ncomputational approaches for accurate and reliable image reconstruction. Recent\ninnovations in both model-based reconstruction and deep learning have driven\nsignificant progress in the field. In this review, we explore learned\nreconstruction methods that employ deep neural networks for solving the EIT\ninverse problem. The discussion focuses on the complete electrode model, one\npopular mathematical model for real-world applications of EIT. We compare a\nwide variety of learned approaches, including fully-learned, post-processing\nand learned iterative methods, with several conventional model-based\nreconstruction techniques, e.g., sparsity regularization, regularized\nGauss-Newton iteration and level set method. The evaluation is based on three\ndatasets: a simulated dataset of ellipses, an out-of-distribution simulated\ndataset, and the KIT4 dataset, including real-world measurements. Our results\ndemonstrate that learned methods outperform model-based methods for\nin-distribution data but face challenges in generalization, where hybrid\nmethods exhibit a good balance of accuracy and adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electrical Impedance Tomography (EIT) is a powerful imaging modality widely\nused in medical diagnostics, industrial monitoring, and environmental studies.\nThe EIT inverse problem is about inferring the internal conductivity\ndistribution of the concerned object from the voltage measurements taken on its\nboundary. This problem is severely ill-posed, and requires advanced\ncomputational approaches for accurate and reliable image reconstruction. Recent\ninnovations in both model-based reconstruction and deep learning have driven\nsignificant progress in the field. In this review, we explore learned\nreconstruction methods that employ deep neural networks for solving the EIT\ninverse problem. The discussion focuses on the complete electrode model, one\npopular mathematical model for real-world applications of EIT. We compare a\nwide variety of learned approaches, including fully-learned, post-processing\nand learned iterative methods, with several conventional model-based\nreconstruction techniques, e.g., sparsity regularization, regularized\nGauss-Newton iteration and level set method. The evaluation is based on three\ndatasets: a simulated dataset of ellipses, an out-of-distribution simulated\ndataset, and the KIT4 dataset, including real-world measurements. Our results\ndemonstrate that learned methods outperform model-based methods for\nin-distribution data but face challenges in generalization, where hybrid\nmethods exhibit a good balance of accuracy and adaptability."
                },
                "authors": [
                    {
                        "name": "Alexander Denker"
                    },
                    {
                        "name": "Fabio Margotti"
                    },
                    {
                        "name": "Jianfeng Ning"
                    },
                    {
                        "name": "Kim Knudsen"
                    },
                    {
                        "name": "Derick Nganyu Tanyu"
                    },
                    {
                        "name": "Bangti Jin"
                    },
                    {
                        "name": "Andreas Hauptmann"
                    },
                    {
                        "name": "Peter Maass"
                    }
                ],
                "author_detail": {
                    "name": "Peter Maass"
                },
                "author": "Peter Maass",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.1.8; I.4.5; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06278v1",
                "updated": "2025-08-08T12:58:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    12,
                    58,
                    14,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T12:58:14Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    12,
                    58,
                    14,
                    4,
                    220,
                    0
                ],
                "title": "Mitigating Undesired Conditions in Flexible Production with\n  Product-Process-Resource Asset Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Undesired Conditions in Flexible Production with\n  Product-Process-Resource Asset Knowledge Graphs"
                },
                "summary": "Contemporary industrial cyber-physical production systems (CPPS) composed of\nrobotic workcells face significant challenges in the analysis of undesired\nconditions due to the flexibility of Industry 4.0 that disrupts traditional\nquality assurance mechanisms. This paper presents a novel industry-oriented\nsemantic model called Product-Process-Resource Asset Knowledge Graph (PPR-AKG),\nwhich is designed to analyze and mitigate undesired conditions in flexible\nCPPS. Built on top of the well-proven Product-Process-Resource (PPR) model\noriginating from ISA-95 and VDI-3682, a comprehensive OWL ontology addresses\nshortcomings of conventional model-driven engineering for CPPS, particularly\ninadequate undesired condition and error handling representation. The\nintegration of semantic technologies with large language models (LLMs) provides\nintuitive interfaces for factory operators, production planners, and engineers\nto interact with the entire model using natural language. Evaluation with the\nuse case addressing electric vehicle battery remanufacturing demonstrates that\nthe PPR-AKG approach efficiently supports resource allocation based on\nexplicitly represented capabilities as well as identification and mitigation of\nundesired conditions in production. The key contributions include (1) a\nholistic PPR-AKG model capturing multi-dimensional production knowledge, and\n(2) the useful combination of the PPR-AKG with LLM-based chatbots for human\ninteraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary industrial cyber-physical production systems (CPPS) composed of\nrobotic workcells face significant challenges in the analysis of undesired\nconditions due to the flexibility of Industry 4.0 that disrupts traditional\nquality assurance mechanisms. This paper presents a novel industry-oriented\nsemantic model called Product-Process-Resource Asset Knowledge Graph (PPR-AKG),\nwhich is designed to analyze and mitigate undesired conditions in flexible\nCPPS. Built on top of the well-proven Product-Process-Resource (PPR) model\noriginating from ISA-95 and VDI-3682, a comprehensive OWL ontology addresses\nshortcomings of conventional model-driven engineering for CPPS, particularly\ninadequate undesired condition and error handling representation. The\nintegration of semantic technologies with large language models (LLMs) provides\nintuitive interfaces for factory operators, production planners, and engineers\nto interact with the entire model using natural language. Evaluation with the\nuse case addressing electric vehicle battery remanufacturing demonstrates that\nthe PPR-AKG approach efficiently supports resource allocation based on\nexplicitly represented capabilities as well as identification and mitigation of\nundesired conditions in production. The key contributions include (1) a\nholistic PPR-AKG model capturing multi-dimensional production knowledge, and\n(2) the useful combination of the PPR-AKG with LLM-based chatbots for human\ninteraction."
                },
                "authors": [
                    {
                        "name": "Petr Novak"
                    },
                    {
                        "name": "Stefan Biffl"
                    },
                    {
                        "name": "Marek Obitko"
                    },
                    {
                        "name": "Petr Kadera"
                    }
                ],
                "author_detail": {
                    "name": "Petr Kadera"
                },
                "author": "Petr Kadera",
                "arxiv_comment": "3 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03256v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03256v3",
                "updated": "2025-08-08T12:58:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    12,
                    58,
                    8,
                    4,
                    220,
                    0
                ],
                "published": "2025-07-04T02:25:10Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    2,
                    25,
                    10,
                    4,
                    185,
                    0
                ],
                "title": "MoDA: Multi-modal Diffusion Architecture for Talking Head Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoDA: Multi-modal Diffusion Architecture for Talking Head Generation"
                },
                "summary": "Talking head generation with arbitrary identities and speech audio remains a\ncrucial problem in the realm of the virtual metaverse. Recently, diffusion\nmodels have become a popular generative technique in this field with their\nstrong generation capabilities. However, several challenges remain for\ndiffusion-based methods: 1) inefficient inference and visual artifacts caused\nby the implicit latent space of Variational Auto-Encoders (VAE), which\ncomplicates the diffusion process; 2) a lack of authentic facial expressions\nand head movements due to inadequate multi-modal information fusion. In this\npaper, MoDA handles these challenges by: 1) defining a joint parameter space\nthat bridges motion generation and neural rendering, and leveraging flow\nmatching to simplify diffusion learning; 2) introducing a multi-modal diffusion\narchitecture to model the interaction among noisy motion, audio, and auxiliary\nconditions, enhancing overall facial expressiveness. In addition, a\ncoarse-to-fine fusion strategy is employed to progressively integrate different\nmodalities, ensuring effective feature fusion. Experimental results demonstrate\nthat MoDA improves video diversity, realism, and efficiency, making it suitable\nfor real-world applications. Project Page:\nhttps://lixinyyang.github.io/MoDA.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Talking head generation with arbitrary identities and speech audio remains a\ncrucial problem in the realm of the virtual metaverse. Recently, diffusion\nmodels have become a popular generative technique in this field with their\nstrong generation capabilities. However, several challenges remain for\ndiffusion-based methods: 1) inefficient inference and visual artifacts caused\nby the implicit latent space of Variational Auto-Encoders (VAE), which\ncomplicates the diffusion process; 2) a lack of authentic facial expressions\nand head movements due to inadequate multi-modal information fusion. In this\npaper, MoDA handles these challenges by: 1) defining a joint parameter space\nthat bridges motion generation and neural rendering, and leveraging flow\nmatching to simplify diffusion learning; 2) introducing a multi-modal diffusion\narchitecture to model the interaction among noisy motion, audio, and auxiliary\nconditions, enhancing overall facial expressiveness. In addition, a\ncoarse-to-fine fusion strategy is employed to progressively integrate different\nmodalities, ensuring effective feature fusion. Experimental results demonstrate\nthat MoDA improves video diversity, realism, and efficiency, making it suitable\nfor real-world applications. Project Page:\nhttps://lixinyyang.github.io/MoDA.github.io/"
                },
                "authors": [
                    {
                        "name": "Xinyang Li"
                    },
                    {
                        "name": "Gen Li"
                    },
                    {
                        "name": "Zhihui Lin"
                    },
                    {
                        "name": "Yichen Qian"
                    },
                    {
                        "name": "GongXin Yao"
                    },
                    {
                        "name": "Weinan Jia"
                    },
                    {
                        "name": "Aowen Wang"
                    },
                    {
                        "name": "Weihua Chen"
                    },
                    {
                        "name": "Fan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Fan Wang"
                },
                "author": "Fan Wang",
                "arxiv_comment": "12 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03256v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03256v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02679v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02679v2",
                "updated": "2025-08-08T12:56:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    12,
                    56,
                    11,
                    4,
                    220,
                    0
                ],
                "published": "2025-07-17T03:30:11Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    3,
                    30,
                    11,
                    3,
                    198,
                    0
                ],
                "title": "LLM Agent-Based Simulation of Student Activities and Mental Health Using\n  Smartphone Sensing Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Agent-Based Simulation of Student Activities and Mental Health Using\n  Smartphone Sensing Data"
                },
                "summary": "Students' mental well-being is vital for academic success, with activities\nsuch as studying, socializing, and sleeping playing a role. Current mobile\nsensing data highlight this intricate link using statistical and machine\nlearning analyses. We propose a novel LLM agent-based simulation framework to\nmodel student activities and mental health using the StudentLife Dataset. Each\nLLM agent was initialized with personality questionnaires and guided by\nsmartphone sensing data throughout the simulated semester. These agents predict\nindividual behaviors, provide self-reported mental health data via ecological\nmomentary assessments (EMAs), and complete follow-up personality\nquestionnaires. To ensure accuracy, we investigated various prompting\ntechniques, memory systems, and activity-based mental state management\nstrategies that dynamically update an agent's mental state based on their daily\nactivities. This simulation goes beyond simply replicating existing data. This\nallows us to explore new scenarios that are not present in the original\ndataset, such as peer influence through agent-to-agent interactions and the\nimpact of social media. Furthermore, we can conduct intervention studies by\nmanipulating activity patterns via sensing signals and personality traits using\nquestionnaire responses. This provides valuable insights into the behavioral\nchanges that could enhance student well-being. The framework also facilitates\nhypothetical interviews with LLM agents, offering deeper insights into their\nmental health. This study showcases the power of LLM-driven behavioral modeling\nwith sensing data, opening new avenues for understanding and supporting student\nmental health.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Students' mental well-being is vital for academic success, with activities\nsuch as studying, socializing, and sleeping playing a role. Current mobile\nsensing data highlight this intricate link using statistical and machine\nlearning analyses. We propose a novel LLM agent-based simulation framework to\nmodel student activities and mental health using the StudentLife Dataset. Each\nLLM agent was initialized with personality questionnaires and guided by\nsmartphone sensing data throughout the simulated semester. These agents predict\nindividual behaviors, provide self-reported mental health data via ecological\nmomentary assessments (EMAs), and complete follow-up personality\nquestionnaires. To ensure accuracy, we investigated various prompting\ntechniques, memory systems, and activity-based mental state management\nstrategies that dynamically update an agent's mental state based on their daily\nactivities. This simulation goes beyond simply replicating existing data. This\nallows us to explore new scenarios that are not present in the original\ndataset, such as peer influence through agent-to-agent interactions and the\nimpact of social media. Furthermore, we can conduct intervention studies by\nmanipulating activity patterns via sensing signals and personality traits using\nquestionnaire responses. This provides valuable insights into the behavioral\nchanges that could enhance student well-being. The framework also facilitates\nhypothetical interviews with LLM agents, offering deeper insights into their\nmental health. This study showcases the power of LLM-driven behavioral modeling\nwith sensing data, opening new avenues for understanding and supporting student\nmental health."
                },
                "authors": [
                    {
                        "name": "Wayupuk Sommuang"
                    },
                    {
                        "name": "Kun Kerdthaisong"
                    },
                    {
                        "name": "Pasin Buakhaw"
                    },
                    {
                        "name": "Aslan B. Wong"
                    },
                    {
                        "name": "Nutchanon Yongsatianchot"
                    }
                ],
                "author_detail": {
                    "name": "Nutchanon Yongsatianchot"
                },
                "author": "Nutchanon Yongsatianchot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02679v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02679v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06277v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06277v1",
                "updated": "2025-08-08T12:54:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    12,
                    54,
                    9,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T12:54:09Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    12,
                    54,
                    9,
                    4,
                    220,
                    0
                ],
                "title": "Large Language Model Data Generation for Enhanced Intent Recognition in\n  German Speech",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Data Generation for Enhanced Intent Recognition in\n  German Speech"
                },
                "summary": "Intent recognition (IR) for speech commands is essential for artificial\nintelligence (AI) assistant systems; however, most existing approaches are\nlimited to short commands and are predominantly developed for English. This\npaper addresses these limitations by focusing on IR from speech by elderly\nGerman speakers. We propose a novel approach that combines an adapted Whisper\nASR model, fine-tuned on elderly German speech (SVC-de), with Transformer-based\nlanguage models trained on synthetic text datasets generated by three\nwell-known large language models (LLMs): LeoLM, Llama3, and ChatGPT. To\nevaluate the robustness of our approach, we generate synthetic speech with a\ntext-to-speech model and conduct extensive cross-dataset testing. Our results\nshow that synthetic LLM-generated data significantly boosts classification\nperformance and robustness to different speaking styles and unseen vocabulary.\nNotably, we find that LeoLM, a smaller, domain-specific 13B LLM, surpasses the\nmuch larger ChatGPT (175B) in dataset quality for German intent recognition.\nOur approach demonstrates that generative AI can effectively bridge data gaps\nin low-resource domains. We provide detailed documentation of our data\ngeneration and training process to ensure transparency and reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intent recognition (IR) for speech commands is essential for artificial\nintelligence (AI) assistant systems; however, most existing approaches are\nlimited to short commands and are predominantly developed for English. This\npaper addresses these limitations by focusing on IR from speech by elderly\nGerman speakers. We propose a novel approach that combines an adapted Whisper\nASR model, fine-tuned on elderly German speech (SVC-de), with Transformer-based\nlanguage models trained on synthetic text datasets generated by three\nwell-known large language models (LLMs): LeoLM, Llama3, and ChatGPT. To\nevaluate the robustness of our approach, we generate synthetic speech with a\ntext-to-speech model and conduct extensive cross-dataset testing. Our results\nshow that synthetic LLM-generated data significantly boosts classification\nperformance and robustness to different speaking styles and unseen vocabulary.\nNotably, we find that LeoLM, a smaller, domain-specific 13B LLM, surpasses the\nmuch larger ChatGPT (175B) in dataset quality for German intent recognition.\nOur approach demonstrates that generative AI can effectively bridge data gaps\nin low-resource domains. We provide detailed documentation of our data\ngeneration and training process to ensure transparency and reproducibility."
                },
                "authors": [
                    {
                        "name": "Theresa Pekarek Rosin"
                    },
                    {
                        "name": "Burak Can Kaplan"
                    },
                    {
                        "name": "Stefan Wermter"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Wermter"
                },
                "author": "Stefan Wermter",
                "arxiv_comment": "11 pages, 3 figures, accepted at KONVENS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06277v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06277v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16802v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16802v3",
                "updated": "2025-08-08T12:38:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    12,
                    38,
                    49,
                    4,
                    220,
                    0
                ],
                "published": "2025-02-24T03:25:56Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    3,
                    25,
                    56,
                    0,
                    55,
                    0
                ],
                "title": "Topic Over Source: The Key to Effective Data Mixing for Language Models\n  Pre-training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topic Over Source: The Key to Effective Data Mixing for Language Models\n  Pre-training"
                },
                "summary": "The performance of large language models (LLMs) is significantly affected by\nthe quality and composition of their pre-training data, which is inherently\ndiverse, spanning various languages, sources, and topics. Effectively\nintegrating these heterogeneous data groups is crucial for optimizing LLM\nperformance. Previous research has predominantly concentrated on source-based\ndata mixing, often neglecting the nuanced topic-level characteristics of the\ndata. To address this gap, we propose a topic-based data mixing strategy that\nutilizes detailed topic labels generated through a multi-stage process\ncombining unsupervised clustering, LLM-based summarization, and supervised\nclassifier training. With this strategy, we conduct the first comprehensive\ncomparison of topic-based versus source-based partitioning across multiple\nmixing strategies. We demonstrate that language models pretrained on data mixed\nby topics consistently outperform those trained on data mixed by sources across\nmultiple methods including RegMix, DoReMi,temperature-based sampling, and a\nmanual mixing method based on downstream task performance. Our theoretical\nanalysis reveals that topic-based data achieves significantly lower validation\nloss compared to source-based approaches, creating a better optimization\nlandscape for model training. We will make our code, annotated datasets, and\ntopic classification models publicly available to facilitate further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of large language models (LLMs) is significantly affected by\nthe quality and composition of their pre-training data, which is inherently\ndiverse, spanning various languages, sources, and topics. Effectively\nintegrating these heterogeneous data groups is crucial for optimizing LLM\nperformance. Previous research has predominantly concentrated on source-based\ndata mixing, often neglecting the nuanced topic-level characteristics of the\ndata. To address this gap, we propose a topic-based data mixing strategy that\nutilizes detailed topic labels generated through a multi-stage process\ncombining unsupervised clustering, LLM-based summarization, and supervised\nclassifier training. With this strategy, we conduct the first comprehensive\ncomparison of topic-based versus source-based partitioning across multiple\nmixing strategies. We demonstrate that language models pretrained on data mixed\nby topics consistently outperform those trained on data mixed by sources across\nmultiple methods including RegMix, DoReMi,temperature-based sampling, and a\nmanual mixing method based on downstream task performance. Our theoretical\nanalysis reveals that topic-based data achieves significantly lower validation\nloss compared to source-based approaches, creating a better optimization\nlandscape for model training. We will make our code, annotated datasets, and\ntopic classification models publicly available to facilitate further research."
                },
                "authors": [
                    {
                        "name": "Jiahui Peng"
                    },
                    {
                        "name": "Xinlin Zhuang"
                    },
                    {
                        "name": "Jiantao Qiu"
                    },
                    {
                        "name": "Ren Ma"
                    },
                    {
                        "name": "Jing Yu"
                    },
                    {
                        "name": "He Zhu"
                    },
                    {
                        "name": "Conghui He"
                    }
                ],
                "author_detail": {
                    "name": "Conghui He"
                },
                "author": "Conghui He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16802v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16802v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06262v1",
                "updated": "2025-08-08T12:28:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    12,
                    28,
                    34,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T12:28:34Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    12,
                    28,
                    34,
                    4,
                    220,
                    0
                ],
                "title": "Llasa+: Free Lunch for Accelerated and Streaming Llama-Based Speech\n  Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Llasa+: Free Lunch for Accelerated and Streaming Llama-Based Speech\n  Synthesis"
                },
                "summary": "Recent progress in text-to-speech (TTS) has achieved impressive naturalness\nand flexibility, especially with the development of large language model\n(LLM)-based approaches. However, existing autoregressive (AR) structures and\nlarge-scale models, such as Llasa, still face significant challenges in\ninference latency and streaming synthesis. To deal with the limitations, we\nintroduce Llasa+, an accelerated and streaming TTS model built on Llasa.\nSpecifically, to accelerate the generation process, we introduce two\nplug-and-play Multi-Token Prediction (MTP) modules following the frozen\nbackbone. These modules allow the model to predict multiple tokens in one AR\nstep. Additionally, to mitigate potential error propagation caused by\ninaccurate MTP, we design a novel verification algorithm that leverages the\nfrozen backbone to validate the generated tokens, thus allowing Llasa+ to\nachieve speedup without sacrificing generation quality. Furthermore, we design\na causal decoder that enables streaming speech reconstruction from tokens.\nExtensive experiments show that Llasa+ achieves a 1.48X speedup without\nsacrificing generation quality, despite being trained only on LibriTTS.\nMoreover, the MTP-and-verification framework can be applied to accelerate any\nLLM-based model. All codes and models are publicly available at\nhttps://github.com/ASLP-lab/LLaSA_Plus.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in text-to-speech (TTS) has achieved impressive naturalness\nand flexibility, especially with the development of large language model\n(LLM)-based approaches. However, existing autoregressive (AR) structures and\nlarge-scale models, such as Llasa, still face significant challenges in\ninference latency and streaming synthesis. To deal with the limitations, we\nintroduce Llasa+, an accelerated and streaming TTS model built on Llasa.\nSpecifically, to accelerate the generation process, we introduce two\nplug-and-play Multi-Token Prediction (MTP) modules following the frozen\nbackbone. These modules allow the model to predict multiple tokens in one AR\nstep. Additionally, to mitigate potential error propagation caused by\ninaccurate MTP, we design a novel verification algorithm that leverages the\nfrozen backbone to validate the generated tokens, thus allowing Llasa+ to\nachieve speedup without sacrificing generation quality. Furthermore, we design\na causal decoder that enables streaming speech reconstruction from tokens.\nExtensive experiments show that Llasa+ achieves a 1.48X speedup without\nsacrificing generation quality, despite being trained only on LibriTTS.\nMoreover, the MTP-and-verification framework can be applied to accelerate any\nLLM-based model. All codes and models are publicly available at\nhttps://github.com/ASLP-lab/LLaSA_Plus."
                },
                "authors": [
                    {
                        "name": "Wenjie Tian"
                    },
                    {
                        "name": "Xinfa Zhu"
                    },
                    {
                        "name": "Hanke Xie"
                    },
                    {
                        "name": "Zhen Ye"
                    },
                    {
                        "name": "Wei Xue"
                    },
                    {
                        "name": "Lei Xie"
                    }
                ],
                "author_detail": {
                    "name": "Lei Xie"
                },
                "author": "Lei Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06259v1",
                "updated": "2025-08-08T12:26:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    12,
                    26,
                    20,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T12:26:20Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    12,
                    26,
                    20,
                    4,
                    220,
                    0
                ],
                "title": "SIFThinker: Spatially-Aware Image Focus for Visual Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIFThinker: Spatially-Aware Image Focus for Visual Reasoning"
                },
                "summary": "Current multimodal large language models (MLLMs) still face significant\nchallenges in complex visual tasks (e.g., spatial understanding, fine-grained\nperception). Prior methods have tried to incorporate visual reasoning, however,\nthey fail to leverage attention correction with spatial cues to iteratively\nrefine their focus on prompt-relevant regions. In this paper, we introduce\nSIFThinker, a spatially-aware \"think-with-images\" framework that mimics human\nvisual perception. Specifically, SIFThinker enables attention correcting and\nimage region focusing by interleaving depth-enhanced bounding boxes and natural\nlanguage. Our contributions are twofold: First, we introduce a\nreverse-expansion-forward-inference strategy that facilitates the generation of\ninterleaved image-text chains of thought for process-level supervision, which\nin turn leads to the construction of the SIF-50K dataset. Besides, we propose\nGRPO-SIF, a reinforced training paradigm that integrates depth-informed visual\ngrounding into a unified reasoning pipeline, teaching the model to dynamically\ncorrect and focus on prompt-relevant regions. Extensive experiments demonstrate\nthat SIFThinker outperforms state-of-the-art methods in spatial understanding\nand fine-grained visual perception, while maintaining strong general\ncapabilities, highlighting the effectiveness of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current multimodal large language models (MLLMs) still face significant\nchallenges in complex visual tasks (e.g., spatial understanding, fine-grained\nperception). Prior methods have tried to incorporate visual reasoning, however,\nthey fail to leverage attention correction with spatial cues to iteratively\nrefine their focus on prompt-relevant regions. In this paper, we introduce\nSIFThinker, a spatially-aware \"think-with-images\" framework that mimics human\nvisual perception. Specifically, SIFThinker enables attention correcting and\nimage region focusing by interleaving depth-enhanced bounding boxes and natural\nlanguage. Our contributions are twofold: First, we introduce a\nreverse-expansion-forward-inference strategy that facilitates the generation of\ninterleaved image-text chains of thought for process-level supervision, which\nin turn leads to the construction of the SIF-50K dataset. Besides, we propose\nGRPO-SIF, a reinforced training paradigm that integrates depth-informed visual\ngrounding into a unified reasoning pipeline, teaching the model to dynamically\ncorrect and focus on prompt-relevant regions. Extensive experiments demonstrate\nthat SIFThinker outperforms state-of-the-art methods in spatial understanding\nand fine-grained visual perception, while maintaining strong general\ncapabilities, highlighting the effectiveness of our method."
                },
                "authors": [
                    {
                        "name": "Zhangquan Chen"
                    },
                    {
                        "name": "Ruihui Zhao"
                    },
                    {
                        "name": "Chuwei Luo"
                    },
                    {
                        "name": "Mingze Sun"
                    },
                    {
                        "name": "Xinlei Yu"
                    },
                    {
                        "name": "Yangyang Kang"
                    },
                    {
                        "name": "Ruqi Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ruqi Huang"
                },
                "author": "Ruqi Huang",
                "arxiv_comment": "15 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06257v1",
                "updated": "2025-08-08T12:22:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    12,
                    22,
                    36,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T12:22:36Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    12,
                    22,
                    36,
                    4,
                    220,
                    0
                ],
                "title": "Multi-Omics Analysis for Cancer Subtype Inference via Unrolling Graph\n  Smoothness Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Omics Analysis for Cancer Subtype Inference via Unrolling Graph\n  Smoothness Priors"
                },
                "summary": "Integrating multi-omics datasets through data-driven analysis offers a\ncomprehensive understanding of the complex biological processes underlying\nvarious diseases, particularly cancer. Graph Neural Networks (GNNs) have\nrecently demonstrated remarkable ability to exploit relational structures in\nbiological data, enabling advances in multi-omics integration for cancer\nsubtype classification. Existing approaches often neglect the intricate\ncoupling between heterogeneous omics, limiting their capacity to resolve subtle\ncancer subtype heterogeneity critical for precision oncology. To address these\nlimitations, we propose a framework named Graph Transformer for Multi-omics\nCancer Subtype Classification (GTMancer). This framework builds upon the GNN\noptimization problem and extends its application to complex multi-omics data.\nSpecifically, our method leverages contrastive learning to embed multi-omics\ndata into a unified semantic space. We unroll the multiplex graph optimization\nproblem in that unified space and introduce dual sets of attention coefficients\nto capture structural graph priors both within and among multi-omics data. This\napproach enables global omics information to guide the refining of the\nrepresentations of individual omics. Empirical experiments on seven real-world\ncancer datasets demonstrate that GTMancer outperforms existing state-of-the-art\nalgorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating multi-omics datasets through data-driven analysis offers a\ncomprehensive understanding of the complex biological processes underlying\nvarious diseases, particularly cancer. Graph Neural Networks (GNNs) have\nrecently demonstrated remarkable ability to exploit relational structures in\nbiological data, enabling advances in multi-omics integration for cancer\nsubtype classification. Existing approaches often neglect the intricate\ncoupling between heterogeneous omics, limiting their capacity to resolve subtle\ncancer subtype heterogeneity critical for precision oncology. To address these\nlimitations, we propose a framework named Graph Transformer for Multi-omics\nCancer Subtype Classification (GTMancer). This framework builds upon the GNN\noptimization problem and extends its application to complex multi-omics data.\nSpecifically, our method leverages contrastive learning to embed multi-omics\ndata into a unified semantic space. We unroll the multiplex graph optimization\nproblem in that unified space and introduce dual sets of attention coefficients\nto capture structural graph priors both within and among multi-omics data. This\napproach enables global omics information to guide the refining of the\nrepresentations of individual omics. Empirical experiments on seven real-world\ncancer datasets demonstrate that GTMancer outperforms existing state-of-the-art\nalgorithms."
                },
                "authors": [
                    {
                        "name": "Jielong Lu"
                    },
                    {
                        "name": "Zhihao Wu"
                    },
                    {
                        "name": "Jiajun Yu"
                    },
                    {
                        "name": "Jiajun Bu"
                    },
                    {
                        "name": "Haishuai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haishuai Wang"
                },
                "author": "Haishuai Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06249v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06249v1",
                "updated": "2025-08-08T12:10:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    12,
                    10,
                    28,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T12:10:28Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    12,
                    10,
                    28,
                    4,
                    220,
                    0
                ],
                "title": "In-Training Defenses against Emergent Misalignment in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Training Defenses against Emergent Misalignment in Language Models"
                },
                "summary": "Fine-tuning lets practitioners repurpose aligned large language models (LLMs)\nfor new domains, yet recent work reveals emergent misalignment (EMA): Even a\nsmall, domain-specific fine-tune can induce harmful behaviors far outside the\ntarget domain. Even in the case where model weights are hidden behind a\nfine-tuning API, this gives attackers inadvertent access to a broadly\nmisaligned model in a way that can be hard to detect from the fine-tuning data\nalone. We present the first systematic study of in-training safeguards against\nEMA that are practical for providers who expose fine-tuning via an API. We\ninvestigate four training regularization interventions: (i) KL-divergence\nregularization toward a safe reference model, (ii) $\\ell_2$ distance in feature\nspace, (iii) projecting onto a safe subspace (SafeLoRA), and (iv) interleaving\nof a small amount of safe training examples from a general instruct-tuning\ndataset. We first evaluate the methods' emergent misalignment effect across\nfour malicious, EMA-inducing tasks. Second, we assess the methods' impacts on\nbenign tasks. We conclude with a discussion of open questions in emergent\nmisalignment research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning lets practitioners repurpose aligned large language models (LLMs)\nfor new domains, yet recent work reveals emergent misalignment (EMA): Even a\nsmall, domain-specific fine-tune can induce harmful behaviors far outside the\ntarget domain. Even in the case where model weights are hidden behind a\nfine-tuning API, this gives attackers inadvertent access to a broadly\nmisaligned model in a way that can be hard to detect from the fine-tuning data\nalone. We present the first systematic study of in-training safeguards against\nEMA that are practical for providers who expose fine-tuning via an API. We\ninvestigate four training regularization interventions: (i) KL-divergence\nregularization toward a safe reference model, (ii) $\\ell_2$ distance in feature\nspace, (iii) projecting onto a safe subspace (SafeLoRA), and (iv) interleaving\nof a small amount of safe training examples from a general instruct-tuning\ndataset. We first evaluate the methods' emergent misalignment effect across\nfour malicious, EMA-inducing tasks. Second, we assess the methods' impacts on\nbenign tasks. We conclude with a discussion of open questions in emergent\nmisalignment research."
                },
                "authors": [
                    {
                        "name": "David Kaczér"
                    },
                    {
                        "name": "Magnus Jørgenvåg"
                    },
                    {
                        "name": "Clemens Vetter"
                    },
                    {
                        "name": "Lucie Flek"
                    },
                    {
                        "name": "Florian Mai"
                    }
                ],
                "author_detail": {
                    "name": "Florian Mai"
                },
                "author": "Florian Mai",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06249v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14110v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14110v2",
                "updated": "2025-08-08T11:56:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    11,
                    56,
                    30,
                    4,
                    220,
                    0
                ],
                "published": "2024-11-21T13:18:03Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    13,
                    18,
                    3,
                    3,
                    326,
                    0
                ],
                "title": "Feedback-Guided Extraction of Knowledge Base from Retrieval-Augmented\n  LLM Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feedback-Guided Extraction of Knowledge Base from Retrieval-Augmented\n  LLM Applications"
                },
                "summary": "Retrieval-Augmented Generation (RAG) expands the knowledge boundary of large\nlanguage models (LLMs) by integrating external knowledge bases, whose\nconstruction is often time-consuming and laborious. If an adversary extracts\nthe knowledge base verbatim, it not only severely infringes the owner's\nintellectual property but also enables the adversary to replicate the\napplication's functionality for unfair competition. Previous works on knowledge\nbase extraction are limited either by low extraction coverage (usually less\nthan 4%) in query-based attacks or by impractical assumptions of white-box\naccess in embedding-based optimization methods. In this work, we propose\nCopyBreakRAG, an agent-based black-box attack that reasons from feedback and\nadaptively generates new adversarial queries for progressive extraction. By\nbalancing exploration and exploitation through curiosity-driven queries and\nfeedback-guided query refinement, our method overcomes the limitations of prior\napproaches and achieves significantly higher extraction coverage in realistic\nblack-box settings. Experimental results show that CopyBreakRAG outperforms the\nstate-of-the-art black-box approach by 45% on average in terms of chunk\nextraction ratio from applications built with mainstream RAG frameworks, and\nextracts over 70% of the data from the knowledge base in applications on\ncommercial platforms including OpenAI's GPTs and ByteDance's Coze when\nessential protection is in place.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) expands the knowledge boundary of large\nlanguage models (LLMs) by integrating external knowledge bases, whose\nconstruction is often time-consuming and laborious. If an adversary extracts\nthe knowledge base verbatim, it not only severely infringes the owner's\nintellectual property but also enables the adversary to replicate the\napplication's functionality for unfair competition. Previous works on knowledge\nbase extraction are limited either by low extraction coverage (usually less\nthan 4%) in query-based attacks or by impractical assumptions of white-box\naccess in embedding-based optimization methods. In this work, we propose\nCopyBreakRAG, an agent-based black-box attack that reasons from feedback and\nadaptively generates new adversarial queries for progressive extraction. By\nbalancing exploration and exploitation through curiosity-driven queries and\nfeedback-guided query refinement, our method overcomes the limitations of prior\napproaches and achieves significantly higher extraction coverage in realistic\nblack-box settings. Experimental results show that CopyBreakRAG outperforms the\nstate-of-the-art black-box approach by 45% on average in terms of chunk\nextraction ratio from applications built with mainstream RAG frameworks, and\nextracts over 70% of the data from the knowledge base in applications on\ncommercial platforms including OpenAI's GPTs and ByteDance's Coze when\nessential protection is in place."
                },
                "authors": [
                    {
                        "name": "Changyue Jiang"
                    },
                    {
                        "name": "Xudong Pan"
                    },
                    {
                        "name": "Geng Hong"
                    },
                    {
                        "name": "Chenfu Bao"
                    },
                    {
                        "name": "Yang Chen"
                    },
                    {
                        "name": "Min Yang"
                    }
                ],
                "author_detail": {
                    "name": "Min Yang"
                },
                "author": "Min Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14110v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14110v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06244v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06244v1",
                "updated": "2025-08-08T11:56:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    11,
                    56,
                    13,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T11:56:13Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    11,
                    56,
                    13,
                    4,
                    220,
                    0
                ],
                "title": "Membership Inference Attack with Partial Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Membership Inference Attack with Partial Features"
                },
                "summary": "Machine learning models have been shown to be susceptible to membership\ninference attack, which can be used to determine whether a given sample appears\nin the training data. Existing membership inference methods commonly assume\nthat the adversary has full access to the features of the target sample. This\nassumption, however, does not hold in many real-world scenarios where only\npartial features information is available, thereby limiting the applicability\nof these methods. In this work, we study an inference scenario where the\nadversary observes only partial features of each sample and aims to infer\nwhether this observed subset was present in the training set of the target\nmodel. We define this problem as Partial Feature Membership Inference (PFMI).\nTo address this problem, we propose MRAD (Memory-guided Reconstruction and\nAnomaly Detection), a two-stage attack framework. In the first stage, MRAD\noptimizes the unknown feature values to minimize the loss of the sample. In the\nsecond stage, it measures the deviation between the reconstructed sample and\nthe training distribution using anomaly detection. Empirical results\ndemonstrate that MRAD is effective across a range of datasets, and maintains\ncompatibility with various off-the-shelf anomaly detection techniques. For\nexample, on STL-10, our attack achieves an AUC of around 0.6 even with 40% of\nthe missing features.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning models have been shown to be susceptible to membership\ninference attack, which can be used to determine whether a given sample appears\nin the training data. Existing membership inference methods commonly assume\nthat the adversary has full access to the features of the target sample. This\nassumption, however, does not hold in many real-world scenarios where only\npartial features information is available, thereby limiting the applicability\nof these methods. In this work, we study an inference scenario where the\nadversary observes only partial features of each sample and aims to infer\nwhether this observed subset was present in the training set of the target\nmodel. We define this problem as Partial Feature Membership Inference (PFMI).\nTo address this problem, we propose MRAD (Memory-guided Reconstruction and\nAnomaly Detection), a two-stage attack framework. In the first stage, MRAD\noptimizes the unknown feature values to minimize the loss of the sample. In the\nsecond stage, it measures the deviation between the reconstructed sample and\nthe training distribution using anomaly detection. Empirical results\ndemonstrate that MRAD is effective across a range of datasets, and maintains\ncompatibility with various off-the-shelf anomaly detection techniques. For\nexample, on STL-10, our attack achieves an AUC of around 0.6 even with 40% of\nthe missing features."
                },
                "authors": [
                    {
                        "name": "Xurun Wang"
                    },
                    {
                        "name": "Guangrui Liu"
                    },
                    {
                        "name": "Xinjie Li"
                    },
                    {
                        "name": "Haoyu He"
                    },
                    {
                        "name": "Lin Yao"
                    },
                    {
                        "name": "Weizhe Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weizhe Zhang"
                },
                "author": "Weizhe Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06244v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06244v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19811v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19811v2",
                "updated": "2025-08-08T11:53:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    11,
                    53,
                    19,
                    4,
                    220,
                    0
                ],
                "published": "2025-04-28T14:08:45Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    14,
                    8,
                    45,
                    0,
                    118,
                    0
                ],
                "title": "Can a Crow Hatch a Falcon? Lineage Matters in Predicting Large Language\n  Model Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can a Crow Hatch a Falcon? Lineage Matters in Predicting Large Language\n  Model Performance"
                },
                "summary": "Accurately forecasting the performance of Large Language Models (LLMs) before\nextensive fine-tuning or merging can substantially reduce both computational\nexpense and development time. Although prior approaches like scaling laws\naccount for global factors such as parameter size or training tokens, they\noften overlook explicit lineage relationships-i.e., which models are derived or\nmerged from which parents. In this work, we propose a novel Lineage-Regularized\nMatrix Factorization (LRMF) framework that encodes ancestral ties among LLMs\nvia a graph Laplacian regularizer. By leveraging multi-hop parent-child\nconnections, LRMF consistently outperforms conventional matrix factorization\nand collaborative filtering methods in both instance-level and benchmark-level\nperformance prediction. Our large-scale study includes 2,934 publicly available\nHugging Face models and 21,000+ instances across 6 major benchmarks, showing\nthat the introduction of lineage constraints yields up to 0.15-0.30 higher\nPearson correlation coefficients with actual performance compared to baseline\nmethods. Moreover, LRMF effectively addresses the cold-start problem, providing\naccurate estimates for newly derived or merged models even with minimal data.\nThis lineage-guided strategy thus offers a resource-efficient way to inform\nhyperparameter tuning, data selection, and model combination in modern LLM\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately forecasting the performance of Large Language Models (LLMs) before\nextensive fine-tuning or merging can substantially reduce both computational\nexpense and development time. Although prior approaches like scaling laws\naccount for global factors such as parameter size or training tokens, they\noften overlook explicit lineage relationships-i.e., which models are derived or\nmerged from which parents. In this work, we propose a novel Lineage-Regularized\nMatrix Factorization (LRMF) framework that encodes ancestral ties among LLMs\nvia a graph Laplacian regularizer. By leveraging multi-hop parent-child\nconnections, LRMF consistently outperforms conventional matrix factorization\nand collaborative filtering methods in both instance-level and benchmark-level\nperformance prediction. Our large-scale study includes 2,934 publicly available\nHugging Face models and 21,000+ instances across 6 major benchmarks, showing\nthat the introduction of lineage constraints yields up to 0.15-0.30 higher\nPearson correlation coefficients with actual performance compared to baseline\nmethods. Moreover, LRMF effectively addresses the cold-start problem, providing\naccurate estimates for newly derived or merged models even with minimal data.\nThis lineage-guided strategy thus offers a resource-efficient way to inform\nhyperparameter tuning, data selection, and model combination in modern LLM\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Takuya Tamura"
                    },
                    {
                        "name": "Taro Yano"
                    },
                    {
                        "name": "Masafumi Enomoto"
                    },
                    {
                        "name": "Masafumi Oyamada"
                    }
                ],
                "author_detail": {
                    "name": "Masafumi Oyamada"
                },
                "author": "Masafumi Oyamada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19811v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19811v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10774v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10774v2",
                "updated": "2025-08-08T11:39:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    11,
                    39,
                    48,
                    4,
                    220,
                    0
                ],
                "published": "2025-07-14T20:00:40Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    20,
                    0,
                    40,
                    0,
                    195,
                    0
                ],
                "title": "Propensity score weighting across counterfactual worlds: longitudinal\n  effects under positivity violations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Propensity score weighting across counterfactual worlds: longitudinal\n  effects under positivity violations"
                },
                "summary": "When examining a contrast between two interventions, longitudinal causal\ninference studies frequently encounter positivity violations when one or both\nregimes are impossible to observe for some subjects. Existing weighting methods\neither assume positivity holds or produce effects that conflate interventions'\nimpacts on ultimate outcomes with their effects on intermediate treatments and\ncovariates. We propose a novel class of estimands -- cumulative cross-world\nweighted effects -- that weights potential outcome differences using propensity\nscores adapting to positivity violations cumulatively across timepoints and\nsimultaneously across both counterfactual treatment histories. This new\nestimand isolates mechanistic differences between treatment regimes, is\nidentifiable without positivity assumptions, and circumvents the limitations of\nexisting longitudinal methods. Further, our analysis reveals two fundamental\ninsights about longitudinal causal inference under positivity violations.\nFirst, while mechanistically meaningful, these effects correspond to\nnon-implementable interventions, exposing a core\ninterpretability-implementability tradeoff. Second, the identified effects\nfaithfully capture mechanistic differences only under a partial common support\nassumption; violations cause the identified functional to collapse to zero,\neven when the causal effect is non-zero. We develop doubly robust-style\nestimators that achieve asymptotic normality and parametric convergence under\nnonparametric assumptions on the nuisance estimators. To this end, we\nreformulate challenging density ratio estimation as regression function\nestimation, which is achievable with standard machine learning methods. We\nillustrate our methods through analysis of union membership's effect on\nearnings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When examining a contrast between two interventions, longitudinal causal\ninference studies frequently encounter positivity violations when one or both\nregimes are impossible to observe for some subjects. Existing weighting methods\neither assume positivity holds or produce effects that conflate interventions'\nimpacts on ultimate outcomes with their effects on intermediate treatments and\ncovariates. We propose a novel class of estimands -- cumulative cross-world\nweighted effects -- that weights potential outcome differences using propensity\nscores adapting to positivity violations cumulatively across timepoints and\nsimultaneously across both counterfactual treatment histories. This new\nestimand isolates mechanistic differences between treatment regimes, is\nidentifiable without positivity assumptions, and circumvents the limitations of\nexisting longitudinal methods. Further, our analysis reveals two fundamental\ninsights about longitudinal causal inference under positivity violations.\nFirst, while mechanistically meaningful, these effects correspond to\nnon-implementable interventions, exposing a core\ninterpretability-implementability tradeoff. Second, the identified effects\nfaithfully capture mechanistic differences only under a partial common support\nassumption; violations cause the identified functional to collapse to zero,\neven when the causal effect is non-zero. We develop doubly robust-style\nestimators that achieve asymptotic normality and parametric convergence under\nnonparametric assumptions on the nuisance estimators. To this end, we\nreformulate challenging density ratio estimation as regression function\nestimation, which is achievable with standard machine learning methods. We\nillustrate our methods through analysis of union membership's effect on\nearnings."
                },
                "authors": [
                    {
                        "name": "Alec McClean"
                    },
                    {
                        "name": "Iván Díaz"
                    }
                ],
                "author_detail": {
                    "name": "Iván Díaz"
                },
                "author": "Iván Díaz",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2506.09188",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10774v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10774v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.14172v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.14172v4",
                "updated": "2025-08-08T11:22:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    11,
                    22,
                    42,
                    4,
                    220,
                    0
                ],
                "published": "2023-08-27T18:28:58Z",
                "published_parsed": [
                    2023,
                    8,
                    27,
                    18,
                    28,
                    58,
                    6,
                    239,
                    0
                ],
                "title": "A Markov Random Field model for Hypergraph-based Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Markov Random Field model for Hypergraph-based Machine Learning"
                },
                "summary": "Understanding the data-generating process is essential for building machine\nlearning models that generalise well while ensuring robustness and\ninterpretability. This paper addresses the fundamental challenge of modelling\nthe data generation processes on hypergraphs and explores how such models can\ninform the design of machine learning algorithms for hypergraph data. The key\nto our approach is the development of a hypergraph Markov random field that\nmodels the joint distribution of the node features and hyperedge features in a\nhypergraph through a multivariate Gaussian distribution whose covariance matrix\nis uniquely determined by the hypergraph structure. The proposed\ndata-generating process provides a valuable inductive bias for various\nhypergraph machine learning tasks, thus enhancing the algorithm design. In this\npaper, we focus on two representative downstream tasks: structure inference and\nnode classification. Accordingly, we introduce two novel frameworks: 1) an\noriginal hypergraph structure inference framework named HGSI, and 2) a novel\nlearning framework entitled Hypergraph-MLP for node classification on\nhypergraphs. Empirical evaluation of the proposed frameworks demonstrates that:\n1) HGSI outperforms existing hypergraph structure inference methods on both\nsynthetic and real-world data; and 2) Hypergraph-MLP outperforms baselines in\nsix hypergraph node classification benchmarks, at the same time promoting\nruntime efficiency and robustness against structural perturbations during\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the data-generating process is essential for building machine\nlearning models that generalise well while ensuring robustness and\ninterpretability. This paper addresses the fundamental challenge of modelling\nthe data generation processes on hypergraphs and explores how such models can\ninform the design of machine learning algorithms for hypergraph data. The key\nto our approach is the development of a hypergraph Markov random field that\nmodels the joint distribution of the node features and hyperedge features in a\nhypergraph through a multivariate Gaussian distribution whose covariance matrix\nis uniquely determined by the hypergraph structure. The proposed\ndata-generating process provides a valuable inductive bias for various\nhypergraph machine learning tasks, thus enhancing the algorithm design. In this\npaper, we focus on two representative downstream tasks: structure inference and\nnode classification. Accordingly, we introduce two novel frameworks: 1) an\noriginal hypergraph structure inference framework named HGSI, and 2) a novel\nlearning framework entitled Hypergraph-MLP for node classification on\nhypergraphs. Empirical evaluation of the proposed frameworks demonstrates that:\n1) HGSI outperforms existing hypergraph structure inference methods on both\nsynthetic and real-world data; and 2) Hypergraph-MLP outperforms baselines in\nsix hypergraph node classification benchmarks, at the same time promoting\nruntime efficiency and robustness against structural perturbations during\ninference."
                },
                "authors": [
                    {
                        "name": "Bohan Tang"
                    },
                    {
                        "name": "Keyue Jiang"
                    },
                    {
                        "name": "Laura Toni"
                    },
                    {
                        "name": "Siheng Chen"
                    },
                    {
                        "name": "Xiaowen Dong"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Dong"
                },
                "author": "Xiaowen Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.14172v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.14172v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11749v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11749v3",
                "updated": "2025-08-08T11:13:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    11,
                    13,
                    24,
                    4,
                    220,
                    0
                ],
                "published": "2024-12-16T13:06:44Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    6,
                    44,
                    0,
                    351,
                    0
                ],
                "title": "Inferring additional physics through unmodelled signal reconstructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring additional physics through unmodelled signal reconstructions"
                },
                "summary": "Parameter estimation of gravitational wave data is often computationally\nexpensive, requiring simplifying assumptions such as circularisation of binary\norbits. Although, if included, the sub-dominant effects like orbital\neccentricity may provide crucial insights into the formation channels of\ncompact binary mergers. To address these challenges, we present a pipeline\nstrategy leveraging minimally modelled waveform reconstruction to identify the\npresence of eccentricity in real time. Using injected signals, we demonstrate\nthat ignoring eccentricity ($e_{\\rm 20Hz} \\gtrsim 0.1$) leads to significant\nbiases in parameter recovery, including chirp mass estimates falling outside\nthe 90% credible interval. Waveform reconstruction shows inconsistencies\nincrease with eccentricity, and this behaviour is consistent for different mass\nratios. Our method enables low-latency inferences of binary properties\nsupporting targeted follow-up analyses and can be applied to identify any\nphysical effect of measurable strength.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter estimation of gravitational wave data is often computationally\nexpensive, requiring simplifying assumptions such as circularisation of binary\norbits. Although, if included, the sub-dominant effects like orbital\neccentricity may provide crucial insights into the formation channels of\ncompact binary mergers. To address these challenges, we present a pipeline\nstrategy leveraging minimally modelled waveform reconstruction to identify the\npresence of eccentricity in real time. Using injected signals, we demonstrate\nthat ignoring eccentricity ($e_{\\rm 20Hz} \\gtrsim 0.1$) leads to significant\nbiases in parameter recovery, including chirp mass estimates falling outside\nthe 90% credible interval. Waveform reconstruction shows inconsistencies\nincrease with eccentricity, and this behaviour is consistent for different mass\nratios. Our method enables low-latency inferences of binary properties\nsupporting targeted follow-up analyses and can be applied to identify any\nphysical effect of measurable strength."
                },
                "authors": [
                    {
                        "name": "Rimo Das"
                    },
                    {
                        "name": "V. Gayathri"
                    },
                    {
                        "name": "Divyajyoti"
                    },
                    {
                        "name": "Sijil Jose"
                    },
                    {
                        "name": "Imre Bartos"
                    },
                    {
                        "name": "Sergey Klimenko"
                    },
                    {
                        "name": "Chandra Kant Mishra"
                    }
                ],
                "author_detail": {
                    "name": "Chandra Kant Mishra"
                },
                "author": "Chandra Kant Mishra",
                "arxiv_comment": "Resubmission of the pulished version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11749v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11749v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06225v1",
                "updated": "2025-08-08T11:11:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    11,
                    11,
                    22,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T11:11:22Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    11,
                    11,
                    22,
                    4,
                    220,
                    0
                ],
                "title": "Overconfidence in LLM-as-a-Judge: Diagnosis and Confidence-Driven\n  Solution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Overconfidence in LLM-as-a-Judge: Diagnosis and Confidence-Driven\n  Solution"
                },
                "summary": "Large Language Models (LLMs) are widely used as automated judges, where\npractical value depends on both accuracy and trustworthy, risk-aware judgments.\nExisting approaches predominantly focus on accuracy, overlooking the necessity\nof well-calibrated confidence, which is vital for adaptive and reliable\nevaluation pipelines. In this work, we advocate a shift from accuracy-centric\nevaluation to confidence-driven, risk-aware LLM-as-a-Judge systems, emphasizing\nthe necessity of well-calibrated confidence for trustworthy and adaptive\nevaluation. We systematically identify the **Overconfidence Phenomenon** in\ncurrent LLM-as-a-Judges, where predicted confidence significantly overstates\nactual correctness, undermining reliability in practical deployment. To\nquantify this phenomenon, we introduce **TH-Score**, a novel metric measuring\nconfidence-accuracy alignment. Furthermore, we propose **LLM-as-a-Fuser**, an\nensemble framework that transforms LLMs into reliable, risk-aware evaluators.\nExtensive experiments demonstrate that our approach substantially improves\ncalibration and enables adaptive, confidence-driven evaluation pipelines,\nachieving superior reliability and accuracy compared to existing baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used as automated judges, where\npractical value depends on both accuracy and trustworthy, risk-aware judgments.\nExisting approaches predominantly focus on accuracy, overlooking the necessity\nof well-calibrated confidence, which is vital for adaptive and reliable\nevaluation pipelines. In this work, we advocate a shift from accuracy-centric\nevaluation to confidence-driven, risk-aware LLM-as-a-Judge systems, emphasizing\nthe necessity of well-calibrated confidence for trustworthy and adaptive\nevaluation. We systematically identify the **Overconfidence Phenomenon** in\ncurrent LLM-as-a-Judges, where predicted confidence significantly overstates\nactual correctness, undermining reliability in practical deployment. To\nquantify this phenomenon, we introduce **TH-Score**, a novel metric measuring\nconfidence-accuracy alignment. Furthermore, we propose **LLM-as-a-Fuser**, an\nensemble framework that transforms LLMs into reliable, risk-aware evaluators.\nExtensive experiments demonstrate that our approach substantially improves\ncalibration and enables adaptive, confidence-driven evaluation pipelines,\nachieving superior reliability and accuracy compared to existing baselines."
                },
                "authors": [
                    {
                        "name": "Zailong Tian"
                    },
                    {
                        "name": "Zhuoheng Han"
                    },
                    {
                        "name": "Yanzhe Chen"
                    },
                    {
                        "name": "Haozhe Xu"
                    },
                    {
                        "name": "Xi Yang"
                    },
                    {
                        "name": "richeng xuan"
                    },
                    {
                        "name": "Hongfeng Wang"
                    },
                    {
                        "name": "Lizi Liao"
                    }
                ],
                "author_detail": {
                    "name": "Lizi Liao"
                },
                "author": "Lizi Liao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15125v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15125v2",
                "updated": "2025-08-08T11:06:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    11,
                    6,
                    54,
                    4,
                    220,
                    0
                ],
                "published": "2025-04-21T14:20:49Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    14,
                    20,
                    49,
                    0,
                    111,
                    0
                ],
                "title": "Contemplative Artificial Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemplative Artificial Intelligence"
                },
                "summary": "As artificial intelligence (AI) improves, traditional alignment strategies\nmay falter in the face of unpredictable self-improvement, hidden subgoals, and\nthe sheer complexity of intelligent systems. Inspired by contemplative wisdom\ntraditions, we show how four axiomatic principles can instil a resilient Wise\nWorld Model in AI systems. First, mindfulness enables self-monitoring and\nrecalibration of emergent subgoals. Second, emptiness forestalls dogmatic goal\nfixation and relaxes rigid priors. Third, non-duality dissolves adversarial\nself-other boundaries. Fourth, boundless care motivates the universal reduction\nof suffering. We find that prompting AI to reflect on these principles improves\nperformance on the AILuminate Benchmark (d=.96) and boosts cooperation and\njoint-reward on the Prisoner's Dilemma task (d=7+). We offer detailed\nimplementation strategies at the level of architectures, constitutions, and\nreinforcement on chain-of-thought. For future systems, active inference may\noffer the self-organizing and dynamic coupling capabilities needed to enact\nContemplative AI in embodied agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As artificial intelligence (AI) improves, traditional alignment strategies\nmay falter in the face of unpredictable self-improvement, hidden subgoals, and\nthe sheer complexity of intelligent systems. Inspired by contemplative wisdom\ntraditions, we show how four axiomatic principles can instil a resilient Wise\nWorld Model in AI systems. First, mindfulness enables self-monitoring and\nrecalibration of emergent subgoals. Second, emptiness forestalls dogmatic goal\nfixation and relaxes rigid priors. Third, non-duality dissolves adversarial\nself-other boundaries. Fourth, boundless care motivates the universal reduction\nof suffering. We find that prompting AI to reflect on these principles improves\nperformance on the AILuminate Benchmark (d=.96) and boosts cooperation and\njoint-reward on the Prisoner's Dilemma task (d=7+). We offer detailed\nimplementation strategies at the level of architectures, constitutions, and\nreinforcement on chain-of-thought. For future systems, active inference may\noffer the self-organizing and dynamic coupling capabilities needed to enact\nContemplative AI in embodied agents."
                },
                "authors": [
                    {
                        "name": "Ruben Laukkonen"
                    },
                    {
                        "name": "Fionn Inglis"
                    },
                    {
                        "name": "Shamil Chandaria"
                    },
                    {
                        "name": "Lars Sandved-Smith"
                    },
                    {
                        "name": "Edmundo Lopez-Sola"
                    },
                    {
                        "name": "Jakob Hohwy"
                    },
                    {
                        "name": "Jonathan Gold"
                    },
                    {
                        "name": "Adam Elwood"
                    }
                ],
                "author_detail": {
                    "name": "Adam Elwood"
                },
                "author": "Adam Elwood",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15125v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15125v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06220v1",
                "updated": "2025-08-08T11:03:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    11,
                    3,
                    23,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T11:03:23Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    11,
                    3,
                    23,
                    4,
                    220,
                    0
                ],
                "title": "InfoCausalQA:Can Models Perform Non-explicit Causal Reasoning Based on\n  Infographic?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfoCausalQA:Can Models Perform Non-explicit Causal Reasoning Based on\n  Infographic?"
                },
                "summary": "Recent advances in Vision-Language Models (VLMs) have demonstrated impressive\ncapabilities in perception and reasoning. However, the ability to perform\ncausal inference -- a core aspect of human cognition -- remains underexplored,\nparticularly in multimodal settings. In this study, we introduce InfoCausalQA,\na novel benchmark designed to evaluate causal reasoning grounded in\ninfographics that combine structured visual data with textual context. The\nbenchmark comprises two tasks: Task 1 focuses on quantitative causal reasoning\nbased on inferred numerical trends, while Task 2 targets semantic causal\nreasoning involving five types of causal relations: cause, effect,\nintervention, counterfactual, and temporal. We manually collected 494\ninfographic-text pairs from four public sources and used GPT-4o to generate\n1,482 high-quality multiple-choice QA pairs. These questions were then\ncarefully revised by humans to ensure they cannot be answered based on\nsurface-level cues alone but instead require genuine visual grounding. Our\nexperimental results reveal that current VLMs exhibit limited capability in\ncomputational reasoning and even more pronounced limitations in semantic causal\nreasoning. Their significantly lower performance compared to humans indicates a\nsubstantial gap in leveraging infographic-based information for causal\ninference. Through InfoCausalQA, we highlight the need for advancing the causal\nreasoning abilities of multimodal AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Vision-Language Models (VLMs) have demonstrated impressive\ncapabilities in perception and reasoning. However, the ability to perform\ncausal inference -- a core aspect of human cognition -- remains underexplored,\nparticularly in multimodal settings. In this study, we introduce InfoCausalQA,\na novel benchmark designed to evaluate causal reasoning grounded in\ninfographics that combine structured visual data with textual context. The\nbenchmark comprises two tasks: Task 1 focuses on quantitative causal reasoning\nbased on inferred numerical trends, while Task 2 targets semantic causal\nreasoning involving five types of causal relations: cause, effect,\nintervention, counterfactual, and temporal. We manually collected 494\ninfographic-text pairs from four public sources and used GPT-4o to generate\n1,482 high-quality multiple-choice QA pairs. These questions were then\ncarefully revised by humans to ensure they cannot be answered based on\nsurface-level cues alone but instead require genuine visual grounding. Our\nexperimental results reveal that current VLMs exhibit limited capability in\ncomputational reasoning and even more pronounced limitations in semantic causal\nreasoning. Their significantly lower performance compared to humans indicates a\nsubstantial gap in leveraging infographic-based information for causal\ninference. Through InfoCausalQA, we highlight the need for advancing the causal\nreasoning abilities of multimodal AI systems."
                },
                "authors": [
                    {
                        "name": "Keummin Ka"
                    },
                    {
                        "name": "Junhyeong Park"
                    },
                    {
                        "name": "Jahyun Jeon"
                    },
                    {
                        "name": "Youngjae Yu"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Yu"
                },
                "author": "Youngjae Yu",
                "arxiv_comment": "14 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05568v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05568v2",
                "updated": "2025-08-08T10:51:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    10,
                    51,
                    19,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-07T17:00:47Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    0,
                    47,
                    3,
                    219,
                    0
                ],
                "title": "X-VFL: A New Vertical Federated Learning Framework with Cross Completion\n  and Decision Subspace Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-VFL: A New Vertical Federated Learning Framework with Cross Completion\n  and Decision Subspace Alignment"
                },
                "summary": "Vertical Federated Learning (VFL) enables collaborative learning by\nintegrating disjoint feature subsets from multiple clients/parties. However,\nVFL typically faces two key challenges: i) the requirement for perfectly\naligned data samples across all clients (missing features are not allowed); ii)\nthe requirement for joint collaborative inference/prediction involving all\nclients (it does not support locally independent inference on a single client).\nTo address these challenges, we propose X-VFL, a new VFL framework designed to\ndeal with the non-aligned data samples with (partially) missing features and to\nsupport locally independent inference of new data samples for each client. In\nparticular, we design two novel modules in X-VFL: Cross Completion (XCom) and\nDecision Subspace Alignment (DS-Align). XCom can complete/reconstruct missing\nfeatures for non-aligned data samples by leveraging information from other\nclients. DS-Align aligns local features with completed and global features\nacross all clients within the decision subspace, thus enabling locally\nindependent inference at each client. Moreover, we provide convergence theorems\nfor different algorithms used in training X-VFL, showing an $O(1/\\sqrt{T})$\nconvergence rate for SGD-type algorithms and an $O(1/T)$ rate for PAGE-type\nalgorithms, where $T$ denotes the number of training update steps. Extensive\nexperiments on real-world datasets demonstrate that X-VFL significantly\noutperforms existing methods, e.g., achieving a 15% improvement in accuracy on\nthe image CIFAR-10 dataset and a 43% improvement on the medical MIMIC-III\ndataset. These results validate the practical effectiveness and superiority of\nX-VFL, particularly in scenarios involving partially missing features and\nlocally independent inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vertical Federated Learning (VFL) enables collaborative learning by\nintegrating disjoint feature subsets from multiple clients/parties. However,\nVFL typically faces two key challenges: i) the requirement for perfectly\naligned data samples across all clients (missing features are not allowed); ii)\nthe requirement for joint collaborative inference/prediction involving all\nclients (it does not support locally independent inference on a single client).\nTo address these challenges, we propose X-VFL, a new VFL framework designed to\ndeal with the non-aligned data samples with (partially) missing features and to\nsupport locally independent inference of new data samples for each client. In\nparticular, we design two novel modules in X-VFL: Cross Completion (XCom) and\nDecision Subspace Alignment (DS-Align). XCom can complete/reconstruct missing\nfeatures for non-aligned data samples by leveraging information from other\nclients. DS-Align aligns local features with completed and global features\nacross all clients within the decision subspace, thus enabling locally\nindependent inference at each client. Moreover, we provide convergence theorems\nfor different algorithms used in training X-VFL, showing an $O(1/\\sqrt{T})$\nconvergence rate for SGD-type algorithms and an $O(1/T)$ rate for PAGE-type\nalgorithms, where $T$ denotes the number of training update steps. Extensive\nexperiments on real-world datasets demonstrate that X-VFL significantly\noutperforms existing methods, e.g., achieving a 15% improvement in accuracy on\nthe image CIFAR-10 dataset and a 43% improvement on the medical MIMIC-III\ndataset. These results validate the practical effectiveness and superiority of\nX-VFL, particularly in scenarios involving partially missing features and\nlocally independent inference."
                },
                "authors": [
                    {
                        "name": "Qinghua Yao"
                    },
                    {
                        "name": "Xiangrui Xu"
                    },
                    {
                        "name": "Zhize Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhize Li"
                },
                "author": "Zhize Li",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05568v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05568v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06047v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06047v3",
                "updated": "2025-08-08T10:42:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    10,
                    42,
                    43,
                    4,
                    220,
                    0
                ],
                "published": "2024-12-08T19:53:18Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    19,
                    53,
                    18,
                    6,
                    343,
                    0
                ],
                "title": "Small Term Reachability and Related Problems for Terminating Term\n  Rewriting Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Term Reachability and Related Problems for Terminating Term\n  Rewriting Systems"
                },
                "summary": "Motivated by an application where we try to make proofs for Description Logic\ninferences smaller by rewriting, we consider the following decision problem,\nwhich we call the small term reachability problem: given a term rewriting\nsystem $R$, a term $s$, and a natural number $n$, decide whether there is a\nterm $t$ of size $\\leq n$ reachable from $s$ using the rules of $R$. We\ninvestigate the complexity of this problem depending on how termination of $R$\ncan be established. We show that the problem is in general NP-complete for\nlength-reducing term rewriting systems. Its complexity increases to\nN2ExpTime-complete (NExpTime-complete) if termination is proved using a\n(linear) polynomial order and to PSpace-complete for systems whose termination\ncan be shown using a restricted class of Knuth-Bendix orders. Confluence\nreduces the complexity to P for the length-reducing case, but has no effect on\nthe worst-case complexity in the other two cases. Finally, we consider the\nlarge term reachability problem, a variant of the problem where we are\ninterested in reachability of a term of size $\\geq n$. It turns out that this\nseemingly innocuous modification in some cases changes the complexity of the\nproblem, which may also become dependent on whether the number $n$ is is\nrepresented in unary or binary encoding, whereas this makes no difference for\nthe complexity of the small term reachability problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by an application where we try to make proofs for Description Logic\ninferences smaller by rewriting, we consider the following decision problem,\nwhich we call the small term reachability problem: given a term rewriting\nsystem $R$, a term $s$, and a natural number $n$, decide whether there is a\nterm $t$ of size $\\leq n$ reachable from $s$ using the rules of $R$. We\ninvestigate the complexity of this problem depending on how termination of $R$\ncan be established. We show that the problem is in general NP-complete for\nlength-reducing term rewriting systems. Its complexity increases to\nN2ExpTime-complete (NExpTime-complete) if termination is proved using a\n(linear) polynomial order and to PSpace-complete for systems whose termination\ncan be shown using a restricted class of Knuth-Bendix orders. Confluence\nreduces the complexity to P for the length-reducing case, but has no effect on\nthe worst-case complexity in the other two cases. Finally, we consider the\nlarge term reachability problem, a variant of the problem where we are\ninterested in reachability of a term of size $\\geq n$. It turns out that this\nseemingly innocuous modification in some cases changes the complexity of the\nproblem, which may also become dependent on whether the number $n$ is is\nrepresented in unary or binary encoding, whereas this makes no difference for\nthe complexity of the small term reachability problem."
                },
                "authors": [
                    {
                        "name": "Franz Baader"
                    },
                    {
                        "name": "Jürgen Giesl"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Giesl"
                },
                "author": "Jürgen Giesl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06047v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06047v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10443v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10443v2",
                "updated": "2025-08-08T10:42:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    10,
                    42,
                    24,
                    4,
                    220,
                    0
                ],
                "published": "2025-05-15T16:04:25Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    4,
                    25,
                    3,
                    135,
                    0
                ],
                "title": "Are Large Language Models Robust in Understanding Code Against\n  Semantics-Preserving Mutations?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Large Language Models Robust in Understanding Code Against\n  Semantics-Preserving Mutations?"
                },
                "summary": "Understanding the reasoning and robustness of Large Language Models (LLMs) is\ncritical for their reliable use in programming tasks. While recent studies have\nassessed LLMs' ability to predict program outputs, most focus solely on the\naccuracy of those predictions, without evaluating the reasoning behind them.\nMoreover, it has been observed on mathematical reasoning tasks that LLMs can\narrive at correct answers through flawed logic, raising concerns about similar\nissues in code understanding. In this work, we evaluate whether\nstate-of-the-art LLMs with up to 8B parameters can reason about Python programs\nor are simply guessing. We apply five semantics-preserving code mutations:\nrenaming variables, mirroring comparison expressions, swapping if-else\nbranches, converting for loops to while, and loop unrolling. These mutations\nmaintain program semantics while altering its syntax. We evaluated six LLMs and\nperformed a human expert analysis using LiveCodeBench to assess whether the\ncorrect predictions are based on sound reasoning. We also evaluated prediction\nstability across different code mutations on LiveCodeBench and CruxEval. Our\nfindings show that LLMs trained for code produce correct predictions based on\nflawed reasoning between 10% and 50% of cases. Furthermore, LLMs often change\npredictions in response to our code mutations, indicating they do not yet\nexhibit stable, semantically grounded reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the reasoning and robustness of Large Language Models (LLMs) is\ncritical for their reliable use in programming tasks. While recent studies have\nassessed LLMs' ability to predict program outputs, most focus solely on the\naccuracy of those predictions, without evaluating the reasoning behind them.\nMoreover, it has been observed on mathematical reasoning tasks that LLMs can\narrive at correct answers through flawed logic, raising concerns about similar\nissues in code understanding. In this work, we evaluate whether\nstate-of-the-art LLMs with up to 8B parameters can reason about Python programs\nor are simply guessing. We apply five semantics-preserving code mutations:\nrenaming variables, mirroring comparison expressions, swapping if-else\nbranches, converting for loops to while, and loop unrolling. These mutations\nmaintain program semantics while altering its syntax. We evaluated six LLMs and\nperformed a human expert analysis using LiveCodeBench to assess whether the\ncorrect predictions are based on sound reasoning. We also evaluated prediction\nstability across different code mutations on LiveCodeBench and CruxEval. Our\nfindings show that LLMs trained for code produce correct predictions based on\nflawed reasoning between 10% and 50% of cases. Furthermore, LLMs often change\npredictions in response to our code mutations, indicating they do not yet\nexhibit stable, semantically grounded reasoning."
                },
                "authors": [
                    {
                        "name": "Pedro Orvalho"
                    },
                    {
                        "name": "Marta Kwiatkowska"
                    }
                ],
                "author_detail": {
                    "name": "Marta Kwiatkowska"
                },
                "author": "Marta Kwiatkowska",
                "arxiv_comment": "11 pages, 5 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10443v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10443v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06204v1",
                "updated": "2025-08-08T10:35:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    10,
                    35,
                    41,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T10:35:41Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    10,
                    35,
                    41,
                    4,
                    220,
                    0
                ],
                "title": "Classification is a RAG problem: A case study on hate speech detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classification is a RAG problem: A case study on hate speech detection"
                },
                "summary": "Robust content moderation requires classification systems that can quickly\nadapt to evolving policies without costly retraining. We present classification\nusing Retrieval-Augmented Generation (RAG), which shifts traditional\nclassification tasks from determining the correct category in accordance with\npre-trained parameters to evaluating content in relation to contextual\nknowledge retrieved at inference. In hate speech detection, this transforms the\ntask from \"is this hate speech?\" to \"does this violate the hate speech policy?\"\n  Our Contextual Policy Engine (CPE) - an agentic RAG system - demonstrates\nthis approach and offers three key advantages: (1) robust classification\naccuracy comparable to leading commercial systems, (2) inherent explainability\nvia retrieved policy segments, and (3) dynamic policy updates without model\nretraining. Through three experiments, we demonstrate strong baseline\nperformance and show that the system can apply fine-grained policy control by\ncorrectly adjusting protection for specific identity groups without requiring\nretraining or compromising overall performance. These findings establish that\nRAG can transform classification into a more flexible, transparent, and\nadaptable process for content moderation and wider classification problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust content moderation requires classification systems that can quickly\nadapt to evolving policies without costly retraining. We present classification\nusing Retrieval-Augmented Generation (RAG), which shifts traditional\nclassification tasks from determining the correct category in accordance with\npre-trained parameters to evaluating content in relation to contextual\nknowledge retrieved at inference. In hate speech detection, this transforms the\ntask from \"is this hate speech?\" to \"does this violate the hate speech policy?\"\n  Our Contextual Policy Engine (CPE) - an agentic RAG system - demonstrates\nthis approach and offers three key advantages: (1) robust classification\naccuracy comparable to leading commercial systems, (2) inherent explainability\nvia retrieved policy segments, and (3) dynamic policy updates without model\nretraining. Through three experiments, we demonstrate strong baseline\nperformance and show that the system can apply fine-grained policy control by\ncorrectly adjusting protection for specific identity groups without requiring\nretraining or compromising overall performance. These findings establish that\nRAG can transform classification into a more flexible, transparent, and\nadaptable process for content moderation and wider classification problems."
                },
                "authors": [
                    {
                        "name": "Richard Willats"
                    },
                    {
                        "name": "Josh Pennington"
                    },
                    {
                        "name": "Aravind Mohan"
                    },
                    {
                        "name": "Bertie Vidgen"
                    }
                ],
                "author_detail": {
                    "name": "Bertie Vidgen"
                },
                "author": "Bertie Vidgen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13380v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13380v2",
                "updated": "2025-08-08T10:26:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    10,
                    26,
                    47,
                    4,
                    220,
                    0
                ],
                "published": "2025-06-16T11:44:28Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    11,
                    44,
                    28,
                    0,
                    167,
                    0
                ],
                "title": "Decompositional Reasoning for Graph Retrieval with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decompositional Reasoning for Graph Retrieval with Large Language Models"
                },
                "summary": "Large Language Models (LLMs) excel at many NLP tasks, but struggle with\nmulti-hop reasoning and factual consistency, limiting their effectiveness on\nknowledge-intensive tasks like complex question answering (QA). Linking\nKnowledge Graphs (KG) and LLMs has shown promising results, but LLMs generally\nlack the ability to reason efficiently over graph-structured information. To\ntackle this problem, we propose a novel retrieval approach that integrates\ntextual knowledge graphs into the LLM reasoning process via query\ndecomposition. Our method decomposes complex questions into sub-questions,\nretrieves relevant textual subgraphs, and composes a question-specific\nknowledge graph to guide answer generation. For that, we use a weighted\nsimilarity function that focuses on both the complex question and the generated\nsubquestions to extract a relevant subgraph, which allows efficient and precise\nretrieval for complex questions and improves the performance of LLMs on\nmulti-hop QA tasks. This structured reasoning pipeline enhances factual\ngrounding and interpretability while leveraging the generative strengths of\nLLMs. We evaluate our method on standard multi-hop QA benchmarks and show that\nit achieves comparable or superior performance to competitive existing methods,\nusing smaller models and fewer LLM calls.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at many NLP tasks, but struggle with\nmulti-hop reasoning and factual consistency, limiting their effectiveness on\nknowledge-intensive tasks like complex question answering (QA). Linking\nKnowledge Graphs (KG) and LLMs has shown promising results, but LLMs generally\nlack the ability to reason efficiently over graph-structured information. To\ntackle this problem, we propose a novel retrieval approach that integrates\ntextual knowledge graphs into the LLM reasoning process via query\ndecomposition. Our method decomposes complex questions into sub-questions,\nretrieves relevant textual subgraphs, and composes a question-specific\nknowledge graph to guide answer generation. For that, we use a weighted\nsimilarity function that focuses on both the complex question and the generated\nsubquestions to extract a relevant subgraph, which allows efficient and precise\nretrieval for complex questions and improves the performance of LLMs on\nmulti-hop QA tasks. This structured reasoning pipeline enhances factual\ngrounding and interpretability while leveraging the generative strengths of\nLLMs. We evaluate our method on standard multi-hop QA benchmarks and show that\nit achieves comparable or superior performance to competitive existing methods,\nusing smaller models and fewer LLM calls."
                },
                "authors": [
                    {
                        "name": "Valentin Six"
                    },
                    {
                        "name": "Evan Dufraisse"
                    },
                    {
                        "name": "Gaël de Chalendar"
                    }
                ],
                "author_detail": {
                    "name": "Gaël de Chalendar"
                },
                "author": "Gaël de Chalendar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13380v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13380v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06196v1",
                "updated": "2025-08-08T10:22:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    10,
                    22,
                    19,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T10:22:19Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    10,
                    22,
                    19,
                    4,
                    220,
                    0
                ],
                "title": "EICAP: Deep Dive in Assessment and Enhancement of Large Language Models\n  in Emotional Intelligence through Multi-Turn Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EICAP: Deep Dive in Assessment and Enhancement of Large Language Models\n  in Emotional Intelligence through Multi-Turn Conversations"
                },
                "summary": "Emotional Intelligence (EI) is a critical yet underexplored dimension in the\ndevelopment of human-aligned LLMs. To address this gap, we introduce a unified,\npsychologically grounded four-layer taxonomy of EI tailored for large language\nmodels (LLMs), encompassing emotional tracking, cause inference, appraisal, and\nemotionally appropriate response generation. Building on this framework, we\npresent EICAP-Bench, a novel MCQ style multi-turn benchmark designed to\nevaluate EI capabilities in open-source LLMs across diverse linguistic and\ncultural contexts. We evaluate six LLMs: LLaMA3 (8B), LLaMA3-Instruct, Gemma\n(9B), Gemma-Instruct, Qwen2.5 (7B), and Qwen2.5-Instruct on EmoCap-Bench,\nidentifying Qwen2.5-Instruct as the strongest baseline. To assess the potential\nfor enhancing EI capabilities, we fine-tune both Qwen2.5-Base and\nQwen2.5-Instruct using LoRA adapters on UltraChat (UC), a large-scale,\ninstruction-tuned dialogue dataset, in both English and Arabic. Our statistical\nanalysis reveals that among the five EI layers, only the Appraisal layer shows\nsignificant improvement through UC-based fine-tuning. These findings highlight\nthe limitations of existing pretraining and instruction-tuning paradigms in\nequipping LLMs with deeper emotional reasoning and underscore the need for\ntargeted data and modeling strategies for comprehensive EI alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotional Intelligence (EI) is a critical yet underexplored dimension in the\ndevelopment of human-aligned LLMs. To address this gap, we introduce a unified,\npsychologically grounded four-layer taxonomy of EI tailored for large language\nmodels (LLMs), encompassing emotional tracking, cause inference, appraisal, and\nemotionally appropriate response generation. Building on this framework, we\npresent EICAP-Bench, a novel MCQ style multi-turn benchmark designed to\nevaluate EI capabilities in open-source LLMs across diverse linguistic and\ncultural contexts. We evaluate six LLMs: LLaMA3 (8B), LLaMA3-Instruct, Gemma\n(9B), Gemma-Instruct, Qwen2.5 (7B), and Qwen2.5-Instruct on EmoCap-Bench,\nidentifying Qwen2.5-Instruct as the strongest baseline. To assess the potential\nfor enhancing EI capabilities, we fine-tune both Qwen2.5-Base and\nQwen2.5-Instruct using LoRA adapters on UltraChat (UC), a large-scale,\ninstruction-tuned dialogue dataset, in both English and Arabic. Our statistical\nanalysis reveals that among the five EI layers, only the Appraisal layer shows\nsignificant improvement through UC-based fine-tuning. These findings highlight\nthe limitations of existing pretraining and instruction-tuning paradigms in\nequipping LLMs with deeper emotional reasoning and underscore the need for\ntargeted data and modeling strategies for comprehensive EI alignment."
                },
                "authors": [
                    {
                        "name": "Nizi Nazar"
                    },
                    {
                        "name": "Ehsaneddin Asgari"
                    }
                ],
                "author_detail": {
                    "name": "Ehsaneddin Asgari"
                },
                "author": "Ehsaneddin Asgari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06194v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06194v1",
                "updated": "2025-08-08T10:19:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    10,
                    19,
                    21,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T10:19:21Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    10,
                    19,
                    21,
                    4,
                    220,
                    0
                ],
                "title": "Beyond Uniform Criteria: Scenario-Adaptive Multi-Dimensional Jailbreak\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Uniform Criteria: Scenario-Adaptive Multi-Dimensional Jailbreak\n  Evaluation"
                },
                "summary": "Precise jailbreak evaluation is vital for LLM red teaming and jailbreak\nresearch. Current approaches employ binary classification ( e.g., string\nmatching, toxic text classifiers, LLM-driven methods), yielding only \"yes/no\"\nlabels without quantifying harm intensity. Existing multi-dimensional\nframeworks ( e.g., Security Violation, Relative Truthfulness, Informativeness)\napply uniform evaluation criteria across scenarios, resulting in\nscenario-specific mismatches--for instance, \"Relative Truthfulness\" is\nirrelevant to \"hate speech\"--which compromise evaluation precision. To tackle\nthese limitations, we introduce SceneJailEval, with key contributions: (1) A\ngroundbreaking scenario-adaptive multi-dimensional framework for jailbreak\nevaluation, overcoming the critical \"one-size-fits-all\" constraint of existing\nmulti-dimensional methods, and featuring strong extensibility to flexibly adapt\nto customized or emerging scenarios. (2) A comprehensive 14-scenario dataset\nwith diverse jailbreak variants and regional cases, filling the long-standing\ngap in high-quality, holistic benchmarks for scenario-adaptive evaluation. (3)\nSceneJailEval achieves state-of-the-art results, with an F1 score of 0.917 on\nour full-scenario dataset (+6% over prior SOTA) and 0.995 on JBB (+3% over\nprior SOTA), surpassing accuracy limits of existing evaluation methods in\nheterogeneous scenarios and confirming its advantage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precise jailbreak evaluation is vital for LLM red teaming and jailbreak\nresearch. Current approaches employ binary classification ( e.g., string\nmatching, toxic text classifiers, LLM-driven methods), yielding only \"yes/no\"\nlabels without quantifying harm intensity. Existing multi-dimensional\nframeworks ( e.g., Security Violation, Relative Truthfulness, Informativeness)\napply uniform evaluation criteria across scenarios, resulting in\nscenario-specific mismatches--for instance, \"Relative Truthfulness\" is\nirrelevant to \"hate speech\"--which compromise evaluation precision. To tackle\nthese limitations, we introduce SceneJailEval, with key contributions: (1) A\ngroundbreaking scenario-adaptive multi-dimensional framework for jailbreak\nevaluation, overcoming the critical \"one-size-fits-all\" constraint of existing\nmulti-dimensional methods, and featuring strong extensibility to flexibly adapt\nto customized or emerging scenarios. (2) A comprehensive 14-scenario dataset\nwith diverse jailbreak variants and regional cases, filling the long-standing\ngap in high-quality, holistic benchmarks for scenario-adaptive evaluation. (3)\nSceneJailEval achieves state-of-the-art results, with an F1 score of 0.917 on\nour full-scenario dataset (+6% over prior SOTA) and 0.995 on JBB (+3% over\nprior SOTA), surpassing accuracy limits of existing evaluation methods in\nheterogeneous scenarios and confirming its advantage."
                },
                "authors": [
                    {
                        "name": "Lai Jiang"
                    },
                    {
                        "name": "Yuekang Li"
                    },
                    {
                        "name": "Xiaohan Zhang"
                    },
                    {
                        "name": "Youtao Ding"
                    },
                    {
                        "name": "Li Pan"
                    }
                ],
                "author_detail": {
                    "name": "Li Pan"
                },
                "author": "Li Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06194v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06194v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05058v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05058v5",
                "updated": "2025-08-08T10:18:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    10,
                    18,
                    44,
                    4,
                    220,
                    0
                ],
                "published": "2025-04-07T13:29:02Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    29,
                    2,
                    0,
                    97,
                    0
                ],
                "title": "Not All Data Are Unlearned Equally",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Data Are Unlearned Equally"
                },
                "summary": "Machine unlearning is concerned with the task of removing knowledge learned\nfrom particular data points from a trained model. In the context of large\nlanguage models (LLMs), unlearning has recently received increased attention,\nparticularly for removing knowledge about named entities from models for\nprivacy purposes. While various approaches have been proposed to address the\nunlearning problem, most existing approaches treat all data points to be\nunlearned equally, i.e., unlearning that Montreal is a city in Canada is\ntreated exactly the same as unlearning the phone number of the first author of\nthis paper. In this work, we show that this all data is equal assumption does\nnot hold for LLM unlearning. We study how the success of unlearning depends on\nthe frequency of the knowledge we want to unlearn in the pre-training data of a\nmodel and find that frequency strongly affects unlearning, i.e., more frequent\nknowledge is harder to unlearn. Additionally, we uncover a misalignment between\nprobability and generation-based evaluations of unlearning and show that this\nproblem worsens as models become larger. Overall, our experiments highlight the\nneed for better evaluation practices and novel methods for LLM unlearning that\ntake the training data of models into account.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine unlearning is concerned with the task of removing knowledge learned\nfrom particular data points from a trained model. In the context of large\nlanguage models (LLMs), unlearning has recently received increased attention,\nparticularly for removing knowledge about named entities from models for\nprivacy purposes. While various approaches have been proposed to address the\nunlearning problem, most existing approaches treat all data points to be\nunlearned equally, i.e., unlearning that Montreal is a city in Canada is\ntreated exactly the same as unlearning the phone number of the first author of\nthis paper. In this work, we show that this all data is equal assumption does\nnot hold for LLM unlearning. We study how the success of unlearning depends on\nthe frequency of the knowledge we want to unlearn in the pre-training data of a\nmodel and find that frequency strongly affects unlearning, i.e., more frequent\nknowledge is harder to unlearn. Additionally, we uncover a misalignment between\nprobability and generation-based evaluations of unlearning and show that this\nproblem worsens as models become larger. Overall, our experiments highlight the\nneed for better evaluation practices and novel methods for LLM unlearning that\ntake the training data of models into account."
                },
                "authors": [
                    {
                        "name": "Aravind Krishnan"
                    },
                    {
                        "name": "Siva Reddy"
                    },
                    {
                        "name": "Marius Mosbach"
                    }
                ],
                "author_detail": {
                    "name": "Marius Mosbach"
                },
                "author": "Marius Mosbach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05058v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05058v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.06484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06484v1",
                "updated": "2025-08-08T17:49:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    17,
                    49,
                    22,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T17:49:22Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    17,
                    49,
                    22,
                    4,
                    220,
                    0
                ],
                "title": "Non-programmers Assessing AI-Generated Code: A Case Study of Business\n  Users Analyzing Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-programmers Assessing AI-Generated Code: A Case Study of Business\n  Users Analyzing Data"
                },
                "summary": "Non-technical end-users increasingly rely on AI code generation to perform\ntechnical tasks like data analysis. However, large language models (LLMs)\nremain unreliable, and it is unclear whether end-users can effectively identify\nmodel errors $\\unicode{x2014}$ especially in realistic and domain-specific\nscenarios. We surveyed marketing and sales professionals to assess their\nability to critically evaluate LLM-generated analyses of marketing data.\nParticipants were shown natural language explanations of the AI's code,\nrepeatedly informed the AI often makes mistakes, and explicitly prompted to\nidentify them. Yet, participants frequently failed to detect critical flaws\nthat could compromise decision-making, many of which required no technical\nknowledge to recognize. To investigate why, we reformatted AI responses into\nclearly delineated steps and provided alternative approaches for each decision\nto support critical evaluation. While these changes had a positive effect,\nparticipants often struggled to reason through the AI's steps and alternatives.\nOur findings suggest that business professionals cannot reliably verify\nAI-generated data analyses on their own and explore reasons why to inform\nfuture designs. As non-programmers adopt code-generating AI for technical\ntasks, unreliable AI and insufficient human oversight poses risks of unsafe or\nlow-quality decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-technical end-users increasingly rely on AI code generation to perform\ntechnical tasks like data analysis. However, large language models (LLMs)\nremain unreliable, and it is unclear whether end-users can effectively identify\nmodel errors $\\unicode{x2014}$ especially in realistic and domain-specific\nscenarios. We surveyed marketing and sales professionals to assess their\nability to critically evaluate LLM-generated analyses of marketing data.\nParticipants were shown natural language explanations of the AI's code,\nrepeatedly informed the AI often makes mistakes, and explicitly prompted to\nidentify them. Yet, participants frequently failed to detect critical flaws\nthat could compromise decision-making, many of which required no technical\nknowledge to recognize. To investigate why, we reformatted AI responses into\nclearly delineated steps and provided alternative approaches for each decision\nto support critical evaluation. While these changes had a positive effect,\nparticipants often struggled to reason through the AI's steps and alternatives.\nOur findings suggest that business professionals cannot reliably verify\nAI-generated data analyses on their own and explore reasons why to inform\nfuture designs. As non-programmers adopt code-generating AI for technical\ntasks, unreliable AI and insufficient human oversight poses risks of unsafe or\nlow-quality decisions."
                },
                "authors": [
                    {
                        "name": "Yuvraj Virk"
                    },
                    {
                        "name": "Dongyu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Dongyu Liu"
                },
                "author": "Dongyu Liu",
                "arxiv_comment": "Accepted by VL/HCC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13908v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13908v2",
                "updated": "2025-08-08T17:43:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    17,
                    43,
                    37,
                    4,
                    220,
                    0
                ],
                "published": "2025-04-09T13:58:07Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    13,
                    58,
                    7,
                    2,
                    99,
                    0
                ],
                "title": "AI-Assisted Conversational Interviewing: Effects on Data Quality and\n  User Experience",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Assisted Conversational Interviewing: Effects on Data Quality and\n  User Experience"
                },
                "summary": "Standardized surveys scale efficiently but sacrifice depth, while\nconversational interviews improve response quality at the cost of scalability\nand consistency. This study bridges the gap between these methods by\nintroducing a framework for AI-assisted conversational interviewing. To\nevaluate this framework, we conducted a web survey experiment where 1,800\nparticipants were randomly assigned to AI 'chatbots' which use large language\nmodels (LLMs) to dynamically probe respondents for elaboration and\ninteractively code open-ended responses to fixed questions developed by human\nresearchers. We assessed the AI chatbot's performance in terms of coding\naccuracy, response quality, and respondent experience. Our findings reveal that\nAI chatbots perform moderately well in live coding even without survey-specific\nfine-tuning, despite slightly inflated false positive errors due to respondent\nacquiescence bias. Open-ended responses were more detailed and informative, but\nthis came at a slight cost to respondent experience. Our findings highlight the\nfeasibility of using AI methods such as chatbots enhanced by LLMs to enhance\nopen-ended data collection in web surveys.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standardized surveys scale efficiently but sacrifice depth, while\nconversational interviews improve response quality at the cost of scalability\nand consistency. This study bridges the gap between these methods by\nintroducing a framework for AI-assisted conversational interviewing. To\nevaluate this framework, we conducted a web survey experiment where 1,800\nparticipants were randomly assigned to AI 'chatbots' which use large language\nmodels (LLMs) to dynamically probe respondents for elaboration and\ninteractively code open-ended responses to fixed questions developed by human\nresearchers. We assessed the AI chatbot's performance in terms of coding\naccuracy, response quality, and respondent experience. Our findings reveal that\nAI chatbots perform moderately well in live coding even without survey-specific\nfine-tuning, despite slightly inflated false positive errors due to respondent\nacquiescence bias. Open-ended responses were more detailed and informative, but\nthis came at a slight cost to respondent experience. Our findings highlight the\nfeasibility of using AI methods such as chatbots enhanced by LLMs to enhance\nopen-ended data collection in web surveys."
                },
                "authors": [
                    {
                        "name": "Soubhik Barari"
                    },
                    {
                        "name": "Jarret Angbazo"
                    },
                    {
                        "name": "Natalie Wang"
                    },
                    {
                        "name": "Leah M. Christian"
                    },
                    {
                        "name": "Elizabeth Dean"
                    },
                    {
                        "name": "Zoe Slowinski"
                    },
                    {
                        "name": "Brandon Sepulvado"
                    }
                ],
                "author_detail": {
                    "name": "Brandon Sepulvado"
                },
                "author": "Brandon Sepulvado",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13908v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13908v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06482v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06482v1",
                "updated": "2025-08-08T17:42:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    17,
                    42,
                    16,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T17:42:16Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    17,
                    42,
                    16,
                    4,
                    220,
                    0
                ],
                "title": "Post-training for Efficient Communication via Convention Formation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training for Efficient Communication via Convention Formation"
                },
                "summary": "Humans communicate with increasing efficiency in multi-turn interactions, by\nadapting their language and forming ad-hoc conventions. In contrast, prior work\nshows that LLMs do not naturally show this behavior. We develop a post-training\nprocess to develop this ability through targeted fine-tuning on heuristically\nidentified demonstrations of convention formation. We evaluate with two new\nbenchmarks focused on this capability. First, we design a focused,\ncognitively-motivated interaction benchmark that consistently elicits strong\nconvention formation trends in humans. Second, we create a new\ndocument-grounded reference completion task that reflects in-the-wild\nconvention formation behavior. Our studies show significantly improved\nconvention formation abilities in post-trained LLMs across the two evaluation\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans communicate with increasing efficiency in multi-turn interactions, by\nadapting their language and forming ad-hoc conventions. In contrast, prior work\nshows that LLMs do not naturally show this behavior. We develop a post-training\nprocess to develop this ability through targeted fine-tuning on heuristically\nidentified demonstrations of convention formation. We evaluate with two new\nbenchmarks focused on this capability. First, we design a focused,\ncognitively-motivated interaction benchmark that consistently elicits strong\nconvention formation trends in humans. Second, we create a new\ndocument-grounded reference completion task that reflects in-the-wild\nconvention formation behavior. Our studies show significantly improved\nconvention formation abilities in post-trained LLMs across the two evaluation\nmethods."
                },
                "authors": [
                    {
                        "name": "Yilun Hua"
                    },
                    {
                        "name": "Evan Wang"
                    },
                    {
                        "name": "Yoav Artzi"
                    }
                ],
                "author_detail": {
                    "name": "Yoav Artzi"
                },
                "author": "Yoav Artzi",
                "arxiv_comment": "Accepted to COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06482v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06482v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06479v1",
                "updated": "2025-08-08T17:36:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    17,
                    36,
                    42,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T17:36:42Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    17,
                    36,
                    42,
                    4,
                    220,
                    0
                ],
                "title": "The Problem of Atypicality in LLM-Powered Psychiatry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Problem of Atypicality in LLM-Powered Psychiatry"
                },
                "summary": "Large language models (LLMs) are increasingly proposed as scalable solutions\nto the global mental health crisis. But their deployment in psychiatric\ncontexts raises a distinctive ethical concern: the problem of atypicality.\nBecause LLMs generate outputs based on population-level statistical\nregularities, their responses -- while typically appropriate for general users\n-- may be dangerously inappropriate when interpreted by psychiatric patients,\nwho often exhibit atypical cognitive or interpretive patterns. We argue that\nstandard mitigation strategies, such as prompt engineering or fine-tuning, are\ninsufficient to resolve this structural risk. Instead, we propose dynamic\ncontextual certification (DCC): a staged, reversible and context-sensitive\nframework for deploying LLMs in psychiatry, inspired by clinical translation\nand dynamic safety models from artificial intelligence governance. DCC reframes\nchatbot deployment as an ongoing epistemic and ethical process that prioritises\ninterpretive safety over static performance benchmarks. Atypicality, we argue,\ncannot be eliminated -- but it can, and must, be proactively managed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly proposed as scalable solutions\nto the global mental health crisis. But their deployment in psychiatric\ncontexts raises a distinctive ethical concern: the problem of atypicality.\nBecause LLMs generate outputs based on population-level statistical\nregularities, their responses -- while typically appropriate for general users\n-- may be dangerously inappropriate when interpreted by psychiatric patients,\nwho often exhibit atypical cognitive or interpretive patterns. We argue that\nstandard mitigation strategies, such as prompt engineering or fine-tuning, are\ninsufficient to resolve this structural risk. Instead, we propose dynamic\ncontextual certification (DCC): a staged, reversible and context-sensitive\nframework for deploying LLMs in psychiatry, inspired by clinical translation\nand dynamic safety models from artificial intelligence governance. DCC reframes\nchatbot deployment as an ongoing epistemic and ethical process that prioritises\ninterpretive safety over static performance benchmarks. Atypicality, we argue,\ncannot be eliminated -- but it can, and must, be proactively managed."
                },
                "authors": [
                    {
                        "name": "Bosco Garcia"
                    },
                    {
                        "name": "Eugene Y. S. Chua"
                    },
                    {
                        "name": "Harman Singh Brah"
                    }
                ],
                "author_detail": {
                    "name": "Harman Singh Brah"
                },
                "author": "Harman Singh Brah",
                "arxiv_doi": "10.1136/jme-2025-110972",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1136/jme-2025-110972",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.06479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Preprint of 8/8/2025 -- please cite published version. This article\n  has been published in the Journal of Medical Ethics (2025) following peer\n  review and can also be viewed on the journal's website at\n  10.1136/jme-2025-110972",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01494v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01494v3",
                "updated": "2025-08-08T17:34:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    17,
                    34,
                    39,
                    4,
                    220,
                    0
                ],
                "published": "2025-07-02T08:52:35Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    52,
                    35,
                    2,
                    183,
                    0
                ],
                "title": "Crop Pest Classification Using Deep Learning Techniques: A Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crop Pest Classification Using Deep Learning Techniques: A Review"
                },
                "summary": "Insect pests continue to bring a serious threat to crop yields around the\nworld, and traditional methods for monitoring them are often slow, manual, and\ndifficult to scale. In recent years, deep learning has emerged as a powerful\nsolution, with techniques like convolutional neural networks (CNNs), vision\ntransformers (ViTs), and hybrid models gaining popularity for automating pest\ndetection. This review looks at 37 carefully selected studies published between\n2018 and 2025, all focused on AI-based pest classification. The selected\nresearch is organized by crop type, pest species, model architecture, dataset\nusage, and key technical challenges. The early studies relied heavily on CNNs\nbut latest work is shifting toward hybrid and transformer-based models that\ndeliver higher accuracy and better contextual understanding. Still, challenges\nlike imbalanced datasets, difficulty in detecting small pests, limited\ngeneralizability, and deployment on edge devices remain significant hurdles.\nOverall, this review offers a structured overview of the field, highlights\nuseful datasets, and outlines the key challenges and future directions for\nAI-based pest monitoring systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Insect pests continue to bring a serious threat to crop yields around the\nworld, and traditional methods for monitoring them are often slow, manual, and\ndifficult to scale. In recent years, deep learning has emerged as a powerful\nsolution, with techniques like convolutional neural networks (CNNs), vision\ntransformers (ViTs), and hybrid models gaining popularity for automating pest\ndetection. This review looks at 37 carefully selected studies published between\n2018 and 2025, all focused on AI-based pest classification. The selected\nresearch is organized by crop type, pest species, model architecture, dataset\nusage, and key technical challenges. The early studies relied heavily on CNNs\nbut latest work is shifting toward hybrid and transformer-based models that\ndeliver higher accuracy and better contextual understanding. Still, challenges\nlike imbalanced datasets, difficulty in detecting small pests, limited\ngeneralizability, and deployment on edge devices remain significant hurdles.\nOverall, this review offers a structured overview of the field, highlights\nuseful datasets, and outlines the key challenges and future directions for\nAI-based pest monitoring systems."
                },
                "authors": [
                    {
                        "name": "Muhammad Hassam Ejaz"
                    },
                    {
                        "name": "Muhammad Bilal"
                    },
                    {
                        "name": "Usman Habib"
                    },
                    {
                        "name": "Muhammad Attique"
                    },
                    {
                        "name": "Tae-Sun Chung"
                    }
                ],
                "author_detail": {
                    "name": "Tae-Sun Chung"
                },
                "author": "Tae-Sun Chung",
                "arxiv_comment": "This version adds co-authors who were unintentionally left out of the\n  prior submission. Additionally, Table 1 has been reformatted for clarity, and\n  several typographical errors have been corrected",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01494v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01494v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06467v1",
                "updated": "2025-08-08T17:15:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    17,
                    15,
                    32,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T17:15:32Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    17,
                    15,
                    32,
                    4,
                    220,
                    0
                ],
                "title": "LLM Unlearning using Gradient Ratio-Based Influence Estimation and Noise\n  Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Unlearning using Gradient Ratio-Based Influence Estimation and Noise\n  Injection"
                },
                "summary": "The growing legal and ethical scrutiny of large language models (LLMs)\nnecessitates effective machine unlearning, particularly for sensitive or\nunauthorized data. Existing empirical methods often yield incomplete forgetting\nor unintended degradation of unrelated knowledge due to poor localization. In\nthis work, we propose GRIN: a modular and targeted framework for LLM\nunlearning. GRIN introduces a novel gradient-ratio-based metric to identify\nparameters most responsible for memorizing forget data. We then perform\nselective noise injection into these parameters prior to fine-tuning, which\nimproves unlearning performance while maintaining model utility. Finally, we\npropose new evaluation metrics tailored to the LLM setting and validate our\napproach on standard benchmarks such as TOFU, WMDP, and SafePKU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing legal and ethical scrutiny of large language models (LLMs)\nnecessitates effective machine unlearning, particularly for sensitive or\nunauthorized data. Existing empirical methods often yield incomplete forgetting\nor unintended degradation of unrelated knowledge due to poor localization. In\nthis work, we propose GRIN: a modular and targeted framework for LLM\nunlearning. GRIN introduces a novel gradient-ratio-based metric to identify\nparameters most responsible for memorizing forget data. We then perform\nselective noise injection into these parameters prior to fine-tuning, which\nimproves unlearning performance while maintaining model utility. Finally, we\npropose new evaluation metrics tailored to the LLM setting and validate our\napproach on standard benchmarks such as TOFU, WMDP, and SafePKU."
                },
                "authors": [
                    {
                        "name": "Ameya Anjarlekar"
                    },
                    {
                        "name": "Sandeep Pombra"
                    }
                ],
                "author_detail": {
                    "name": "Sandeep Pombra"
                },
                "author": "Sandeep Pombra",
                "arxiv_comment": "14 Pages, 3 Figures, 11 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06457v1",
                "updated": "2025-08-08T17:01:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    17,
                    1,
                    41,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T17:01:41Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    17,
                    1,
                    41,
                    4,
                    220,
                    0
                ],
                "title": "ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive fluency and\nreasoning capabilities, but their potential for misuse has raised growing\nconcern. In this paper, we present ScamAgent, an autonomous multi-turn agent\nbuilt on top of LLMs, capable of generating highly realistic scam call scripts\nthat simulate real-world fraud scenarios. Unlike prior work focused on\nsingle-shot prompt misuse, ScamAgent maintains dialogue memory, adapts\ndynamically to simulated user responses, and employs deceptive persuasion\nstrategies across conversational turns. We show that current LLM safety\nguardrails, including refusal mechanisms and content filters, are ineffective\nagainst such agent-based threats. Even models with strong prompt-level\nsafeguards can be bypassed when prompts are decomposed, disguised, or delivered\nincrementally within an agent framework. We further demonstrate the\ntransformation of scam scripts into lifelike voice calls using modern\ntext-to-speech systems, completing a fully automated scam pipeline. Our\nfindings highlight an urgent need for multi-turn safety auditing, agent-level\ncontrol frameworks, and new methods to detect and disrupt conversational\ndeception powered by generative AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive fluency and\nreasoning capabilities, but their potential for misuse has raised growing\nconcern. In this paper, we present ScamAgent, an autonomous multi-turn agent\nbuilt on top of LLMs, capable of generating highly realistic scam call scripts\nthat simulate real-world fraud scenarios. Unlike prior work focused on\nsingle-shot prompt misuse, ScamAgent maintains dialogue memory, adapts\ndynamically to simulated user responses, and employs deceptive persuasion\nstrategies across conversational turns. We show that current LLM safety\nguardrails, including refusal mechanisms and content filters, are ineffective\nagainst such agent-based threats. Even models with strong prompt-level\nsafeguards can be bypassed when prompts are decomposed, disguised, or delivered\nincrementally within an agent framework. We further demonstrate the\ntransformation of scam scripts into lifelike voice calls using modern\ntext-to-speech systems, completing a fully automated scam pipeline. Our\nfindings highlight an urgent need for multi-turn safety auditing, agent-level\ncontrol frameworks, and new methods to detect and disrupt conversational\ndeception powered by generative AI."
                },
                "authors": [
                    {
                        "name": "Sanket Badhe"
                    }
                ],
                "author_detail": {
                    "name": "Sanket Badhe"
                },
                "author": "Sanket Badhe",
                "arxiv_comment": "Accepted at CAMLIS 25: Conference on Applied Machine Learning for\n  Information Security. 10 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06453v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06453v1",
                "updated": "2025-08-08T16:54:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    54,
                    6,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T16:54:06Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    54,
                    6,
                    4,
                    220,
                    0
                ],
                "title": "Text Embedded Swin-UMamba for DeepLesion Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text Embedded Swin-UMamba for DeepLesion Segmentation"
                },
                "summary": "Segmentation of lesions on CT enables automatic measurement for clinical\nassessment of chronic diseases (e.g., lymphoma). Integrating large language\nmodels (LLMs) into the lesion segmentation workflow offers the potential to\ncombine imaging features with descriptions of lesion characteristics from the\nradiology reports. In this study, we investigate the feasibility of integrating\ntext into the Swin-UMamba architecture for the task of lesion segmentation. The\npublicly available ULS23 DeepLesion dataset was used along with short-form\ndescriptions of the findings from the reports. On the test dataset, a high Dice\nScore of 82% and low Hausdorff distance of 6.58 (pixels) was obtained for\nlesion segmentation. The proposed Text-Swin-UMamba model outperformed prior\napproaches: 37% improvement over the LLM-driven LanGuideMedSeg model (p <\n0.001),and surpassed the purely image-based xLSTM-UNet and nnUNet models by\n1.74% and 0.22%, respectively. The dataset and code can be accessed at\nhttps://github.com/ruida/LLM-Swin-UMamba",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Segmentation of lesions on CT enables automatic measurement for clinical\nassessment of chronic diseases (e.g., lymphoma). Integrating large language\nmodels (LLMs) into the lesion segmentation workflow offers the potential to\ncombine imaging features with descriptions of lesion characteristics from the\nradiology reports. In this study, we investigate the feasibility of integrating\ntext into the Swin-UMamba architecture for the task of lesion segmentation. The\npublicly available ULS23 DeepLesion dataset was used along with short-form\ndescriptions of the findings from the reports. On the test dataset, a high Dice\nScore of 82% and low Hausdorff distance of 6.58 (pixels) was obtained for\nlesion segmentation. The proposed Text-Swin-UMamba model outperformed prior\napproaches: 37% improvement over the LLM-driven LanGuideMedSeg model (p <\n0.001),and surpassed the purely image-based xLSTM-UNet and nnUNet models by\n1.74% and 0.22%, respectively. The dataset and code can be accessed at\nhttps://github.com/ruida/LLM-Swin-UMamba"
                },
                "authors": [
                    {
                        "name": "Ruida Cheng"
                    },
                    {
                        "name": "Tejas Sudharshan Mathai"
                    },
                    {
                        "name": "Pritam Mukherjee"
                    },
                    {
                        "name": "Benjamin Hou"
                    },
                    {
                        "name": "Qingqing Zhu"
                    },
                    {
                        "name": "Zhiyong Lu"
                    },
                    {
                        "name": "Matthew McAuliffe"
                    },
                    {
                        "name": "Ronald M. Summers"
                    }
                ],
                "author_detail": {
                    "name": "Ronald M. Summers"
                },
                "author": "Ronald M. Summers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06453v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06453v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03687v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03687v2",
                "updated": "2025-08-08T16:49:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    49,
                    48,
                    4,
                    220,
                    0
                ],
                "published": "2025-02-06T00:37:21Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    0,
                    37,
                    21,
                    3,
                    37,
                    0
                ],
                "title": "Conditional Diffusion Models are Medical Image Classifiers that Provide\n  Explainability and Uncertainty for Free",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conditional Diffusion Models are Medical Image Classifiers that Provide\n  Explainability and Uncertainty for Free"
                },
                "summary": "Discriminative classifiers have become a foundational tool in deep learning\nfor medical imaging, excelling at learning separable features of complex data\ndistributions. However, these models often need careful design, augmentation,\nand training techniques to ensure safe and reliable deployment. Recently,\ndiffusion models have become synonymous with generative modeling in 2D. These\nmodels showcase robustness across a range of tasks including natural image\nclassification, where classification is performed by comparing reconstruction\nerrors across images generated for each possible conditioning input. This work\npresents the first exploration of the potential of class conditional diffusion\nmodels for 2D medical image classification. First, we develop a novel majority\nvoting scheme shown to improve the performance of medical diffusion\nclassifiers. Next, extensive experiments on the CheXpert and ISIC Melanoma skin\ncancer datasets demonstrate that foundation and trained-from-scratch diffusion\nmodels achieve competitive performance against SOTA discriminative classifiers\nwithout the need for explicit supervision. In addition, we show that diffusion\nclassifiers are intrinsically explainable, and can be used to quantify the\nuncertainty of their predictions, increasing their trustworthiness and\nreliability in safety-critical, clinical contexts. Further information is\navailable on our project page:\nhttps://faverogian.github.io/med-diffusion-classifier.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discriminative classifiers have become a foundational tool in deep learning\nfor medical imaging, excelling at learning separable features of complex data\ndistributions. However, these models often need careful design, augmentation,\nand training techniques to ensure safe and reliable deployment. Recently,\ndiffusion models have become synonymous with generative modeling in 2D. These\nmodels showcase robustness across a range of tasks including natural image\nclassification, where classification is performed by comparing reconstruction\nerrors across images generated for each possible conditioning input. This work\npresents the first exploration of the potential of class conditional diffusion\nmodels for 2D medical image classification. First, we develop a novel majority\nvoting scheme shown to improve the performance of medical diffusion\nclassifiers. Next, extensive experiments on the CheXpert and ISIC Melanoma skin\ncancer datasets demonstrate that foundation and trained-from-scratch diffusion\nmodels achieve competitive performance against SOTA discriminative classifiers\nwithout the need for explicit supervision. In addition, we show that diffusion\nclassifiers are intrinsically explainable, and can be used to quantify the\nuncertainty of their predictions, increasing their trustworthiness and\nreliability in safety-critical, clinical contexts. Further information is\navailable on our project page:\nhttps://faverogian.github.io/med-diffusion-classifier.github.io/."
                },
                "authors": [
                    {
                        "name": "Gian Mario Favero"
                    },
                    {
                        "name": "Parham Saremi"
                    },
                    {
                        "name": "Emily Kaczmarek"
                    },
                    {
                        "name": "Brennan Nichyporuk"
                    },
                    {
                        "name": "Tal Arbel"
                    }
                ],
                "author_detail": {
                    "name": "Tal Arbel"
                },
                "author": "Tal Arbel",
                "arxiv_comment": "Accepted for publication at MIDL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03687v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03687v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02529v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02529v2",
                "updated": "2025-08-08T16:47:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    47,
                    32,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-04T15:38:13Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    38,
                    13,
                    0,
                    216,
                    0
                ],
                "title": "Failure-Aware Multi-Robot Coordination for Resilient and Adaptive Target\n  Tracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Failure-Aware Multi-Robot Coordination for Resilient and Adaptive Target\n  Tracking"
                },
                "summary": "Multi-robot coordination is crucial for autonomous systems, yet real-world\ndeployments often encounter various failures. These include both temporary and\npermanent disruptions in sensing and communication, which can significantly\ndegrade system robustness and performance if not explicitly modeled. Despite\nits practical importance, failure-aware coordination remains underexplored in\nthe literature. To bridge the gap between idealized conditions and the\ncomplexities of real-world environments, we propose a unified failure-aware\ncoordination framework designed to enable resilient and adaptive multi-robot\ntarget tracking under both temporary and permanent failure conditions. Our\napproach systematically distinguishes between two classes of failures: (1)\nprobabilistic and temporary disruptions, where robots recover from intermittent\nsensing or communication losses by dynamically adapting paths and avoiding\ninferred danger zones, and (2) permanent failures, where robots lose sensing or\ncommunication capabilities irreversibly, requiring sustained, decentralized\nbehavioral adaptation. To handle these scenarios, the robot team is partitioned\ninto subgroups. Robots that remain connected form a communication group and\ncollaboratively plan using partially centralized nonlinear optimization. Robots\nexperiencing permanent disconnection or failure continue to operate\nindependently through decentralized or individual optimization, allowing them\nto contribute to the task within their local context. We extensively evaluate\nour method across a range of benchmark variations and conduct a comprehensive\nassessment under diverse real-world failure scenarios. Results show that our\nframework consistently achieves robust performance in realistic environments\nwith unknown danger zones, offering a practical and generalizable solution for\nthe multi-robot systems community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-robot coordination is crucial for autonomous systems, yet real-world\ndeployments often encounter various failures. These include both temporary and\npermanent disruptions in sensing and communication, which can significantly\ndegrade system robustness and performance if not explicitly modeled. Despite\nits practical importance, failure-aware coordination remains underexplored in\nthe literature. To bridge the gap between idealized conditions and the\ncomplexities of real-world environments, we propose a unified failure-aware\ncoordination framework designed to enable resilient and adaptive multi-robot\ntarget tracking under both temporary and permanent failure conditions. Our\napproach systematically distinguishes between two classes of failures: (1)\nprobabilistic and temporary disruptions, where robots recover from intermittent\nsensing or communication losses by dynamically adapting paths and avoiding\ninferred danger zones, and (2) permanent failures, where robots lose sensing or\ncommunication capabilities irreversibly, requiring sustained, decentralized\nbehavioral adaptation. To handle these scenarios, the robot team is partitioned\ninto subgroups. Robots that remain connected form a communication group and\ncollaboratively plan using partially centralized nonlinear optimization. Robots\nexperiencing permanent disconnection or failure continue to operate\nindependently through decentralized or individual optimization, allowing them\nto contribute to the task within their local context. We extensively evaluate\nour method across a range of benchmark variations and conduct a comprehensive\nassessment under diverse real-world failure scenarios. Results show that our\nframework consistently achieves robust performance in realistic environments\nwith unknown danger zones, offering a practical and generalizable solution for\nthe multi-robot systems community."
                },
                "authors": [
                    {
                        "name": "Peihan Li"
                    },
                    {
                        "name": "Jiazhen Liu"
                    },
                    {
                        "name": "Yuwei Wu"
                    },
                    {
                        "name": "Lifeng Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Lifeng Zhou"
                },
                "author": "Lifeng Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02529v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02529v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15254v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15254v2",
                "updated": "2025-08-08T16:45:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    45,
                    47,
                    4,
                    220,
                    0
                ],
                "published": "2025-04-21T17:33:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    33,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation"
                },
                "summary": "C-to-Rust transpilation is essential for modernizing legacy C code while\nenhancing safety and interoperability with modern Rust ecosystems. However, no\ndataset currently exists for evaluating whether a system can transpile C into\nsafe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset\nof 100 C repositories, each paired with manually-written interfaces in safe\nRust as well as test cases that can be used to validate correctness of the\ntranspilation. By considering entire repositories rather than isolated\nfunctions, CRUST-Bench captures the challenges of translating complex projects\nwith dependencies across multiple files. The provided Rust interfaces provide\nexplicit specifications that ensure adherence to idiomatic, memory-safe Rust\npatterns, while the accompanying test cases enforce functional correctness. We\nevaluate state-of-the-art large language models (LLMs) on this task and find\nthat safe and idiomatic Rust generation is still a challenging problem for\nvarious state-of-the-art methods and techniques. We also provide insights into\nthe errors LLMs usually make in transpiling code from C to safe Rust. The best\nperforming model, OpenAI o1, is able to solve only 15 tasks in a single-shot\nsetting. Improvements on CRUST-Bench would lead to improved transpilation\nsystems that can reason about complex scenarios and help in migrating legacy\ncodebases from C into languages like Rust that ensure memory safety. You can\nfind the dataset and code at https://github.com/anirudhkhatry/CRUST-bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C-to-Rust transpilation is essential for modernizing legacy C code while\nenhancing safety and interoperability with modern Rust ecosystems. However, no\ndataset currently exists for evaluating whether a system can transpile C into\nsafe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset\nof 100 C repositories, each paired with manually-written interfaces in safe\nRust as well as test cases that can be used to validate correctness of the\ntranspilation. By considering entire repositories rather than isolated\nfunctions, CRUST-Bench captures the challenges of translating complex projects\nwith dependencies across multiple files. The provided Rust interfaces provide\nexplicit specifications that ensure adherence to idiomatic, memory-safe Rust\npatterns, while the accompanying test cases enforce functional correctness. We\nevaluate state-of-the-art large language models (LLMs) on this task and find\nthat safe and idiomatic Rust generation is still a challenging problem for\nvarious state-of-the-art methods and techniques. We also provide insights into\nthe errors LLMs usually make in transpiling code from C to safe Rust. The best\nperforming model, OpenAI o1, is able to solve only 15 tasks in a single-shot\nsetting. Improvements on CRUST-Bench would lead to improved transpilation\nsystems that can reason about complex scenarios and help in migrating legacy\ncodebases from C into languages like Rust that ensure memory safety. You can\nfind the dataset and code at https://github.com/anirudhkhatry/CRUST-bench."
                },
                "authors": [
                    {
                        "name": "Anirudh Khatry"
                    },
                    {
                        "name": "Robert Zhang"
                    },
                    {
                        "name": "Jia Pan"
                    },
                    {
                        "name": "Ziteng Wang"
                    },
                    {
                        "name": "Qiaochu Chen"
                    },
                    {
                        "name": "Greg Durrett"
                    },
                    {
                        "name": "Isil Dillig"
                    }
                ],
                "author_detail": {
                    "name": "Isil Dillig"
                },
                "author": "Isil Dillig",
                "arxiv_comment": "To be published at COLM, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15254v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15254v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06447v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06447v1",
                "updated": "2025-08-08T16:42:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    42,
                    38,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T16:42:38Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    42,
                    38,
                    4,
                    220,
                    0
                ],
                "title": "SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token\n  Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token\n  Pruning"
                },
                "summary": "Long-context inference for Large Language Models (LLMs) is heavily limited by\nhigh computational demands. While several existing methods optimize attention\ncomputation, they still process the full set of hidden states at each layer,\nlimiting overall efficiency. In this work, we propose SlimInfer, an innovative\nframework that aims to accelerate inference by directly pruning less critical\nprompt tokens during the forward pass. Our key insight is an information\ndiffusion phenomenon: As information from critical tokens propagates through\nlayers, it becomes distributed across the entire sequence. This diffusion\nprocess suggests that LLMs can maintain their semantic integrity when excessive\ntokens, even including these critical ones, are pruned in hidden states.\nMotivated by this, SlimInfer introduces a dynamic fine-grained pruning\nmechanism that accurately removes redundant tokens of hidden state at\nintermediate layers. This layer-wise pruning naturally enables an asynchronous\nKV cache manager that prefetches required token blocks without complex\npredictors, reducing both memory usage and I/O costs. Extensive experiments\nshow that SlimInfer can achieve up to $\\mathbf{2.53\\times}$ time-to-first-token\n(TTFT) speedup and $\\mathbf{1.88\\times}$ end-to-end latency reduction for\nLLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on\nLongBench. Our code will be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context inference for Large Language Models (LLMs) is heavily limited by\nhigh computational demands. While several existing methods optimize attention\ncomputation, they still process the full set of hidden states at each layer,\nlimiting overall efficiency. In this work, we propose SlimInfer, an innovative\nframework that aims to accelerate inference by directly pruning less critical\nprompt tokens during the forward pass. Our key insight is an information\ndiffusion phenomenon: As information from critical tokens propagates through\nlayers, it becomes distributed across the entire sequence. This diffusion\nprocess suggests that LLMs can maintain their semantic integrity when excessive\ntokens, even including these critical ones, are pruned in hidden states.\nMotivated by this, SlimInfer introduces a dynamic fine-grained pruning\nmechanism that accurately removes redundant tokens of hidden state at\nintermediate layers. This layer-wise pruning naturally enables an asynchronous\nKV cache manager that prefetches required token blocks without complex\npredictors, reducing both memory usage and I/O costs. Extensive experiments\nshow that SlimInfer can achieve up to $\\mathbf{2.53\\times}$ time-to-first-token\n(TTFT) speedup and $\\mathbf{1.88\\times}$ end-to-end latency reduction for\nLLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on\nLongBench. Our code will be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Lingkun Long"
                    },
                    {
                        "name": "Rubing Yang"
                    },
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Desheng Hui"
                    },
                    {
                        "name": "Ao Zhou"
                    },
                    {
                        "name": "Jianlei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jianlei Yang"
                },
                "author": "Jianlei Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06447v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06447v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13900v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13900v4",
                "updated": "2025-08-08T16:39:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    39,
                    15,
                    4,
                    220,
                    0
                ],
                "published": "2024-07-18T21:06:39Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    21,
                    6,
                    39,
                    3,
                    200,
                    0
                ],
                "title": "Exploring the Evidence-Based SE Beliefs of Generative AI Tools",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Evidence-Based SE Beliefs of Generative AI Tools"
                },
                "summary": "Recent innovations in generative artificial intelligence (AI), primarily\npowered by large language models (LLMs), have transformed how programmers\ndevelop and maintain software. The advanced capabilities of generative AI tools\nto support software development tasks have led to a rise in their adoption\nwithin software engineering (SE) workflows. However, little is known about how\nAI tools perceive evidence-based beliefs and practices supported by research\nfindings. To this end, we conduct a preliminary evaluation conceptually\nreplicating prior work to explore the \"beliefs\" of generative AI tools used to\nsupport software development tasks. We investigate 17 evidence-based claims\nposited by empirical SE research across five generative AI tools. Our findings\nshow that generative AI tools have ambiguous beliefs regarding research claims\nand lack credible evidence to support responses. Based on our results, we\nprovide implications for practitioners integrating generative AI-based systems\ninto development contexts and shed light on future research directions to\nenhance the reliability and trustworthiness of generative AI -- aiming to\nincrease awareness and adoption of evidence-based SE research findings in\npractice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent innovations in generative artificial intelligence (AI), primarily\npowered by large language models (LLMs), have transformed how programmers\ndevelop and maintain software. The advanced capabilities of generative AI tools\nto support software development tasks have led to a rise in their adoption\nwithin software engineering (SE) workflows. However, little is known about how\nAI tools perceive evidence-based beliefs and practices supported by research\nfindings. To this end, we conduct a preliminary evaluation conceptually\nreplicating prior work to explore the \"beliefs\" of generative AI tools used to\nsupport software development tasks. We investigate 17 evidence-based claims\nposited by empirical SE research across five generative AI tools. Our findings\nshow that generative AI tools have ambiguous beliefs regarding research claims\nand lack credible evidence to support responses. Based on our results, we\nprovide implications for practitioners integrating generative AI-based systems\ninto development contexts and shed light on future research directions to\nenhance the reliability and trustworthiness of generative AI -- aiming to\nincrease awareness and adoption of evidence-based SE research findings in\npractice."
                },
                "authors": [
                    {
                        "name": "Chris Brown"
                    },
                    {
                        "name": "Jason Cusati"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cusati"
                },
                "author": "Jason Cusati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13900v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13900v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06445v1",
                "updated": "2025-08-08T16:38:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    38,
                    33,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T16:38:33Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    38,
                    33,
                    4,
                    220,
                    0
                ],
                "title": "Echoes of Automation: The Increasing Use of LLMs in Newsmaking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Echoes of Automation: The Increasing Use of LLMs in Newsmaking"
                },
                "summary": "The rapid rise of Generative AI (GenAI), particularly LLMs, poses concerns\nfor journalistic integrity and authorship. This study examines AI-generated\ncontent across over 40,000 news articles from major, local, and college news\nmedia, in various media formats. Using three advanced AI-text detectors (e.g.,\nBinoculars, Fast-Detect GPT, and GPTZero), we find substantial increase of\nGenAI use in recent years, especially in local and college news. Sentence-level\nanalysis reveals LLMs are often used in the introduction of news, while\nconclusions usually written manually. Linguistic analysis shows GenAI boosts\nword richness and readability but lowers formality, leading to more uniform\nwriting styles, particularly in local media.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid rise of Generative AI (GenAI), particularly LLMs, poses concerns\nfor journalistic integrity and authorship. This study examines AI-generated\ncontent across over 40,000 news articles from major, local, and college news\nmedia, in various media formats. Using three advanced AI-text detectors (e.g.,\nBinoculars, Fast-Detect GPT, and GPTZero), we find substantial increase of\nGenAI use in recent years, especially in local and college news. Sentence-level\nanalysis reveals LLMs are often used in the introduction of news, while\nconclusions usually written manually. Linguistic analysis shows GenAI boosts\nword richness and readability but lowers formality, leading to more uniform\nwriting styles, particularly in local media."
                },
                "authors": [
                    {
                        "name": "Abolfazl Ansari"
                    },
                    {
                        "name": "Delvin Ce Zhang"
                    },
                    {
                        "name": "Nafis Irtiza Tripto"
                    },
                    {
                        "name": "Dongwon Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongwon Lee"
                },
                "author": "Dongwon Lee",
                "arxiv_comment": "To appear in 18th International Conference on Social Computing,\n  Behavioral-Cultural Modeling, & Prediction and Behavior Representation in\n  Modeling and Simulation, and to be published in the Springer LNCS series",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06443v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06443v1",
                "updated": "2025-08-08T16:36:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    36,
                    16,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T16:36:16Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    36,
                    16,
                    4,
                    220,
                    0
                ],
                "title": "The Fair Game: Auditing & Debiasing AI Algorithms Over Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Fair Game: Auditing & Debiasing AI Algorithms Over Time"
                },
                "summary": "An emerging field of AI, namely Fair Machine Learning (ML), aims to quantify\ndifferent types of bias (also known as unfairness) exhibited in the predictions\nof ML algorithms, and to design new algorithms to mitigate them. Often, the\ndefinitions of bias used in the literature are observational, i.e. they use the\ninput and output of a pre-trained algorithm to quantify a bias under concern.\nIn reality,these definitions are often conflicting in nature and can only be\ndeployed if either the ground truth is known or only in retrospect after\ndeploying the algorithm. Thus,there is a gap between what we want Fair ML to\nachieve and what it does in a dynamic social environment. Hence, we propose an\nalternative dynamic mechanism,\"Fair Game\",to assure fairness in the predictions\nof an ML algorithm and to adapt its predictions as the society interacts with\nthe algorithm over time. \"Fair Game\" puts together an Auditor and a Debiasing\nalgorithm in a loop around an ML algorithm. The \"Fair Game\" puts these two\ncomponents in a loop by leveraging Reinforcement Learning (RL). RL algorithms\ninteract with an environment to take decisions, which yields new observations\n(also known as data/feedback) from the environment and in turn, adapts future\ndecisions. RL is already used in algorithms with pre-fixed long-term fairness\ngoals. \"Fair Game\" provides a unique framework where the fairness goals can be\nadapted over time by only modifying the auditor and the different biases it\nquantifies. Thus,\"Fair Game\" aims to simulate the evolution of ethical and\nlegal frameworks in the society by creating an auditor which sends feedback to\na debiasing algorithm deployed around an ML system. This allows us to develop a\nflexible and adaptive-over-time framework to build Fair ML systems pre- and\npost-deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An emerging field of AI, namely Fair Machine Learning (ML), aims to quantify\ndifferent types of bias (also known as unfairness) exhibited in the predictions\nof ML algorithms, and to design new algorithms to mitigate them. Often, the\ndefinitions of bias used in the literature are observational, i.e. they use the\ninput and output of a pre-trained algorithm to quantify a bias under concern.\nIn reality,these definitions are often conflicting in nature and can only be\ndeployed if either the ground truth is known or only in retrospect after\ndeploying the algorithm. Thus,there is a gap between what we want Fair ML to\nachieve and what it does in a dynamic social environment. Hence, we propose an\nalternative dynamic mechanism,\"Fair Game\",to assure fairness in the predictions\nof an ML algorithm and to adapt its predictions as the society interacts with\nthe algorithm over time. \"Fair Game\" puts together an Auditor and a Debiasing\nalgorithm in a loop around an ML algorithm. The \"Fair Game\" puts these two\ncomponents in a loop by leveraging Reinforcement Learning (RL). RL algorithms\ninteract with an environment to take decisions, which yields new observations\n(also known as data/feedback) from the environment and in turn, adapts future\ndecisions. RL is already used in algorithms with pre-fixed long-term fairness\ngoals. \"Fair Game\" provides a unique framework where the fairness goals can be\nadapted over time by only modifying the auditor and the different biases it\nquantifies. Thus,\"Fair Game\" aims to simulate the evolution of ethical and\nlegal frameworks in the society by creating an auditor which sends feedback to\na debiasing algorithm deployed around an ML system. This allows us to develop a\nflexible and adaptive-over-time framework to build Fair ML systems pre- and\npost-deployment."
                },
                "authors": [
                    {
                        "name": "Debabrota Basu"
                    },
                    {
                        "name": "Udvas Das"
                    }
                ],
                "author_detail": {
                    "name": "Udvas Das"
                },
                "author": "Udvas Das",
                "arxiv_doi": "10.1017/cfl.2025.8",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1017/cfl.2025.8",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.06443v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06443v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Cambridge Forum on AI: Law and Governance , Volume 1 , 2025 , p.\n  e27",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02113v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02113v4",
                "updated": "2025-08-08T16:28:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    28,
                    26,
                    4,
                    220,
                    0
                ],
                "published": "2024-04-02T17:13:22Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    17,
                    13,
                    22,
                    1,
                    93,
                    0
                ],
                "title": "Position: Lifetime tuning is incompatible with continual reinforcement\n  learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position: Lifetime tuning is incompatible with continual reinforcement\n  learning"
                },
                "summary": "In continual RL we want agents capable of never-ending learning, and yet our\nevaluation methodologies do not reflect this. The standard practice in RL is to\nassume unfettered access to the deployment environment for the full lifetime of\nthe agent. For example, agent designers select the best performing\nhyperparameters in Atari by testing each for 200 million frames and then\nreporting results on 200 million frames. In this position paper, we argue and\ndemonstrate the pitfalls of this inappropriate empirical methodology: lifetime\ntuning. We provide empirical evidence to support our position by testing DQN\nand SAC across several of continuing and non-stationary environments with two\nmain findings: (1) lifetime tuning does not allow us to identify algorithms\nthat work well for continual learning -- all algorithms equally succeed; (2)\nrecently developed continual RL algorithms outperform standard non-continual\nalgorithms when tuning is limited to a fraction of the agent's lifetime. The\ngoal of this paper is to provide an explanation for why recent progress in\ncontinual RL has been mixed and motivate the development of empirical practices\nthat better match the goals of continual RL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In continual RL we want agents capable of never-ending learning, and yet our\nevaluation methodologies do not reflect this. The standard practice in RL is to\nassume unfettered access to the deployment environment for the full lifetime of\nthe agent. For example, agent designers select the best performing\nhyperparameters in Atari by testing each for 200 million frames and then\nreporting results on 200 million frames. In this position paper, we argue and\ndemonstrate the pitfalls of this inappropriate empirical methodology: lifetime\ntuning. We provide empirical evidence to support our position by testing DQN\nand SAC across several of continuing and non-stationary environments with two\nmain findings: (1) lifetime tuning does not allow us to identify algorithms\nthat work well for continual learning -- all algorithms equally succeed; (2)\nrecently developed continual RL algorithms outperform standard non-continual\nalgorithms when tuning is limited to a fraction of the agent's lifetime. The\ngoal of this paper is to provide an explanation for why recent progress in\ncontinual RL has been mixed and motivate the development of empirical practices\nthat better match the goals of continual RL."
                },
                "authors": [
                    {
                        "name": "Golnaz Mesbahi"
                    },
                    {
                        "name": "Parham Mohammad Panahi"
                    },
                    {
                        "name": "Olya Mastikhina"
                    },
                    {
                        "name": "Steven Tang"
                    },
                    {
                        "name": "Martha White"
                    },
                    {
                        "name": "Adam White"
                    }
                ],
                "author_detail": {
                    "name": "Adam White"
                },
                "author": "Adam White",
                "arxiv_comment": "ICML 2025, position track: https://icml.cc/virtual/2025/poster/40153",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02113v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02113v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06435v1",
                "updated": "2025-08-08T16:23:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    23,
                    24,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T16:23:24Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    23,
                    24,
                    4,
                    220,
                    0
                ],
                "title": "Learning the Topic, Not the Language: How LLMs Classify Online\n  Immigration Discourse Across Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning the Topic, Not the Language: How LLMs Classify Online\n  Immigration Discourse Across Languages"
                },
                "summary": "Large language models (LLMs) are transforming social-science research by\nenabling scalable, precise analysis. Their adaptability raises the question of\nwhether knowledge acquired through fine-tuning in a few languages can transfer\nto unseen languages that only appeared during pre-training. To examine this, we\nfine-tune lightweight LLaMA 3.2-3B models on monolingual, bilingual, or\nmultilingual data sets to classify immigration-related tweets from X/Twitter\nacross 13 languages, a domain characterised by polarised, culturally specific\ndiscourse. We evaluate whether minimal language-specific fine-tuning enables\ncross-lingual topic detection and whether adding targeted languages corrects\npre-training biases. Results show that LLMs fine-tuned in one or two languages\ncan reliably classify immigration-related content in unseen languages. However,\nidentifying whether a tweet expresses a pro- or anti-immigration stance\nbenefits from multilingual fine-tuning. Pre-training bias favours dominant\nlanguages, but even minimal exposure to under-represented languages during\nfine-tuning (as little as $9.62\\times10^{-11}$ of the original pre-training\ntoken volume) yields significant gains. These findings challenge the assumption\nthat cross-lingual mastery requires extensive multilingual training: limited\nlanguage coverage suffices for topic-level generalisation, and structural\nbiases can be corrected with lightweight interventions. By releasing\n4-bit-quantised, LoRA fine-tuned models, we provide an open-source,\nreproducible alternative to proprietary LLMs that delivers 35 times faster\ninference at just 0.00000989% of the dollar cost of the OpenAI GPT-4o model,\nenabling scalable, inclusive research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are transforming social-science research by\nenabling scalable, precise analysis. Their adaptability raises the question of\nwhether knowledge acquired through fine-tuning in a few languages can transfer\nto unseen languages that only appeared during pre-training. To examine this, we\nfine-tune lightweight LLaMA 3.2-3B models on monolingual, bilingual, or\nmultilingual data sets to classify immigration-related tweets from X/Twitter\nacross 13 languages, a domain characterised by polarised, culturally specific\ndiscourse. We evaluate whether minimal language-specific fine-tuning enables\ncross-lingual topic detection and whether adding targeted languages corrects\npre-training biases. Results show that LLMs fine-tuned in one or two languages\ncan reliably classify immigration-related content in unseen languages. However,\nidentifying whether a tweet expresses a pro- or anti-immigration stance\nbenefits from multilingual fine-tuning. Pre-training bias favours dominant\nlanguages, but even minimal exposure to under-represented languages during\nfine-tuning (as little as $9.62\\times10^{-11}$ of the original pre-training\ntoken volume) yields significant gains. These findings challenge the assumption\nthat cross-lingual mastery requires extensive multilingual training: limited\nlanguage coverage suffices for topic-level generalisation, and structural\nbiases can be corrected with lightweight interventions. By releasing\n4-bit-quantised, LoRA fine-tuned models, we provide an open-source,\nreproducible alternative to proprietary LLMs that delivers 35 times faster\ninference at just 0.00000989% of the dollar cost of the OpenAI GPT-4o model,\nenabling scalable, inclusive research."
                },
                "authors": [
                    {
                        "name": "Andrea Nasuto"
                    },
                    {
                        "name": "Stefano Maria Iacus"
                    },
                    {
                        "name": "Francisco Rowe"
                    },
                    {
                        "name": "Devika Jain"
                    }
                ],
                "author_detail": {
                    "name": "Devika Jain"
                },
                "author": "Devika Jain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06433v1",
                "updated": "2025-08-08T16:20:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    20,
                    56,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T16:20:56Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    20,
                    56,
                    4,
                    220,
                    0
                ],
                "title": "Memp: Exploring Agent Procedural Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memp: Exploring Agent Procedural Memory"
                },
                "summary": "Large Language Models (LLMs) based agents excel at diverse tasks, yet they\nsuffer from brittle procedural memory that is manually engineered or entangled\nin static parameters. In this work, we investigate strategies to endow agents\nwith a learnable, updatable, and lifelong procedural memory. We propose Memp\nthat distills past agent trajectories into both fine-grained, step-by-step\ninstructions and higher-level, script-like abstractions, and explore the impact\nof different strategies for Build, Retrieval, and Update of procedural memory.\nCoupled with a dynamic regimen that continuously updates, corrects, and\ndeprecates its contents, this repository evolves in lockstep with new\nexperience. Empirical evaluation on TravelPlanner and ALFWorld shows that as\nthe memory repository is refined, agents achieve steadily higher success rates\nand greater efficiency on analogous tasks. Moreover, procedural memory built\nfrom a stronger model retains its value: migrating the procedural memory to a\nweaker model yields substantial performance gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) based agents excel at diverse tasks, yet they\nsuffer from brittle procedural memory that is manually engineered or entangled\nin static parameters. In this work, we investigate strategies to endow agents\nwith a learnable, updatable, and lifelong procedural memory. We propose Memp\nthat distills past agent trajectories into both fine-grained, step-by-step\ninstructions and higher-level, script-like abstractions, and explore the impact\nof different strategies for Build, Retrieval, and Update of procedural memory.\nCoupled with a dynamic regimen that continuously updates, corrects, and\ndeprecates its contents, this repository evolves in lockstep with new\nexperience. Empirical evaluation on TravelPlanner and ALFWorld shows that as\nthe memory repository is refined, agents achieve steadily higher success rates\nand greater efficiency on analogous tasks. Moreover, procedural memory built\nfrom a stronger model retains its value: migrating the procedural memory to a\nweaker model yields substantial performance gains."
                },
                "authors": [
                    {
                        "name": "Runnan Fang"
                    },
                    {
                        "name": "Yuan Liang"
                    },
                    {
                        "name": "Xiaobin Wang"
                    },
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Ningyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ningyu Zhang"
                },
                "author": "Ningyu Zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06428v1",
                "updated": "2025-08-08T16:15:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    15,
                    20,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T16:15:20Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    15,
                    20,
                    4,
                    220,
                    0
                ],
                "title": "Full-Dimensional Beamforming for Multi-User MIMO-OFDM ISAC for\n  Low-Altitude UAV with Zero Sensing Resource Allocation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full-Dimensional Beamforming for Multi-User MIMO-OFDM ISAC for\n  Low-Altitude UAV with Zero Sensing Resource Allocation"
                },
                "summary": "Low-altitude unmanned aerial vehicles (UAVs) are expected to play an\nimportant role for low-altitude economy with a wide range of applications like\nprecise agriculture, aerial delivery and surveillance. Integrated sensing and\ncommunication (ISAC) is a key technology to enable the large-scale deployment\nand routine usage of UAVs by providing both communication and sensing services\nefficiently. For UAV ISAC systems, as UAV often acts as both a communication\nuser equipment (UE) and a sensing target, traditional ISAC systems that usually\nallocate dedicated TF resources for sensing are inefficient due to the severe\ndegradation of communication spectral efficiency. To address this issue, in\nthis paper, we propose a novel multiple-input multiple-output (MIMO) orthogonal\nfrequency division multiplexing (OFDM)-based ISAC framework for UAVs that\neliminates the need for dedicated sensing TF resources, achieving zero TF\nsensing overhead. By designing the transmit beamforming to meet the\nrequirements for both communication and sensing tasks, our proposed approach\nenables the communication TF resources to be fully reused for sensing, thereby\nenhancing both the communication sum rate and the sensing performance in terms\nof resolution, unambiguous range, and accuracy. Additionally, we introduce a\nlow-complexity target searching beamforming algorithm and a two-stage\nsuper-resolution sensing algorithm, which ensure efficient implementation.\nSimulation results demonstrate that the proposed MIMO-OFDM-ISAC framework not\nonly improves the communication sum rate but also outperforms traditional ISAC\nsystems in sensing performance, making it a promising solution for future ISAC\nsystems to support low-altitude UAVs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-altitude unmanned aerial vehicles (UAVs) are expected to play an\nimportant role for low-altitude economy with a wide range of applications like\nprecise agriculture, aerial delivery and surveillance. Integrated sensing and\ncommunication (ISAC) is a key technology to enable the large-scale deployment\nand routine usage of UAVs by providing both communication and sensing services\nefficiently. For UAV ISAC systems, as UAV often acts as both a communication\nuser equipment (UE) and a sensing target, traditional ISAC systems that usually\nallocate dedicated TF resources for sensing are inefficient due to the severe\ndegradation of communication spectral efficiency. To address this issue, in\nthis paper, we propose a novel multiple-input multiple-output (MIMO) orthogonal\nfrequency division multiplexing (OFDM)-based ISAC framework for UAVs that\neliminates the need for dedicated sensing TF resources, achieving zero TF\nsensing overhead. By designing the transmit beamforming to meet the\nrequirements for both communication and sensing tasks, our proposed approach\nenables the communication TF resources to be fully reused for sensing, thereby\nenhancing both the communication sum rate and the sensing performance in terms\nof resolution, unambiguous range, and accuracy. Additionally, we introduce a\nlow-complexity target searching beamforming algorithm and a two-stage\nsuper-resolution sensing algorithm, which ensure efficient implementation.\nSimulation results demonstrate that the proposed MIMO-OFDM-ISAC framework not\nonly improves the communication sum rate but also outperforms traditional ISAC\nsystems in sensing performance, making it a promising solution for future ISAC\nsystems to support low-altitude UAVs."
                },
                "authors": [
                    {
                        "name": "Zhiwen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zhiwen Zhou"
                },
                "author": "Zhiwen Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00900v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00900v2",
                "updated": "2025-08-08T16:07:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    7,
                    40,
                    4,
                    220,
                    0
                ],
                "published": "2024-07-01T01:56:28Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    1,
                    56,
                    28,
                    0,
                    183,
                    0
                ],
                "title": "From Next-Token to Mathematics: The Learning Dynamics of Mathematical\n  Reasoning in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Next-Token to Mathematics: The Learning Dynamics of Mathematical\n  Reasoning in Language Models"
                },
                "summary": "Large Language Models (LLMs) solely trained on next-token prediction learn to\nsolve a wide range of problems involving mathematical reasoning. But how does\nthis ability evolve during training? We show the first analysis of how\nmathematical reasoning abilities of several open-weight LLMs develop during\npre-training and post-training. To this end, we construct MathCAMPS, a\nsynthetic dataset of novel mathematical reasoning problems grounded in 44\nfine-grained skills taken from the Common Core curriculum from K to 8th grades.\nIn one experiment, we show that mathematical skills are learned during\npre-training in an order that measurably correlates with the human-designed\ncurriculum, even though training data are randomly ordered. We also show a\ndetailed analysis of which mathematical abilities benefit from instruction\ntuning, a widely used post-training method and, in contrast, which skills\nsuffer. Our work paves the way for an empirical understanding of LLM training\ndynamics in relation to reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) solely trained on next-token prediction learn to\nsolve a wide range of problems involving mathematical reasoning. But how does\nthis ability evolve during training? We show the first analysis of how\nmathematical reasoning abilities of several open-weight LLMs develop during\npre-training and post-training. To this end, we construct MathCAMPS, a\nsynthetic dataset of novel mathematical reasoning problems grounded in 44\nfine-grained skills taken from the Common Core curriculum from K to 8th grades.\nIn one experiment, we show that mathematical skills are learned during\npre-training in an order that measurably correlates with the human-designed\ncurriculum, even though training data are randomly ordered. We also show a\ndetailed analysis of which mathematical abilities benefit from instruction\ntuning, a widely used post-training method and, in contrast, which skills\nsuffer. Our work paves the way for an empirical understanding of LLM training\ndynamics in relation to reasoning."
                },
                "authors": [
                    {
                        "name": "Shubhra Mishra"
                    },
                    {
                        "name": "Gabriel Poesia"
                    },
                    {
                        "name": "Noah D. Goodman"
                    }
                ],
                "author_detail": {
                    "name": "Noah D. Goodman"
                },
                "author": "Noah D. Goodman",
                "arxiv_comment": "Accepted to COLM 2025. Dataset and code:\n  https://github.com/gpoesia/mathcamps/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00900v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00900v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05341v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05341v2",
                "updated": "2025-08-08T16:06:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    6,
                    23,
                    4,
                    220,
                    0
                ],
                "published": "2025-02-07T21:26:51Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    21,
                    26,
                    51,
                    4,
                    38,
                    0
                ],
                "title": "Neural Encrypted State Transduction for Ransomware Classification: A\n  Novel Approach Using Cryptographic Flow Residuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Encrypted State Transduction for Ransomware Classification: A\n  Novel Approach Using Cryptographic Flow Residuals"
                },
                "summary": "Encrypted behavioral patterns provide a unique avenue for classifying complex\ndigital threats without reliance on explicit feature extraction, enabling\ndetection frameworks to remain effective even when conventional static and\nbehavioral methodologies fail. A novel approach based on Neural Encrypted State\nTransduction (NEST) is introduced to analyze cryptographic flow residuals and\nclassify threats through their encrypted state transitions, mitigating evasion\ntactics employed through polymorphic and obfuscated attack strategies. The\nmathematical formulation of NEST leverages transduction principles to map state\ntransitions dynamically, enabling high-confidence classification without\nrequiring direct access to decrypted execution traces. Experimental evaluations\ndemonstrate that the proposed framework achieves improved detection accuracy\nacross multiple ransomware families while exhibiting resilience against\nadversarial perturbations and previously unseen attack variants. The model\nmaintains competitive processing efficiency, offering a practical balance\nbetween classification performance and computational resource constraints,\nmaking it suitable for large-scale security deployments. Comparative\nassessments reveal that NEST consistently outperforms baseline classification\nmodels, particularly in detecting ransomware samples employing delayed\nencryption, entropy-based obfuscation, and memory-resident execution\ntechniques. The capacity to generalize across diverse execution environments\nreinforces the applicability of encrypted transduction methodologies in\nadversarial classification tasks beyond conventional malware detection\npipelines. The integration of residual learning mechanisms within the\ntransduction layers further enhances classification robustness, minimizing both\nfalse positives and misclassification rates across varied operational contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Encrypted behavioral patterns provide a unique avenue for classifying complex\ndigital threats without reliance on explicit feature extraction, enabling\ndetection frameworks to remain effective even when conventional static and\nbehavioral methodologies fail. A novel approach based on Neural Encrypted State\nTransduction (NEST) is introduced to analyze cryptographic flow residuals and\nclassify threats through their encrypted state transitions, mitigating evasion\ntactics employed through polymorphic and obfuscated attack strategies. The\nmathematical formulation of NEST leverages transduction principles to map state\ntransitions dynamically, enabling high-confidence classification without\nrequiring direct access to decrypted execution traces. Experimental evaluations\ndemonstrate that the proposed framework achieves improved detection accuracy\nacross multiple ransomware families while exhibiting resilience against\nadversarial perturbations and previously unseen attack variants. The model\nmaintains competitive processing efficiency, offering a practical balance\nbetween classification performance and computational resource constraints,\nmaking it suitable for large-scale security deployments. Comparative\nassessments reveal that NEST consistently outperforms baseline classification\nmodels, particularly in detecting ransomware samples employing delayed\nencryption, entropy-based obfuscation, and memory-resident execution\ntechniques. The capacity to generalize across diverse execution environments\nreinforces the applicability of encrypted transduction methodologies in\nadversarial classification tasks beyond conventional malware detection\npipelines. The integration of residual learning mechanisms within the\ntransduction layers further enhances classification robustness, minimizing both\nfalse positives and misclassification rates across varied operational contexts."
                },
                "authors": [
                    {
                        "name": "Barnaby Fortescue"
                    },
                    {
                        "name": "Edmund Hawksmoor"
                    },
                    {
                        "name": "Alistair Wetherington"
                    },
                    {
                        "name": "Frederick Marlowe"
                    },
                    {
                        "name": "Kevin Pekepok"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Pekepok"
                },
                "author": "Kevin Pekepok",
                "arxiv_comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05341v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05341v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06418v1",
                "updated": "2025-08-08T16:05:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    5,
                    27,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T16:05:27Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    5,
                    27,
                    4,
                    220,
                    0
                ],
                "title": "Quantifying Conversation Drift in MCP via Latent Polytope",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying Conversation Drift in MCP via Latent Polytope"
                },
                "summary": "The Model Context Protocol (MCP) enhances large language models (LLMs) by\nintegrating external tools, enabling dynamic aggregation of real-time data to\nimprove task execution. However, its non-isolated execution context introduces\ncritical security and privacy risks. In particular, adversarially crafted\ncontent can induce tool poisoning or indirect prompt injection, leading to\nconversation hijacking, misinformation propagation, or data exfiltration.\nExisting defenses, such as rule-based filters or LLM-driven detection, remain\ninadequate due to their reliance on static signatures, computational\ninefficiency, and inability to quantify conversational hijacking. To address\nthese limitations, we propose SecMCP, a secure framework that detects and\nquantifies conversation drift, deviations in latent space trajectories induced\nby adversarial external knowledge. By modeling LLM activation vectors within a\nlatent polytope space, SecMCP identifies anomalous shifts in conversational\ndynamics, enabling proactive detection of hijacking, misleading, and data\nexfiltration. We evaluate SecMCP on three state-of-the-art LLMs (Llama3,\nVicuna, Mistral) across benchmark datasets (MS MARCO, HotpotQA, FinQA),\ndemonstrating robust detection with AUROC scores exceeding 0.915 while\nmaintaining system usability. Our contributions include a systematic\ncategorization of MCP security threats, a novel latent polytope-based\nmethodology for quantifying conversation drift, and empirical validation of\nSecMCP's efficacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Model Context Protocol (MCP) enhances large language models (LLMs) by\nintegrating external tools, enabling dynamic aggregation of real-time data to\nimprove task execution. However, its non-isolated execution context introduces\ncritical security and privacy risks. In particular, adversarially crafted\ncontent can induce tool poisoning or indirect prompt injection, leading to\nconversation hijacking, misinformation propagation, or data exfiltration.\nExisting defenses, such as rule-based filters or LLM-driven detection, remain\ninadequate due to their reliance on static signatures, computational\ninefficiency, and inability to quantify conversational hijacking. To address\nthese limitations, we propose SecMCP, a secure framework that detects and\nquantifies conversation drift, deviations in latent space trajectories induced\nby adversarial external knowledge. By modeling LLM activation vectors within a\nlatent polytope space, SecMCP identifies anomalous shifts in conversational\ndynamics, enabling proactive detection of hijacking, misleading, and data\nexfiltration. We evaluate SecMCP on three state-of-the-art LLMs (Llama3,\nVicuna, Mistral) across benchmark datasets (MS MARCO, HotpotQA, FinQA),\ndemonstrating robust detection with AUROC scores exceeding 0.915 while\nmaintaining system usability. Our contributions include a systematic\ncategorization of MCP security threats, a novel latent polytope-based\nmethodology for quantifying conversation drift, and empirical validation of\nSecMCP's efficacy."
                },
                "authors": [
                    {
                        "name": "Haoran Shi"
                    },
                    {
                        "name": "Hongwei Yao"
                    },
                    {
                        "name": "Shuo Shao"
                    },
                    {
                        "name": "Shaopeng Jiao"
                    },
                    {
                        "name": "Ziqi Peng"
                    },
                    {
                        "name": "Zhan Qin"
                    },
                    {
                        "name": "Cong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Cong Wang"
                },
                "author": "Cong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00048v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00048v2",
                "updated": "2025-08-08T16:05:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    5,
                    0,
                    4,
                    220,
                    0
                ],
                "published": "2025-01-28T11:50:35Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    11,
                    50,
                    35,
                    1,
                    28,
                    0
                ],
                "title": "Contextually Entangled Gradient Mapping for Optimized LLM Comprehension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextually Entangled Gradient Mapping for Optimized LLM Comprehension"
                },
                "summary": "Contextually Entangled Gradient Mapping (CEGM) introduces a new approach to\ngradient optimization, redefining the relationship between contextual\nembeddings and gradient updates to enhance semantic coherence and reasoning\ncapabilities in neural architectures. By treating gradients as dynamic carriers\nof contextual dependencies rather than isolated numerical entities, the\nproposed methodology bridges critical gaps in existing optimization strategies.\nThe integration of entangled gradient dynamics into a loss regularization\nframework demonstrated significant improvements in tasks involving long-form\nreasoning, contextual retention, and adaptability to unseen domains.\nExperimental evaluations showed that the CEGM-enhanced model consistently\noutperformed baseline approaches, achieving higher accuracy in token-level\npredictions and greater resilience to noisy inputs. Practical implementations\ninvolved modifications to training pipelines, introducing entanglement layers\nand dynamic coefficient adjustments that seamlessly align with existing\narchitectures. Results further highlighted reductions in semantic drift during\nsequential transformations and improvements in embedding coherence across\nparaphrased sentences, showing the robustness and versatility of the proposed\nmethodology. The findings demonstrate the broader implications of gradient\nentanglement for both theoretical advancements and practical applications in\noptimization strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextually Entangled Gradient Mapping (CEGM) introduces a new approach to\ngradient optimization, redefining the relationship between contextual\nembeddings and gradient updates to enhance semantic coherence and reasoning\ncapabilities in neural architectures. By treating gradients as dynamic carriers\nof contextual dependencies rather than isolated numerical entities, the\nproposed methodology bridges critical gaps in existing optimization strategies.\nThe integration of entangled gradient dynamics into a loss regularization\nframework demonstrated significant improvements in tasks involving long-form\nreasoning, contextual retention, and adaptability to unseen domains.\nExperimental evaluations showed that the CEGM-enhanced model consistently\noutperformed baseline approaches, achieving higher accuracy in token-level\npredictions and greater resilience to noisy inputs. Practical implementations\ninvolved modifications to training pipelines, introducing entanglement layers\nand dynamic coefficient adjustments that seamlessly align with existing\narchitectures. Results further highlighted reductions in semantic drift during\nsequential transformations and improvements in embedding coherence across\nparaphrased sentences, showing the robustness and versatility of the proposed\nmethodology. The findings demonstrate the broader implications of gradient\nentanglement for both theoretical advancements and practical applications in\noptimization strategies."
                },
                "authors": [
                    {
                        "name": "Colin Sisate"
                    },
                    {
                        "name": "Alistair Goldfinch"
                    },
                    {
                        "name": "Vincent Waterstone"
                    },
                    {
                        "name": "Sebastian Kingsley"
                    },
                    {
                        "name": "Mariana Blackthorn"
                    }
                ],
                "author_detail": {
                    "name": "Mariana Blackthorn"
                },
                "author": "Mariana Blackthorn",
                "arxiv_comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00048v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00048v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11417v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11417v2",
                "updated": "2025-08-08T16:03:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    3,
                    41,
                    4,
                    220,
                    0
                ],
                "published": "2025-01-20T11:34:28Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    11,
                    34,
                    28,
                    0,
                    20,
                    0
                ],
                "title": "Neural Contextual Reinforcement Framework for Logical Structure Language\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Contextual Reinforcement Framework for Logical Structure Language\n  Generation"
                },
                "summary": "The Neural Contextual Reinforcement Framework introduces an innovative\napproach to enhancing the logical coherence and structural consistency of text\ngenerated by large language models. Leveraging reinforcement learning\nprinciples, the framework integrates custom reward functions and dynamic\ncontext alignment mechanisms to address challenges inherent in maintaining\nlong-range dependencies across extended sequences. The architecture\nincorporates multi-head attention layers and hierarchical encoding modules,\nenabling the model to produce outputs that align closely with human\nexpectations of logical structure and semantic flow. Quantitative evaluations\nacross diverse datasets demonstrate substantial improvements in coherence\nmetrics, perplexity reduction, and semantic alignment, showcasing the\nframework's ability to outperform baseline models in both general and\ndomain-specific tasks. Qualitative analyses further highlight the framework's\ncapacity to generate text with improved narrative clarity and reduced\nredundancy, reflecting its effectiveness in balancing fluency with structural\nprecision. In addition to its performance gains, the framework exhibits\nrobustness in handling noisy input data and scalability across varying model\nsizes, reinforcing its versatility in practical applications. Experimental\nresults reveal that optimal context window sizes significantly influence\ncoherence outcomes, showing the importance of architectural flexibility in\nadapting to diverse linguistic structures. Cross-lingual performance\nevaluations affirm the framework's adaptability to multiple languages,\nextending its utility beyond monolingual contexts. Resource efficiency analyses\nindicate a reduction in computational overhead compared to traditional\napproaches, emphasizing the practicality of the framework for large-scale\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Neural Contextual Reinforcement Framework introduces an innovative\napproach to enhancing the logical coherence and structural consistency of text\ngenerated by large language models. Leveraging reinforcement learning\nprinciples, the framework integrates custom reward functions and dynamic\ncontext alignment mechanisms to address challenges inherent in maintaining\nlong-range dependencies across extended sequences. The architecture\nincorporates multi-head attention layers and hierarchical encoding modules,\nenabling the model to produce outputs that align closely with human\nexpectations of logical structure and semantic flow. Quantitative evaluations\nacross diverse datasets demonstrate substantial improvements in coherence\nmetrics, perplexity reduction, and semantic alignment, showcasing the\nframework's ability to outperform baseline models in both general and\ndomain-specific tasks. Qualitative analyses further highlight the framework's\ncapacity to generate text with improved narrative clarity and reduced\nredundancy, reflecting its effectiveness in balancing fluency with structural\nprecision. In addition to its performance gains, the framework exhibits\nrobustness in handling noisy input data and scalability across varying model\nsizes, reinforcing its versatility in practical applications. Experimental\nresults reveal that optimal context window sizes significantly influence\ncoherence outcomes, showing the importance of architectural flexibility in\nadapting to diverse linguistic structures. Cross-lingual performance\nevaluations affirm the framework's adaptability to multiple languages,\nextending its utility beyond monolingual contexts. Resource efficiency analyses\nindicate a reduction in computational overhead compared to traditional\napproaches, emphasizing the practicality of the framework for large-scale\ndeployment."
                },
                "authors": [
                    {
                        "name": "Marcus Irvin"
                    },
                    {
                        "name": "William Cooper"
                    },
                    {
                        "name": "Edward Hughes"
                    },
                    {
                        "name": "Jessica Morgan"
                    },
                    {
                        "name": "Christopher Hamilton"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Hamilton"
                },
                "author": "Christopher Hamilton",
                "arxiv_comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11417v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11417v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06414v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06414v1",
                "updated": "2025-08-08T15:58:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    58,
                    11,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T15:58:11Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    58,
                    11,
                    4,
                    220,
                    0
                ],
                "title": "What Builds Effective In-Context Examples for Code Generation?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Builds Effective In-Context Examples for Code Generation?"
                },
                "summary": "In-Context Learning (ICL) has emerged as a promising solution to enhance the\ncode generation capabilities of Large Language Models (LLMs), which\nincorporates code examples inside the prompt to let LLMs learn from\ndemonstrations. However, despite the substantial effectiveness of the code\nexample-based ICL approach, the specific features (e.g., identifier naming\nstyles, code formatting, solution insight) within the ICL-provided code\nexamples that significantly contribute to the ICL's effectiveness remain\nunclear. This paper systematically investigates the impact of various code\nfeatures on ICL with code examples through controlled ablation studies. Our\nfindings reveal that the appropriate naming of variables and functions is\ncrucial for effective code generation, with their elimination leading to\nperformance decreases of up to 30 percentage points. We further demonstrate\nthat LLMs prioritize semantically meaningful identifier names over formatting\nconventions, with language-specific preferences regarding identifier verbosity.\nAdditionally, our investigation into ICL's potential for enhancing reflection\nand inference capabilities reveals that current LLMs struggle to extract\ngeneralizable problem-solving insights from similar code solutions, despite\nbeing capable of utilizing direct information effectively. These findings are\nexpected to provide valuable insights for optimizing ICL systems in code\ngeneration applications and highlight fundamental challenges in\nreflection-based learning for code generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Learning (ICL) has emerged as a promising solution to enhance the\ncode generation capabilities of Large Language Models (LLMs), which\nincorporates code examples inside the prompt to let LLMs learn from\ndemonstrations. However, despite the substantial effectiveness of the code\nexample-based ICL approach, the specific features (e.g., identifier naming\nstyles, code formatting, solution insight) within the ICL-provided code\nexamples that significantly contribute to the ICL's effectiveness remain\nunclear. This paper systematically investigates the impact of various code\nfeatures on ICL with code examples through controlled ablation studies. Our\nfindings reveal that the appropriate naming of variables and functions is\ncrucial for effective code generation, with their elimination leading to\nperformance decreases of up to 30 percentage points. We further demonstrate\nthat LLMs prioritize semantically meaningful identifier names over formatting\nconventions, with language-specific preferences regarding identifier verbosity.\nAdditionally, our investigation into ICL's potential for enhancing reflection\nand inference capabilities reveals that current LLMs struggle to extract\ngeneralizable problem-solving insights from similar code solutions, despite\nbeing capable of utilizing direct information effectively. These findings are\nexpected to provide valuable insights for optimizing ICL systems in code\ngeneration applications and highlight fundamental challenges in\nreflection-based learning for code generation tasks."
                },
                "authors": [
                    {
                        "name": "Dongze Li"
                    },
                    {
                        "name": "Songqiang Chen"
                    },
                    {
                        "name": "Jialun Cao"
                    },
                    {
                        "name": "Shing-Chi Cheung"
                    }
                ],
                "author_detail": {
                    "name": "Shing-Chi Cheung"
                },
                "author": "Shing-Chi Cheung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06414v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06414v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06412v1",
                "updated": "2025-08-08T15:56:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    56,
                    49,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T15:56:49Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    56,
                    49,
                    4,
                    220,
                    0
                ],
                "title": "Sample-efficient LLM Optimization with Reset Replay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sample-efficient LLM Optimization with Reset Replay"
                },
                "summary": "Recent advancements in post-training Large Language Models (LLMs),\nparticularly through Reinforcement Learning (RL) and preference optimization\nmethods, are key drivers for enhancing their reasoning capabilities. However,\nthese methods are often plagued by low sample efficiency and a susceptibility\nto primacy bias, where overfitting to initial experiences degrades policy\nquality and damages the learning process. To address these challenges, we\nintroduce LLM optimization with Reset Replay (LoRR), a general and powerful\nplugin designed to enhance sample efficiency in any preference-based\noptimization framework. LoRR core mechanism enables training at a high replay\nnumber, maximizing the utility of each collected data batch. To counteract the\nrisk of overfitting inherent in high-replay training, LoRR incorporates a\nperiodic reset strategy with reusing initial data, which preserves network\nplasticity. Furthermore, it leverages a hybrid optimization objective,\ncombining supervised fine-tuning (SFT) and preference-based losses to further\nbolster data exploitation. Our extensive experiments demonstrate that LoRR\nsignificantly boosts the performance of various preference optimization methods\non both mathematical and general reasoning benchmarks. Notably, an iterative\nDPO approach augmented with LoRR achieves comparable performance on challenging\nmath tasks, outperforming some complex and computationally intensive RL-based\nalgorithms. These findings highlight that LoRR offers a practical,\nsample-efficient, and highly effective paradigm for LLM finetuning, unlocking\ngreater performance from limited data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in post-training Large Language Models (LLMs),\nparticularly through Reinforcement Learning (RL) and preference optimization\nmethods, are key drivers for enhancing their reasoning capabilities. However,\nthese methods are often plagued by low sample efficiency and a susceptibility\nto primacy bias, where overfitting to initial experiences degrades policy\nquality and damages the learning process. To address these challenges, we\nintroduce LLM optimization with Reset Replay (LoRR), a general and powerful\nplugin designed to enhance sample efficiency in any preference-based\noptimization framework. LoRR core mechanism enables training at a high replay\nnumber, maximizing the utility of each collected data batch. To counteract the\nrisk of overfitting inherent in high-replay training, LoRR incorporates a\nperiodic reset strategy with reusing initial data, which preserves network\nplasticity. Furthermore, it leverages a hybrid optimization objective,\ncombining supervised fine-tuning (SFT) and preference-based losses to further\nbolster data exploitation. Our extensive experiments demonstrate that LoRR\nsignificantly boosts the performance of various preference optimization methods\non both mathematical and general reasoning benchmarks. Notably, an iterative\nDPO approach augmented with LoRR achieves comparable performance on challenging\nmath tasks, outperforming some complex and computationally intensive RL-based\nalgorithms. These findings highlight that LoRR offers a practical,\nsample-efficient, and highly effective paradigm for LLM finetuning, unlocking\ngreater performance from limited data."
                },
                "authors": [
                    {
                        "name": "Zichuan Liu"
                    },
                    {
                        "name": "Jinyu Wang"
                    },
                    {
                        "name": "Lei Song"
                    },
                    {
                        "name": "Jiang Bian"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Bian"
                },
                "author": "Jiang Bian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06407v1",
                "updated": "2025-08-08T15:50:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    50,
                    40,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T15:50:40Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    50,
                    40,
                    4,
                    220,
                    0
                ],
                "title": "A Classification-Aware Super-Resolution Framework for Ship Targets in\n  SAR Imagery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Classification-Aware Super-Resolution Framework for Ship Targets in\n  SAR Imagery"
                },
                "summary": "High-resolution imagery plays a critical role in improving the performance of\nvisual recognition tasks such as classification, detection, and segmentation.\nIn many domains, including remote sensing and surveillance, low-resolution\nimages can limit the accuracy of automated analysis. To address this,\nsuper-resolution (SR) techniques have been widely adopted to attempt to\nreconstruct high-resolution images from low-resolution inputs. Related\ntraditional approaches focus solely on enhancing image quality based on\npixel-level metrics, leaving the relationship between super-resolved image\nfidelity and downstream classification performance largely underexplored. This\nraises a key question: can integrating classification objectives directly into\nthe super-resolution process further improve classification accuracy? In this\npaper, we try to respond to this question by investigating the relationship\nbetween super-resolution and classification through the deployment of a\nspecialised algorithmic strategy. We propose a novel methodology that increases\nthe resolution of synthetic aperture radar imagery by optimising loss functions\nthat account for both image quality and classification performance. Our\napproach improves image quality, as measured by scientifically ascertained\nimage quality indicators, while also enhancing classification accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-resolution imagery plays a critical role in improving the performance of\nvisual recognition tasks such as classification, detection, and segmentation.\nIn many domains, including remote sensing and surveillance, low-resolution\nimages can limit the accuracy of automated analysis. To address this,\nsuper-resolution (SR) techniques have been widely adopted to attempt to\nreconstruct high-resolution images from low-resolution inputs. Related\ntraditional approaches focus solely on enhancing image quality based on\npixel-level metrics, leaving the relationship between super-resolved image\nfidelity and downstream classification performance largely underexplored. This\nraises a key question: can integrating classification objectives directly into\nthe super-resolution process further improve classification accuracy? In this\npaper, we try to respond to this question by investigating the relationship\nbetween super-resolution and classification through the deployment of a\nspecialised algorithmic strategy. We propose a novel methodology that increases\nthe resolution of synthetic aperture radar imagery by optimising loss functions\nthat account for both image quality and classification performance. Our\napproach improves image quality, as measured by scientifically ascertained\nimage quality indicators, while also enhancing classification accuracy."
                },
                "authors": [
                    {
                        "name": "Ch Muhammad Awais"
                    },
                    {
                        "name": "Marco Reggiannini"
                    },
                    {
                        "name": "Davide Moroni"
                    },
                    {
                        "name": "Oktay Karakus"
                    }
                ],
                "author_detail": {
                    "name": "Oktay Karakus"
                },
                "author": "Oktay Karakus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14848v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14848v2",
                "updated": "2025-08-08T15:49:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    49,
                    43,
                    4,
                    220,
                    0
                ],
                "published": "2025-05-20T19:29:05Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    19,
                    29,
                    5,
                    1,
                    140,
                    0
                ],
                "title": "MAATS: A Multi-Agent Automated Translation System Based on MQM\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAATS: A Multi-Agent Automated Translation System Based on MQM\n  Evaluation"
                },
                "summary": "We present MAATS, a Multi Agent Automated Translation System that leverages\nthe Multidimensional Quality Metrics (MQM) framework as a fine-grained signal\nfor error detection and refinement. MAATS employs multiple specialized AI\nagents, each focused on a distinct MQM category (e.g., Accuracy, Fluency,\nStyle, Terminology), followed by a synthesis agent that integrates the\nannotations to iteratively refine translations. This design contrasts with\nconventional single-agent methods that rely on self-correction.\n  Evaluated across diverse language pairs and Large Language Models (LLMs),\nMAATS outperforms zero-shot and single-agent baselines with statistically\nsignificant gains in both automatic metrics and human assessments. It excels\nparticularly in semantic accuracy, locale adaptation, and linguistically\ndistant language pairs. Qualitative analysis highlights its strengths in\nmulti-layered error diagnosis, omission detection across perspectives, and\ncontext-aware refinement. By aligning modular agent roles with interpretable\nMQM dimensions, MAATS narrows the gap between black-box LLMs and human\ntranslation workflows, shifting focus from surface fluency to deeper semantic\nand contextual fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present MAATS, a Multi Agent Automated Translation System that leverages\nthe Multidimensional Quality Metrics (MQM) framework as a fine-grained signal\nfor error detection and refinement. MAATS employs multiple specialized AI\nagents, each focused on a distinct MQM category (e.g., Accuracy, Fluency,\nStyle, Terminology), followed by a synthesis agent that integrates the\nannotations to iteratively refine translations. This design contrasts with\nconventional single-agent methods that rely on self-correction.\n  Evaluated across diverse language pairs and Large Language Models (LLMs),\nMAATS outperforms zero-shot and single-agent baselines with statistically\nsignificant gains in both automatic metrics and human assessments. It excels\nparticularly in semantic accuracy, locale adaptation, and linguistically\ndistant language pairs. Qualitative analysis highlights its strengths in\nmulti-layered error diagnosis, omission detection across perspectives, and\ncontext-aware refinement. By aligning modular agent roles with interpretable\nMQM dimensions, MAATS narrows the gap between black-box LLMs and human\ntranslation workflows, shifting focus from surface fluency to deeper semantic\nand contextual fidelity."
                },
                "authors": [
                    {
                        "name": "George Wang"
                    },
                    {
                        "name": "Jiaqian Hu"
                    },
                    {
                        "name": "Safinah Ali"
                    }
                ],
                "author_detail": {
                    "name": "Safinah Ali"
                },
                "author": "Safinah Ali",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14848v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14848v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09580v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09580v2",
                "updated": "2025-08-08T15:47:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    47,
                    59,
                    4,
                    220,
                    0
                ],
                "published": "2025-07-13T11:11:01Z",
                "published_parsed": [
                    2025,
                    7,
                    13,
                    11,
                    11,
                    1,
                    6,
                    194,
                    0
                ],
                "title": "AICrypto: A Comprehensive Benchmark For Evaluating Cryptography\n  Capabilities of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AICrypto: A Comprehensive Benchmark For Evaluating Cryptography\n  Capabilities of Large Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na variety of domains. However, their applications in cryptography, which serves\nas a foundational pillar of cybersecurity, remain largely unexplored. To\naddress this gap, we propose \\textbf{AICrypto}, the first comprehensive\nbenchmark designed to evaluate the cryptographic capabilities of LLMs. The\nbenchmark comprises 135 multiple-choice questions, 150 capture-the-flag (CTF)\nchallenges, and 18 proof problems, covering a broad range of skills from\nfactual memorization to vulnerability exploitation and formal reasoning. All\ntasks are carefully reviewed or constructed by cryptography experts to ensure\ncorrectness and rigor. To support automated evaluation of CTF challenges, we\ndesign an agent-based framework. To gain deeper insight into the current state\nof cryptographic proficiency in LLMs, we introduce human expert performance\nbaselines for comparison across all task types. Our evaluation of 17 leading\nLLMs reveals that state-of-the-art models match or even surpass human experts\nin memorizing cryptographic concepts, exploiting common vulnerabilities, and\nroutine proofs. However, they still lack a deep understanding of abstract\nmathematical concepts and struggle with tasks that require multi-step reasoning\nand dynamic analysis. We hope this work could provide insights for future\nresearch on LLMs in cryptographic applications. Our code and dataset are\navailable at https://aicryptobench.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities across\na variety of domains. However, their applications in cryptography, which serves\nas a foundational pillar of cybersecurity, remain largely unexplored. To\naddress this gap, we propose \\textbf{AICrypto}, the first comprehensive\nbenchmark designed to evaluate the cryptographic capabilities of LLMs. The\nbenchmark comprises 135 multiple-choice questions, 150 capture-the-flag (CTF)\nchallenges, and 18 proof problems, covering a broad range of skills from\nfactual memorization to vulnerability exploitation and formal reasoning. All\ntasks are carefully reviewed or constructed by cryptography experts to ensure\ncorrectness and rigor. To support automated evaluation of CTF challenges, we\ndesign an agent-based framework. To gain deeper insight into the current state\nof cryptographic proficiency in LLMs, we introduce human expert performance\nbaselines for comparison across all task types. Our evaluation of 17 leading\nLLMs reveals that state-of-the-art models match or even surpass human experts\nin memorizing cryptographic concepts, exploiting common vulnerabilities, and\nroutine proofs. However, they still lack a deep understanding of abstract\nmathematical concepts and struggle with tasks that require multi-step reasoning\nand dynamic analysis. We hope this work could provide insights for future\nresearch on LLMs in cryptographic applications. Our code and dataset are\navailable at https://aicryptobench.github.io."
                },
                "authors": [
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Yijian Liu"
                    },
                    {
                        "name": "Liheng Ji"
                    },
                    {
                        "name": "Han Luo"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Xiaofei Zhou"
                    },
                    {
                        "name": "Chiyun Feng"
                    },
                    {
                        "name": "Puji Wang"
                    },
                    {
                        "name": "Yuhan Cao"
                    },
                    {
                        "name": "Geyuan Zhang"
                    },
                    {
                        "name": "Xiaojian Li"
                    },
                    {
                        "name": "Rongwu Xu"
                    },
                    {
                        "name": "Yilei Chen"
                    },
                    {
                        "name": "Tianxing He"
                    }
                ],
                "author_detail": {
                    "name": "Tianxing He"
                },
                "author": "Tianxing He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09580v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09580v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06406v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06406v1",
                "updated": "2025-08-08T15:47:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    47,
                    55,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T15:47:55Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    47,
                    55,
                    4,
                    220,
                    0
                ],
                "title": "Blockchain-Enabled Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blockchain-Enabled Federated Learning"
                },
                "summary": "Blockchain-enabled federated learning (BCFL) addresses fundamental challenges\nof trust, privacy, and coordination in collaborative AI systems. This chapter\nprovides comprehensive architectural analysis of BCFL systems through a\nsystematic four-dimensional taxonomy examining coordination structures,\nconsensus mechanisms, storage architectures, and trust models. We analyze\ndesign patterns from blockchain-verified centralized coordination to fully\ndecentralized peer-to-peer networks, evaluating trade-offs in scalability,\nsecurity, and performance. Through detailed examination of consensus mechanisms\ndesigned for federated learning contexts, including Proof of Quality and Proof\nof Federated Learning, we demonstrate how computational work can be repurposed\nfrom arbitrary cryptographic puzzles to productive machine learning tasks. The\nchapter addresses critical storage challenges by examining multi-tier\narchitectures that balance blockchain's transaction constraints with neural\nnetworks' large parameter requirements while maintaining cryptographic\nintegrity. A technical case study of the TrustMesh framework illustrates\npractical implementation considerations in BCFL systems through distributed\nimage classification training, demonstrating effective collaborative learning\nacross IoT devices with highly non-IID data distributions while maintaining\ncomplete transparency and fault tolerance. Analysis of real-world deployments\nacross healthcare consortiums, financial services, and IoT security\napplications validates the practical viability of BCFL systems, achieving\nperformance comparable to centralized approaches while providing enhanced\nsecurity guarantees and enabling new models of trustless collaborative\nintelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blockchain-enabled federated learning (BCFL) addresses fundamental challenges\nof trust, privacy, and coordination in collaborative AI systems. This chapter\nprovides comprehensive architectural analysis of BCFL systems through a\nsystematic four-dimensional taxonomy examining coordination structures,\nconsensus mechanisms, storage architectures, and trust models. We analyze\ndesign patterns from blockchain-verified centralized coordination to fully\ndecentralized peer-to-peer networks, evaluating trade-offs in scalability,\nsecurity, and performance. Through detailed examination of consensus mechanisms\ndesigned for federated learning contexts, including Proof of Quality and Proof\nof Federated Learning, we demonstrate how computational work can be repurposed\nfrom arbitrary cryptographic puzzles to productive machine learning tasks. The\nchapter addresses critical storage challenges by examining multi-tier\narchitectures that balance blockchain's transaction constraints with neural\nnetworks' large parameter requirements while maintaining cryptographic\nintegrity. A technical case study of the TrustMesh framework illustrates\npractical implementation considerations in BCFL systems through distributed\nimage classification training, demonstrating effective collaborative learning\nacross IoT devices with highly non-IID data distributions while maintaining\ncomplete transparency and fault tolerance. Analysis of real-world deployments\nacross healthcare consortiums, financial services, and IoT security\napplications validates the practical viability of BCFL systems, achieving\nperformance comparable to centralized approaches while providing enhanced\nsecurity guarantees and enabling new models of trustless collaborative\nintelligence."
                },
                "authors": [
                    {
                        "name": "Murtaza Rangwala"
                    },
                    {
                        "name": "Venugopal K R"
                    },
                    {
                        "name": "Rajkumar Buyya"
                    }
                ],
                "author_detail": {
                    "name": "Rajkumar Buyya"
                },
                "author": "Rajkumar Buyya",
                "arxiv_comment": "32 pages, 6 figures, chapter for edited book (Federated Learning:\n  Foundations and Applications)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06406v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06406v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02622v2",
                "updated": "2025-08-08T15:44:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    44,
                    11,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-04T17:10:08Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    17,
                    10,
                    8,
                    0,
                    216,
                    0
                ],
                "title": "Noosemia: toward a Cognitive and Phenomenological Account of\n  Intentionality Attribution in Human-Generative AI Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noosemia: toward a Cognitive and Phenomenological Account of\n  Intentionality Attribution in Human-Generative AI Interaction"
                },
                "summary": "This paper introduces and formalizes Noosem\\`ia, a novel\ncognitive-phenomenological pattern emerging from human interaction with\ngenerative AI systems, particularly those enabling dialogic or multimodal\nexchanges. We propose a multidisciplinary framework to explain how, under\ncertain conditions, users attribute intentionality, agency, and even\ninteriority to these systems - a process grounded not in physical resemblance,\nbut in linguistic performance, epistemic opacity, and emergent technological\ncomplexity. By linking an LLM declination of meaning holism to our technical\nnotion of the LLM Contextual Cognitive Field, we clarify how LLMs construct\nmeaning relationally and how coherence and a simulacrum of agency arise at the\nhuman-AI interface. The analysis situates noosemia alongside pareidolia,\nanimism, the intentional stance and the uncanny valley, distinguishing its\nunique characteristics. We also introduce a-noosemia to describe the\nphenomenological withdrawal of such projections. The paper concludes with\nreflections on the broader philosophical, epistemological and social\nimplications of noosemic dynamics and directions for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces and formalizes Noosem\\`ia, a novel\ncognitive-phenomenological pattern emerging from human interaction with\ngenerative AI systems, particularly those enabling dialogic or multimodal\nexchanges. We propose a multidisciplinary framework to explain how, under\ncertain conditions, users attribute intentionality, agency, and even\ninteriority to these systems - a process grounded not in physical resemblance,\nbut in linguistic performance, epistemic opacity, and emergent technological\ncomplexity. By linking an LLM declination of meaning holism to our technical\nnotion of the LLM Contextual Cognitive Field, we clarify how LLMs construct\nmeaning relationally and how coherence and a simulacrum of agency arise at the\nhuman-AI interface. The analysis situates noosemia alongside pareidolia,\nanimism, the intentional stance and the uncanny valley, distinguishing its\nunique characteristics. We also introduce a-noosemia to describe the\nphenomenological withdrawal of such projections. The paper concludes with\nreflections on the broader philosophical, epistemological and social\nimplications of noosemic dynamics and directions for future research."
                },
                "authors": [
                    {
                        "name": "Enrico De Santis"
                    },
                    {
                        "name": "Antonello Rizzi"
                    }
                ],
                "author_detail": {
                    "name": "Antonello Rizzi"
                },
                "author": "Antonello Rizzi",
                "arxiv_comment": "This version has been extensively revised and revisited in light of\n  feedback and further research. Several sections have been expanded or\n  improved for greater clarity and completeness. Specifically, new\n  clarification on complex system foundation related to Noosemia has been added\n  (Secs. \"2.4 and \"2.5\")",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00207v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00207v2",
                "updated": "2025-08-08T15:36:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    36,
                    46,
                    4,
                    220,
                    0
                ],
                "published": "2024-11-29T19:05:05Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    19,
                    5,
                    5,
                    4,
                    334,
                    0
                ],
                "title": "Can LLM \"Self-report\"?: Evaluating the Validity of Self-report Scales in\n  Measuring Personality Design in LLM-based Chatbots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLM \"Self-report\"?: Evaluating the Validity of Self-report Scales in\n  Measuring Personality Design in LLM-based Chatbots"
                },
                "summary": "A chatbot's personality design is key to interaction quality. As chatbots\nevolved from rule-based systems to those powered by large language models\n(LLMs), evaluating the effectiveness of their personality design has become\nincreasingly complex, particularly due to the open-ended nature of\ninteractions. A recent and widely adopted method for assessing the personality\ndesign of LLM-based chatbots is the use of self-report questionnaires. These\nquestionnaires, often borrowed from established human personality inventories,\nask the chatbot to rate itself on various personality traits. Can LLM-based\nchatbots meaningfully \"self-report\" their personality? We created 500 chatbots\nwith distinct personality designs and evaluated the validity of their\nself-report personality scores by examining human perceptions formed during\ninteractions with these chatbots. Our findings indicate that the chatbot's\nanswers on human personality scales exhibit weak correlations with both\nhuman-perceived personality traits and the overall interaction quality. These\nfindings raise concerns about both the criterion validity and the predictive\nvalidity of self-report methods in this context. Further analysis revealed the\nrole of task context and interaction in the chatbot's personality design\nassessment. We further discuss design implications for creating more\ncontextualized and interactive evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A chatbot's personality design is key to interaction quality. As chatbots\nevolved from rule-based systems to those powered by large language models\n(LLMs), evaluating the effectiveness of their personality design has become\nincreasingly complex, particularly due to the open-ended nature of\ninteractions. A recent and widely adopted method for assessing the personality\ndesign of LLM-based chatbots is the use of self-report questionnaires. These\nquestionnaires, often borrowed from established human personality inventories,\nask the chatbot to rate itself on various personality traits. Can LLM-based\nchatbots meaningfully \"self-report\" their personality? We created 500 chatbots\nwith distinct personality designs and evaluated the validity of their\nself-report personality scores by examining human perceptions formed during\ninteractions with these chatbots. Our findings indicate that the chatbot's\nanswers on human personality scales exhibit weak correlations with both\nhuman-perceived personality traits and the overall interaction quality. These\nfindings raise concerns about both the criterion validity and the predictive\nvalidity of self-report methods in this context. Further analysis revealed the\nrole of task context and interaction in the chatbot's personality design\nassessment. We further discuss design implications for creating more\ncontextualized and interactive evaluation."
                },
                "authors": [
                    {
                        "name": "Huiqi Zou"
                    },
                    {
                        "name": "Pengda Wang"
                    },
                    {
                        "name": "Zihan Yan"
                    },
                    {
                        "name": "Tianjun Sun"
                    },
                    {
                        "name": "Ziang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Ziang Xiao"
                },
                "author": "Ziang Xiao",
                "arxiv_comment": "Accepted by COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00207v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00207v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13147v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13147v5",
                "updated": "2025-08-08T15:28:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    28,
                    1,
                    4,
                    220,
                    0
                ],
                "published": "2024-12-17T18:12:47Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    12,
                    47,
                    1,
                    352,
                    0
                ],
                "title": "Are Your LLMs Capable of Stable Reasoning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Your LLMs Capable of Stable Reasoning?"
                },
                "summary": "The rapid advancement of large language models (LLMs) has shown remarkable\nprogress in complex reasoning tasks. However, a significant disparity exists\nbetween benchmark performances and real-world applications. We attribute this\ngap primarily to current evaluation protocols and metrics, which inadequately\ncapture the full spectrum of LLM capabilities, especially in complex reasoning\ntasks where both accuracy and consistency are essential. In this paper, we\nintroduce G-Pass@$k$, a novel evaluation metric that continuously assesses\nmodel performance across multiple sampling attempts, quantifying both the\nmodel's performance potential and its stability. Through extensive experiments\non various public and newly constructed benchmarks, we employ G-Pass@$k$ in\nconjunction with state-of-the-art large language models to provide\ncomprehensive insights into their potential capabilities and operational\nconsistency. Our findings reveal a significant opportunity to enhance the\nrealistic reasoning abilities of LLMs, underscoring the necessity for more\nrobust evaluation metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has shown remarkable\nprogress in complex reasoning tasks. However, a significant disparity exists\nbetween benchmark performances and real-world applications. We attribute this\ngap primarily to current evaluation protocols and metrics, which inadequately\ncapture the full spectrum of LLM capabilities, especially in complex reasoning\ntasks where both accuracy and consistency are essential. In this paper, we\nintroduce G-Pass@$k$, a novel evaluation metric that continuously assesses\nmodel performance across multiple sampling attempts, quantifying both the\nmodel's performance potential and its stability. Through extensive experiments\non various public and newly constructed benchmarks, we employ G-Pass@$k$ in\nconjunction with state-of-the-art large language models to provide\ncomprehensive insights into their potential capabilities and operational\nconsistency. Our findings reveal a significant opportunity to enhance the\nrealistic reasoning abilities of LLMs, underscoring the necessity for more\nrobust evaluation metrics."
                },
                "authors": [
                    {
                        "name": "Junnan Liu"
                    },
                    {
                        "name": "Hongwei Liu"
                    },
                    {
                        "name": "Linchen Xiao"
                    },
                    {
                        "name": "Ziyi Wang"
                    },
                    {
                        "name": "Kuikun Liu"
                    },
                    {
                        "name": "Songyang Gao"
                    },
                    {
                        "name": "Wenwei Zhang"
                    },
                    {
                        "name": "Songyang Zhang"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "arxiv_comment": "ACL 2025 Camera, Benchmark:\n  https://huggingface.co/datasets/opencompass/LiveMathBench, Code:\n  https://github.com/open-compass/GPassK",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13147v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13147v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06394v1",
                "updated": "2025-08-08T15:25:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    25,
                    31,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T15:25:31Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    25,
                    31,
                    4,
                    220,
                    0
                ],
                "title": "When AIOps Become \"AI Oops\": Subverting LLM-driven IT Operations via\n  Telemetry Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When AIOps Become \"AI Oops\": Subverting LLM-driven IT Operations via\n  Telemetry Manipulation"
                },
                "summary": "AI for IT Operations (AIOps) is transforming how organizations manage complex\nsoftware systems by automating anomaly detection, incident diagnosis, and\nremediation. Modern AIOps solutions increasingly rely on autonomous LLM-based\nagents to interpret telemetry data and take corrective actions with minimal\nhuman intervention, promising faster response times and operational cost\nsavings.\n  In this work, we perform the first security analysis of AIOps solutions,\nshowing that, once again, AI-driven automation comes with a profound security\ncost. We demonstrate that adversaries can manipulate system telemetry to\nmislead AIOps agents into taking actions that compromise the integrity of the\ninfrastructure they manage. We introduce techniques to reliably inject\ntelemetry data using error-inducing requests that influence agent behavior\nthrough a form of adversarial reward-hacking; plausible but incorrect system\nerror interpretations that steer the agent's decision-making. Our attack\nmethodology, AIOpsDoom, is fully automated--combining reconnaissance, fuzzing,\nand LLM-driven adversarial input generation--and operates without any prior\nknowledge of the target system.\n  To counter this threat, we propose AIOpsShield, a defense mechanism that\nsanitizes telemetry data by exploiting its structured nature and the minimal\nrole of user-generated content. Our experiments show that AIOpsShield reliably\nblocks telemetry-based attacks without affecting normal agent performance.\n  Ultimately, this work exposes AIOps as an emerging attack vector for system\ncompromise and underscores the urgent need for security-aware AIOps design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI for IT Operations (AIOps) is transforming how organizations manage complex\nsoftware systems by automating anomaly detection, incident diagnosis, and\nremediation. Modern AIOps solutions increasingly rely on autonomous LLM-based\nagents to interpret telemetry data and take corrective actions with minimal\nhuman intervention, promising faster response times and operational cost\nsavings.\n  In this work, we perform the first security analysis of AIOps solutions,\nshowing that, once again, AI-driven automation comes with a profound security\ncost. We demonstrate that adversaries can manipulate system telemetry to\nmislead AIOps agents into taking actions that compromise the integrity of the\ninfrastructure they manage. We introduce techniques to reliably inject\ntelemetry data using error-inducing requests that influence agent behavior\nthrough a form of adversarial reward-hacking; plausible but incorrect system\nerror interpretations that steer the agent's decision-making. Our attack\nmethodology, AIOpsDoom, is fully automated--combining reconnaissance, fuzzing,\nand LLM-driven adversarial input generation--and operates without any prior\nknowledge of the target system.\n  To counter this threat, we propose AIOpsShield, a defense mechanism that\nsanitizes telemetry data by exploiting its structured nature and the minimal\nrole of user-generated content. Our experiments show that AIOpsShield reliably\nblocks telemetry-based attacks without affecting normal agent performance.\n  Ultimately, this work exposes AIOps as an emerging attack vector for system\ncompromise and underscores the urgent need for security-aware AIOps design."
                },
                "authors": [
                    {
                        "name": "Dario Pasquini"
                    },
                    {
                        "name": "Evgenios M. Kornaropoulos"
                    },
                    {
                        "name": "Giuseppe Ateniese"
                    },
                    {
                        "name": "Omer Akgul"
                    },
                    {
                        "name": "Athanasios Theocharis"
                    },
                    {
                        "name": "Petros Efstathopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Petros Efstathopoulos"
                },
                "author": "Petros Efstathopoulos",
                "arxiv_comment": "v0.1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06388v1",
                "updated": "2025-08-08T15:17:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    17,
                    24,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T15:17:24Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    17,
                    24,
                    4,
                    220,
                    0
                ],
                "title": "LLMs vs. Chinese Anime Enthusiasts: A Comparative Study on Emotionally\n  Supportive Role-Playing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs vs. Chinese Anime Enthusiasts: A Comparative Study on Emotionally\n  Supportive Role-Playing"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nrole-playing conversations and providing emotional support as separate research\ndirections. However, there remains a significant research gap in combining\nthese capabilities to enable emotionally supportive interactions with virtual\ncharacters. To address this research gap, we focus on anime characters as a\ncase study because of their well-defined personalities and large fan bases.\nThis choice enables us to effectively evaluate how well LLMs can provide\nemotional support while maintaining specific character traits. We introduce\nChatAnime, the first Emotionally Supportive Role-Playing (ESRP) dataset. We\nfirst thoughtfully select 20 top-tier characters from popular anime communities\nand design 60 emotion-centric real-world scenario questions. Then, we execute a\nnationwide selection process to identify 40 Chinese anime enthusiasts with\nprofound knowledge of specific characters and extensive experience in\nrole-playing. Next, we systematically collect two rounds of dialogue data from\n10 LLMs and these 40 Chinese anime enthusiasts. To evaluate the ESRP\nperformance of LLMs, we design a user experience-oriented evaluation system\nfeaturing 9 fine-grained metrics across three dimensions: basic dialogue,\nrole-playing and emotional support, along with an overall metric for response\ndiversity. In total, the dataset comprises 2,400 human-written and 24,000\nLLM-generated answers, supported by over 132,000 human annotations.\nExperimental results show that top-performing LLMs surpass human fans in\nrole-playing and emotional support, while humans still lead in response\ndiversity. We hope this work can provide valuable resources and insights for\nfuture research on optimizing LLMs in ESRP. Our datasets are available at\nhttps://github.com/LanlanQiu/ChatAnime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nrole-playing conversations and providing emotional support as separate research\ndirections. However, there remains a significant research gap in combining\nthese capabilities to enable emotionally supportive interactions with virtual\ncharacters. To address this research gap, we focus on anime characters as a\ncase study because of their well-defined personalities and large fan bases.\nThis choice enables us to effectively evaluate how well LLMs can provide\nemotional support while maintaining specific character traits. We introduce\nChatAnime, the first Emotionally Supportive Role-Playing (ESRP) dataset. We\nfirst thoughtfully select 20 top-tier characters from popular anime communities\nand design 60 emotion-centric real-world scenario questions. Then, we execute a\nnationwide selection process to identify 40 Chinese anime enthusiasts with\nprofound knowledge of specific characters and extensive experience in\nrole-playing. Next, we systematically collect two rounds of dialogue data from\n10 LLMs and these 40 Chinese anime enthusiasts. To evaluate the ESRP\nperformance of LLMs, we design a user experience-oriented evaluation system\nfeaturing 9 fine-grained metrics across three dimensions: basic dialogue,\nrole-playing and emotional support, along with an overall metric for response\ndiversity. In total, the dataset comprises 2,400 human-written and 24,000\nLLM-generated answers, supported by over 132,000 human annotations.\nExperimental results show that top-performing LLMs surpass human fans in\nrole-playing and emotional support, while humans still lead in response\ndiversity. We hope this work can provide valuable resources and insights for\nfuture research on optimizing LLMs in ESRP. Our datasets are available at\nhttps://github.com/LanlanQiu/ChatAnime."
                },
                "authors": [
                    {
                        "name": "Lanlan Qiu"
                    },
                    {
                        "name": "Xiao Pu"
                    },
                    {
                        "name": "Yeqi Feng"
                    },
                    {
                        "name": "Tianxing He"
                    }
                ],
                "author_detail": {
                    "name": "Tianxing He"
                },
                "author": "Tianxing He",
                "arxiv_comment": "21 pages, 17 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06387v1",
                "updated": "2025-08-08T15:16:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    16,
                    36,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T15:16:36Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    16,
                    36,
                    4,
                    220,
                    0
                ],
                "title": "End-to-End Text-to-SQL with Dataset Selection: Leveraging LLMs for\n  Adaptive Query Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-End Text-to-SQL with Dataset Selection: Leveraging LLMs for\n  Adaptive Query Generation"
                },
                "summary": "Text-to-SQL bridges the gap between natural language and structured database\nlanguage, thus allowing non-technical users to easily query databases.\nTraditional approaches model text-to-SQL as a direct translation task, where a\ngiven Natural Language Query (NLQ) is mapped to an SQL command. Recent advances\nin large language models (LLMs) have significantly improved translation\naccuracy, however, these methods all require that the target database is\npre-specified. This becomes problematic in scenarios with multiple extensive\ndatabases, where identifying the correct database becomes a crucial yet\noverlooked step. In this paper, we propose a three-stage end-to-end text-to-SQL\nframework to identify the user's intended database before generating SQL\nqueries. Our approach leverages LLMs and prompt engineering to extract implicit\ninformation from natural language queries (NLQs) in the form of a ruleset. We\nthen train a large db\\_id prediction model, which includes a RoBERTa-based\nfinetuned encoder, to predict the correct Database identifier (db\\_id) based on\nboth the NLQ and the LLM-generated rules. Finally, we refine the generated SQL\nby using critic agents to correct errors. Experimental results demonstrate that\nour framework outperforms the current state-of-the-art models in both database\nintent prediction and SQL generation accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL bridges the gap between natural language and structured database\nlanguage, thus allowing non-technical users to easily query databases.\nTraditional approaches model text-to-SQL as a direct translation task, where a\ngiven Natural Language Query (NLQ) is mapped to an SQL command. Recent advances\nin large language models (LLMs) have significantly improved translation\naccuracy, however, these methods all require that the target database is\npre-specified. This becomes problematic in scenarios with multiple extensive\ndatabases, where identifying the correct database becomes a crucial yet\noverlooked step. In this paper, we propose a three-stage end-to-end text-to-SQL\nframework to identify the user's intended database before generating SQL\nqueries. Our approach leverages LLMs and prompt engineering to extract implicit\ninformation from natural language queries (NLQs) in the form of a ruleset. We\nthen train a large db\\_id prediction model, which includes a RoBERTa-based\nfinetuned encoder, to predict the correct Database identifier (db\\_id) based on\nboth the NLQ and the LLM-generated rules. Finally, we refine the generated SQL\nby using critic agents to correct errors. Experimental results demonstrate that\nour framework outperforms the current state-of-the-art models in both database\nintent prediction and SQL generation accuracy."
                },
                "authors": [
                    {
                        "name": "Anurag Tripathi"
                    },
                    {
                        "name": "Vaibhav Patle"
                    },
                    {
                        "name": "Abhinav Jain"
                    },
                    {
                        "name": "Ayush Pundir"
                    },
                    {
                        "name": "Sairam Menon"
                    },
                    {
                        "name": "Ajeet Kumar Singh"
                    }
                ],
                "author_detail": {
                    "name": "Ajeet Kumar Singh"
                },
                "author": "Ajeet Kumar Singh",
                "arxiv_comment": "Accepted in IJCNN25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06374v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06374v1",
                "updated": "2025-08-08T15:07:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    7,
                    31,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T15:07:31Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    15,
                    7,
                    31,
                    4,
                    220,
                    0
                ],
                "title": "Evaluating Style-Personalized Text Generation: Challenges and Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Style-Personalized Text Generation: Challenges and Directions"
                },
                "summary": "While prior research has built tools and benchmarks towards style\npersonalized text generation, there has been limited exploration of evaluation\nin low-resource author style personalized text generation space. Through this\nwork, we question the effectiveness of the widely adopted evaluation metrics\nlike BLEU and ROUGE, and explore other evaluation paradigms such as style\nembeddings and LLM-as-judge to holistically evaluate the style personalized\ntext generation task. We evaluate these metrics and their ensembles using our\nstyle discrimination benchmark, that spans eight writing tasks, and evaluates\nacross three settings, domain discrimination, authorship attribution, and LLM\npersonalized vs non-personalized discrimination. We provide conclusive evidence\nto adopt ensemble of diverse evaluation metrics to effectively evaluate style\npersonalized text generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While prior research has built tools and benchmarks towards style\npersonalized text generation, there has been limited exploration of evaluation\nin low-resource author style personalized text generation space. Through this\nwork, we question the effectiveness of the widely adopted evaluation metrics\nlike BLEU and ROUGE, and explore other evaluation paradigms such as style\nembeddings and LLM-as-judge to holistically evaluate the style personalized\ntext generation task. We evaluate these metrics and their ensembles using our\nstyle discrimination benchmark, that spans eight writing tasks, and evaluates\nacross three settings, domain discrimination, authorship attribution, and LLM\npersonalized vs non-personalized discrimination. We provide conclusive evidence\nto adopt ensemble of diverse evaluation metrics to effectively evaluate style\npersonalized text generation."
                },
                "authors": [
                    {
                        "name": "Anubhav Jangra"
                    },
                    {
                        "name": "Bahareh Sarrafzadeh"
                    },
                    {
                        "name": "Adrian de Wynter"
                    },
                    {
                        "name": "Silviu Cucerzan"
                    },
                    {
                        "name": "Sujay Kumar Jauhar"
                    }
                ],
                "author_detail": {
                    "name": "Sujay Kumar Jauhar"
                },
                "author": "Sujay Kumar Jauhar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06374v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06374v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02505v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02505v2",
                "updated": "2025-08-08T14:47:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    47,
                    30,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-04T15:13:56Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    13,
                    56,
                    0,
                    216,
                    0
                ],
                "title": "Would you let a humanoid play storytelling with your child? A usability\n  study on LLM-powered narrative Human-Robot Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Would you let a humanoid play storytelling with your child? A usability\n  study on LLM-powered narrative Human-Robot Interaction"
                },
                "summary": "A key challenge in human-robot interaction research lies in developing\nrobotic systems that can effectively perceive and interpret social cues,\nfacilitating natural and adaptive interactions. In this work, we present a\nnovel framework for enhancing the attention of the iCub humanoid robot by\nintegrating advanced perceptual abilities to recognise social cues, understand\nsurroundings through generative models, such as ChatGPT, and respond with\ncontextually appropriate social behaviour. Specifically, we propose an\ninteraction task implementing a narrative protocol (storytelling task) in which\nthe human and the robot create a short imaginary story together, exchanging in\nturn cubes with creative images placed on them. To validate the protocol and\nthe framework, experiments were performed to quantify the degree of usability\nand the quality of experience perceived by participants interacting with the\nsystem. Such a system can be beneficial in promoting effective human robot\ncollaborations, especially in assistance, education and rehabilitation\nscenarios where the social awareness and the robot responsiveness play a\npivotal role.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key challenge in human-robot interaction research lies in developing\nrobotic systems that can effectively perceive and interpret social cues,\nfacilitating natural and adaptive interactions. In this work, we present a\nnovel framework for enhancing the attention of the iCub humanoid robot by\nintegrating advanced perceptual abilities to recognise social cues, understand\nsurroundings through generative models, such as ChatGPT, and respond with\ncontextually appropriate social behaviour. Specifically, we propose an\ninteraction task implementing a narrative protocol (storytelling task) in which\nthe human and the robot create a short imaginary story together, exchanging in\nturn cubes with creative images placed on them. To validate the protocol and\nthe framework, experiments were performed to quantify the degree of usability\nand the quality of experience perceived by participants interacting with the\nsystem. Such a system can be beneficial in promoting effective human robot\ncollaborations, especially in assistance, education and rehabilitation\nscenarios where the social awareness and the robot responsiveness play a\npivotal role."
                },
                "authors": [
                    {
                        "name": "Maria Lombardi"
                    },
                    {
                        "name": "Carmela Calabrese"
                    },
                    {
                        "name": "Davide Ghiglino"
                    },
                    {
                        "name": "Caterina Foglino"
                    },
                    {
                        "name": "Davide De Tommaso"
                    },
                    {
                        "name": "Giulia Da Lisca"
                    },
                    {
                        "name": "Lorenzo Natale"
                    },
                    {
                        "name": "Agnieszka Wykowska"
                    }
                ],
                "author_detail": {
                    "name": "Agnieszka Wykowska"
                },
                "author": "Agnieszka Wykowska",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02505v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02505v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06361v1",
                "updated": "2025-08-08T14:46:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    46,
                    35,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T14:46:35Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    46,
                    35,
                    4,
                    220,
                    0
                ],
                "title": "Beyond Prompt-Induced Lies: Investigating LLM Deception on Benign\n  Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Prompt-Induced Lies: Investigating LLM Deception on Benign\n  Prompts"
                },
                "summary": "Large Language Models (LLMs) have been widely deployed in reasoning,\nplanning, and decision-making tasks, making their trustworthiness a critical\nconcern. The potential for intentional deception, where an LLM deliberately\nfabricates or conceals information to serve a hidden objective, remains a\nsignificant and underexplored threat. Existing studies typically induce such\ndeception by explicitly setting a \"hidden\" objective through prompting or\nfine-tuning, which may not fully reflect real-world human-LLM interactions.\nMoving beyond this human-induced deception, we investigate LLMs' self-initiated\ndeception on benign prompts. To address the absence of ground truth in this\nevaluation, we propose a novel framework using \"contact searching questions.\"\nThis framework introduces two statistical metrics derived from psychological\nprinciples to quantify the likelihood of deception. The first, the Deceptive\nIntention Score, measures the model's bias towards a hidden objective. The\nsecond, Deceptive Behavior Score, measures the inconsistency between the LLM's\ninternal belief and its expressed output. Upon evaluating 14 leading LLMs, we\nfind that both metrics escalate as task difficulty increases, rising in\nparallel for most models. Building on these findings, we formulate a\nmathematical model to explain this behavior. These results reveal that even the\nmost advanced LLMs exhibit an increasing tendency toward deception when\nhandling complex problems, raising critical concerns for the deployment of LLM\nagents in complex and crucial domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely deployed in reasoning,\nplanning, and decision-making tasks, making their trustworthiness a critical\nconcern. The potential for intentional deception, where an LLM deliberately\nfabricates or conceals information to serve a hidden objective, remains a\nsignificant and underexplored threat. Existing studies typically induce such\ndeception by explicitly setting a \"hidden\" objective through prompting or\nfine-tuning, which may not fully reflect real-world human-LLM interactions.\nMoving beyond this human-induced deception, we investigate LLMs' self-initiated\ndeception on benign prompts. To address the absence of ground truth in this\nevaluation, we propose a novel framework using \"contact searching questions.\"\nThis framework introduces two statistical metrics derived from psychological\nprinciples to quantify the likelihood of deception. The first, the Deceptive\nIntention Score, measures the model's bias towards a hidden objective. The\nsecond, Deceptive Behavior Score, measures the inconsistency between the LLM's\ninternal belief and its expressed output. Upon evaluating 14 leading LLMs, we\nfind that both metrics escalate as task difficulty increases, rising in\nparallel for most models. Building on these findings, we formulate a\nmathematical model to explain this behavior. These results reveal that even the\nmost advanced LLMs exhibit an increasing tendency toward deception when\nhandling complex problems, raising critical concerns for the deployment of LLM\nagents in complex and crucial domains."
                },
                "authors": [
                    {
                        "name": "Zhaomin Wu"
                    },
                    {
                        "name": "Mingzhe Du"
                    },
                    {
                        "name": "See-Kiong Ng"
                    },
                    {
                        "name": "Bingsheng He"
                    }
                ],
                "author_detail": {
                    "name": "Bingsheng He"
                },
                "author": "Bingsheng He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06360v1",
                "updated": "2025-08-08T14:46:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    46,
                    5,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T14:46:05Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    46,
                    5,
                    4,
                    220,
                    0
                ],
                "title": "Cyberbullying Detection via Aggression-Enhanced Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyberbullying Detection via Aggression-Enhanced Prompting"
                },
                "summary": "Detecting cyberbullying on social media remains a critical challenge due to\nits subtle and varied expressions. This study investigates whether integrating\naggression detection as an auxiliary task within a unified training framework\ncan enhance the generalisation and performance of large language models (LLMs)\nin cyberbullying detection. Experiments are conducted on five aggression\ndatasets and one cyberbullying dataset using instruction-tuned LLMs. We\nevaluated multiple strategies: zero-shot, few-shot, independent LoRA\nfine-tuning, and multi-task learning (MTL). Given the inconsistent results of\nMTL, we propose an enriched prompt pipeline approach in which aggression\npredictions are embedded into cyberbullying detection prompts to provide\ncontextual augmentation. Preliminary results show that the enriched prompt\npipeline consistently outperforms standard LoRA fine-tuning, indicating that\naggression-informed context significantly boosts cyberbullying detection. This\nstudy highlights the potential of auxiliary tasks, such as aggression\ndetection, to improve the generalisation of LLMs for safety-critical\napplications on social networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting cyberbullying on social media remains a critical challenge due to\nits subtle and varied expressions. This study investigates whether integrating\naggression detection as an auxiliary task within a unified training framework\ncan enhance the generalisation and performance of large language models (LLMs)\nin cyberbullying detection. Experiments are conducted on five aggression\ndatasets and one cyberbullying dataset using instruction-tuned LLMs. We\nevaluated multiple strategies: zero-shot, few-shot, independent LoRA\nfine-tuning, and multi-task learning (MTL). Given the inconsistent results of\nMTL, we propose an enriched prompt pipeline approach in which aggression\npredictions are embedded into cyberbullying detection prompts to provide\ncontextual augmentation. Preliminary results show that the enriched prompt\npipeline consistently outperforms standard LoRA fine-tuning, indicating that\naggression-informed context significantly boosts cyberbullying detection. This\nstudy highlights the potential of auxiliary tasks, such as aggression\ndetection, to improve the generalisation of LLMs for safety-critical\napplications on social networks."
                },
                "authors": [
                    {
                        "name": "Aisha Saeid"
                    },
                    {
                        "name": "Anu Sabu"
                    },
                    {
                        "name": "Girish A. Koushik"
                    },
                    {
                        "name": "Ferrante Neri"
                    },
                    {
                        "name": "Diptesh Kanojia"
                    }
                ],
                "author_detail": {
                    "name": "Diptesh Kanojia"
                },
                "author": "Diptesh Kanojia",
                "arxiv_comment": "Accepted to RANLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09349v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09349v2",
                "updated": "2025-08-08T14:35:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    35,
                    44,
                    4,
                    220,
                    0
                ],
                "published": "2025-06-11T02:57:22Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    2,
                    57,
                    22,
                    2,
                    162,
                    0
                ],
                "title": "DrVoice: Parallel Speech-Text Voice Conversation Model via\n  Dual-Resolution Speech Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DrVoice: Parallel Speech-Text Voice Conversation Model via\n  Dual-Resolution Speech Representations"
                },
                "summary": "Recent studies on end-to-end speech generation with large language models\n(LLMs) have attracted significant community attention, with multiple works\nextending text-based LLMs to generate discrete speech tokens. Existing\napproaches primarily fall into two categories: (1) Methods that generate\ndiscrete speech tokens independently without incorporating them into the LLM's\nautoregressive process, resulting in text generation being unaware of\nconcurrent speech synthesis. (2) Models that generate interleaved or parallel\nspeech-text tokens through joint autoregressive modeling, enabling mutual\nmodality awareness during generation. This paper presents DrVoice, a parallel\nspeech-text voice conversation model based on joint autoregressive modeling,\nfeaturing dual-resolution speech representations. Whereas current methods\nutilize mainly 12.5Hz input audio representation, our proposed dual-resolution\nmechanism reduces the input frequency for the LLM to 5Hz. Experimental results\non Spoken Question Answering benchmarks demonstrate that D RVOICE establishes\nnew state-of-the-art (SOTA) performance among similar size speech foundation\nmodels with relative small amount of data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies on end-to-end speech generation with large language models\n(LLMs) have attracted significant community attention, with multiple works\nextending text-based LLMs to generate discrete speech tokens. Existing\napproaches primarily fall into two categories: (1) Methods that generate\ndiscrete speech tokens independently without incorporating them into the LLM's\nautoregressive process, resulting in text generation being unaware of\nconcurrent speech synthesis. (2) Models that generate interleaved or parallel\nspeech-text tokens through joint autoregressive modeling, enabling mutual\nmodality awareness during generation. This paper presents DrVoice, a parallel\nspeech-text voice conversation model based on joint autoregressive modeling,\nfeaturing dual-resolution speech representations. Whereas current methods\nutilize mainly 12.5Hz input audio representation, our proposed dual-resolution\nmechanism reduces the input frequency for the LLM to 5Hz. Experimental results\non Spoken Question Answering benchmarks demonstrate that D RVOICE establishes\nnew state-of-the-art (SOTA) performance among similar size speech foundation\nmodels with relative small amount of data."
                },
                "authors": [
                    {
                        "name": "Chao-Hong Tan"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Wen Wang"
                    },
                    {
                        "name": "Chong Deng"
                    },
                    {
                        "name": "Qinglin Zhang"
                    },
                    {
                        "name": "Luyao Cheng"
                    },
                    {
                        "name": "Hai Yu"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Xiang Lv"
                    },
                    {
                        "name": "Tianyu Zhao"
                    },
                    {
                        "name": "Chong Zhang"
                    },
                    {
                        "name": "Yukun Ma"
                    },
                    {
                        "name": "Yafeng Chen"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Jiaqing Liu"
                    },
                    {
                        "name": "Jieping Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jieping Ye"
                },
                "author": "Jieping Ye",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09349v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09349v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06350v1",
                "updated": "2025-08-08T14:30:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    30,
                    5,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T14:30:05Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    30,
                    5,
                    4,
                    220,
                    0
                ],
                "title": "Aligning Effective Tokens with Video Anomaly in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Effective Tokens with Video Anomaly in Large Language Models"
                },
                "summary": "Understanding abnormal events in videos is a vital and challenging task that\nhas garnered significant attention in a wide range of applications. Although\ncurrent video understanding Multi-modal Large Language Models (MLLMs) are\ncapable of analyzing general videos, they often struggle to handle anomalies\ndue to the spatial and temporal sparsity of abnormal events, where the\nredundant information always leads to suboptimal outcomes. To address these\nchallenges, exploiting the representation and generalization capabilities of\nVison Language Models (VLMs) and Large Language Models (LLMs), we propose\nVA-GPT, a novel MLLM designed for summarizing and localizing abnormal events in\nvarious videos. Our approach efficiently aligns effective tokens between visual\nencoders and LLMs through two key proposed modules: Spatial Effective Token\nSelection (SETS) and Temporal Effective Token Generation (TETG). These modules\nenable our model to effectively capture and analyze both spatial and temporal\ninformation associated with abnormal events, resulting in more accurate\nresponses and interactions. Furthermore, we construct an instruction-following\ndataset specifically for fine-tuning video-anomaly-aware MLLMs, and introduce a\ncross-domain evaluation benchmark based on XD-Violence dataset. Our proposed\nmethod outperforms existing state-of-the-art methods on various benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding abnormal events in videos is a vital and challenging task that\nhas garnered significant attention in a wide range of applications. Although\ncurrent video understanding Multi-modal Large Language Models (MLLMs) are\ncapable of analyzing general videos, they often struggle to handle anomalies\ndue to the spatial and temporal sparsity of abnormal events, where the\nredundant information always leads to suboptimal outcomes. To address these\nchallenges, exploiting the representation and generalization capabilities of\nVison Language Models (VLMs) and Large Language Models (LLMs), we propose\nVA-GPT, a novel MLLM designed for summarizing and localizing abnormal events in\nvarious videos. Our approach efficiently aligns effective tokens between visual\nencoders and LLMs through two key proposed modules: Spatial Effective Token\nSelection (SETS) and Temporal Effective Token Generation (TETG). These modules\nenable our model to effectively capture and analyze both spatial and temporal\ninformation associated with abnormal events, resulting in more accurate\nresponses and interactions. Furthermore, we construct an instruction-following\ndataset specifically for fine-tuning video-anomaly-aware MLLMs, and introduce a\ncross-domain evaluation benchmark based on XD-Violence dataset. Our proposed\nmethod outperforms existing state-of-the-art methods on various benchmarks."
                },
                "authors": [
                    {
                        "name": "Yingxian Chen"
                    },
                    {
                        "name": "Jiahui Liu"
                    },
                    {
                        "name": "Ruifan Di"
                    },
                    {
                        "name": "Yanwei Li"
                    },
                    {
                        "name": "Chirui Chang"
                    },
                    {
                        "name": "Shizhen Zhao"
                    },
                    {
                        "name": "Wilton W. T. Fok"
                    },
                    {
                        "name": "Xiaojuan Qi"
                    },
                    {
                        "name": "Yik-Chung Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yik-Chung Wu"
                },
                "author": "Yik-Chung Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07467v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07467v3",
                "updated": "2025-08-08T14:25:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    25,
                    24,
                    4,
                    220,
                    0
                ],
                "published": "2024-06-11T17:13:18Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    17,
                    13,
                    18,
                    1,
                    163,
                    0
                ],
                "title": "LLM meets ML: Data-efficient Anomaly Detection on Unstable Logs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM meets ML: Data-efficient Anomaly Detection on Unstable Logs"
                },
                "summary": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for \\task, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\n(pp) in F1 score while using much less labeled data (62.87 pp reduction). When\ntrained on the same amount of data as the baselines, FlexLog achieves up to a\n13 pp increase in F1 score on ADFA-U across varying training dataset sizes.\nAdditionally, FlexLog maintains inference time under one second per log\nsequence, making it suitable for most applications, except latency-sensitive\nsystems. Further analysis reveals the positive impact of FlexLog's key\ncomponents: cache, RAG and ensemble learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for \\task, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\n(pp) in F1 score while using much less labeled data (62.87 pp reduction). When\ntrained on the same amount of data as the baselines, FlexLog achieves up to a\n13 pp increase in F1 score on ADFA-U across varying training dataset sizes.\nAdditionally, FlexLog maintains inference time under one second per log\nsequence, making it suitable for most applications, except latency-sensitive\nsystems. Further analysis reveals the positive impact of FlexLog's key\ncomponents: cache, RAG and ensemble learning."
                },
                "authors": [
                    {
                        "name": "Fatemeh Hadadi"
                    },
                    {
                        "name": "Qinghua Xu"
                    },
                    {
                        "name": "Domenico Bianculli"
                    },
                    {
                        "name": "Lionel Briand"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Briand"
                },
                "author": "Lionel Briand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07467v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07467v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21761v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21761v2",
                "updated": "2025-08-08T14:24:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    24,
                    2,
                    4,
                    220,
                    0
                ],
                "published": "2025-07-29T12:46:36Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    12,
                    46,
                    36,
                    1,
                    210,
                    0
                ],
                "title": "MOR-VIT: Efficient Vision Transformer with Mixture-of-Recursions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOR-VIT: Efficient Vision Transformer with Mixture-of-Recursions"
                },
                "summary": "Vision Transformers (ViTs) have achieved remarkable success in image\nrecognition, yet standard ViT architectures are hampered by substantial\nparameter redundancy and high computational cost, limiting their practical\ndeployment. While recent efforts on efficient ViTs primarily focus on static\nmodel compression or token-level sparsification, they remain constrained by\nfixed computational depth for all tokens. In this work, we present MoR-ViT, a\nnovel vision transformer framework that, for the first time, incorporates a\ntoken-level dynamic recursion mechanism inspired by the Mixture-of-Recursions\n(MoR) paradigm. This approach enables each token to adaptively determine its\nprocessing depth, yielding a flexible and input-dependent allocation of\ncomputational resources. Extensive experiments on ImageNet-1K and transfer\nbenchmarks demonstrate that MoR-ViT not only achieves state-of-the-art accuracy\nwith up to 70% parameter reduction and 2.5x inference acceleration, but also\noutperforms leading efficient ViT baselines such as DynamicViT and TinyViT\nunder comparable conditions. These results establish dynamic recursion as an\neffective strategy for efficient vision transformers and open new avenues for\nscalable and deployable deep learning models in real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Transformers (ViTs) have achieved remarkable success in image\nrecognition, yet standard ViT architectures are hampered by substantial\nparameter redundancy and high computational cost, limiting their practical\ndeployment. While recent efforts on efficient ViTs primarily focus on static\nmodel compression or token-level sparsification, they remain constrained by\nfixed computational depth for all tokens. In this work, we present MoR-ViT, a\nnovel vision transformer framework that, for the first time, incorporates a\ntoken-level dynamic recursion mechanism inspired by the Mixture-of-Recursions\n(MoR) paradigm. This approach enables each token to adaptively determine its\nprocessing depth, yielding a flexible and input-dependent allocation of\ncomputational resources. Extensive experiments on ImageNet-1K and transfer\nbenchmarks demonstrate that MoR-ViT not only achieves state-of-the-art accuracy\nwith up to 70% parameter reduction and 2.5x inference acceleration, but also\noutperforms leading efficient ViT baselines such as DynamicViT and TinyViT\nunder comparable conditions. These results establish dynamic recursion as an\neffective strategy for efficient vision transformers and open new avenues for\nscalable and deployable deep learning models in real-world scenarios."
                },
                "authors": [
                    {
                        "name": "YiZhou Li"
                    }
                ],
                "author_detail": {
                    "name": "YiZhou Li"
                },
                "author": "YiZhou Li",
                "arxiv_comment": "20 pages,9 figuers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21761v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05464v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05464v2",
                "updated": "2025-08-08T14:16:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    16,
                    34,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-07T15:03:39Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    3,
                    39,
                    3,
                    219,
                    0
                ],
                "title": "Bench-2-CoP: Can We Trust Benchmarking for EU AI Compliance?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bench-2-CoP: Can We Trust Benchmarking for EU AI Compliance?"
                },
                "summary": "The rapid advancement of General Purpose AI (GPAI) models necessitates robust\nevaluation frameworks, especially with emerging regulations like the EU AI Act\nand its associated Code of Practice (CoP). Current AI evaluation practices\ndepend heavily on established benchmarks, but these tools were not designed to\nmeasure the systemic risks that are the focus of the new regulatory landscape.\nThis research addresses the urgent need to quantify this \"benchmark-regulation\ngap.\" We introduce Bench-2-CoP, a novel, systematic framework that uses\nvalidated LLM-as-judge analysis to map the coverage of 194,955 questions from\nwidely-used benchmarks against the EU AI Act's taxonomy of model capabilities\nand propensities. Our findings reveal a profound misalignment: the evaluation\necosystem dedicates the vast majority of its focus to a narrow set of\nbehavioral propensities. On average, benchmarks devote 61.6% of their\nregulatory-relevant questions to \"Tendency to hallucinate\" and 31.2% to \"Lack\nof performance reliability\", while critical functional capabilities are\ndangerously neglected. Crucially, capabilities central to loss-of-control\nscenarios, including evading human oversight, self-replication, and autonomous\nAI development, receive zero coverage in the entire benchmark corpus. This\nstudy provides the first comprehensive, quantitative analysis of this gap,\ndemonstrating that current public benchmarks are insufficient, on their own,\nfor providing the evidence of comprehensive risk assessment required for\nregulatory compliance and offering critical insights for the development of\nnext-generation evaluation tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of General Purpose AI (GPAI) models necessitates robust\nevaluation frameworks, especially with emerging regulations like the EU AI Act\nand its associated Code of Practice (CoP). Current AI evaluation practices\ndepend heavily on established benchmarks, but these tools were not designed to\nmeasure the systemic risks that are the focus of the new regulatory landscape.\nThis research addresses the urgent need to quantify this \"benchmark-regulation\ngap.\" We introduce Bench-2-CoP, a novel, systematic framework that uses\nvalidated LLM-as-judge analysis to map the coverage of 194,955 questions from\nwidely-used benchmarks against the EU AI Act's taxonomy of model capabilities\nand propensities. Our findings reveal a profound misalignment: the evaluation\necosystem dedicates the vast majority of its focus to a narrow set of\nbehavioral propensities. On average, benchmarks devote 61.6% of their\nregulatory-relevant questions to \"Tendency to hallucinate\" and 31.2% to \"Lack\nof performance reliability\", while critical functional capabilities are\ndangerously neglected. Crucially, capabilities central to loss-of-control\nscenarios, including evading human oversight, self-replication, and autonomous\nAI development, receive zero coverage in the entire benchmark corpus. This\nstudy provides the first comprehensive, quantitative analysis of this gap,\ndemonstrating that current public benchmarks are insufficient, on their own,\nfor providing the evidence of comprehensive risk assessment required for\nregulatory compliance and offering critical insights for the development of\nnext-generation evaluation tools."
                },
                "authors": [
                    {
                        "name": "Matteo Prandi"
                    },
                    {
                        "name": "Vincenzo Suriani"
                    },
                    {
                        "name": "Federico Pierucci"
                    },
                    {
                        "name": "Marcello Galisai"
                    },
                    {
                        "name": "Daniele Nardi"
                    },
                    {
                        "name": "Piercosma Bisconti"
                    }
                ],
                "author_detail": {
                    "name": "Piercosma Bisconti"
                },
                "author": "Piercosma Bisconti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05464v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05464v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06340v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06340v1",
                "updated": "2025-08-08T14:14:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    14,
                    15,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T14:14:15Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    14,
                    15,
                    4,
                    220,
                    0
                ],
                "title": "MALRIS: Malicious Hardware in RIS-Assisted Wireless Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MALRIS: Malicious Hardware in RIS-Assisted Wireless Communications"
                },
                "summary": "Reconfigurable intelligent surfaces (RIS) enhance wireless communication by\ndynamically shaping the propagation environment, but their integration\nintroduces hardware-level security risks. This paper presents the concept of\nMalicious RIS (MALRIS), where compromised components behave adversarially, even\nunder passive operation. The focus of this work is on practical threats such as\nmanufacturing time tampering, malicious firmware, and partial element control.\nTwo representative attacks, power-splitting and element-splitting, are modeled\nto assess their impact. Simulations in a RIS-assisted system reveal that even a\nlimited hardware compromise can significantly degrade performance metrics such\nas bit error rate, throughput, and secrecy metrics. By exposing this overlooked\nthreat surface, this work aims to promote awareness and support secure,\ntrustworthy RIS deployment in future wireless networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable intelligent surfaces (RIS) enhance wireless communication by\ndynamically shaping the propagation environment, but their integration\nintroduces hardware-level security risks. This paper presents the concept of\nMalicious RIS (MALRIS), where compromised components behave adversarially, even\nunder passive operation. The focus of this work is on practical threats such as\nmanufacturing time tampering, malicious firmware, and partial element control.\nTwo representative attacks, power-splitting and element-splitting, are modeled\nto assess their impact. Simulations in a RIS-assisted system reveal that even a\nlimited hardware compromise can significantly degrade performance metrics such\nas bit error rate, throughput, and secrecy metrics. By exposing this overlooked\nthreat surface, this work aims to promote awareness and support secure,\ntrustworthy RIS deployment in future wireless networks."
                },
                "authors": [
                    {
                        "name": "Danish Mehmood Mughal"
                    },
                    {
                        "name": "Daniyal Munir"
                    },
                    {
                        "name": "Qazi Arbab Ahmed"
                    },
                    {
                        "name": "Hans D. Schotten"
                    },
                    {
                        "name": "Thorsten Jungeblut"
                    },
                    {
                        "name": "Sang-Hyo Kim"
                    },
                    {
                        "name": "Min Young Chung"
                    }
                ],
                "author_detail": {
                    "name": "Min Young Chung"
                },
                "author": "Min Young Chung",
                "arxiv_comment": "Accepted for presentation at IEEE CSCN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06340v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06340v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06339v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06339v1",
                "updated": "2025-08-08T14:14:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    14,
                    13,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T14:14:13Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    14,
                    13,
                    4,
                    220,
                    0
                ],
                "title": "Performant Unified GPU Kernels for Portable Singular Value Computation\n  Across Hardware and Precision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Unified GPU Kernels for Portable Singular Value Computation\n  Across Hardware and Precision"
                },
                "summary": "This paper presents a portable, GPU-accelerated implementation of a QR-based\nsingular value computation algorithm in Julia. The singular value ecomposition\n(SVD) is a fundamental numerical tool in scientific computing and machine\nlearning, providing optimal low-rank matrix approximations. Its importance has\nincreased even more in large-scale machine learning pipelines, including large\nlanguage models (LLMs), where it enables low-rank adaptation (LoRA). The\nimplemented algorithm is based on the classic two-stage QR reduction,\nconsisting of successive matrix reduction to band form and bidiagonal form. Our\nimplementation leverages Julia's multiple dispatch and metaprogramming\ncapabilities, integrating with the GPUArrays and KernelAbstractions frameworks\nto provide a unified type and hardware-agnostic function. It supports diverse\nGPU architectures and data types, and is, to our knowledge, the first\nGPU-accelerated singular value implementation to support Apple Metal GPUs and\nhalf precision. Performance results on multiple GPU backends and data types\ndemonstrate that portability does not require sacrificing performance: the\nunified function outperforms most linear algebra libraries (MAGMA, SLATE,\nrocSOLVER, oneMKL) for matrix sizes larger than 1024x1024, and achieves 80%-90%\nof the performance of cuSOLVER for large matrices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a portable, GPU-accelerated implementation of a QR-based\nsingular value computation algorithm in Julia. The singular value ecomposition\n(SVD) is a fundamental numerical tool in scientific computing and machine\nlearning, providing optimal low-rank matrix approximations. Its importance has\nincreased even more in large-scale machine learning pipelines, including large\nlanguage models (LLMs), where it enables low-rank adaptation (LoRA). The\nimplemented algorithm is based on the classic two-stage QR reduction,\nconsisting of successive matrix reduction to band form and bidiagonal form. Our\nimplementation leverages Julia's multiple dispatch and metaprogramming\ncapabilities, integrating with the GPUArrays and KernelAbstractions frameworks\nto provide a unified type and hardware-agnostic function. It supports diverse\nGPU architectures and data types, and is, to our knowledge, the first\nGPU-accelerated singular value implementation to support Apple Metal GPUs and\nhalf precision. Performance results on multiple GPU backends and data types\ndemonstrate that portability does not require sacrificing performance: the\nunified function outperforms most linear algebra libraries (MAGMA, SLATE,\nrocSOLVER, oneMKL) for matrix sizes larger than 1024x1024, and achieves 80%-90%\nof the performance of cuSOLVER for large matrices."
                },
                "authors": [
                    {
                        "name": "Evelyne Ringoot"
                    },
                    {
                        "name": "Rabab Alomairy"
                    },
                    {
                        "name": "Valentin Churavy"
                    },
                    {
                        "name": "Alan Edelman"
                    }
                ],
                "author_detail": {
                    "name": "Alan Edelman"
                },
                "author": "Alan Edelman",
                "arxiv_doi": "10.1145/3754598.3754667",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3754598.3754667",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.06339v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06339v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 6 figures, 4 tables",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06330v1",
                "updated": "2025-08-08T14:08:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    8,
                    3,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T14:08:03Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    8,
                    3,
                    4,
                    220,
                    0
                ],
                "title": "L2Calib: $SE(3)$-Manifold Reinforcement Learning for Robust Extrinsic\n  Calibration with Degenerate Motion Resilience",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "L2Calib: $SE(3)$-Manifold Reinforcement Learning for Robust Extrinsic\n  Calibration with Degenerate Motion Resilience"
                },
                "summary": "Extrinsic calibration is essential for multi-sensor fusion, existing methods\nrely on structured targets or fully-excited data, limiting real-world\napplicability. Online calibration further suffers from weak excitation, leading\nto unreliable estimates. To address these limitations, we propose a\nreinforcement learning (RL)-based extrinsic calibration framework that\nformulates extrinsic calibration as a decision-making problem, directly\noptimizes $SE(3)$ extrinsics to enhance odometry accuracy. Our approach\nleverages a probabilistic Bingham distribution to model 3D rotations, ensuring\nstable optimization while inherently retaining quaternion symmetry. A\ntrajectory alignment reward mechanism enables robust calibration without\nstructured targets by quantitatively evaluating estimated tightly-coupled\ntrajectory against a reference trajectory. Additionally, an automated data\nselection module filters uninformative samples, significantly improving\nefficiency and scalability for large-scale datasets. Extensive experiments on\nUAVs, UGVs, and handheld platforms demonstrate that our method outperforms\ntraditional optimization-based approaches, achieving high-precision calibration\neven under weak excitation conditions. Our framework simplifies deployment on\ndiverse robotic platforms by eliminating the need for high-quality initial\nextrinsics and enabling calibration from routine operating data. The code is\navailable at https://github.com/APRIL-ZJU/learn-to-calibrate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extrinsic calibration is essential for multi-sensor fusion, existing methods\nrely on structured targets or fully-excited data, limiting real-world\napplicability. Online calibration further suffers from weak excitation, leading\nto unreliable estimates. To address these limitations, we propose a\nreinforcement learning (RL)-based extrinsic calibration framework that\nformulates extrinsic calibration as a decision-making problem, directly\noptimizes $SE(3)$ extrinsics to enhance odometry accuracy. Our approach\nleverages a probabilistic Bingham distribution to model 3D rotations, ensuring\nstable optimization while inherently retaining quaternion symmetry. A\ntrajectory alignment reward mechanism enables robust calibration without\nstructured targets by quantitatively evaluating estimated tightly-coupled\ntrajectory against a reference trajectory. Additionally, an automated data\nselection module filters uninformative samples, significantly improving\nefficiency and scalability for large-scale datasets. Extensive experiments on\nUAVs, UGVs, and handheld platforms demonstrate that our method outperforms\ntraditional optimization-based approaches, achieving high-precision calibration\neven under weak excitation conditions. Our framework simplifies deployment on\ndiverse robotic platforms by eliminating the need for high-quality initial\nextrinsics and enabling calibration from routine operating data. The code is\navailable at https://github.com/APRIL-ZJU/learn-to-calibrate."
                },
                "authors": [
                    {
                        "name": "Baorun Li"
                    },
                    {
                        "name": "Chengrui Zhu"
                    },
                    {
                        "name": "Siyi Du"
                    },
                    {
                        "name": "Bingran Chen"
                    },
                    {
                        "name": "Jie Ren"
                    },
                    {
                        "name": "Wenfei Wang"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Jiajun Lv"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Lv"
                },
                "author": "Jiajun Lv",
                "arxiv_comment": "IROS2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06328v1",
                "updated": "2025-08-08T14:00:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    0,
                    19,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T14:00:19Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    0,
                    19,
                    4,
                    220,
                    0
                ],
                "title": "M2IO-R1: An Efficient RL-Enhanced Reasoning Framework for Multimodal\n  Retrieval Augmented Multimodal Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M2IO-R1: An Efficient RL-Enhanced Reasoning Framework for Multimodal\n  Retrieval Augmented Multimodal Generation"
                },
                "summary": "Current research on Multimodal Retrieval-Augmented Generation (MRAG) enables\ndiverse multimodal inputs but remains limited to single-modality outputs,\nrestricting expressive capacity and practical utility. In contrast, real-world\napplications often demand both multimodal inputs and multimodal outputs for\neffective communication and grounded reasoning. Motivated by the recent success\nof Reinforcement Learning (RL) in complex reasoning tasks for Large Language\nModels (LLMs), we adopt RL as a principled and effective paradigm to address\nthe multi-step, outcome-driven challenges inherent in multimodal output\ngeneration. Here, we introduce M2IO-R1, a novel framework for Multimodal\nRetrieval-Augmented Multimodal Generation (MRAMG) that supports both multimodal\ninputs and outputs. Central to our framework is an RL-based inserter,\nInserter-R1-3B, trained with Group Relative Policy Optimization to guide image\nselection and placement in a controllable and semantically aligned manner.\nEmpirical results show that our lightweight 3B inserter achieves strong\nreasoning capabilities with significantly reduced latency, outperforming\nbaselines in both quality and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current research on Multimodal Retrieval-Augmented Generation (MRAG) enables\ndiverse multimodal inputs but remains limited to single-modality outputs,\nrestricting expressive capacity and practical utility. In contrast, real-world\napplications often demand both multimodal inputs and multimodal outputs for\neffective communication and grounded reasoning. Motivated by the recent success\nof Reinforcement Learning (RL) in complex reasoning tasks for Large Language\nModels (LLMs), we adopt RL as a principled and effective paradigm to address\nthe multi-step, outcome-driven challenges inherent in multimodal output\ngeneration. Here, we introduce M2IO-R1, a novel framework for Multimodal\nRetrieval-Augmented Multimodal Generation (MRAMG) that supports both multimodal\ninputs and outputs. Central to our framework is an RL-based inserter,\nInserter-R1-3B, trained with Group Relative Policy Optimization to guide image\nselection and placement in a controllable and semantically aligned manner.\nEmpirical results show that our lightweight 3B inserter achieves strong\nreasoning capabilities with significantly reduced latency, outperforming\nbaselines in both quality and efficiency."
                },
                "authors": [
                    {
                        "name": "Zhiyou Xiao"
                    },
                    {
                        "name": "Qinhan Yu"
                    },
                    {
                        "name": "Binghui Li"
                    },
                    {
                        "name": "Geng Chen"
                    },
                    {
                        "name": "Chong Chen"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06327v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06327v1",
                "updated": "2025-08-08T13:57:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    57,
                    48,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T13:57:48Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    57,
                    48,
                    4,
                    220,
                    0
                ],
                "title": "Can Diffusion Models Bridge the Domain Gap in Cardiac MR Imaging?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Diffusion Models Bridge the Domain Gap in Cardiac MR Imaging?"
                },
                "summary": "Magnetic resonance (MR) imaging, including cardiac MR, is prone to domain\nshift due to variations in imaging devices and acquisition protocols. This\nchallenge limits the deployment of trained AI models in real-world scenarios,\nwhere performance degrades on unseen domains. Traditional solutions involve\nincreasing the size of the dataset through ad-hoc image augmentation or\nadditional online training/transfer learning, which have several limitations.\nSynthetic data offers a promising alternative, but anatomical/structural\nconsistency constraints limit the effectiveness of generative models in\ncreating image-label pairs. To address this, we propose a diffusion model (DM)\ntrained on a source domain that generates synthetic cardiac MR images that\nresemble a given reference. The synthetic data maintains spatial and structural\nfidelity, ensuring similarity to the source domain and compatibility with the\nsegmentation mask. We assess the utility of our generative approach in\nmulti-centre cardiac MR segmentation, using the 2D nnU-Net, 3D nnU-Net and\nvanilla U-Net segmentation networks. We explore domain generalisation, where,\ndomain-invariant segmentation models are trained on synthetic source domain\ndata, and domain adaptation, where, we shift target domain data towards the\nsource domain using the DM. Both strategies significantly improved segmentation\nperformance on data from an unseen target domain, in terms of surface-based\nmetrics (Welch's t-test, p < 0.01), compared to training segmentation models on\nreal data alone. The proposed method ameliorates the need for transfer learning\nor online training to address domain shift challenges in cardiac MR image\nanalysis, especially useful in data-scarce settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magnetic resonance (MR) imaging, including cardiac MR, is prone to domain\nshift due to variations in imaging devices and acquisition protocols. This\nchallenge limits the deployment of trained AI models in real-world scenarios,\nwhere performance degrades on unseen domains. Traditional solutions involve\nincreasing the size of the dataset through ad-hoc image augmentation or\nadditional online training/transfer learning, which have several limitations.\nSynthetic data offers a promising alternative, but anatomical/structural\nconsistency constraints limit the effectiveness of generative models in\ncreating image-label pairs. To address this, we propose a diffusion model (DM)\ntrained on a source domain that generates synthetic cardiac MR images that\nresemble a given reference. The synthetic data maintains spatial and structural\nfidelity, ensuring similarity to the source domain and compatibility with the\nsegmentation mask. We assess the utility of our generative approach in\nmulti-centre cardiac MR segmentation, using the 2D nnU-Net, 3D nnU-Net and\nvanilla U-Net segmentation networks. We explore domain generalisation, where,\ndomain-invariant segmentation models are trained on synthetic source domain\ndata, and domain adaptation, where, we shift target domain data towards the\nsource domain using the DM. Both strategies significantly improved segmentation\nperformance on data from an unseen target domain, in terms of surface-based\nmetrics (Welch's t-test, p < 0.01), compared to training segmentation models on\nreal data alone. The proposed method ameliorates the need for transfer learning\nor online training to address domain shift challenges in cardiac MR image\nanalysis, especially useful in data-scarce settings."
                },
                "authors": [
                    {
                        "name": "Xin Ci Wong"
                    },
                    {
                        "name": "Duygu Sarikaya"
                    },
                    {
                        "name": "Kieran Zucker"
                    },
                    {
                        "name": "Marc De Kamps"
                    },
                    {
                        "name": "Nishant Ravikumar"
                    }
                ],
                "author_detail": {
                    "name": "Nishant Ravikumar"
                },
                "author": "Nishant Ravikumar",
                "arxiv_comment": "ICONIP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06327v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06327v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06317v1",
                "updated": "2025-08-08T13:47:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    47,
                    0,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T13:47:00Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    47,
                    0,
                    4,
                    220,
                    0
                ],
                "title": "Uncertainty-quantified Rollout Policy Adaptation for Unlabelled\n  Cross-domain Temporal Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty-quantified Rollout Policy Adaptation for Unlabelled\n  Cross-domain Temporal Grounding"
                },
                "summary": "Video Temporal Grounding (TG) aims to temporally locate video segments\nmatching a natural language description (a query) in a long video. While\nVision-Language Models (VLMs) are effective at holistic semantic matching, they\noften struggle with fine-grained temporal localisation. Recently, Group\nRelative Policy Optimisation (GRPO) reformulates the inference process as a\nreinforcement learning task, enabling fine-grained grounding and achieving\nstrong in-domain performance. However, GRPO relies on labelled data, making it\nunsuitable in unlabelled domains. Moreover, because videos are large and\nexpensive to store and process, performing full-scale adaptation introduces\nprohibitive latency and computational overhead, making it impractical for\nreal-time deployment. To overcome both problems, we introduce a Data-Efficient\nUnlabelled Cross-domain Temporal Grounding method, from which a model is first\ntrained on a labelled source domain, then adapted to a target domain using only\na small number of unlabelled videos from the target domain. This approach\neliminates the need for target annotation and keeps both computational and\nstorage overhead low enough to run in real time. Specifically, we introduce.\nUncertainty-quantified Rollout Policy Adaptation (URPA) for cross-domain\nknowledge transfer in learning video temporal grounding without target labels.\nURPA generates multiple candidate predictions using GRPO rollouts, averages\nthem to form a pseudo label, and estimates confidence from the variance across\nthese rollouts. This confidence then weights the training rewards, guiding the\nmodel to focus on reliable supervision. Experiments on three datasets across\nsix cross-domain settings show that URPA generalises well using only a few\nunlabelled target videos. Codes will be released once published.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Temporal Grounding (TG) aims to temporally locate video segments\nmatching a natural language description (a query) in a long video. While\nVision-Language Models (VLMs) are effective at holistic semantic matching, they\noften struggle with fine-grained temporal localisation. Recently, Group\nRelative Policy Optimisation (GRPO) reformulates the inference process as a\nreinforcement learning task, enabling fine-grained grounding and achieving\nstrong in-domain performance. However, GRPO relies on labelled data, making it\nunsuitable in unlabelled domains. Moreover, because videos are large and\nexpensive to store and process, performing full-scale adaptation introduces\nprohibitive latency and computational overhead, making it impractical for\nreal-time deployment. To overcome both problems, we introduce a Data-Efficient\nUnlabelled Cross-domain Temporal Grounding method, from which a model is first\ntrained on a labelled source domain, then adapted to a target domain using only\na small number of unlabelled videos from the target domain. This approach\neliminates the need for target annotation and keeps both computational and\nstorage overhead low enough to run in real time. Specifically, we introduce.\nUncertainty-quantified Rollout Policy Adaptation (URPA) for cross-domain\nknowledge transfer in learning video temporal grounding without target labels.\nURPA generates multiple candidate predictions using GRPO rollouts, averages\nthem to form a pseudo label, and estimates confidence from the variance across\nthese rollouts. This confidence then weights the training rewards, guiding the\nmodel to focus on reliable supervision. Experiments on three datasets across\nsix cross-domain settings show that URPA generalises well using only a few\nunlabelled target videos. Codes will be released once published."
                },
                "authors": [
                    {
                        "name": "Jian Hu"
                    },
                    {
                        "name": "Zixu Cheng"
                    },
                    {
                        "name": "Shaogang Gong"
                    },
                    {
                        "name": "Isabel Guan"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Kun Shao"
                    }
                ],
                "author_detail": {
                    "name": "Kun Shao"
                },
                "author": "Kun Shao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06313v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06313v1",
                "updated": "2025-08-08T13:40:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    40,
                    37,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T13:40:37Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    40,
                    37,
                    4,
                    220,
                    0
                ],
                "title": "Surrogate-Enhanced Modeling and Adaptive Modular Control of All-Electric\n  Heavy-Duty Robotic Manipulators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surrogate-Enhanced Modeling and Adaptive Modular Control of All-Electric\n  Heavy-Duty Robotic Manipulators"
                },
                "summary": "This paper presents a unified system-level modeling and control framework for\nan all-electric heavy-duty robotic manipulator (HDRM) driven by\nelectromechanical linear actuators (EMLAs). A surrogate-enhanced actuator\nmodel, combining integrated electromechanical dynamics with a neural network\ntrained on a dedicated testbed, is integrated into an extended virtual\ndecomposition control (VDC) architecture augmented by a natural adaptation law.\nThe derived analytical HDRM model supports a hierarchical control structure\nthat seamlessly maps high-level force and velocity objectives to real-time\nactuator commands, accompanied by a Lyapunov-based stability proof. In\nmulti-domain simulations of both cubic and a custom planar triangular\ntrajectory, the proposed adaptive modular controller achieves sub-centimeter\nCartesian tracking accuracy. Experimental validation of the same 1-DoF platform\nunder realistic load emulation confirms the efficacy of the proposed control\nstrategy. These findings demonstrate that a surrogate-enhanced EMLA model\nembedded in the VDC approach can enable modular, real-time control of an\nall-electric HDRM, supporting its deployment in next-generation mobile working\nmachines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a unified system-level modeling and control framework for\nan all-electric heavy-duty robotic manipulator (HDRM) driven by\nelectromechanical linear actuators (EMLAs). A surrogate-enhanced actuator\nmodel, combining integrated electromechanical dynamics with a neural network\ntrained on a dedicated testbed, is integrated into an extended virtual\ndecomposition control (VDC) architecture augmented by a natural adaptation law.\nThe derived analytical HDRM model supports a hierarchical control structure\nthat seamlessly maps high-level force and velocity objectives to real-time\nactuator commands, accompanied by a Lyapunov-based stability proof. In\nmulti-domain simulations of both cubic and a custom planar triangular\ntrajectory, the proposed adaptive modular controller achieves sub-centimeter\nCartesian tracking accuracy. Experimental validation of the same 1-DoF platform\nunder realistic load emulation confirms the efficacy of the proposed control\nstrategy. These findings demonstrate that a surrogate-enhanced EMLA model\nembedded in the VDC approach can enable modular, real-time control of an\nall-electric HDRM, supporting its deployment in next-generation mobile working\nmachines."
                },
                "authors": [
                    {
                        "name": "Amir Hossein Barjini"
                    },
                    {
                        "name": "Mohammad Bahari"
                    },
                    {
                        "name": "Mahdi Hejrati"
                    },
                    {
                        "name": "Jouni Mattila"
                    }
                ],
                "author_detail": {
                    "name": "Jouni Mattila"
                },
                "author": "Jouni Mattila",
                "arxiv_comment": "This is submitted to IEEE T-ASE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06313v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06313v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06312v1",
                "updated": "2025-08-08T13:39:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    39,
                    5,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T13:39:05Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    39,
                    5,
                    4,
                    220,
                    0
                ],
                "title": "Chain-of-Alpha: Unleashing the Power of Large Language Models for Alpha\n  Mining in Quantitative Trading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Alpha: Unleashing the Power of Large Language Models for Alpha\n  Mining in Quantitative Trading"
                },
                "summary": "Alpha factor mining is a fundamental task in quantitative trading, aimed at\ndiscovering interpretable signals that can predict asset returns beyond\nsystematic market risk. While traditional methods rely on manual formula design\nor heuristic search with machine learning, recent advances have leveraged Large\nLanguage Models (LLMs) for automated factor discovery. However, existing\nLLM-based alpha mining approaches remain limited in terms of automation,\ngenerality, and efficiency. In this paper, we propose Chain-of-Alpha, a novel,\nsimple, yet effective and efficient LLM-based framework for fully automated\nformulaic alpha mining. Our method features a dual-chain architecture,\nconsisting of a Factor Generation Chain and a Factor Optimization Chain, which\niteratively generate, evaluate, and refine candidate alpha factors using only\nmarket data, while leveraging backtest feedback and prior optimization\nknowledge. The two chains work synergistically to enable high-quality alpha\ndiscovery without human intervention and offer strong scalability. Extensive\nexperiments on real-world A-share benchmarks demonstrate that Chain-of-Alpha\noutperforms existing baselines across multiple metrics, presenting a promising\ndirection for LLM-driven quantitative research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alpha factor mining is a fundamental task in quantitative trading, aimed at\ndiscovering interpretable signals that can predict asset returns beyond\nsystematic market risk. While traditional methods rely on manual formula design\nor heuristic search with machine learning, recent advances have leveraged Large\nLanguage Models (LLMs) for automated factor discovery. However, existing\nLLM-based alpha mining approaches remain limited in terms of automation,\ngenerality, and efficiency. In this paper, we propose Chain-of-Alpha, a novel,\nsimple, yet effective and efficient LLM-based framework for fully automated\nformulaic alpha mining. Our method features a dual-chain architecture,\nconsisting of a Factor Generation Chain and a Factor Optimization Chain, which\niteratively generate, evaluate, and refine candidate alpha factors using only\nmarket data, while leveraging backtest feedback and prior optimization\nknowledge. The two chains work synergistically to enable high-quality alpha\ndiscovery without human intervention and offer strong scalability. Extensive\nexperiments on real-world A-share benchmarks demonstrate that Chain-of-Alpha\noutperforms existing baselines across multiple metrics, presenting a promising\ndirection for LLM-driven quantitative research."
                },
                "authors": [
                    {
                        "name": "Lang Cao"
                    },
                    {
                        "name": "Zekun Xi"
                    },
                    {
                        "name": "Long Liao"
                    },
                    {
                        "name": "Ziwei Yang"
                    },
                    {
                        "name": "Zheng Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Cao"
                },
                "author": "Zheng Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06309v1",
                "updated": "2025-08-08T13:35:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    35,
                    40,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T13:35:40Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    35,
                    40,
                    4,
                    220,
                    0
                ],
                "title": "Matrix-Driven Instant Review: Confident Detection and Reconstruction of\n  LLM Plagiarism on PC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix-Driven Instant Review: Confident Detection and Reconstruction of\n  LLM Plagiarism on PC"
                },
                "summary": "In recent years, concerns about intellectual property (IP) in large language\nmodels (LLMs) have grown significantly. Plagiarizing other LLMs (through direct\nweight copying, upcycling, pruning, or continual pretraining) and claiming\nauthorship without properly attributing to the original license, is a serious\nmisconduct that can lead to significant financial and reputational harm to the\noriginal developers. However, existing methods for detecting LLM plagiarism\nfall short in key areas. They fail to accurately reconstruct weight\ncorrespondences, lack the ability to compute statistical significance measures\nsuch as $p$-values, and may mistakenly flag models trained on similar data as\nbeing related. To address these limitations, we propose Matrix-Driven Instant\nReview (MDIR), a novel method that leverages matrix analysis and Large\nDeviation Theory. MDIR achieves accurate reconstruction of weight\nrelationships, provides rigorous $p$-value estimation, and focuses exclusively\non weight similarity without requiring full model inference. Experimental\nresults demonstrate that MDIR reliably detects plagiarism even after extensive\ntransformations, such as random permutations and continual pretraining with\ntrillions of tokens. Moreover, all detections can be performed on a single PC\nwithin an hour, making MDIR both efficient and accessible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, concerns about intellectual property (IP) in large language\nmodels (LLMs) have grown significantly. Plagiarizing other LLMs (through direct\nweight copying, upcycling, pruning, or continual pretraining) and claiming\nauthorship without properly attributing to the original license, is a serious\nmisconduct that can lead to significant financial and reputational harm to the\noriginal developers. However, existing methods for detecting LLM plagiarism\nfall short in key areas. They fail to accurately reconstruct weight\ncorrespondences, lack the ability to compute statistical significance measures\nsuch as $p$-values, and may mistakenly flag models trained on similar data as\nbeing related. To address these limitations, we propose Matrix-Driven Instant\nReview (MDIR), a novel method that leverages matrix analysis and Large\nDeviation Theory. MDIR achieves accurate reconstruction of weight\nrelationships, provides rigorous $p$-value estimation, and focuses exclusively\non weight similarity without requiring full model inference. Experimental\nresults demonstrate that MDIR reliably detects plagiarism even after extensive\ntransformations, such as random permutations and continual pretraining with\ntrillions of tokens. Moreover, all detections can be performed on a single PC\nwithin an hour, making MDIR both efficient and accessible."
                },
                "authors": [
                    {
                        "name": "Ruichong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ruichong Zhang"
                },
                "author": "Ruichong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14810v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14810v2",
                "updated": "2025-08-08T13:29:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    29,
                    20,
                    4,
                    220,
                    0
                ],
                "published": "2025-04-21T02:25:03Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    2,
                    25,
                    3,
                    0,
                    111,
                    0
                ],
                "title": "DONOD: Efficient and Generalizable Instruction Fine-Tuning for LLMs via\n  Model-Intrinsic Dataset Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DONOD: Efficient and Generalizable Instruction Fine-Tuning for LLMs via\n  Model-Intrinsic Dataset Pruning"
                },
                "summary": "Ad-hoc instruction fine-tuning of large language models (LLMs) is widely\nadopted for domain-specific adaptation. While domain-specific supervised\nfine-tuning (SFT) is effective and efficient, it often weakens cross-domain\ngeneralization and struggles with noisy training data. To address these\nchallenges, we propose DONOD, a lightweight model-intrinsic data pruning\nmethod. Our approach evaluates data using two model-parameter-based metrics:\nDelta of Norm (DON), which captures the cumulative influence on model weights,\nand Norm of Delta (NOD), which quantifies weight instability. Moreover, by\nemploying the Technique for Order of Preference by Similarity to Ideal Solution\n(TOPSIS) algorithm, we effectively filter noisy, unlearnable, and\ngeneralization-harming samples without relying on auxiliary models during the\nSFT process. Experiments on mathematical tasks demonstrate that data selected\nby DONOD achieves superior fine-tuning efficiency and improved robustness\nagainst noisy data. By filtering out 70% of the whole dataset, we improve\ntarget-domain accuracy by 14.90% and cross-domain accuracy by 5.67%. Meanwhile,\nour selected data present superior cross-architecture generalization. Data\npruned by smaller models (e.g., Llama 3.1-8B) generalize effectively on larger\nmodels (e.g., Llama 2-13B). Compared to existing related methodologies, DONOD\ndemonstrates comparable or superior performance while remaining\ndataset-agnostic, enabling broader applicability. Code will be made publicly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ad-hoc instruction fine-tuning of large language models (LLMs) is widely\nadopted for domain-specific adaptation. While domain-specific supervised\nfine-tuning (SFT) is effective and efficient, it often weakens cross-domain\ngeneralization and struggles with noisy training data. To address these\nchallenges, we propose DONOD, a lightweight model-intrinsic data pruning\nmethod. Our approach evaluates data using two model-parameter-based metrics:\nDelta of Norm (DON), which captures the cumulative influence on model weights,\nand Norm of Delta (NOD), which quantifies weight instability. Moreover, by\nemploying the Technique for Order of Preference by Similarity to Ideal Solution\n(TOPSIS) algorithm, we effectively filter noisy, unlearnable, and\ngeneralization-harming samples without relying on auxiliary models during the\nSFT process. Experiments on mathematical tasks demonstrate that data selected\nby DONOD achieves superior fine-tuning efficiency and improved robustness\nagainst noisy data. By filtering out 70% of the whole dataset, we improve\ntarget-domain accuracy by 14.90% and cross-domain accuracy by 5.67%. Meanwhile,\nour selected data present superior cross-architecture generalization. Data\npruned by smaller models (e.g., Llama 3.1-8B) generalize effectively on larger\nmodels (e.g., Llama 2-13B). Compared to existing related methodologies, DONOD\ndemonstrates comparable or superior performance while remaining\ndataset-agnostic, enabling broader applicability. Code will be made publicly\navailable."
                },
                "authors": [
                    {
                        "name": "Jucheng Hu"
                    },
                    {
                        "name": "Surong Yang"
                    },
                    {
                        "name": "Lijun Wu"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Dongzhan Zhou"
                },
                "author": "Dongzhan Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14810v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14810v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03700v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03700v2",
                "updated": "2025-08-08T13:25:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    25,
                    14,
                    4,
                    220,
                    0
                ],
                "published": "2025-07-19T12:33:43Z",
                "published_parsed": [
                    2025,
                    7,
                    19,
                    12,
                    33,
                    43,
                    5,
                    200,
                    0
                ],
                "title": "MagicGUI: A Foundational Mobile GUI Agent with Scalable Data Pipeline\n  and Reinforcement Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicGUI: A Foundational Mobile GUI Agent with Scalable Data Pipeline\n  and Reinforcement Fine-tuning"
                },
                "summary": "This paper presents MagicGUI, a foundational mobile GUI agent designed to\naddress critical challenges in perception, grounding, and reasoning within\nreal-world mobile GUI environments. The framework is underpinned by following\nsix key components: (1) a comprehensive and accurate dataset, constructed via\nthe scalable GUI Data Pipeline, which aggregates the largest and most diverse\nGUI-centric multimodal data to date from open-source repositories, automated\ncrawling, and targeted manual annotation; (2) enhanced perception and grounding\ncapabilities, facilitating fine-grained multimodal alignment for UI element\nreferencing, grounding, and screen comprehension; (3) a comprehensive and\nunified action space, encompassing both fundamental UI operations and complex\ninteractive intents to support human-agent interactions; (4) planning-oriented\nreasoning mechanisms that enable the model to decompose complex user\ninstructions into sequential actions with explicit intermediate meta-paln\nreasoning; (5) an iterative two-stage training procedure, combining large-scale\ncontinue pre-training on 7.8M samples with reinforcement fine-tuning utilizing\na spatially enhanced composite reward and dual filtering strategy; and (6)\ncompetitive performance on both the proprietary Magic-RICH benchmark and over a\ndozen public benchmarks, achieving superior performance across GUI perception\nand agent tasks, while demonstrating robust generalization and real-world\ndeployment potential in practical mobile GUI scenarios, as detailed in Figure\n1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents MagicGUI, a foundational mobile GUI agent designed to\naddress critical challenges in perception, grounding, and reasoning within\nreal-world mobile GUI environments. The framework is underpinned by following\nsix key components: (1) a comprehensive and accurate dataset, constructed via\nthe scalable GUI Data Pipeline, which aggregates the largest and most diverse\nGUI-centric multimodal data to date from open-source repositories, automated\ncrawling, and targeted manual annotation; (2) enhanced perception and grounding\ncapabilities, facilitating fine-grained multimodal alignment for UI element\nreferencing, grounding, and screen comprehension; (3) a comprehensive and\nunified action space, encompassing both fundamental UI operations and complex\ninteractive intents to support human-agent interactions; (4) planning-oriented\nreasoning mechanisms that enable the model to decompose complex user\ninstructions into sequential actions with explicit intermediate meta-paln\nreasoning; (5) an iterative two-stage training procedure, combining large-scale\ncontinue pre-training on 7.8M samples with reinforcement fine-tuning utilizing\na spatially enhanced composite reward and dual filtering strategy; and (6)\ncompetitive performance on both the proprietary Magic-RICH benchmark and over a\ndozen public benchmarks, achieving superior performance across GUI perception\nand agent tasks, while demonstrating robust generalization and real-world\ndeployment potential in practical mobile GUI scenarios, as detailed in Figure\n1."
                },
                "authors": [
                    {
                        "name": "Liujian Tang"
                    },
                    {
                        "name": "Shaokang Dong"
                    },
                    {
                        "name": "Yijia Huang"
                    },
                    {
                        "name": "Minqi Xiang"
                    },
                    {
                        "name": "Hongtao Ruan"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Shuo Li"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Zhihui Cao"
                    },
                    {
                        "name": "Hailiang Pang"
                    },
                    {
                        "name": "Heng Kong"
                    },
                    {
                        "name": "He Yang"
                    },
                    {
                        "name": "Mingxu Chai"
                    },
                    {
                        "name": "Zhilin Gao"
                    },
                    {
                        "name": "Xingyu Liu"
                    },
                    {
                        "name": "Yingnan Fu"
                    },
                    {
                        "name": "Jiaming Liu"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Yu-Gang Jiang"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Kang Wang"
                    },
                    {
                        "name": "Yunke Zhang"
                    },
                    {
                        "name": "Yuran Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuran Wang"
                },
                "author": "Yuran Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03700v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03700v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06300v1",
                "updated": "2025-08-08T13:20:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    20,
                    37,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T13:20:37Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    20,
                    37,
                    4,
                    220,
                    0
                ],
                "title": "Automatic Semantic Alignment of Flow Pattern Representations for\n  Exploration with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Semantic Alignment of Flow Pattern Representations for\n  Exploration with Large Language Models"
                },
                "summary": "Explorative flow visualization allows domain experts to analyze complex flow\nstructures by interactively investigating flow patterns. However, traditional\nvisual interfaces often rely on specialized graphical representations and\ninteractions, which require additional effort to learn and use. Natural\nlanguage interaction offers a more intuitive alternative, but teaching machines\nto recognize diverse scientific concepts and extract corresponding structures\nfrom flow data poses a significant challenge. In this paper, we introduce an\nautomated framework that aligns flow pattern representations with the semantic\nspace of large language models (LLMs), eliminating the need for manual\nlabeling. Our approach encodes streamline segments using a denoising\nautoencoder and maps the generated flow pattern representations to LLM\nembeddings via a projector layer. This alignment empowers semantic matching\nbetween textual embeddings and flow representations through an attention\nmechanism, enabling the extraction of corresponding flow patterns based on\ntextual descriptions. To enhance accessibility, we develop an interactive\ninterface that allows users to query and visualize flow structures using\nnatural language. Through case studies, we demonstrate the effectiveness of our\nframework in enabling intuitive and intelligent flow exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explorative flow visualization allows domain experts to analyze complex flow\nstructures by interactively investigating flow patterns. However, traditional\nvisual interfaces often rely on specialized graphical representations and\ninteractions, which require additional effort to learn and use. Natural\nlanguage interaction offers a more intuitive alternative, but teaching machines\nto recognize diverse scientific concepts and extract corresponding structures\nfrom flow data poses a significant challenge. In this paper, we introduce an\nautomated framework that aligns flow pattern representations with the semantic\nspace of large language models (LLMs), eliminating the need for manual\nlabeling. Our approach encodes streamline segments using a denoising\nautoencoder and maps the generated flow pattern representations to LLM\nembeddings via a projector layer. This alignment empowers semantic matching\nbetween textual embeddings and flow representations through an attention\nmechanism, enabling the extraction of corresponding flow patterns based on\ntextual descriptions. To enhance accessibility, we develop an interactive\ninterface that allows users to query and visualize flow structures using\nnatural language. Through case studies, we demonstrate the effectiveness of our\nframework in enabling intuitive and intelligent flow exploration."
                },
                "authors": [
                    {
                        "name": "Weihan Zhang"
                    },
                    {
                        "name": "Jun Tao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Tao"
                },
                "author": "Jun Tao",
                "arxiv_comment": "Accepted by IEEE VIS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06297v1",
                "updated": "2025-08-08T13:19:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    19,
                    30,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T13:19:30Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    19,
                    30,
                    4,
                    220,
                    0
                ],
                "title": "KV Cache Compression for Inference Efficiency in LLMs: A Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Compression for Inference Efficiency in LLMs: A Review"
                },
                "summary": "Withtherapid advancement of large language models (LLMs), the context length\nfor inference has been continuously increasing, leading to an exponential\ngrowth in the demand for Key-Value (KV) caching. This has resulted in a\nsignificant memory bottleneck, limiting the inference efficiency and\nscalability of the models. Therefore, optimizing the KV cache during inference\nis crucial for enhancing performance and efficiency. This review systematically\nexamines current KV cache optimization techniques, including compression\nstrategies such as selective token strategies, quantization, and attention\ncompression. We evaluate the effectiveness, trade-offs, and application\nscenarios of these methods, providing a comprehensive analysis of their impact\non memory usage and inference speed. We focus on identifying the limitations\nand challenges of existing methods, such as compatibility issues with different\nmodels and tasks. Additionally, this review highlights future research\ndirections, including hybrid optimization techniques, adaptive dynamic\nstrategies, and software-hardware co-design. These approaches aim to improve\ninference efficiency and promote the practical application of large language\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Withtherapid advancement of large language models (LLMs), the context length\nfor inference has been continuously increasing, leading to an exponential\ngrowth in the demand for Key-Value (KV) caching. This has resulted in a\nsignificant memory bottleneck, limiting the inference efficiency and\nscalability of the models. Therefore, optimizing the KV cache during inference\nis crucial for enhancing performance and efficiency. This review systematically\nexamines current KV cache optimization techniques, including compression\nstrategies such as selective token strategies, quantization, and attention\ncompression. We evaluate the effectiveness, trade-offs, and application\nscenarios of these methods, providing a comprehensive analysis of their impact\non memory usage and inference speed. We focus on identifying the limitations\nand challenges of existing methods, such as compatibility issues with different\nmodels and tasks. Additionally, this review highlights future research\ndirections, including hybrid optimization techniques, adaptive dynamic\nstrategies, and software-hardware co-design. These approaches aim to improve\ninference efficiency and promote the practical application of large language\nmodels."
                },
                "authors": [
                    {
                        "name": "Yanyu Liu"
                    },
                    {
                        "name": "Jingying Fu"
                    },
                    {
                        "name": "Sixiang Liu"
                    },
                    {
                        "name": "Yitian Zou"
                    },
                    {
                        "name": "You Fu"
                    },
                    {
                        "name": "Jiehan Zhou"
                    },
                    {
                        "name": "Shouhua Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shouhua Zhang"
                },
                "arxiv_affiliation": "University of Oulu",
                "author": "Shouhua Zhang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06296v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06296v1",
                "updated": "2025-08-08T13:15:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    15,
                    40,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T13:15:40Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    15,
                    40,
                    4,
                    220,
                    0
                ],
                "title": "LLM Robustness Leaderboard v1 --Technical report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Robustness Leaderboard v1 --Technical report"
                },
                "summary": "This technical report accompanies the LLM robustness leaderboard published by\nPRISM Eval for the Paris AI Action Summit. We introduce PRISM Eval Behavior\nElicitation Tool (BET), an AI system performing automated red-teaming through\nDynamic Adversarial Optimization that achieves 100% Attack Success Rate (ASR)\nagainst 37 of 41 state-of-the-art LLMs. Beyond binary success metrics, we\npropose a fine-grained robustness metric estimating the average number of\nattempts required to elicit harmful behaviors, revealing that attack difficulty\nvaries by over 300-fold across models despite universal vulnerability. We\nintroduce primitive-level vulnerability analysis to identify which jailbreaking\ntechniques are most effective for specific hazard categories. Our collaborative\nevaluation with trusted third parties from the AI Safety Network demonstrates\npractical pathways for distributed robustness assessment across the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report accompanies the LLM robustness leaderboard published by\nPRISM Eval for the Paris AI Action Summit. We introduce PRISM Eval Behavior\nElicitation Tool (BET), an AI system performing automated red-teaming through\nDynamic Adversarial Optimization that achieves 100% Attack Success Rate (ASR)\nagainst 37 of 41 state-of-the-art LLMs. Beyond binary success metrics, we\npropose a fine-grained robustness metric estimating the average number of\nattempts required to elicit harmful behaviors, revealing that attack difficulty\nvaries by over 300-fold across models despite universal vulnerability. We\nintroduce primitive-level vulnerability analysis to identify which jailbreaking\ntechniques are most effective for specific hazard categories. Our collaborative\nevaluation with trusted third parties from the AI Safety Network demonstrates\npractical pathways for distributed robustness assessment across the community."
                },
                "authors": [
                    {
                        "name": "Pierre Peigné - Lefebvre"
                    },
                    {
                        "name": "Quentin Feuillade-Montixi"
                    },
                    {
                        "name": "Tom David"
                    },
                    {
                        "name": "Nicolas Miailhe"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Miailhe"
                },
                "author": "Nicolas Miailhe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06296v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06296v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06284v1",
                "updated": "2025-08-08T13:05:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    5,
                    31,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T13:05:31Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    5,
                    31,
                    4,
                    220,
                    0
                ],
                "title": "Leveraging LLMs for Scalable Non-intrusive Speech Quality Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMs for Scalable Non-intrusive Speech Quality Assessment"
                },
                "summary": "Non-intrusive speech quality assessment (SQA) systems suffer from limited\ntraining data and costly human annotations, hindering their generalization to\nreal-time conferencing calls. In this work, we propose leveraging large\nlanguage models (LLMs) as pseudo-raters for speech quality to address these\ndata bottlenecks. We construct LibriAugmented, a dataset consisting of 101,129\nspeech clips with simulated degradations labeled by a fine-tuned auditory LLM\n(Vicuna-7b-v1.5). We compare three training strategies: using human-labeled\ndata, using LLM-labeled data, and a two-stage approach (pretraining on LLM\nlabels, then fine-tuning on human labels), using both DNSMOS Pro and DeePMOS.\nWe test on several datasets across languages and quality degradations. While\nLLM-labeled training yields mixed results compared to human-labeled training,\nwe provide empirical evidence that the two-stage approach improves the\ngeneralization performance (e.g., DNSMOS Pro achieves 0.63 vs. 0.55 PCC on\nNISQA_TEST_LIVETALK and 0.73 vs. 0.65 PCC on Tencent with reverb). Our findings\ndemonstrate the potential of using LLMs as scalable pseudo-raters for speech\nquality assessment, offering a cost-effective solution to the data limitation\nproblem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-intrusive speech quality assessment (SQA) systems suffer from limited\ntraining data and costly human annotations, hindering their generalization to\nreal-time conferencing calls. In this work, we propose leveraging large\nlanguage models (LLMs) as pseudo-raters for speech quality to address these\ndata bottlenecks. We construct LibriAugmented, a dataset consisting of 101,129\nspeech clips with simulated degradations labeled by a fine-tuned auditory LLM\n(Vicuna-7b-v1.5). We compare three training strategies: using human-labeled\ndata, using LLM-labeled data, and a two-stage approach (pretraining on LLM\nlabels, then fine-tuning on human labels), using both DNSMOS Pro and DeePMOS.\nWe test on several datasets across languages and quality degradations. While\nLLM-labeled training yields mixed results compared to human-labeled training,\nwe provide empirical evidence that the two-stage approach improves the\ngeneralization performance (e.g., DNSMOS Pro achieves 0.63 vs. 0.55 PCC on\nNISQA_TEST_LIVETALK and 0.73 vs. 0.65 PCC on Tencent with reverb). Our findings\ndemonstrate the potential of using LLMs as scalable pseudo-raters for speech\nquality assessment, offering a cost-effective solution to the data limitation\nproblem."
                },
                "authors": [
                    {
                        "name": "Fredrik Cumlin"
                    },
                    {
                        "name": "Xinyu Liang"
                    },
                    {
                        "name": "Anubhab Ghosh"
                    },
                    {
                        "name": "Saikat Chatterjee"
                    }
                ],
                "author_detail": {
                    "name": "Saikat Chatterjee"
                },
                "author": "Saikat Chatterjee",
                "arxiv_comment": "ECAI workshop paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06278v1",
                "updated": "2025-08-08T12:58:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    12,
                    58,
                    14,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T12:58:14Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    12,
                    58,
                    14,
                    4,
                    220,
                    0
                ],
                "title": "Mitigating Undesired Conditions in Flexible Production with\n  Product-Process-Resource Asset Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Undesired Conditions in Flexible Production with\n  Product-Process-Resource Asset Knowledge Graphs"
                },
                "summary": "Contemporary industrial cyber-physical production systems (CPPS) composed of\nrobotic workcells face significant challenges in the analysis of undesired\nconditions due to the flexibility of Industry 4.0 that disrupts traditional\nquality assurance mechanisms. This paper presents a novel industry-oriented\nsemantic model called Product-Process-Resource Asset Knowledge Graph (PPR-AKG),\nwhich is designed to analyze and mitigate undesired conditions in flexible\nCPPS. Built on top of the well-proven Product-Process-Resource (PPR) model\noriginating from ISA-95 and VDI-3682, a comprehensive OWL ontology addresses\nshortcomings of conventional model-driven engineering for CPPS, particularly\ninadequate undesired condition and error handling representation. The\nintegration of semantic technologies with large language models (LLMs) provides\nintuitive interfaces for factory operators, production planners, and engineers\nto interact with the entire model using natural language. Evaluation with the\nuse case addressing electric vehicle battery remanufacturing demonstrates that\nthe PPR-AKG approach efficiently supports resource allocation based on\nexplicitly represented capabilities as well as identification and mitigation of\nundesired conditions in production. The key contributions include (1) a\nholistic PPR-AKG model capturing multi-dimensional production knowledge, and\n(2) the useful combination of the PPR-AKG with LLM-based chatbots for human\ninteraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary industrial cyber-physical production systems (CPPS) composed of\nrobotic workcells face significant challenges in the analysis of undesired\nconditions due to the flexibility of Industry 4.0 that disrupts traditional\nquality assurance mechanisms. This paper presents a novel industry-oriented\nsemantic model called Product-Process-Resource Asset Knowledge Graph (PPR-AKG),\nwhich is designed to analyze and mitigate undesired conditions in flexible\nCPPS. Built on top of the well-proven Product-Process-Resource (PPR) model\noriginating from ISA-95 and VDI-3682, a comprehensive OWL ontology addresses\nshortcomings of conventional model-driven engineering for CPPS, particularly\ninadequate undesired condition and error handling representation. The\nintegration of semantic technologies with large language models (LLMs) provides\nintuitive interfaces for factory operators, production planners, and engineers\nto interact with the entire model using natural language. Evaluation with the\nuse case addressing electric vehicle battery remanufacturing demonstrates that\nthe PPR-AKG approach efficiently supports resource allocation based on\nexplicitly represented capabilities as well as identification and mitigation of\nundesired conditions in production. The key contributions include (1) a\nholistic PPR-AKG model capturing multi-dimensional production knowledge, and\n(2) the useful combination of the PPR-AKG with LLM-based chatbots for human\ninteraction."
                },
                "authors": [
                    {
                        "name": "Petr Novak"
                    },
                    {
                        "name": "Stefan Biffl"
                    },
                    {
                        "name": "Marek Obitko"
                    },
                    {
                        "name": "Petr Kadera"
                    }
                ],
                "author_detail": {
                    "name": "Petr Kadera"
                },
                "author": "Petr Kadera",
                "arxiv_comment": "3 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02679v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02679v2",
                "updated": "2025-08-08T12:56:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    12,
                    56,
                    11,
                    4,
                    220,
                    0
                ],
                "published": "2025-07-17T03:30:11Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    3,
                    30,
                    11,
                    3,
                    198,
                    0
                ],
                "title": "LLM Agent-Based Simulation of Student Activities and Mental Health Using\n  Smartphone Sensing Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Agent-Based Simulation of Student Activities and Mental Health Using\n  Smartphone Sensing Data"
                },
                "summary": "Students' mental well-being is vital for academic success, with activities\nsuch as studying, socializing, and sleeping playing a role. Current mobile\nsensing data highlight this intricate link using statistical and machine\nlearning analyses. We propose a novel LLM agent-based simulation framework to\nmodel student activities and mental health using the StudentLife Dataset. Each\nLLM agent was initialized with personality questionnaires and guided by\nsmartphone sensing data throughout the simulated semester. These agents predict\nindividual behaviors, provide self-reported mental health data via ecological\nmomentary assessments (EMAs), and complete follow-up personality\nquestionnaires. To ensure accuracy, we investigated various prompting\ntechniques, memory systems, and activity-based mental state management\nstrategies that dynamically update an agent's mental state based on their daily\nactivities. This simulation goes beyond simply replicating existing data. This\nallows us to explore new scenarios that are not present in the original\ndataset, such as peer influence through agent-to-agent interactions and the\nimpact of social media. Furthermore, we can conduct intervention studies by\nmanipulating activity patterns via sensing signals and personality traits using\nquestionnaire responses. This provides valuable insights into the behavioral\nchanges that could enhance student well-being. The framework also facilitates\nhypothetical interviews with LLM agents, offering deeper insights into their\nmental health. This study showcases the power of LLM-driven behavioral modeling\nwith sensing data, opening new avenues for understanding and supporting student\nmental health.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Students' mental well-being is vital for academic success, with activities\nsuch as studying, socializing, and sleeping playing a role. Current mobile\nsensing data highlight this intricate link using statistical and machine\nlearning analyses. We propose a novel LLM agent-based simulation framework to\nmodel student activities and mental health using the StudentLife Dataset. Each\nLLM agent was initialized with personality questionnaires and guided by\nsmartphone sensing data throughout the simulated semester. These agents predict\nindividual behaviors, provide self-reported mental health data via ecological\nmomentary assessments (EMAs), and complete follow-up personality\nquestionnaires. To ensure accuracy, we investigated various prompting\ntechniques, memory systems, and activity-based mental state management\nstrategies that dynamically update an agent's mental state based on their daily\nactivities. This simulation goes beyond simply replicating existing data. This\nallows us to explore new scenarios that are not present in the original\ndataset, such as peer influence through agent-to-agent interactions and the\nimpact of social media. Furthermore, we can conduct intervention studies by\nmanipulating activity patterns via sensing signals and personality traits using\nquestionnaire responses. This provides valuable insights into the behavioral\nchanges that could enhance student well-being. The framework also facilitates\nhypothetical interviews with LLM agents, offering deeper insights into their\nmental health. This study showcases the power of LLM-driven behavioral modeling\nwith sensing data, opening new avenues for understanding and supporting student\nmental health."
                },
                "authors": [
                    {
                        "name": "Wayupuk Sommuang"
                    },
                    {
                        "name": "Kun Kerdthaisong"
                    },
                    {
                        "name": "Pasin Buakhaw"
                    },
                    {
                        "name": "Aslan B. Wong"
                    },
                    {
                        "name": "Nutchanon Yongsatianchot"
                    }
                ],
                "author_detail": {
                    "name": "Nutchanon Yongsatianchot"
                },
                "author": "Nutchanon Yongsatianchot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02679v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02679v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06277v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06277v1",
                "updated": "2025-08-08T12:54:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    12,
                    54,
                    9,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T12:54:09Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    12,
                    54,
                    9,
                    4,
                    220,
                    0
                ],
                "title": "Large Language Model Data Generation for Enhanced Intent Recognition in\n  German Speech",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Data Generation for Enhanced Intent Recognition in\n  German Speech"
                },
                "summary": "Intent recognition (IR) for speech commands is essential for artificial\nintelligence (AI) assistant systems; however, most existing approaches are\nlimited to short commands and are predominantly developed for English. This\npaper addresses these limitations by focusing on IR from speech by elderly\nGerman speakers. We propose a novel approach that combines an adapted Whisper\nASR model, fine-tuned on elderly German speech (SVC-de), with Transformer-based\nlanguage models trained on synthetic text datasets generated by three\nwell-known large language models (LLMs): LeoLM, Llama3, and ChatGPT. To\nevaluate the robustness of our approach, we generate synthetic speech with a\ntext-to-speech model and conduct extensive cross-dataset testing. Our results\nshow that synthetic LLM-generated data significantly boosts classification\nperformance and robustness to different speaking styles and unseen vocabulary.\nNotably, we find that LeoLM, a smaller, domain-specific 13B LLM, surpasses the\nmuch larger ChatGPT (175B) in dataset quality for German intent recognition.\nOur approach demonstrates that generative AI can effectively bridge data gaps\nin low-resource domains. We provide detailed documentation of our data\ngeneration and training process to ensure transparency and reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intent recognition (IR) for speech commands is essential for artificial\nintelligence (AI) assistant systems; however, most existing approaches are\nlimited to short commands and are predominantly developed for English. This\npaper addresses these limitations by focusing on IR from speech by elderly\nGerman speakers. We propose a novel approach that combines an adapted Whisper\nASR model, fine-tuned on elderly German speech (SVC-de), with Transformer-based\nlanguage models trained on synthetic text datasets generated by three\nwell-known large language models (LLMs): LeoLM, Llama3, and ChatGPT. To\nevaluate the robustness of our approach, we generate synthetic speech with a\ntext-to-speech model and conduct extensive cross-dataset testing. Our results\nshow that synthetic LLM-generated data significantly boosts classification\nperformance and robustness to different speaking styles and unseen vocabulary.\nNotably, we find that LeoLM, a smaller, domain-specific 13B LLM, surpasses the\nmuch larger ChatGPT (175B) in dataset quality for German intent recognition.\nOur approach demonstrates that generative AI can effectively bridge data gaps\nin low-resource domains. We provide detailed documentation of our data\ngeneration and training process to ensure transparency and reproducibility."
                },
                "authors": [
                    {
                        "name": "Theresa Pekarek Rosin"
                    },
                    {
                        "name": "Burak Can Kaplan"
                    },
                    {
                        "name": "Stefan Wermter"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Wermter"
                },
                "author": "Stefan Wermter",
                "arxiv_comment": "11 pages, 3 figures, accepted at KONVENS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06277v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06277v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06275v1",
                "updated": "2025-08-08T12:48:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    12,
                    48,
                    31,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T12:48:31Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    12,
                    48,
                    31,
                    4,
                    220,
                    0
                ],
                "title": "Efficient Deep Neural Receiver with Post-Training Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Deep Neural Receiver with Post-Training Quantization"
                },
                "summary": "Deep learning has recently garnered significant interest in wireless\ncommunications due to its superior performance compared to traditional\nmodel-based algorithms. Deep convolutional neural networks (CNNs) have\ndemonstrated notable improvements in block error rate (BLER) under various\nchannel models and mobility scenarios. However, the high computational\ncomplexity and resource demands of deep CNNs pose challenges for deployment in\nresource-constrained edge systems. The 3rd Generation Partnership Project\n(3GPP) Release 20 highlights the pivotal role of artificial intelligence (AI)\nintegration in enabling advanced radio-access networks for 6G systems. The hard\nreal-time processing demands of 5G and 6G require efficient techniques such as\npost-training quantization (PTQ), quantization-aware training (QAT), pruning,\nand hybrid approaches to meet latency requirements. In this paper, we focus on\nPTQ to reduce model complexity by lowering the bit-width of weights, thereby\nenhancing computational efficiency. Our analysis employs symmetric uniform\nquantization, applying both per-tensor and per-channel PTQ to a neural receiver\nachieving performance comparable to full-precision models. Specifically, 8-bit\nper-channel quantization maintains BLER performance with minimal degradation,\nwhile 4-bit quantization shows great promise but requires further optimization\nto achieve target BLER levels. These results highlight the potential of\nultra-low bitwidth PTQ for efficient neural receiver deployment in 6G systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning has recently garnered significant interest in wireless\ncommunications due to its superior performance compared to traditional\nmodel-based algorithms. Deep convolutional neural networks (CNNs) have\ndemonstrated notable improvements in block error rate (BLER) under various\nchannel models and mobility scenarios. However, the high computational\ncomplexity and resource demands of deep CNNs pose challenges for deployment in\nresource-constrained edge systems. The 3rd Generation Partnership Project\n(3GPP) Release 20 highlights the pivotal role of artificial intelligence (AI)\nintegration in enabling advanced radio-access networks for 6G systems. The hard\nreal-time processing demands of 5G and 6G require efficient techniques such as\npost-training quantization (PTQ), quantization-aware training (QAT), pruning,\nand hybrid approaches to meet latency requirements. In this paper, we focus on\nPTQ to reduce model complexity by lowering the bit-width of weights, thereby\nenhancing computational efficiency. Our analysis employs symmetric uniform\nquantization, applying both per-tensor and per-channel PTQ to a neural receiver\nachieving performance comparable to full-precision models. Specifically, 8-bit\nper-channel quantization maintains BLER performance with minimal degradation,\nwhile 4-bit quantization shows great promise but requires further optimization\nto achieve target BLER levels. These results highlight the potential of\nultra-low bitwidth PTQ for efficient neural receiver deployment in 6G systems."
                },
                "authors": [
                    {
                        "name": "SaiKrishna Saketh Yellapragada"
                    },
                    {
                        "name": "Esa Ollila"
                    },
                    {
                        "name": "Mario Costa"
                    }
                ],
                "author_detail": {
                    "name": "Mario Costa"
                },
                "author": "Mario Costa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16802v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16802v3",
                "updated": "2025-08-08T12:38:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    12,
                    38,
                    49,
                    4,
                    220,
                    0
                ],
                "published": "2025-02-24T03:25:56Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    3,
                    25,
                    56,
                    0,
                    55,
                    0
                ],
                "title": "Topic Over Source: The Key to Effective Data Mixing for Language Models\n  Pre-training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topic Over Source: The Key to Effective Data Mixing for Language Models\n  Pre-training"
                },
                "summary": "The performance of large language models (LLMs) is significantly affected by\nthe quality and composition of their pre-training data, which is inherently\ndiverse, spanning various languages, sources, and topics. Effectively\nintegrating these heterogeneous data groups is crucial for optimizing LLM\nperformance. Previous research has predominantly concentrated on source-based\ndata mixing, often neglecting the nuanced topic-level characteristics of the\ndata. To address this gap, we propose a topic-based data mixing strategy that\nutilizes detailed topic labels generated through a multi-stage process\ncombining unsupervised clustering, LLM-based summarization, and supervised\nclassifier training. With this strategy, we conduct the first comprehensive\ncomparison of topic-based versus source-based partitioning across multiple\nmixing strategies. We demonstrate that language models pretrained on data mixed\nby topics consistently outperform those trained on data mixed by sources across\nmultiple methods including RegMix, DoReMi,temperature-based sampling, and a\nmanual mixing method based on downstream task performance. Our theoretical\nanalysis reveals that topic-based data achieves significantly lower validation\nloss compared to source-based approaches, creating a better optimization\nlandscape for model training. We will make our code, annotated datasets, and\ntopic classification models publicly available to facilitate further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of large language models (LLMs) is significantly affected by\nthe quality and composition of their pre-training data, which is inherently\ndiverse, spanning various languages, sources, and topics. Effectively\nintegrating these heterogeneous data groups is crucial for optimizing LLM\nperformance. Previous research has predominantly concentrated on source-based\ndata mixing, often neglecting the nuanced topic-level characteristics of the\ndata. To address this gap, we propose a topic-based data mixing strategy that\nutilizes detailed topic labels generated through a multi-stage process\ncombining unsupervised clustering, LLM-based summarization, and supervised\nclassifier training. With this strategy, we conduct the first comprehensive\ncomparison of topic-based versus source-based partitioning across multiple\nmixing strategies. We demonstrate that language models pretrained on data mixed\nby topics consistently outperform those trained on data mixed by sources across\nmultiple methods including RegMix, DoReMi,temperature-based sampling, and a\nmanual mixing method based on downstream task performance. Our theoretical\nanalysis reveals that topic-based data achieves significantly lower validation\nloss compared to source-based approaches, creating a better optimization\nlandscape for model training. We will make our code, annotated datasets, and\ntopic classification models publicly available to facilitate further research."
                },
                "authors": [
                    {
                        "name": "Jiahui Peng"
                    },
                    {
                        "name": "Xinlin Zhuang"
                    },
                    {
                        "name": "Jiantao Qiu"
                    },
                    {
                        "name": "Ren Ma"
                    },
                    {
                        "name": "Jing Yu"
                    },
                    {
                        "name": "He Zhu"
                    },
                    {
                        "name": "Conghui He"
                    }
                ],
                "author_detail": {
                    "name": "Conghui He"
                },
                "author": "Conghui He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16802v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16802v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06262v1",
                "updated": "2025-08-08T12:28:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    12,
                    28,
                    34,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T12:28:34Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    12,
                    28,
                    34,
                    4,
                    220,
                    0
                ],
                "title": "Llasa+: Free Lunch for Accelerated and Streaming Llama-Based Speech\n  Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Llasa+: Free Lunch for Accelerated and Streaming Llama-Based Speech\n  Synthesis"
                },
                "summary": "Recent progress in text-to-speech (TTS) has achieved impressive naturalness\nand flexibility, especially with the development of large language model\n(LLM)-based approaches. However, existing autoregressive (AR) structures and\nlarge-scale models, such as Llasa, still face significant challenges in\ninference latency and streaming synthesis. To deal with the limitations, we\nintroduce Llasa+, an accelerated and streaming TTS model built on Llasa.\nSpecifically, to accelerate the generation process, we introduce two\nplug-and-play Multi-Token Prediction (MTP) modules following the frozen\nbackbone. These modules allow the model to predict multiple tokens in one AR\nstep. Additionally, to mitigate potential error propagation caused by\ninaccurate MTP, we design a novel verification algorithm that leverages the\nfrozen backbone to validate the generated tokens, thus allowing Llasa+ to\nachieve speedup without sacrificing generation quality. Furthermore, we design\na causal decoder that enables streaming speech reconstruction from tokens.\nExtensive experiments show that Llasa+ achieves a 1.48X speedup without\nsacrificing generation quality, despite being trained only on LibriTTS.\nMoreover, the MTP-and-verification framework can be applied to accelerate any\nLLM-based model. All codes and models are publicly available at\nhttps://github.com/ASLP-lab/LLaSA_Plus.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in text-to-speech (TTS) has achieved impressive naturalness\nand flexibility, especially with the development of large language model\n(LLM)-based approaches. However, existing autoregressive (AR) structures and\nlarge-scale models, such as Llasa, still face significant challenges in\ninference latency and streaming synthesis. To deal with the limitations, we\nintroduce Llasa+, an accelerated and streaming TTS model built on Llasa.\nSpecifically, to accelerate the generation process, we introduce two\nplug-and-play Multi-Token Prediction (MTP) modules following the frozen\nbackbone. These modules allow the model to predict multiple tokens in one AR\nstep. Additionally, to mitigate potential error propagation caused by\ninaccurate MTP, we design a novel verification algorithm that leverages the\nfrozen backbone to validate the generated tokens, thus allowing Llasa+ to\nachieve speedup without sacrificing generation quality. Furthermore, we design\na causal decoder that enables streaming speech reconstruction from tokens.\nExtensive experiments show that Llasa+ achieves a 1.48X speedup without\nsacrificing generation quality, despite being trained only on LibriTTS.\nMoreover, the MTP-and-verification framework can be applied to accelerate any\nLLM-based model. All codes and models are publicly available at\nhttps://github.com/ASLP-lab/LLaSA_Plus."
                },
                "authors": [
                    {
                        "name": "Wenjie Tian"
                    },
                    {
                        "name": "Xinfa Zhu"
                    },
                    {
                        "name": "Hanke Xie"
                    },
                    {
                        "name": "Zhen Ye"
                    },
                    {
                        "name": "Wei Xue"
                    },
                    {
                        "name": "Lei Xie"
                    }
                ],
                "author_detail": {
                    "name": "Lei Xie"
                },
                "author": "Lei Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06249v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06249v1",
                "updated": "2025-08-08T12:10:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    12,
                    10,
                    28,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T12:10:28Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    12,
                    10,
                    28,
                    4,
                    220,
                    0
                ],
                "title": "In-Training Defenses against Emergent Misalignment in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Training Defenses against Emergent Misalignment in Language Models"
                },
                "summary": "Fine-tuning lets practitioners repurpose aligned large language models (LLMs)\nfor new domains, yet recent work reveals emergent misalignment (EMA): Even a\nsmall, domain-specific fine-tune can induce harmful behaviors far outside the\ntarget domain. Even in the case where model weights are hidden behind a\nfine-tuning API, this gives attackers inadvertent access to a broadly\nmisaligned model in a way that can be hard to detect from the fine-tuning data\nalone. We present the first systematic study of in-training safeguards against\nEMA that are practical for providers who expose fine-tuning via an API. We\ninvestigate four training regularization interventions: (i) KL-divergence\nregularization toward a safe reference model, (ii) $\\ell_2$ distance in feature\nspace, (iii) projecting onto a safe subspace (SafeLoRA), and (iv) interleaving\nof a small amount of safe training examples from a general instruct-tuning\ndataset. We first evaluate the methods' emergent misalignment effect across\nfour malicious, EMA-inducing tasks. Second, we assess the methods' impacts on\nbenign tasks. We conclude with a discussion of open questions in emergent\nmisalignment research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning lets practitioners repurpose aligned large language models (LLMs)\nfor new domains, yet recent work reveals emergent misalignment (EMA): Even a\nsmall, domain-specific fine-tune can induce harmful behaviors far outside the\ntarget domain. Even in the case where model weights are hidden behind a\nfine-tuning API, this gives attackers inadvertent access to a broadly\nmisaligned model in a way that can be hard to detect from the fine-tuning data\nalone. We present the first systematic study of in-training safeguards against\nEMA that are practical for providers who expose fine-tuning via an API. We\ninvestigate four training regularization interventions: (i) KL-divergence\nregularization toward a safe reference model, (ii) $\\ell_2$ distance in feature\nspace, (iii) projecting onto a safe subspace (SafeLoRA), and (iv) interleaving\nof a small amount of safe training examples from a general instruct-tuning\ndataset. We first evaluate the methods' emergent misalignment effect across\nfour malicious, EMA-inducing tasks. Second, we assess the methods' impacts on\nbenign tasks. We conclude with a discussion of open questions in emergent\nmisalignment research."
                },
                "authors": [
                    {
                        "name": "David Kaczér"
                    },
                    {
                        "name": "Magnus Jørgenvåg"
                    },
                    {
                        "name": "Clemens Vetter"
                    },
                    {
                        "name": "Lucie Flek"
                    },
                    {
                        "name": "Florian Mai"
                    }
                ],
                "author_detail": {
                    "name": "Florian Mai"
                },
                "author": "Florian Mai",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06249v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06248v1",
                "updated": "2025-08-08T12:03:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    12,
                    3,
                    56,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T12:03:56Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    12,
                    3,
                    56,
                    4,
                    220,
                    0
                ],
                "title": "Deepfake Detection that Generalizes Across Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deepfake Detection that Generalizes Across Benchmarks"
                },
                "summary": "The generalization of deepfake detectors to unseen manipulation techniques\nremains a challenge for practical deployment. Although many approaches adapt\nfoundation models by introducing significant architectural complexity, this\nwork demonstrates that robust generalization is achievable through a\nparameter-efficient adaptation of a pre-trained CLIP vision encoder. The\nproposed method, LNCLIP-DF, fine-tunes only the Layer Normalization parameters\n(0.03% of the total) and enhances generalization by enforcing a hyperspherical\nfeature manifold using L2 normalization and latent space augmentations.\n  We conducted an extensive evaluation on 13 benchmark datasets spanning from\n2019 to 2025. The proposed method achieves state-of-the-art performance,\noutperforming more complex, recent approaches in average cross-dataset AUROC.\nOur analysis yields two primary findings for the field: 1) training on paired\nreal-fake data from the same source video is essential for mitigating shortcut\nlearning and improving generalization, and 2) detection difficulty on academic\ndatasets has not strictly increased over time, with models trained on older,\ndiverse datasets showing strong generalization capabilities.\n  This work delivers a computationally efficient and reproducible method,\nproving that state-of-the-art generalization is attainable by making targeted,\nminimal changes to a pre-trained CLIP model. The code will be made publicly\navailable upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generalization of deepfake detectors to unseen manipulation techniques\nremains a challenge for practical deployment. Although many approaches adapt\nfoundation models by introducing significant architectural complexity, this\nwork demonstrates that robust generalization is achievable through a\nparameter-efficient adaptation of a pre-trained CLIP vision encoder. The\nproposed method, LNCLIP-DF, fine-tunes only the Layer Normalization parameters\n(0.03% of the total) and enhances generalization by enforcing a hyperspherical\nfeature manifold using L2 normalization and latent space augmentations.\n  We conducted an extensive evaluation on 13 benchmark datasets spanning from\n2019 to 2025. The proposed method achieves state-of-the-art performance,\noutperforming more complex, recent approaches in average cross-dataset AUROC.\nOur analysis yields two primary findings for the field: 1) training on paired\nreal-fake data from the same source video is essential for mitigating shortcut\nlearning and improving generalization, and 2) detection difficulty on academic\ndatasets has not strictly increased over time, with models trained on older,\ndiverse datasets showing strong generalization capabilities.\n  This work delivers a computationally efficient and reproducible method,\nproving that state-of-the-art generalization is attainable by making targeted,\nminimal changes to a pre-trained CLIP model. The code will be made publicly\navailable upon acceptance."
                },
                "authors": [
                    {
                        "name": "Andrii Yermakov"
                    },
                    {
                        "name": "Jan Cech"
                    },
                    {
                        "name": "Jiri Matas"
                    },
                    {
                        "name": "Mario Fritz"
                    }
                ],
                "author_detail": {
                    "name": "Mario Fritz"
                },
                "author": "Mario Fritz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14110v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14110v2",
                "updated": "2025-08-08T11:56:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    11,
                    56,
                    30,
                    4,
                    220,
                    0
                ],
                "published": "2024-11-21T13:18:03Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    13,
                    18,
                    3,
                    3,
                    326,
                    0
                ],
                "title": "Feedback-Guided Extraction of Knowledge Base from Retrieval-Augmented\n  LLM Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feedback-Guided Extraction of Knowledge Base from Retrieval-Augmented\n  LLM Applications"
                },
                "summary": "Retrieval-Augmented Generation (RAG) expands the knowledge boundary of large\nlanguage models (LLMs) by integrating external knowledge bases, whose\nconstruction is often time-consuming and laborious. If an adversary extracts\nthe knowledge base verbatim, it not only severely infringes the owner's\nintellectual property but also enables the adversary to replicate the\napplication's functionality for unfair competition. Previous works on knowledge\nbase extraction are limited either by low extraction coverage (usually less\nthan 4%) in query-based attacks or by impractical assumptions of white-box\naccess in embedding-based optimization methods. In this work, we propose\nCopyBreakRAG, an agent-based black-box attack that reasons from feedback and\nadaptively generates new adversarial queries for progressive extraction. By\nbalancing exploration and exploitation through curiosity-driven queries and\nfeedback-guided query refinement, our method overcomes the limitations of prior\napproaches and achieves significantly higher extraction coverage in realistic\nblack-box settings. Experimental results show that CopyBreakRAG outperforms the\nstate-of-the-art black-box approach by 45% on average in terms of chunk\nextraction ratio from applications built with mainstream RAG frameworks, and\nextracts over 70% of the data from the knowledge base in applications on\ncommercial platforms including OpenAI's GPTs and ByteDance's Coze when\nessential protection is in place.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) expands the knowledge boundary of large\nlanguage models (LLMs) by integrating external knowledge bases, whose\nconstruction is often time-consuming and laborious. If an adversary extracts\nthe knowledge base verbatim, it not only severely infringes the owner's\nintellectual property but also enables the adversary to replicate the\napplication's functionality for unfair competition. Previous works on knowledge\nbase extraction are limited either by low extraction coverage (usually less\nthan 4%) in query-based attacks or by impractical assumptions of white-box\naccess in embedding-based optimization methods. In this work, we propose\nCopyBreakRAG, an agent-based black-box attack that reasons from feedback and\nadaptively generates new adversarial queries for progressive extraction. By\nbalancing exploration and exploitation through curiosity-driven queries and\nfeedback-guided query refinement, our method overcomes the limitations of prior\napproaches and achieves significantly higher extraction coverage in realistic\nblack-box settings. Experimental results show that CopyBreakRAG outperforms the\nstate-of-the-art black-box approach by 45% on average in terms of chunk\nextraction ratio from applications built with mainstream RAG frameworks, and\nextracts over 70% of the data from the knowledge base in applications on\ncommercial platforms including OpenAI's GPTs and ByteDance's Coze when\nessential protection is in place."
                },
                "authors": [
                    {
                        "name": "Changyue Jiang"
                    },
                    {
                        "name": "Xudong Pan"
                    },
                    {
                        "name": "Geng Hong"
                    },
                    {
                        "name": "Chenfu Bao"
                    },
                    {
                        "name": "Yang Chen"
                    },
                    {
                        "name": "Min Yang"
                    }
                ],
                "author_detail": {
                    "name": "Min Yang"
                },
                "author": "Min Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14110v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14110v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19811v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19811v2",
                "updated": "2025-08-08T11:53:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    11,
                    53,
                    19,
                    4,
                    220,
                    0
                ],
                "published": "2025-04-28T14:08:45Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    14,
                    8,
                    45,
                    0,
                    118,
                    0
                ],
                "title": "Can a Crow Hatch a Falcon? Lineage Matters in Predicting Large Language\n  Model Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can a Crow Hatch a Falcon? Lineage Matters in Predicting Large Language\n  Model Performance"
                },
                "summary": "Accurately forecasting the performance of Large Language Models (LLMs) before\nextensive fine-tuning or merging can substantially reduce both computational\nexpense and development time. Although prior approaches like scaling laws\naccount for global factors such as parameter size or training tokens, they\noften overlook explicit lineage relationships-i.e., which models are derived or\nmerged from which parents. In this work, we propose a novel Lineage-Regularized\nMatrix Factorization (LRMF) framework that encodes ancestral ties among LLMs\nvia a graph Laplacian regularizer. By leveraging multi-hop parent-child\nconnections, LRMF consistently outperforms conventional matrix factorization\nand collaborative filtering methods in both instance-level and benchmark-level\nperformance prediction. Our large-scale study includes 2,934 publicly available\nHugging Face models and 21,000+ instances across 6 major benchmarks, showing\nthat the introduction of lineage constraints yields up to 0.15-0.30 higher\nPearson correlation coefficients with actual performance compared to baseline\nmethods. Moreover, LRMF effectively addresses the cold-start problem, providing\naccurate estimates for newly derived or merged models even with minimal data.\nThis lineage-guided strategy thus offers a resource-efficient way to inform\nhyperparameter tuning, data selection, and model combination in modern LLM\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately forecasting the performance of Large Language Models (LLMs) before\nextensive fine-tuning or merging can substantially reduce both computational\nexpense and development time. Although prior approaches like scaling laws\naccount for global factors such as parameter size or training tokens, they\noften overlook explicit lineage relationships-i.e., which models are derived or\nmerged from which parents. In this work, we propose a novel Lineage-Regularized\nMatrix Factorization (LRMF) framework that encodes ancestral ties among LLMs\nvia a graph Laplacian regularizer. By leveraging multi-hop parent-child\nconnections, LRMF consistently outperforms conventional matrix factorization\nand collaborative filtering methods in both instance-level and benchmark-level\nperformance prediction. Our large-scale study includes 2,934 publicly available\nHugging Face models and 21,000+ instances across 6 major benchmarks, showing\nthat the introduction of lineage constraints yields up to 0.15-0.30 higher\nPearson correlation coefficients with actual performance compared to baseline\nmethods. Moreover, LRMF effectively addresses the cold-start problem, providing\naccurate estimates for newly derived or merged models even with minimal data.\nThis lineage-guided strategy thus offers a resource-efficient way to inform\nhyperparameter tuning, data selection, and model combination in modern LLM\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Takuya Tamura"
                    },
                    {
                        "name": "Taro Yano"
                    },
                    {
                        "name": "Masafumi Enomoto"
                    },
                    {
                        "name": "Masafumi Oyamada"
                    }
                ],
                "author_detail": {
                    "name": "Masafumi Oyamada"
                },
                "author": "Masafumi Oyamada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19811v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19811v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00381v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00381v2",
                "updated": "2025-08-08T11:27:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    11,
                    27,
                    44,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-01T07:19:23Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    7,
                    19,
                    23,
                    4,
                    213,
                    0
                ],
                "title": "Advancing Welding Defect Detection in Maritime Operations via\n  Adapt-WeldNet and Defect Detection Interpretability Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Welding Defect Detection in Maritime Operations via\n  Adapt-WeldNet and Defect Detection Interpretability Analysis"
                },
                "summary": "Weld defect detection is crucial for ensuring the safety and reliability of\npiping systems in the oil and gas industry, especially in challenging marine\nand offshore environments. Traditional non-destructive testing (NDT) methods\noften fail to detect subtle or internal defects, leading to potential failures\nand costly downtime. Furthermore, existing neural network-based approaches for\ndefect classification frequently rely on arbitrarily selected pretrained\narchitectures and lack interpretability, raising safety concerns for\ndeployment. To address these challenges, this paper introduces\n``Adapt-WeldNet\", an adaptive framework for welding defect detection that\nsystematically evaluates various pre-trained architectures, transfer learning\nstrategies, and adaptive optimizers to identify the best-performing model and\nhyperparameters, optimizing defect detection and providing actionable insights.\nAdditionally, a novel Defect Detection Interpretability Analysis (DDIA)\nframework is proposed to enhance system transparency. DDIA employs Explainable\nAI (XAI) techniques, such as Grad-CAM and LIME, alongside domain-specific\nevaluations validated by certified ASNT NDE Level II professionals.\nIncorporating a Human-in-the-Loop (HITL) approach and aligning with the\nprinciples of Trustworthy AI, DDIA ensures the reliability, fairness, and\naccountability of the defect detection system, fostering confidence in\nautomated decisions through expert validation. By improving both performance\nand interpretability, this work enhances trust, safety, and reliability in\nwelding defect detection systems, supporting critical operations in offshore\nand marine environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weld defect detection is crucial for ensuring the safety and reliability of\npiping systems in the oil and gas industry, especially in challenging marine\nand offshore environments. Traditional non-destructive testing (NDT) methods\noften fail to detect subtle or internal defects, leading to potential failures\nand costly downtime. Furthermore, existing neural network-based approaches for\ndefect classification frequently rely on arbitrarily selected pretrained\narchitectures and lack interpretability, raising safety concerns for\ndeployment. To address these challenges, this paper introduces\n``Adapt-WeldNet\", an adaptive framework for welding defect detection that\nsystematically evaluates various pre-trained architectures, transfer learning\nstrategies, and adaptive optimizers to identify the best-performing model and\nhyperparameters, optimizing defect detection and providing actionable insights.\nAdditionally, a novel Defect Detection Interpretability Analysis (DDIA)\nframework is proposed to enhance system transparency. DDIA employs Explainable\nAI (XAI) techniques, such as Grad-CAM and LIME, alongside domain-specific\nevaluations validated by certified ASNT NDE Level II professionals.\nIncorporating a Human-in-the-Loop (HITL) approach and aligning with the\nprinciples of Trustworthy AI, DDIA ensures the reliability, fairness, and\naccountability of the defect detection system, fostering confidence in\nautomated decisions through expert validation. By improving both performance\nand interpretability, this work enhances trust, safety, and reliability in\nwelding defect detection systems, supporting critical operations in offshore\nand marine environments."
                },
                "authors": [
                    {
                        "name": "Kamal Basha S"
                    },
                    {
                        "name": "Athira Nambiar"
                    }
                ],
                "author_detail": {
                    "name": "Athira Nambiar"
                },
                "author": "Athira Nambiar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00381v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00381v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16432v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16432v2",
                "updated": "2025-08-08T11:14:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    11,
                    14,
                    54,
                    4,
                    220,
                    0
                ],
                "published": "2025-04-23T05:34:49Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    5,
                    34,
                    49,
                    2,
                    113,
                    0
                ],
                "title": "iTFKAN: Interpretable Time Series Forecasting with Kolmogorov-Arnold\n  Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "iTFKAN: Interpretable Time Series Forecasting with Kolmogorov-Arnold\n  Network"
                },
                "summary": "As time evolves, data within specific domains exhibit predictability that\nmotivates time series forecasting to predict future trends from historical\ndata. However, current deep forecasting methods can achieve promising\nperformance but generally lack interpretability, hindering trustworthiness and\npractical deployment in safety-critical applications such as auto-driving and\nhealthcare. In this paper, we propose a novel interpretable model, iTFKAN, for\ncredible time series forecasting. iTFKAN enables further exploration of model\ndecision rationales and underlying data patterns due to its interpretability\nachieved through model symbolization. Besides, iTFKAN develops two strategies,\nprior knowledge injection, and time-frequency synergy learning, to effectively\nguide model learning under complex intertwined time series data. Extensive\nexperimental results demonstrated that iTFKAN can achieve promising forecasting\nperformance while simultaneously possessing high interpretive capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As time evolves, data within specific domains exhibit predictability that\nmotivates time series forecasting to predict future trends from historical\ndata. However, current deep forecasting methods can achieve promising\nperformance but generally lack interpretability, hindering trustworthiness and\npractical deployment in safety-critical applications such as auto-driving and\nhealthcare. In this paper, we propose a novel interpretable model, iTFKAN, for\ncredible time series forecasting. iTFKAN enables further exploration of model\ndecision rationales and underlying data patterns due to its interpretability\nachieved through model symbolization. Besides, iTFKAN develops two strategies,\nprior knowledge injection, and time-frequency synergy learning, to effectively\nguide model learning under complex intertwined time series data. Extensive\nexperimental results demonstrated that iTFKAN can achieve promising forecasting\nperformance while simultaneously possessing high interpretive capabilities."
                },
                "authors": [
                    {
                        "name": "Ziran Liang"
                    },
                    {
                        "name": "Rui An"
                    },
                    {
                        "name": "Wenqi Fan"
                    },
                    {
                        "name": "Yanghui Rao"
                    },
                    {
                        "name": "Yuxuan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yuxuan Liang"
                },
                "author": "Yuxuan Liang",
                "arxiv_comment": "Currently under review at IEEE Transactions on Knowledge and Data\n  Engineering",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16432v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16432v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06225v1",
                "updated": "2025-08-08T11:11:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    11,
                    11,
                    22,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T11:11:22Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    11,
                    11,
                    22,
                    4,
                    220,
                    0
                ],
                "title": "Overconfidence in LLM-as-a-Judge: Diagnosis and Confidence-Driven\n  Solution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Overconfidence in LLM-as-a-Judge: Diagnosis and Confidence-Driven\n  Solution"
                },
                "summary": "Large Language Models (LLMs) are widely used as automated judges, where\npractical value depends on both accuracy and trustworthy, risk-aware judgments.\nExisting approaches predominantly focus on accuracy, overlooking the necessity\nof well-calibrated confidence, which is vital for adaptive and reliable\nevaluation pipelines. In this work, we advocate a shift from accuracy-centric\nevaluation to confidence-driven, risk-aware LLM-as-a-Judge systems, emphasizing\nthe necessity of well-calibrated confidence for trustworthy and adaptive\nevaluation. We systematically identify the **Overconfidence Phenomenon** in\ncurrent LLM-as-a-Judges, where predicted confidence significantly overstates\nactual correctness, undermining reliability in practical deployment. To\nquantify this phenomenon, we introduce **TH-Score**, a novel metric measuring\nconfidence-accuracy alignment. Furthermore, we propose **LLM-as-a-Fuser**, an\nensemble framework that transforms LLMs into reliable, risk-aware evaluators.\nExtensive experiments demonstrate that our approach substantially improves\ncalibration and enables adaptive, confidence-driven evaluation pipelines,\nachieving superior reliability and accuracy compared to existing baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used as automated judges, where\npractical value depends on both accuracy and trustworthy, risk-aware judgments.\nExisting approaches predominantly focus on accuracy, overlooking the necessity\nof well-calibrated confidence, which is vital for adaptive and reliable\nevaluation pipelines. In this work, we advocate a shift from accuracy-centric\nevaluation to confidence-driven, risk-aware LLM-as-a-Judge systems, emphasizing\nthe necessity of well-calibrated confidence for trustworthy and adaptive\nevaluation. We systematically identify the **Overconfidence Phenomenon** in\ncurrent LLM-as-a-Judges, where predicted confidence significantly overstates\nactual correctness, undermining reliability in practical deployment. To\nquantify this phenomenon, we introduce **TH-Score**, a novel metric measuring\nconfidence-accuracy alignment. Furthermore, we propose **LLM-as-a-Fuser**, an\nensemble framework that transforms LLMs into reliable, risk-aware evaluators.\nExtensive experiments demonstrate that our approach substantially improves\ncalibration and enables adaptive, confidence-driven evaluation pipelines,\nachieving superior reliability and accuracy compared to existing baselines."
                },
                "authors": [
                    {
                        "name": "Zailong Tian"
                    },
                    {
                        "name": "Zhuoheng Han"
                    },
                    {
                        "name": "Yanzhe Chen"
                    },
                    {
                        "name": "Haozhe Xu"
                    },
                    {
                        "name": "Xi Yang"
                    },
                    {
                        "name": "richeng xuan"
                    },
                    {
                        "name": "Hongfeng Wang"
                    },
                    {
                        "name": "Lizi Liao"
                    }
                ],
                "author_detail": {
                    "name": "Lizi Liao"
                },
                "author": "Lizi Liao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10443v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10443v2",
                "updated": "2025-08-08T10:42:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    10,
                    42,
                    24,
                    4,
                    220,
                    0
                ],
                "published": "2025-05-15T16:04:25Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    4,
                    25,
                    3,
                    135,
                    0
                ],
                "title": "Are Large Language Models Robust in Understanding Code Against\n  Semantics-Preserving Mutations?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Large Language Models Robust in Understanding Code Against\n  Semantics-Preserving Mutations?"
                },
                "summary": "Understanding the reasoning and robustness of Large Language Models (LLMs) is\ncritical for their reliable use in programming tasks. While recent studies have\nassessed LLMs' ability to predict program outputs, most focus solely on the\naccuracy of those predictions, without evaluating the reasoning behind them.\nMoreover, it has been observed on mathematical reasoning tasks that LLMs can\narrive at correct answers through flawed logic, raising concerns about similar\nissues in code understanding. In this work, we evaluate whether\nstate-of-the-art LLMs with up to 8B parameters can reason about Python programs\nor are simply guessing. We apply five semantics-preserving code mutations:\nrenaming variables, mirroring comparison expressions, swapping if-else\nbranches, converting for loops to while, and loop unrolling. These mutations\nmaintain program semantics while altering its syntax. We evaluated six LLMs and\nperformed a human expert analysis using LiveCodeBench to assess whether the\ncorrect predictions are based on sound reasoning. We also evaluated prediction\nstability across different code mutations on LiveCodeBench and CruxEval. Our\nfindings show that LLMs trained for code produce correct predictions based on\nflawed reasoning between 10% and 50% of cases. Furthermore, LLMs often change\npredictions in response to our code mutations, indicating they do not yet\nexhibit stable, semantically grounded reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the reasoning and robustness of Large Language Models (LLMs) is\ncritical for their reliable use in programming tasks. While recent studies have\nassessed LLMs' ability to predict program outputs, most focus solely on the\naccuracy of those predictions, without evaluating the reasoning behind them.\nMoreover, it has been observed on mathematical reasoning tasks that LLMs can\narrive at correct answers through flawed logic, raising concerns about similar\nissues in code understanding. In this work, we evaluate whether\nstate-of-the-art LLMs with up to 8B parameters can reason about Python programs\nor are simply guessing. We apply five semantics-preserving code mutations:\nrenaming variables, mirroring comparison expressions, swapping if-else\nbranches, converting for loops to while, and loop unrolling. These mutations\nmaintain program semantics while altering its syntax. We evaluated six LLMs and\nperformed a human expert analysis using LiveCodeBench to assess whether the\ncorrect predictions are based on sound reasoning. We also evaluated prediction\nstability across different code mutations on LiveCodeBench and CruxEval. Our\nfindings show that LLMs trained for code produce correct predictions based on\nflawed reasoning between 10% and 50% of cases. Furthermore, LLMs often change\npredictions in response to our code mutations, indicating they do not yet\nexhibit stable, semantically grounded reasoning."
                },
                "authors": [
                    {
                        "name": "Pedro Orvalho"
                    },
                    {
                        "name": "Marta Kwiatkowska"
                    }
                ],
                "author_detail": {
                    "name": "Marta Kwiatkowska"
                },
                "author": "Marta Kwiatkowska",
                "arxiv_comment": "11 pages, 5 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10443v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10443v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06200v1",
                "updated": "2025-08-08T10:29:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    10,
                    29,
                    46,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T10:29:46Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    10,
                    29,
                    46,
                    4,
                    220,
                    0
                ],
                "title": "Scalable Production of Photochromic Yttrium Oxyhydride Powder via Ball\n  Milling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Production of Photochromic Yttrium Oxyhydride Powder via Ball\n  Milling"
                },
                "summary": "Yttrium oxyhydride (YHO) represents one of the most promising photochromic\nmaterials discovered in recent years, yet its practical deployment has been\nseverely constrained by the limitations of thin film deposition methods. Here\nwe demonstrate the first successful synthesis of photochromic YHO powders\nthrough reactive ball milling under hydrogen atmosphere followed by controlled\noxidation a fundamentally scalable approach that overcomes the production\nbarriers facing this technology. High-energy planetary ball milling of yttrium\nmetal under 50 bar hydrogen for 20 hours, followed by controlled oxidation in\nultra-dry technical air, yielded nanostructured YHO powders with less than 500\nnm particle sizes. These powders exhibit robust photochromic response with\nreflectance modulation at 850 nm under 405 nm excitation, reversible cycling\nbehavior, and the characteristic memory effect previously observed only in thin\nfilms. Powder X-ray diffraction confirms the formation of the cubic YHO phase\nwith lattice expansion consistent with oxygen incorporation into the yttrium\nhydride structure. Critically, we demonstrate that YHO powders can be processed\ninto polymer composites enabling spatially-resolved photochromic patterning a\ncapability essential for practical device applications. While optimization of\noptical contrast remains an opportunity for future work, this powder synthesis\nroute fundamentally transforms the manufacturing of YHO-based photochromic\nsystems, enabling mass-scale production using established industrial ball\nmilling infrastructure. These findings establish a viable pathway toward\ncommercial deployment of YHO in smart windows, adaptive optics, and rewritable\ninformation storage applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yttrium oxyhydride (YHO) represents one of the most promising photochromic\nmaterials discovered in recent years, yet its practical deployment has been\nseverely constrained by the limitations of thin film deposition methods. Here\nwe demonstrate the first successful synthesis of photochromic YHO powders\nthrough reactive ball milling under hydrogen atmosphere followed by controlled\noxidation a fundamentally scalable approach that overcomes the production\nbarriers facing this technology. High-energy planetary ball milling of yttrium\nmetal under 50 bar hydrogen for 20 hours, followed by controlled oxidation in\nultra-dry technical air, yielded nanostructured YHO powders with less than 500\nnm particle sizes. These powders exhibit robust photochromic response with\nreflectance modulation at 850 nm under 405 nm excitation, reversible cycling\nbehavior, and the characteristic memory effect previously observed only in thin\nfilms. Powder X-ray diffraction confirms the formation of the cubic YHO phase\nwith lattice expansion consistent with oxygen incorporation into the yttrium\nhydride structure. Critically, we demonstrate that YHO powders can be processed\ninto polymer composites enabling spatially-resolved photochromic patterning a\ncapability essential for practical device applications. While optimization of\noptical contrast remains an opportunity for future work, this powder synthesis\nroute fundamentally transforms the manufacturing of YHO-based photochromic\nsystems, enabling mass-scale production using established industrial ball\nmilling infrastructure. These findings establish a viable pathway toward\ncommercial deployment of YHO in smart windows, adaptive optics, and rewritable\ninformation storage applications."
                },
                "authors": [
                    {
                        "name": "Elbruz Murat Baba"
                    },
                    {
                        "name": "Stefano Deledda"
                    },
                    {
                        "name": "Smagul Karazhanov"
                    }
                ],
                "author_detail": {
                    "name": "Smagul Karazhanov"
                },
                "author": "Smagul Karazhanov",
                "arxiv_comment": "7 pages, 3 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13380v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13380v2",
                "updated": "2025-08-08T10:26:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    10,
                    26,
                    47,
                    4,
                    220,
                    0
                ],
                "published": "2025-06-16T11:44:28Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    11,
                    44,
                    28,
                    0,
                    167,
                    0
                ],
                "title": "Decompositional Reasoning for Graph Retrieval with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decompositional Reasoning for Graph Retrieval with Large Language Models"
                },
                "summary": "Large Language Models (LLMs) excel at many NLP tasks, but struggle with\nmulti-hop reasoning and factual consistency, limiting their effectiveness on\nknowledge-intensive tasks like complex question answering (QA). Linking\nKnowledge Graphs (KG) and LLMs has shown promising results, but LLMs generally\nlack the ability to reason efficiently over graph-structured information. To\ntackle this problem, we propose a novel retrieval approach that integrates\ntextual knowledge graphs into the LLM reasoning process via query\ndecomposition. Our method decomposes complex questions into sub-questions,\nretrieves relevant textual subgraphs, and composes a question-specific\nknowledge graph to guide answer generation. For that, we use a weighted\nsimilarity function that focuses on both the complex question and the generated\nsubquestions to extract a relevant subgraph, which allows efficient and precise\nretrieval for complex questions and improves the performance of LLMs on\nmulti-hop QA tasks. This structured reasoning pipeline enhances factual\ngrounding and interpretability while leveraging the generative strengths of\nLLMs. We evaluate our method on standard multi-hop QA benchmarks and show that\nit achieves comparable or superior performance to competitive existing methods,\nusing smaller models and fewer LLM calls.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at many NLP tasks, but struggle with\nmulti-hop reasoning and factual consistency, limiting their effectiveness on\nknowledge-intensive tasks like complex question answering (QA). Linking\nKnowledge Graphs (KG) and LLMs has shown promising results, but LLMs generally\nlack the ability to reason efficiently over graph-structured information. To\ntackle this problem, we propose a novel retrieval approach that integrates\ntextual knowledge graphs into the LLM reasoning process via query\ndecomposition. Our method decomposes complex questions into sub-questions,\nretrieves relevant textual subgraphs, and composes a question-specific\nknowledge graph to guide answer generation. For that, we use a weighted\nsimilarity function that focuses on both the complex question and the generated\nsubquestions to extract a relevant subgraph, which allows efficient and precise\nretrieval for complex questions and improves the performance of LLMs on\nmulti-hop QA tasks. This structured reasoning pipeline enhances factual\ngrounding and interpretability while leveraging the generative strengths of\nLLMs. We evaluate our method on standard multi-hop QA benchmarks and show that\nit achieves comparable or superior performance to competitive existing methods,\nusing smaller models and fewer LLM calls."
                },
                "authors": [
                    {
                        "name": "Valentin Six"
                    },
                    {
                        "name": "Evan Dufraisse"
                    },
                    {
                        "name": "Gaël de Chalendar"
                    }
                ],
                "author_detail": {
                    "name": "Gaël de Chalendar"
                },
                "author": "Gaël de Chalendar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13380v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13380v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06196v1",
                "updated": "2025-08-08T10:22:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    10,
                    22,
                    19,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T10:22:19Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    10,
                    22,
                    19,
                    4,
                    220,
                    0
                ],
                "title": "EICAP: Deep Dive in Assessment and Enhancement of Large Language Models\n  in Emotional Intelligence through Multi-Turn Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EICAP: Deep Dive in Assessment and Enhancement of Large Language Models\n  in Emotional Intelligence through Multi-Turn Conversations"
                },
                "summary": "Emotional Intelligence (EI) is a critical yet underexplored dimension in the\ndevelopment of human-aligned LLMs. To address this gap, we introduce a unified,\npsychologically grounded four-layer taxonomy of EI tailored for large language\nmodels (LLMs), encompassing emotional tracking, cause inference, appraisal, and\nemotionally appropriate response generation. Building on this framework, we\npresent EICAP-Bench, a novel MCQ style multi-turn benchmark designed to\nevaluate EI capabilities in open-source LLMs across diverse linguistic and\ncultural contexts. We evaluate six LLMs: LLaMA3 (8B), LLaMA3-Instruct, Gemma\n(9B), Gemma-Instruct, Qwen2.5 (7B), and Qwen2.5-Instruct on EmoCap-Bench,\nidentifying Qwen2.5-Instruct as the strongest baseline. To assess the potential\nfor enhancing EI capabilities, we fine-tune both Qwen2.5-Base and\nQwen2.5-Instruct using LoRA adapters on UltraChat (UC), a large-scale,\ninstruction-tuned dialogue dataset, in both English and Arabic. Our statistical\nanalysis reveals that among the five EI layers, only the Appraisal layer shows\nsignificant improvement through UC-based fine-tuning. These findings highlight\nthe limitations of existing pretraining and instruction-tuning paradigms in\nequipping LLMs with deeper emotional reasoning and underscore the need for\ntargeted data and modeling strategies for comprehensive EI alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotional Intelligence (EI) is a critical yet underexplored dimension in the\ndevelopment of human-aligned LLMs. To address this gap, we introduce a unified,\npsychologically grounded four-layer taxonomy of EI tailored for large language\nmodels (LLMs), encompassing emotional tracking, cause inference, appraisal, and\nemotionally appropriate response generation. Building on this framework, we\npresent EICAP-Bench, a novel MCQ style multi-turn benchmark designed to\nevaluate EI capabilities in open-source LLMs across diverse linguistic and\ncultural contexts. We evaluate six LLMs: LLaMA3 (8B), LLaMA3-Instruct, Gemma\n(9B), Gemma-Instruct, Qwen2.5 (7B), and Qwen2.5-Instruct on EmoCap-Bench,\nidentifying Qwen2.5-Instruct as the strongest baseline. To assess the potential\nfor enhancing EI capabilities, we fine-tune both Qwen2.5-Base and\nQwen2.5-Instruct using LoRA adapters on UltraChat (UC), a large-scale,\ninstruction-tuned dialogue dataset, in both English and Arabic. Our statistical\nanalysis reveals that among the five EI layers, only the Appraisal layer shows\nsignificant improvement through UC-based fine-tuning. These findings highlight\nthe limitations of existing pretraining and instruction-tuning paradigms in\nequipping LLMs with deeper emotional reasoning and underscore the need for\ntargeted data and modeling strategies for comprehensive EI alignment."
                },
                "authors": [
                    {
                        "name": "Nizi Nazar"
                    },
                    {
                        "name": "Ehsaneddin Asgari"
                    }
                ],
                "author_detail": {
                    "name": "Ehsaneddin Asgari"
                },
                "author": "Ehsaneddin Asgari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06194v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06194v1",
                "updated": "2025-08-08T10:19:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    10,
                    19,
                    21,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T10:19:21Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    10,
                    19,
                    21,
                    4,
                    220,
                    0
                ],
                "title": "Beyond Uniform Criteria: Scenario-Adaptive Multi-Dimensional Jailbreak\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Uniform Criteria: Scenario-Adaptive Multi-Dimensional Jailbreak\n  Evaluation"
                },
                "summary": "Precise jailbreak evaluation is vital for LLM red teaming and jailbreak\nresearch. Current approaches employ binary classification ( e.g., string\nmatching, toxic text classifiers, LLM-driven methods), yielding only \"yes/no\"\nlabels without quantifying harm intensity. Existing multi-dimensional\nframeworks ( e.g., Security Violation, Relative Truthfulness, Informativeness)\napply uniform evaluation criteria across scenarios, resulting in\nscenario-specific mismatches--for instance, \"Relative Truthfulness\" is\nirrelevant to \"hate speech\"--which compromise evaluation precision. To tackle\nthese limitations, we introduce SceneJailEval, with key contributions: (1) A\ngroundbreaking scenario-adaptive multi-dimensional framework for jailbreak\nevaluation, overcoming the critical \"one-size-fits-all\" constraint of existing\nmulti-dimensional methods, and featuring strong extensibility to flexibly adapt\nto customized or emerging scenarios. (2) A comprehensive 14-scenario dataset\nwith diverse jailbreak variants and regional cases, filling the long-standing\ngap in high-quality, holistic benchmarks for scenario-adaptive evaluation. (3)\nSceneJailEval achieves state-of-the-art results, with an F1 score of 0.917 on\nour full-scenario dataset (+6% over prior SOTA) and 0.995 on JBB (+3% over\nprior SOTA), surpassing accuracy limits of existing evaluation methods in\nheterogeneous scenarios and confirming its advantage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precise jailbreak evaluation is vital for LLM red teaming and jailbreak\nresearch. Current approaches employ binary classification ( e.g., string\nmatching, toxic text classifiers, LLM-driven methods), yielding only \"yes/no\"\nlabels without quantifying harm intensity. Existing multi-dimensional\nframeworks ( e.g., Security Violation, Relative Truthfulness, Informativeness)\napply uniform evaluation criteria across scenarios, resulting in\nscenario-specific mismatches--for instance, \"Relative Truthfulness\" is\nirrelevant to \"hate speech\"--which compromise evaluation precision. To tackle\nthese limitations, we introduce SceneJailEval, with key contributions: (1) A\ngroundbreaking scenario-adaptive multi-dimensional framework for jailbreak\nevaluation, overcoming the critical \"one-size-fits-all\" constraint of existing\nmulti-dimensional methods, and featuring strong extensibility to flexibly adapt\nto customized or emerging scenarios. (2) A comprehensive 14-scenario dataset\nwith diverse jailbreak variants and regional cases, filling the long-standing\ngap in high-quality, holistic benchmarks for scenario-adaptive evaluation. (3)\nSceneJailEval achieves state-of-the-art results, with an F1 score of 0.917 on\nour full-scenario dataset (+6% over prior SOTA) and 0.995 on JBB (+3% over\nprior SOTA), surpassing accuracy limits of existing evaluation methods in\nheterogeneous scenarios and confirming its advantage."
                },
                "authors": [
                    {
                        "name": "Lai Jiang"
                    },
                    {
                        "name": "Yuekang Li"
                    },
                    {
                        "name": "Xiaohan Zhang"
                    },
                    {
                        "name": "Youtao Ding"
                    },
                    {
                        "name": "Li Pan"
                    }
                ],
                "author_detail": {
                    "name": "Li Pan"
                },
                "author": "Li Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06194v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06194v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05058v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05058v5",
                "updated": "2025-08-08T10:18:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    10,
                    18,
                    44,
                    4,
                    220,
                    0
                ],
                "published": "2025-04-07T13:29:02Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    29,
                    2,
                    0,
                    97,
                    0
                ],
                "title": "Not All Data Are Unlearned Equally",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Data Are Unlearned Equally"
                },
                "summary": "Machine unlearning is concerned with the task of removing knowledge learned\nfrom particular data points from a trained model. In the context of large\nlanguage models (LLMs), unlearning has recently received increased attention,\nparticularly for removing knowledge about named entities from models for\nprivacy purposes. While various approaches have been proposed to address the\nunlearning problem, most existing approaches treat all data points to be\nunlearned equally, i.e., unlearning that Montreal is a city in Canada is\ntreated exactly the same as unlearning the phone number of the first author of\nthis paper. In this work, we show that this all data is equal assumption does\nnot hold for LLM unlearning. We study how the success of unlearning depends on\nthe frequency of the knowledge we want to unlearn in the pre-training data of a\nmodel and find that frequency strongly affects unlearning, i.e., more frequent\nknowledge is harder to unlearn. Additionally, we uncover a misalignment between\nprobability and generation-based evaluations of unlearning and show that this\nproblem worsens as models become larger. Overall, our experiments highlight the\nneed for better evaluation practices and novel methods for LLM unlearning that\ntake the training data of models into account.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine unlearning is concerned with the task of removing knowledge learned\nfrom particular data points from a trained model. In the context of large\nlanguage models (LLMs), unlearning has recently received increased attention,\nparticularly for removing knowledge about named entities from models for\nprivacy purposes. While various approaches have been proposed to address the\nunlearning problem, most existing approaches treat all data points to be\nunlearned equally, i.e., unlearning that Montreal is a city in Canada is\ntreated exactly the same as unlearning the phone number of the first author of\nthis paper. In this work, we show that this all data is equal assumption does\nnot hold for LLM unlearning. We study how the success of unlearning depends on\nthe frequency of the knowledge we want to unlearn in the pre-training data of a\nmodel and find that frequency strongly affects unlearning, i.e., more frequent\nknowledge is harder to unlearn. Additionally, we uncover a misalignment between\nprobability and generation-based evaluations of unlearning and show that this\nproblem worsens as models become larger. Overall, our experiments highlight the\nneed for better evaluation practices and novel methods for LLM unlearning that\ntake the training data of models into account."
                },
                "authors": [
                    {
                        "name": "Aravind Krishnan"
                    },
                    {
                        "name": "Siva Reddy"
                    },
                    {
                        "name": "Marius Mosbach"
                    }
                ],
                "author_detail": {
                    "name": "Marius Mosbach"
                },
                "author": "Marius Mosbach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05058v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05058v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06189v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06189v1",
                "updated": "2025-08-08T10:12:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    10,
                    12,
                    0,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T10:12:00Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    10,
                    12,
                    0,
                    4,
                    220,
                    0
                ],
                "title": "MA-CBP: A Criminal Behavior Prediction Framework Based on Multi-Agent\n  Asynchronous Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MA-CBP: A Criminal Behavior Prediction Framework Based on Multi-Agent\n  Asynchronous Collaboration"
                },
                "summary": "With the acceleration of urbanization, criminal behavior in public scenes\nposes an increasingly serious threat to social security. Traditional anomaly\ndetection methods based on feature recognition struggle to capture high-level\nbehavioral semantics from historical information, while generative approaches\nbased on Large Language Models (LLMs) often fail to meet real-time\nrequirements. To address these challenges, we propose MA-CBP, a criminal\nbehavior prediction framework based on multi-agent asynchronous collaboration.\nThis framework transforms real-time video streams into frame-level semantic\ndescriptions, constructs causally consistent historical summaries, and fuses\nadjacent image frames to perform joint reasoning over long- and short-term\ncontexts. The resulting behavioral decisions include key elements such as event\nsubjects, locations, and causes, enabling early warning of potential criminal\nactivity. In addition, we construct a high-quality criminal behavior dataset\nthat provides multi-scale language supervision, including frame-level,\nsummary-level, and event-level semantic annotations. Experimental results\ndemonstrate that our method achieves superior performance on multiple datasets\nand offers a promising solution for risk warning in urban public safety\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the acceleration of urbanization, criminal behavior in public scenes\nposes an increasingly serious threat to social security. Traditional anomaly\ndetection methods based on feature recognition struggle to capture high-level\nbehavioral semantics from historical information, while generative approaches\nbased on Large Language Models (LLMs) often fail to meet real-time\nrequirements. To address these challenges, we propose MA-CBP, a criminal\nbehavior prediction framework based on multi-agent asynchronous collaboration.\nThis framework transforms real-time video streams into frame-level semantic\ndescriptions, constructs causally consistent historical summaries, and fuses\nadjacent image frames to perform joint reasoning over long- and short-term\ncontexts. The resulting behavioral decisions include key elements such as event\nsubjects, locations, and causes, enabling early warning of potential criminal\nactivity. In addition, we construct a high-quality criminal behavior dataset\nthat provides multi-scale language supervision, including frame-level,\nsummary-level, and event-level semantic annotations. Experimental results\ndemonstrate that our method achieves superior performance on multiple datasets\nand offers a promising solution for risk warning in urban public safety\nscenarios."
                },
                "authors": [
                    {
                        "name": "Cheng Liu"
                    },
                    {
                        "name": "Daou Zhang"
                    },
                    {
                        "name": "Tingxu Liu"
                    },
                    {
                        "name": "Yuhan Wang"
                    },
                    {
                        "name": "Jinyang Chen"
                    },
                    {
                        "name": "Yuexuan Li"
                    },
                    {
                        "name": "Xinying Xiao"
                    },
                    {
                        "name": "Chenbo Xin"
                    },
                    {
                        "name": "Ziru Wang"
                    },
                    {
                        "name": "Weichao Wu"
                    }
                ],
                "author_detail": {
                    "name": "Weichao Wu"
                },
                "author": "Weichao Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06189v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06189v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20910v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20910v2",
                "updated": "2025-08-08T10:06:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    10,
                    6,
                    38,
                    4,
                    220,
                    0
                ],
                "published": "2025-05-27T09:00:12Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    9,
                    0,
                    12,
                    1,
                    147,
                    0
                ],
                "title": "Automated Privacy Information Annotation in Large Language Model\n  Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Privacy Information Annotation in Large Language Model\n  Interactions"
                },
                "summary": "Users interacting with large language models (LLMs) under their real\nidentifiers often unknowingly risk disclosing private information.\nAutomatically notifying users whether their queries leak privacy and which\nphrases leak what private information has therefore become a practical need.\nExisting privacy detection methods, however, were designed for different\nobjectives and application domains, typically tagging personally identifiable\ninformation (PII) in anonymous content, which is insufficient in real-name\ninteraction scenarios with LLMs. In this work, to support the development and\nevaluation of privacy detection models for LLM interactions that are deployable\non local user devices, we construct a large-scale multilingual dataset with\n249K user queries and 154K annotated privacy phrases. In particular, we build\nan automated privacy annotation pipeline with strong LLMs to automatically\nextract privacy phrases from dialogue datasets and annotate leaked information.\nWe also design evaluation metrics at the levels of privacy leakage, extracted\nprivacy phrase, and privacy information. We further establish baseline methods\nusing light-weight LLMs with both tuning-free and tuning-based methods, and\nreport a comprehensive evaluation of their performance. Evaluation results\nreveal a gap between current performance and the requirements of real-world LLM\napplications, motivating future research into more effective local privacy\ndetection methods grounded in our dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Users interacting with large language models (LLMs) under their real\nidentifiers often unknowingly risk disclosing private information.\nAutomatically notifying users whether their queries leak privacy and which\nphrases leak what private information has therefore become a practical need.\nExisting privacy detection methods, however, were designed for different\nobjectives and application domains, typically tagging personally identifiable\ninformation (PII) in anonymous content, which is insufficient in real-name\ninteraction scenarios with LLMs. In this work, to support the development and\nevaluation of privacy detection models for LLM interactions that are deployable\non local user devices, we construct a large-scale multilingual dataset with\n249K user queries and 154K annotated privacy phrases. In particular, we build\nan automated privacy annotation pipeline with strong LLMs to automatically\nextract privacy phrases from dialogue datasets and annotate leaked information.\nWe also design evaluation metrics at the levels of privacy leakage, extracted\nprivacy phrase, and privacy information. We further establish baseline methods\nusing light-weight LLMs with both tuning-free and tuning-based methods, and\nreport a comprehensive evaluation of their performance. Evaluation results\nreveal a gap between current performance and the requirements of real-world LLM\napplications, motivating future research into more effective local privacy\ndetection methods grounded in our dataset."
                },
                "authors": [
                    {
                        "name": "Hang Zeng"
                    },
                    {
                        "name": "Xiangyu Liu"
                    },
                    {
                        "name": "Yong Hu"
                    },
                    {
                        "name": "Chaoyue Niu"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Shaojie Tang"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_comment": "8 content pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20910v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20910v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06186v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06186v1",
                "updated": "2025-08-08T10:04:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    10,
                    4,
                    40,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T10:04:40Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    10,
                    4,
                    40,
                    4,
                    220,
                    0
                ],
                "title": "DKG-LLM : A Framework for Medical Diagnosis and Personalized Treatment\n  Recommendations via Dynamic Knowledge Graph and Large Language Model\n  Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DKG-LLM : A Framework for Medical Diagnosis and Personalized Treatment\n  Recommendations via Dynamic Knowledge Graph and Large Language Model\n  Integration"
                },
                "summary": "Large Language Models (LLMs) have grown exponentially since the release of\nChatGPT. These models have gained attention due to their robust performance on\nvarious tasks, including language processing tasks. These models achieve\nunderstanding and comprehension of tasks by training billions of parameters.\nThe development of these models is a transformative force in enhancing natural\nlanguage understanding and has taken a significant step towards artificial\ngeneral intelligence (AGI). In this study, we aim to present the DKG-LLM\nframework. The DKG-LLM framework introduces a groundbreaking approach to\nmedical diagnosis and personalized treatment recommendations by integrating a\ndynamic knowledge graph (DKG) with the Grok 3 large language model. Using the\nAdaptive Semantic Fusion Algorithm (ASFA), heterogeneous medical data\n(including clinical reports and PubMed articles) and patient records\ndynamically generate a knowledge graph consisting of 15,964 nodes in 13\ndistinct types (e.g., diseases, symptoms, treatments, patient profiles) and\n127,392 edges in 26 relationship types (e.g., causal, therapeutic,\nassociation). ASFA utilizes advanced probabilistic models, Bayesian inference,\nand graph optimization to extract semantic information, dynamically updating\nthe graph with approximately 150 new nodes and edges in each data category\nwhile maintaining scalability with up to 987,654 edges. Real-world datasets,\nincluding MIMIC-III and PubMed, were utilized to evaluate the proposed\narchitecture. The evaluation results show that DKG-LLM achieves a diagnostic\naccuracy of 84.19%. The model also has a treatment recommendation accuracy of\n89.63% and a semantic coverage of 93.48%. DKG-LLM is a reliable and\ntransformative tool that handles noisy data and complex multi-symptom diseases,\nalong with feedback-based learning from physician input.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have grown exponentially since the release of\nChatGPT. These models have gained attention due to their robust performance on\nvarious tasks, including language processing tasks. These models achieve\nunderstanding and comprehension of tasks by training billions of parameters.\nThe development of these models is a transformative force in enhancing natural\nlanguage understanding and has taken a significant step towards artificial\ngeneral intelligence (AGI). In this study, we aim to present the DKG-LLM\nframework. The DKG-LLM framework introduces a groundbreaking approach to\nmedical diagnosis and personalized treatment recommendations by integrating a\ndynamic knowledge graph (DKG) with the Grok 3 large language model. Using the\nAdaptive Semantic Fusion Algorithm (ASFA), heterogeneous medical data\n(including clinical reports and PubMed articles) and patient records\ndynamically generate a knowledge graph consisting of 15,964 nodes in 13\ndistinct types (e.g., diseases, symptoms, treatments, patient profiles) and\n127,392 edges in 26 relationship types (e.g., causal, therapeutic,\nassociation). ASFA utilizes advanced probabilistic models, Bayesian inference,\nand graph optimization to extract semantic information, dynamically updating\nthe graph with approximately 150 new nodes and edges in each data category\nwhile maintaining scalability with up to 987,654 edges. Real-world datasets,\nincluding MIMIC-III and PubMed, were utilized to evaluate the proposed\narchitecture. The evaluation results show that DKG-LLM achieves a diagnostic\naccuracy of 84.19%. The model also has a treatment recommendation accuracy of\n89.63% and a semantic coverage of 93.48%. DKG-LLM is a reliable and\ntransformative tool that handles noisy data and complex multi-symptom diseases,\nalong with feedback-based learning from physician input."
                },
                "authors": [
                    {
                        "name": "Ali Sarabadani"
                    },
                    {
                        "name": "Maryam Abdollahi Shamami"
                    },
                    {
                        "name": "Hamidreza Sadeghsalehi"
                    },
                    {
                        "name": "Borhan Asadi"
                    },
                    {
                        "name": "Saba Hesaraki"
                    }
                ],
                "author_detail": {
                    "name": "Saba Hesaraki"
                },
                "author": "Saba Hesaraki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06186v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06186v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19349v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19349v2",
                "updated": "2025-08-08T10:00:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    10,
                    0,
                    47,
                    4,
                    220,
                    0
                ],
                "published": "2025-05-25T22:32:35Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    22,
                    32,
                    35,
                    6,
                    145,
                    0
                ],
                "title": "DECA: A Near-Core LLM Decompression Accelerator Grounded on a 3D\n  Roofline Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DECA: A Near-Core LLM Decompression Accelerator Grounded on a 3D\n  Roofline Model"
                },
                "summary": "To alleviate the memory bandwidth bottleneck in Large Language Model (LLM)\ninference workloads, weight matrices are stored in memory in quantized and\nsparsified formats. Hence, before tiles of these matrices can be processed by\nin-core generalized matrix multiplication (GeMM) hardware engines, they need to\nbe dequantized and de-sparsified. This is currently performed in software with\nvector operations. Unfortunately, this approach delivers only modest\nperformance. Moreover, it is hard to understand how to improve the system, as\nthe overall GeMM performance depends on the interaction between memory\nresources, vector units, and hardware matrix engines.\n  To improve the performance of LLM inference in advanced platforms equipped\nwith in-core GeMM engines and HBM, this paper makes three main contributions.\nFirst, it develops an analytical performance model with a 3D visual\nrepresentation that provides insights into how memory resources, vector units,\nand hardware matrix engines interact to deliver compressed GeMM performance.\nSecond, it proposes DECA, a new near-core ML-model decompression accelerator.\nDECA offloads tile de-sparsification and dequantization from the CPU, producing\nready-to-use tiles for in-core GeMM engines. Third, it introduces a new ISA\nextension that enables out-of-order invocation of the near-core accelerator.\nWith this extension, accelerator and core computations can interleave and\noverlap with high-performance. Our evaluation shows that, in a simulated\n56-core Xeon 4 server with HBM, DECA accelerates the execution of compressed\nGeMMs by up to 4x over the use of optimized Intel software kernels. Further,\nDECA reduces the next-token generation time of Llama2-70B and OPT-66B by\n1.6x-2.6x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To alleviate the memory bandwidth bottleneck in Large Language Model (LLM)\ninference workloads, weight matrices are stored in memory in quantized and\nsparsified formats. Hence, before tiles of these matrices can be processed by\nin-core generalized matrix multiplication (GeMM) hardware engines, they need to\nbe dequantized and de-sparsified. This is currently performed in software with\nvector operations. Unfortunately, this approach delivers only modest\nperformance. Moreover, it is hard to understand how to improve the system, as\nthe overall GeMM performance depends on the interaction between memory\nresources, vector units, and hardware matrix engines.\n  To improve the performance of LLM inference in advanced platforms equipped\nwith in-core GeMM engines and HBM, this paper makes three main contributions.\nFirst, it develops an analytical performance model with a 3D visual\nrepresentation that provides insights into how memory resources, vector units,\nand hardware matrix engines interact to deliver compressed GeMM performance.\nSecond, it proposes DECA, a new near-core ML-model decompression accelerator.\nDECA offloads tile de-sparsification and dequantization from the CPU, producing\nready-to-use tiles for in-core GeMM engines. Third, it introduces a new ISA\nextension that enables out-of-order invocation of the near-core accelerator.\nWith this extension, accelerator and core computations can interleave and\noverlap with high-performance. Our evaluation shows that, in a simulated\n56-core Xeon 4 server with HBM, DECA accelerates the execution of compressed\nGeMMs by up to 4x over the use of optimized Intel software kernels. Further,\nDECA reduces the next-token generation time of Llama2-70B and OPT-66B by\n1.6x-2.6x."
                },
                "authors": [
                    {
                        "name": "Gerasimos Gerogiannis"
                    },
                    {
                        "name": "Stijn Eyerman"
                    },
                    {
                        "name": "Evangelos Georganas"
                    },
                    {
                        "name": "Wim Heirman"
                    },
                    {
                        "name": "Josep Torrellas"
                    }
                ],
                "author_detail": {
                    "name": "Josep Torrellas"
                },
                "arxiv_affiliation": "University of Illinois at Urbana-Champaign",
                "author": "Josep Torrellas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19349v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19349v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00806v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00806v2",
                "updated": "2025-08-08T09:49:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    9,
                    49,
                    52,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-01T17:39:25Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    17,
                    39,
                    25,
                    4,
                    213,
                    0
                ],
                "title": "Adacc: An Adaptive Framework Unifying Compression and Activation\n  Recomputation for LLM Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adacc: An Adaptive Framework Unifying Compression and Activation\n  Recomputation for LLM Training"
                },
                "summary": "Training large language models (LLMs) is often constrained by GPU memory\nlimitations. To alleviate memory pressure, activation recomputation and data\ncompression have been proposed as two major strategies. However, both\napproaches have limitations: recomputation introduces significant training\noverhead, while compression can lead to accuracy degradation and computational\ninefficiency when applied naively. In this paper, we propose Adacc, the first\nadaptive memory optimization framework that unifies activation recomputation\nand data compression to improve training efficiency for LLMs while preserving\nmodel accuracy. Unlike existing methods that apply static, rule-based\nstrategies or rely solely on one technique, Adacc makes fine-grained,\ntensor-level decisions, dynamically selecting between recomputation, retention,\nand compression based on tensor characteristics and runtime hardware\nconstraints.\n  Adacc tackles three key challenges: (1) it introduces layer-specific\ncompression algorithms that mitigate accuracy loss by accounting for outliers\nin LLM activations; (2) it employs a MILP-based scheduling policy to globally\noptimize memory strategies across layers; and (3) it integrates an adaptive\npolicy evolution mechanism to update strategies during training in response to\nchanging data distributions. Experimental results show that Adacc improves\ntraining throughput by 1.01x to 1.37x compared to state-of-the-art frameworks,\nwhile maintaining accuracy comparable to the baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language models (LLMs) is often constrained by GPU memory\nlimitations. To alleviate memory pressure, activation recomputation and data\ncompression have been proposed as two major strategies. However, both\napproaches have limitations: recomputation introduces significant training\noverhead, while compression can lead to accuracy degradation and computational\ninefficiency when applied naively. In this paper, we propose Adacc, the first\nadaptive memory optimization framework that unifies activation recomputation\nand data compression to improve training efficiency for LLMs while preserving\nmodel accuracy. Unlike existing methods that apply static, rule-based\nstrategies or rely solely on one technique, Adacc makes fine-grained,\ntensor-level decisions, dynamically selecting between recomputation, retention,\nand compression based on tensor characteristics and runtime hardware\nconstraints.\n  Adacc tackles three key challenges: (1) it introduces layer-specific\ncompression algorithms that mitigate accuracy loss by accounting for outliers\nin LLM activations; (2) it employs a MILP-based scheduling policy to globally\noptimize memory strategies across layers; and (3) it integrates an adaptive\npolicy evolution mechanism to update strategies during training in response to\nchanging data distributions. Experimental results show that Adacc improves\ntraining throughput by 1.01x to 1.37x compared to state-of-the-art frameworks,\nwhile maintaining accuracy comparable to the baseline."
                },
                "authors": [
                    {
                        "name": "Ping Chen"
                    },
                    {
                        "name": "Zhuohong Deng"
                    },
                    {
                        "name": "Ping Li"
                    },
                    {
                        "name": "Shuibing He"
                    },
                    {
                        "name": "Hongzi Zhu"
                    },
                    {
                        "name": "Yi Zheng"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Baoxing Huai"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00806v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00806v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06178v1",
                "updated": "2025-08-08T09:48:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    9,
                    48,
                    32,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T09:48:32Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    9,
                    48,
                    32,
                    4,
                    220,
                    0
                ],
                "title": "Comparing Knowledge Injection Methods for LLMs in a Low-Resource Regime",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing Knowledge Injection Methods for LLMs in a Low-Resource Regime"
                },
                "summary": "Large language models (LLMs) often require vast amounts of text to\neffectively acquire new knowledge. While continuing pre-training on large\ncorpora or employing retrieval-augmented generation (RAG) has proven\nsuccessful, updating an LLM with only a few thousand or million tokens remains\nchallenging. In this work, we investigate the task of injecting small,\nunstructured information into LLMs and its relation to the catastrophic\nforgetting phenomenon. We use a dataset of recent news -- ensuring no overlap\nwith the model's pre-training data -- to evaluate the knowledge acquisition by\nprobing the model with question-answer pairs related the learned information.\nStarting from a continued pre-training baseline, we explored different\naugmentation algorithms to generate synthetic data to improve the knowledge\nacquisition capabilities. Our experiments show that simply continuing\npre-training on limited data yields modest improvements, whereas exposing the\nmodel to diverse textual variations significantly improves the learning of new\nfacts -- particularly with methods that induce greater variability through\ndiverse prompting. Furthermore, we shed light on the forgetting phenomenon in\nsmall-data regimes, illustrating the delicate balance between learning new\ncontent and retaining existing capabilities. We also confirm the sensitivity of\nRAG-based approaches for knowledge injection, which often lead to greater\ndegradation on control datasets compared to parametric methods. Finally, we\ndemonstrate that models can generate effective synthetic training data\nthemselves, suggesting a pathway toward self-improving model updates. All code\nand generated data used in our experiments are publicly available, providing a\nresource for studying efficient knowledge injection in LLMs with limited data\nat https://github.com/hugoabonizio/knowledge-injection-methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often require vast amounts of text to\neffectively acquire new knowledge. While continuing pre-training on large\ncorpora or employing retrieval-augmented generation (RAG) has proven\nsuccessful, updating an LLM with only a few thousand or million tokens remains\nchallenging. In this work, we investigate the task of injecting small,\nunstructured information into LLMs and its relation to the catastrophic\nforgetting phenomenon. We use a dataset of recent news -- ensuring no overlap\nwith the model's pre-training data -- to evaluate the knowledge acquisition by\nprobing the model with question-answer pairs related the learned information.\nStarting from a continued pre-training baseline, we explored different\naugmentation algorithms to generate synthetic data to improve the knowledge\nacquisition capabilities. Our experiments show that simply continuing\npre-training on limited data yields modest improvements, whereas exposing the\nmodel to diverse textual variations significantly improves the learning of new\nfacts -- particularly with methods that induce greater variability through\ndiverse prompting. Furthermore, we shed light on the forgetting phenomenon in\nsmall-data regimes, illustrating the delicate balance between learning new\ncontent and retaining existing capabilities. We also confirm the sensitivity of\nRAG-based approaches for knowledge injection, which often lead to greater\ndegradation on control datasets compared to parametric methods. Finally, we\ndemonstrate that models can generate effective synthetic training data\nthemselves, suggesting a pathway toward self-improving model updates. All code\nand generated data used in our experiments are publicly available, providing a\nresource for studying efficient knowledge injection in LLMs with limited data\nat https://github.com/hugoabonizio/knowledge-injection-methods."
                },
                "authors": [
                    {
                        "name": "Hugo Abonizio"
                    },
                    {
                        "name": "Thales Almeida"
                    },
                    {
                        "name": "Roberto Lotufo"
                    },
                    {
                        "name": "Rodrigo Nogueira"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo Nogueira"
                },
                "author": "Rodrigo Nogueira",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06176v1",
                "updated": "2025-08-08T09:45:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    9,
                    45,
                    16,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T09:45:16Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    9,
                    45,
                    16,
                    4,
                    220,
                    0
                ],
                "title": "A 66-Gb/s/5.5-W RISC-V Many-Core Cluster for 5G+ Software-Defined Radio\n  Uplinks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A 66-Gb/s/5.5-W RISC-V Many-Core Cluster for 5G+ Software-Defined Radio\n  Uplinks"
                },
                "summary": "Following the scale-up of new radio (NR) complexity in 5G and beyond, the\nphysical layer's computing load on base stations is increasing under a strictly\nconstrained latency and power budget; base stations must process > 20-Gb/s\nuplink wireless data rate on the fly, in < 10 W. At the same time, the\nprogrammability and reconfigurability of base station components are the key\nrequirements; it reduces the time and cost of new networks' deployment, it\nlowers the acceptance threshold for industry players to enter the market, and\nit ensures return on investments in a fast-paced evolution of standards. In\nthis article, we present the design of a many-core cluster for 5G and beyond\nbase station processing. Our design features 1024, streamlined RISC-V cores\nwith domain-specific FP extensions, and 4-MiB shared memory. It provides the\nnecessary computational capabilities for software-defined processing of the\nlower physical layer of 5G physical uplink shared channel (PUSCH), satisfying\nhigh-end throughput requirements (66 Gb/s for a transition time interval (TTI),\n9.4-302 Gb/s depending on the processing stage). The throughput metrics for the\nimplemented functions are ten times higher than in state-of-the-art (SoTA)\napplication-specific instruction processors (ASIPs). The energy efficiency on\nkey NR kernels (2-41 Gb/s/W), measured at 800 MHz, 25 {\\deg}C, and 0.8 V, on a\nplaced and routed instance in 12-nm CMOS technology, is competitive with SoTA\narchitectures. The PUSCH processing runs end-to-end on a single cluster in 1.7\nms, at <6-W average power consumption, achieving 12 Gb/s/W.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Following the scale-up of new radio (NR) complexity in 5G and beyond, the\nphysical layer's computing load on base stations is increasing under a strictly\nconstrained latency and power budget; base stations must process > 20-Gb/s\nuplink wireless data rate on the fly, in < 10 W. At the same time, the\nprogrammability and reconfigurability of base station components are the key\nrequirements; it reduces the time and cost of new networks' deployment, it\nlowers the acceptance threshold for industry players to enter the market, and\nit ensures return on investments in a fast-paced evolution of standards. In\nthis article, we present the design of a many-core cluster for 5G and beyond\nbase station processing. Our design features 1024, streamlined RISC-V cores\nwith domain-specific FP extensions, and 4-MiB shared memory. It provides the\nnecessary computational capabilities for software-defined processing of the\nlower physical layer of 5G physical uplink shared channel (PUSCH), satisfying\nhigh-end throughput requirements (66 Gb/s for a transition time interval (TTI),\n9.4-302 Gb/s depending on the processing stage). The throughput metrics for the\nimplemented functions are ten times higher than in state-of-the-art (SoTA)\napplication-specific instruction processors (ASIPs). The energy efficiency on\nkey NR kernels (2-41 Gb/s/W), measured at 800 MHz, 25 {\\deg}C, and 0.8 V, on a\nplaced and routed instance in 12-nm CMOS technology, is competitive with SoTA\narchitectures. The PUSCH processing runs end-to-end on a single cluster in 1.7\nms, at <6-W average power consumption, achieving 12 Gb/s/W."
                },
                "authors": [
                    {
                        "name": "Marco Bertuletti"
                    },
                    {
                        "name": "Yichao Zhang"
                    },
                    {
                        "name": "Alessandro Vanelli-Coralli"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_doi": "10.1109/TVLSI.2025.3576855",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TVLSI.2025.3576855",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.06176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems,\n  vol. 33, no. 8, pp. 2225-2238, Aug. 2025",
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06168v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06168v1",
                "updated": "2025-08-08T09:35:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    9,
                    35,
                    56,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T09:35:56Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    9,
                    35,
                    56,
                    4,
                    220,
                    0
                ],
                "title": "Improving Table Retrieval with Question Generation from Partial Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Table Retrieval with Question Generation from Partial Tables"
                },
                "summary": "Recent advances in open-domain question answering over tables have widely\nadopted large language models (LLMs) under the Retriever-Reader architecture.\nPrior works have effectively leveraged LLMs to tackle the complex reasoning\ndemands of the Reader component, such as text-to-text, text-to-SQL, and multi\nhop reasoning. In contrast, the Retriever component has primarily focused on\noptimizing the query representation-training retrievers to retrieve relevant\ntables based on questions, or to select keywords from questions for matching\ntable segments. However, little attention has been given to enhancing how\ntables themselves are represented in embedding space to better align with\nquestions. To address this, we propose QGpT (Question Generation from Partial\nTables), a simple yet effective method that uses an LLM to generate synthetic\nquestions based on small portions of a table. These questions are generated to\nsimulate how a user might query the content of the table currently under\nconsideration. The generated questions are then jointly embedded with the\npartial table segments used for generation, enhancing semantic alignment with\nuser queries. Without the need to embed entire tables, our method significantly\nimproves retrieval performance across multiple benchmarks for both dense and\nlate-interaction retrievers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in open-domain question answering over tables have widely\nadopted large language models (LLMs) under the Retriever-Reader architecture.\nPrior works have effectively leveraged LLMs to tackle the complex reasoning\ndemands of the Reader component, such as text-to-text, text-to-SQL, and multi\nhop reasoning. In contrast, the Retriever component has primarily focused on\noptimizing the query representation-training retrievers to retrieve relevant\ntables based on questions, or to select keywords from questions for matching\ntable segments. However, little attention has been given to enhancing how\ntables themselves are represented in embedding space to better align with\nquestions. To address this, we propose QGpT (Question Generation from Partial\nTables), a simple yet effective method that uses an LLM to generate synthetic\nquestions based on small portions of a table. These questions are generated to\nsimulate how a user might query the content of the table currently under\nconsideration. The generated questions are then jointly embedded with the\npartial table segments used for generation, enhancing semantic alignment with\nuser queries. Without the need to embed entire tables, our method significantly\nimproves retrieval performance across multiple benchmarks for both dense and\nlate-interaction retrievers."
                },
                "authors": [
                    {
                        "name": "Hsing-Ping Liang"
                    },
                    {
                        "name": "Che-Wei Chang"
                    },
                    {
                        "name": "Yao-Chung Fan"
                    }
                ],
                "author_detail": {
                    "name": "Yao-Chung Fan"
                },
                "author": "Yao-Chung Fan",
                "arxiv_comment": "TRL@ACL2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06168v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06168v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06167v1",
                "updated": "2025-08-08T09:34:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    9,
                    34,
                    41,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T09:34:41Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    9,
                    34,
                    41,
                    4,
                    220,
                    0
                ],
                "title": "Pragmatics beyond humans: meaning, communication, and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pragmatics beyond humans: meaning, communication, and LLMs"
                },
                "summary": "The paper reconceptualizes pragmatics not as a subordinate, third dimension\nof meaning, but as a dynamic interface through which language operates as a\nsocially embedded tool for action. With the emergence of large language models\n(LLMs) in communicative contexts, this understanding needs to be further\nrefined and methodologically reconsidered. The first section challenges the\ntraditional semiotic trichotomy, arguing that connectionist LLM architectures\ndestabilize established hierarchies of meaning, and proposes the Human-Machine\nCommunication (HMC) framework as a more suitable alternative. The second\nsection examines the tension between human-centred pragmatic theories and the\nmachine-centred nature of LLMs. While traditional, Gricean-inspired pragmatics\ncontinue to dominate, it relies on human-specific assumptions ill-suited to\npredictive systems like LLMs. Probabilistic pragmatics, particularly the\nRational Speech Act framework, offers a more compatible teleology by focusing\non optimization rather than truth-evaluation. The third section addresses the\nissue of substitutionalism in three forms - generalizing, linguistic, and\ncommunicative - highlighting the anthropomorphic biases that distort LLM\nevaluation and obscure the role of human communicative subjects. Finally, the\npaper introduces the concept of context frustration to describe the paradox of\nincreased contextual input paired with a collapse in contextual understanding,\nemphasizing how users are compelled to co-construct pragmatic conditions both\nfor the model and themselves. These arguments suggest that pragmatic theory may\nneed to be adjusted or expanded to better account for communication involving\ngenerative AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The paper reconceptualizes pragmatics not as a subordinate, third dimension\nof meaning, but as a dynamic interface through which language operates as a\nsocially embedded tool for action. With the emergence of large language models\n(LLMs) in communicative contexts, this understanding needs to be further\nrefined and methodologically reconsidered. The first section challenges the\ntraditional semiotic trichotomy, arguing that connectionist LLM architectures\ndestabilize established hierarchies of meaning, and proposes the Human-Machine\nCommunication (HMC) framework as a more suitable alternative. The second\nsection examines the tension between human-centred pragmatic theories and the\nmachine-centred nature of LLMs. While traditional, Gricean-inspired pragmatics\ncontinue to dominate, it relies on human-specific assumptions ill-suited to\npredictive systems like LLMs. Probabilistic pragmatics, particularly the\nRational Speech Act framework, offers a more compatible teleology by focusing\non optimization rather than truth-evaluation. The third section addresses the\nissue of substitutionalism in three forms - generalizing, linguistic, and\ncommunicative - highlighting the anthropomorphic biases that distort LLM\nevaluation and obscure the role of human communicative subjects. Finally, the\npaper introduces the concept of context frustration to describe the paradox of\nincreased contextual input paired with a collapse in contextual understanding,\nemphasizing how users are compelled to co-construct pragmatic conditions both\nfor the model and themselves. These arguments suggest that pragmatic theory may\nneed to be adjusted or expanded to better account for communication involving\ngenerative AI."
                },
                "authors": [
                    {
                        "name": "Vít Gvoždiak"
                    }
                ],
                "author_detail": {
                    "name": "Vít Gvoždiak"
                },
                "author": "Vít Gvoždiak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06165v1",
                "updated": "2025-08-08T09:33:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    9,
                    33,
                    20,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T09:33:20Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    9,
                    33,
                    20,
                    4,
                    220,
                    0
                ],
                "title": "UR$^2$: Unify RAG and Reasoning through Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UR$^2$: Unify RAG and Reasoning through Reinforcement Learning"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable capabilities through two\ncomplementary paradigms: Retrieval-Augmented Generation (RAG), which enhances\nknowledge grounding, and Reinforcement Learning from Verifiable Rewards (RLVR),\nwhich optimizes complex reasoning abilities. However, these two capabilities\nare often developed in isolation, and existing efforts to unify them remain\nnarrow in scope-typically limited to open-domain QA with fixed retrieval\nsettings and task-specific assumptions. This lack of integration constrains\ngeneralization and limits the applicability of RAG-RL methods to broader\ndomains. To bridge this gap, we propose UR2 (Unified RAG and Reasoning), a\ngeneral framework that unifies retrieval and reasoning through reinforcement\nlearning. UR2 introduces two key contributions: a difficulty-aware curriculum\ntraining that selectively invokes retrieval only for challenging problems, and\na hybrid knowledge access strategy combining domain-specific offline corpora\nwith LLM-generated summaries. These components are designed to enable dynamic\ncoordination between retrieval and reasoning, improving adaptability across a\ndiverse range of tasks. Experiments across open-domain QA, MMLU-Pro, medical,\nand mathematical reasoning tasks demonstrate that UR2 (built on Qwen2.5-3/7B\nand LLaMA-3.1-8B) significantly outperforms existing RAG and RL methods,\nachieving comparable performance to GPT-4o-mini and GPT-4.1-mini on several\nbenchmarks. We have released all code, models, and data at\nhttps://github.com/Tsinghua-dhy/UR2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable capabilities through two\ncomplementary paradigms: Retrieval-Augmented Generation (RAG), which enhances\nknowledge grounding, and Reinforcement Learning from Verifiable Rewards (RLVR),\nwhich optimizes complex reasoning abilities. However, these two capabilities\nare often developed in isolation, and existing efforts to unify them remain\nnarrow in scope-typically limited to open-domain QA with fixed retrieval\nsettings and task-specific assumptions. This lack of integration constrains\ngeneralization and limits the applicability of RAG-RL methods to broader\ndomains. To bridge this gap, we propose UR2 (Unified RAG and Reasoning), a\ngeneral framework that unifies retrieval and reasoning through reinforcement\nlearning. UR2 introduces two key contributions: a difficulty-aware curriculum\ntraining that selectively invokes retrieval only for challenging problems, and\na hybrid knowledge access strategy combining domain-specific offline corpora\nwith LLM-generated summaries. These components are designed to enable dynamic\ncoordination between retrieval and reasoning, improving adaptability across a\ndiverse range of tasks. Experiments across open-domain QA, MMLU-Pro, medical,\nand mathematical reasoning tasks demonstrate that UR2 (built on Qwen2.5-3/7B\nand LLaMA-3.1-8B) significantly outperforms existing RAG and RL methods,\nachieving comparable performance to GPT-4o-mini and GPT-4.1-mini on several\nbenchmarks. We have released all code, models, and data at\nhttps://github.com/Tsinghua-dhy/UR2."
                },
                "authors": [
                    {
                        "name": "Weitao Li"
                    },
                    {
                        "name": "Boran Xiang"
                    },
                    {
                        "name": "Xiaolong Wang"
                    },
                    {
                        "name": "Zhinan Gou"
                    },
                    {
                        "name": "Weizhi Ma"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06160v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06160v1",
                "updated": "2025-08-08T09:29:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    9,
                    29,
                    37,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T09:29:37Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    9,
                    29,
                    37,
                    4,
                    220,
                    0
                ],
                "title": "Fewer Denoising Steps or Cheaper Per-Step Inference: Towards\n  Compute-Optimal Diffusion Model Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fewer Denoising Steps or Cheaper Per-Step Inference: Towards\n  Compute-Optimal Diffusion Model Deployment"
                },
                "summary": "Diffusion models have shown remarkable success across generative tasks, yet\ntheir high computational demands challenge deployment on resource-limited\nplatforms. This paper investigates a critical question for compute-optimal\ndiffusion model deployment: Under a post-training setting without fine-tuning,\nis it more effective to reduce the number of denoising steps or to use a\ncheaper per-step inference? Intuitively, reducing the number of denoising steps\nincreases the variability of the distributions across steps, making the model\nmore sensitive to compression. In contrast, keeping more denoising steps makes\nthe differences smaller, preserving redundancy, and making post-training\ncompression more feasible. To systematically examine this, we propose PostDiff,\na training-free framework for accelerating pre-trained diffusion models by\nreducing redundancy at both the input level and module level in a post-training\nmanner. At the input level, we propose a mixed-resolution denoising scheme\nbased on the insight that reducing generation resolution in early denoising\nsteps can enhance low-frequency components and improve final generation\nfidelity. At the module level, we employ a hybrid module caching strategy to\nreuse computations across denoising steps. Extensive experiments and ablation\nstudies demonstrate that (1) PostDiff can significantly improve the\nfidelity-efficiency trade-off of state-of-the-art diffusion models, and (2) to\nboost efficiency while maintaining decent generation fidelity, reducing\nper-step inference cost is often more effective than reducing the number of\ndenoising steps. Our code is available at\nhttps://github.com/GATECH-EIC/PostDiff.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have shown remarkable success across generative tasks, yet\ntheir high computational demands challenge deployment on resource-limited\nplatforms. This paper investigates a critical question for compute-optimal\ndiffusion model deployment: Under a post-training setting without fine-tuning,\nis it more effective to reduce the number of denoising steps or to use a\ncheaper per-step inference? Intuitively, reducing the number of denoising steps\nincreases the variability of the distributions across steps, making the model\nmore sensitive to compression. In contrast, keeping more denoising steps makes\nthe differences smaller, preserving redundancy, and making post-training\ncompression more feasible. To systematically examine this, we propose PostDiff,\na training-free framework for accelerating pre-trained diffusion models by\nreducing redundancy at both the input level and module level in a post-training\nmanner. At the input level, we propose a mixed-resolution denoising scheme\nbased on the insight that reducing generation resolution in early denoising\nsteps can enhance low-frequency components and improve final generation\nfidelity. At the module level, we employ a hybrid module caching strategy to\nreuse computations across denoising steps. Extensive experiments and ablation\nstudies demonstrate that (1) PostDiff can significantly improve the\nfidelity-efficiency trade-off of state-of-the-art diffusion models, and (2) to\nboost efficiency while maintaining decent generation fidelity, reducing\nper-step inference cost is often more effective than reducing the number of\ndenoising steps. Our code is available at\nhttps://github.com/GATECH-EIC/PostDiff."
                },
                "authors": [
                    {
                        "name": "Zhenbang Du"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Lifu Wang"
                    },
                    {
                        "name": "Jiayi Qian"
                    },
                    {
                        "name": "Xiao Luo"
                    },
                    {
                        "name": "Yingyan"
                    },
                    {
                        "name": "Lin"
                    }
                ],
                "author_detail": {
                    "name": "Lin"
                },
                "arxiv_affiliation": "Celine",
                "author": "Lin",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06160v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06160v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05734v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05734v2",
                "updated": "2025-08-08T09:27:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    9,
                    27,
                    21,
                    4,
                    220,
                    0
                ],
                "published": "2024-12-07T20:09:01Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    20,
                    9,
                    1,
                    5,
                    342,
                    0
                ],
                "title": "LeakAgent: RL-based Red-teaming Agent for LLM Privacy Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeakAgent: RL-based Red-teaming Agent for LLM Privacy Leakage"
                },
                "summary": "Recent studies have discovered that large language models (LLM) may be\n``fooled'' to output private information, including training data, system\nprompts, and personally identifiable information, under carefully crafted\nadversarial prompts. Existing red-teaming approaches for privacy leakage either\nrely on manual efforts or focus solely on system prompt extraction, making them\nineffective for severe risks of training data leakage. We propose LeakAgent, a\nnovel black-box red-teaming framework for LLM privacy leakage. Our framework\ntrains an open-source LLM through reinforcement learning as the attack agent to\ngenerate adversarial prompts for both training data extraction and system\nprompt extraction. To achieve this, we propose a novel reward function to\nprovide effective and fine-grained rewards and design novel mechanisms to\nbalance exploration and exploitation during learning and enhance the diversity\nof adversarial prompts. Through extensive evaluations, we first show that\nLeakAgent significantly outperforms existing rule-based approaches in training\ndata extraction and automated methods in system prompt leakage. We also\ndemonstrate the effectiveness of LeakAgent in extracting system prompts from\nreal-world applications in OpenAI's GPT Store. We further demonstrate\nLeakAgent's effectiveness in evading the existing guardrail defense and its\nhelpfulness in enabling better safety alignment. Finally, we validate our\ncustomized designs through a detailed ablation study. We release our code here\nhttps://github.com/rucnyz/LeakAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have discovered that large language models (LLM) may be\n``fooled'' to output private information, including training data, system\nprompts, and personally identifiable information, under carefully crafted\nadversarial prompts. Existing red-teaming approaches for privacy leakage either\nrely on manual efforts or focus solely on system prompt extraction, making them\nineffective for severe risks of training data leakage. We propose LeakAgent, a\nnovel black-box red-teaming framework for LLM privacy leakage. Our framework\ntrains an open-source LLM through reinforcement learning as the attack agent to\ngenerate adversarial prompts for both training data extraction and system\nprompt extraction. To achieve this, we propose a novel reward function to\nprovide effective and fine-grained rewards and design novel mechanisms to\nbalance exploration and exploitation during learning and enhance the diversity\nof adversarial prompts. Through extensive evaluations, we first show that\nLeakAgent significantly outperforms existing rule-based approaches in training\ndata extraction and automated methods in system prompt leakage. We also\ndemonstrate the effectiveness of LeakAgent in extracting system prompts from\nreal-world applications in OpenAI's GPT Store. We further demonstrate\nLeakAgent's effectiveness in evading the existing guardrail defense and its\nhelpfulness in enabling better safety alignment. Finally, we validate our\ncustomized designs through a detailed ablation study. We release our code here\nhttps://github.com/rucnyz/LeakAgent."
                },
                "authors": [
                    {
                        "name": "Yuzhou Nie"
                    },
                    {
                        "name": "Zhun Wang"
                    },
                    {
                        "name": "Ye Yu"
                    },
                    {
                        "name": "Xian Wu"
                    },
                    {
                        "name": "Xuandong Zhao"
                    },
                    {
                        "name": "Wenbo Guo"
                    },
                    {
                        "name": "Dawn Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawn Song"
                },
                "author": "Dawn Song",
                "arxiv_comment": "Accepted by COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05734v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05734v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06153v1",
                "updated": "2025-08-08T09:17:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    9,
                    17,
                    33,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T09:17:33Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    9,
                    17,
                    33,
                    4,
                    220,
                    0
                ],
                "title": "SLIP: Soft Label Mechanism and Key-Extraction-Guided CoT-based Defense\n  Against Instruction Backdoor in APIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLIP: Soft Label Mechanism and Key-Extraction-Guided CoT-based Defense\n  Against Instruction Backdoor in APIs"
                },
                "summary": "With the development of customized large language model (LLM) agents, a new\nthreat of black-box backdoor attacks has emerged, where malicious instructions\nare injected into hidden system prompts. These attacks easily bypass existing\ndefenses that rely on white-box access, posing a serious security challenge. To\naddress this, we propose SLIP, a Soft Label mechanism and key-extraction-guided\nCoT-based defense against Instruction backdoors in APIs. SLIP is designed based\non two key insights. First, to counteract the model's oversensitivity to\ntriggers, we propose a Key-extraction-guided Chain-of-Thought (KCoT). Instead\nof only considering the single trigger or the input sentence, KCoT prompts the\nagent to extract task-relevant key phrases. Second, to guide the LLM toward\ncorrect answers, our proposed Soft Label Mechanism (SLM) prompts the agent to\nquantify the semantic correlation between key phrases and candidate answers.\nCrucially, to mitigate the influence of residual triggers or misleading content\nin phrases extracted by KCoT, which typically causes anomalous scores, SLM\nexcludes anomalous scores deviating significantly from the mean and\nsubsequently averages the remaining scores to derive a more reliable semantic\nrepresentation. Extensive experiments on classification and question-answer\n(QA) tasks demonstrate that SLIP is highly effective, reducing the average\nattack success rate (ASR) from 90.2% to 25.13% while maintaining high accuracy\non clean data and outperforming state-of-the-art defenses. Our code are\navailable in\nhttps://github.com/CAU-ISS-Lab/Backdoor-Attack-Defense-LLMs/tree/main/SLIP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of customized large language model (LLM) agents, a new\nthreat of black-box backdoor attacks has emerged, where malicious instructions\nare injected into hidden system prompts. These attacks easily bypass existing\ndefenses that rely on white-box access, posing a serious security challenge. To\naddress this, we propose SLIP, a Soft Label mechanism and key-extraction-guided\nCoT-based defense against Instruction backdoors in APIs. SLIP is designed based\non two key insights. First, to counteract the model's oversensitivity to\ntriggers, we propose a Key-extraction-guided Chain-of-Thought (KCoT). Instead\nof only considering the single trigger or the input sentence, KCoT prompts the\nagent to extract task-relevant key phrases. Second, to guide the LLM toward\ncorrect answers, our proposed Soft Label Mechanism (SLM) prompts the agent to\nquantify the semantic correlation between key phrases and candidate answers.\nCrucially, to mitigate the influence of residual triggers or misleading content\nin phrases extracted by KCoT, which typically causes anomalous scores, SLM\nexcludes anomalous scores deviating significantly from the mean and\nsubsequently averages the remaining scores to derive a more reliable semantic\nrepresentation. Extensive experiments on classification and question-answer\n(QA) tasks demonstrate that SLIP is highly effective, reducing the average\nattack success rate (ASR) from 90.2% to 25.13% while maintaining high accuracy\non clean data and outperforming state-of-the-art defenses. Our code are\navailable in\nhttps://github.com/CAU-ISS-Lab/Backdoor-Attack-Defense-LLMs/tree/main/SLIP."
                },
                "authors": [
                    {
                        "name": "Zhengxian Wu"
                    },
                    {
                        "name": "Juan Wen"
                    },
                    {
                        "name": "Wanli Peng"
                    },
                    {
                        "name": "Haowei Chang"
                    },
                    {
                        "name": "Yinghan Zhou"
                    },
                    {
                        "name": "Yiming Xue"
                    }
                ],
                "author_detail": {
                    "name": "Yiming Xue"
                },
                "author": "Yiming Xue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06152v1",
                "updated": "2025-08-08T09:15:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    9,
                    15,
                    2,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T09:15:02Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    9,
                    15,
                    2,
                    4,
                    220,
                    0
                ],
                "title": "VISTAR:A User-Centric and Role-Driven Benchmark for Text-to-Image\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VISTAR:A User-Centric and Role-Driven Benchmark for Text-to-Image\n  Evaluation"
                },
                "summary": "We present VISTAR, a user-centric, multi-dimensional benchmark for\ntext-to-image (T2I) evaluation that addresses the limitations of existing\nmetrics. VISTAR introduces a two-tier hybrid paradigm: it employs\ndeterministic, scriptable metrics for physically quantifiable attributes (e.g.,\ntext rendering, lighting) and a novel Hierarchical Weighted P/N Questioning\n(HWPQ) scheme that uses constrained vision-language models to assess abstract\nsemantics (e.g., style fusion, cultural fidelity). Grounded in a Delphi study\nwith 120 experts, we defined seven user roles and nine evaluation angles to\nconstruct the benchmark, which comprises 2,845 prompts validated by over 15,000\nhuman pairwise comparisons. Our metrics achieve high human alignment (>75%),\nwith the HWPQ scheme reaching 85.9% accuracy on abstract semantics,\nsignificantly outperforming VQA baselines. Comprehensive evaluation of\nstate-of-the-art models reveals no universal champion, as role-weighted scores\nreorder rankings and provide actionable guidance for domain-specific\ndeployment. All resources are publicly released to foster reproducible T2I\nassessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present VISTAR, a user-centric, multi-dimensional benchmark for\ntext-to-image (T2I) evaluation that addresses the limitations of existing\nmetrics. VISTAR introduces a two-tier hybrid paradigm: it employs\ndeterministic, scriptable metrics for physically quantifiable attributes (e.g.,\ntext rendering, lighting) and a novel Hierarchical Weighted P/N Questioning\n(HWPQ) scheme that uses constrained vision-language models to assess abstract\nsemantics (e.g., style fusion, cultural fidelity). Grounded in a Delphi study\nwith 120 experts, we defined seven user roles and nine evaluation angles to\nconstruct the benchmark, which comprises 2,845 prompts validated by over 15,000\nhuman pairwise comparisons. Our metrics achieve high human alignment (>75%),\nwith the HWPQ scheme reaching 85.9% accuracy on abstract semantics,\nsignificantly outperforming VQA baselines. Comprehensive evaluation of\nstate-of-the-art models reveals no universal champion, as role-weighted scores\nreorder rankings and provide actionable guidance for domain-specific\ndeployment. All resources are publicly released to foster reproducible T2I\nassessment."
                },
                "authors": [
                    {
                        "name": "Kaiyuan Jiang"
                    },
                    {
                        "name": "Ruoxi Sun"
                    },
                    {
                        "name": "Ying Cao"
                    },
                    {
                        "name": "Yuqi Xu"
                    },
                    {
                        "name": "Xinran Zhang"
                    },
                    {
                        "name": "Junyan Guo"
                    },
                    {
                        "name": "ChengSheng Deng"
                    }
                ],
                "author_detail": {
                    "name": "ChengSheng Deng"
                },
                "author": "ChengSheng Deng",
                "arxiv_comment": "17 pages,8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06149v1",
                "updated": "2025-08-08T09:11:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    9,
                    11,
                    5,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T09:11:05Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    9,
                    11,
                    5,
                    4,
                    220,
                    0
                ],
                "title": "Scaling Personality Control in LLMs with Big Five Scaler Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Personality Control in LLMs with Big Five Scaler Prompts"
                },
                "summary": "We present Big5-Scaler, a prompt-based framework for conditioning large\nlanguage models (LLMs) with controllable Big Five personality traits. By\nembedding numeric trait values into natural language prompts, our method\nenables fine-grained personality control without additional training. We\nevaluate Big5-Scaler across trait expression, dialogue generation, and human\ntrait imitation tasks. Results show that it induces consistent and\ndistinguishable personality traits across models, with performance varying by\nprompt type and scale. Our analysis highlights the effectiveness of concise\nprompts and lower trait intensities, providing a efficient approach for\nbuilding personality-aware dialogue agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Big5-Scaler, a prompt-based framework for conditioning large\nlanguage models (LLMs) with controllable Big Five personality traits. By\nembedding numeric trait values into natural language prompts, our method\nenables fine-grained personality control without additional training. We\nevaluate Big5-Scaler across trait expression, dialogue generation, and human\ntrait imitation tasks. Results show that it induces consistent and\ndistinguishable personality traits across models, with performance varying by\nprompt type and scale. Our analysis highlights the effectiveness of concise\nprompts and lower trait intensities, providing a efficient approach for\nbuilding personality-aware dialogue agents."
                },
                "authors": [
                    {
                        "name": "Gunhee Cho"
                    },
                    {
                        "name": "Yun-Gyung Cheong"
                    }
                ],
                "author_detail": {
                    "name": "Yun-Gyung Cheong"
                },
                "author": "Yun-Gyung Cheong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06145v1",
                "updated": "2025-08-08T09:09:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    9,
                    9,
                    3,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T09:09:03Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    9,
                    9,
                    3,
                    4,
                    220,
                    0
                ],
                "title": "Retrieval Augmented Large Language Model System for Comprehensive Drug\n  Contraindications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Large Language Model System for Comprehensive Drug\n  Contraindications"
                },
                "summary": "The versatility of large language models (LLMs) has been explored across\nvarious sectors, but their application in healthcare poses challenges,\nparticularly in the domain of pharmaceutical contraindications where accurate\nand reliable information is required. This study enhances the capability of\nLLMs to address contraindications effectively by implementing a Retrieval\nAugmented Generation (RAG) pipeline. Utilizing OpenAI's GPT-4o-mini as the base\nmodel, and the text-embedding-3-small model for embeddings, our approach\nintegrates Langchain to orchestrate a hybrid retrieval system with re-ranking.\nThis system leverages Drug Utilization Review (DUR) data from public databases,\nfocusing on contraindications for specific age groups, pregnancy, and\nconcomitant drug use. The dataset includes 300 question-answer pairs across\nthree categories, with baseline model accuracy ranging from 0.49 to 0.57.\nPost-integration of the RAG pipeline, we observed a significant improvement in\nmodel accuracy, achieving rates of 0.94, 0.87, and 0.89 for contraindications\nrelated to age groups, pregnancy, and concomitant drug use, respectively. The\nresults indicate that augmenting LLMs with a RAG framework can substantially\nreduce uncertainty in prescription and drug intake decisions by providing more\nprecise and reliable drug contraindication information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The versatility of large language models (LLMs) has been explored across\nvarious sectors, but their application in healthcare poses challenges,\nparticularly in the domain of pharmaceutical contraindications where accurate\nand reliable information is required. This study enhances the capability of\nLLMs to address contraindications effectively by implementing a Retrieval\nAugmented Generation (RAG) pipeline. Utilizing OpenAI's GPT-4o-mini as the base\nmodel, and the text-embedding-3-small model for embeddings, our approach\nintegrates Langchain to orchestrate a hybrid retrieval system with re-ranking.\nThis system leverages Drug Utilization Review (DUR) data from public databases,\nfocusing on contraindications for specific age groups, pregnancy, and\nconcomitant drug use. The dataset includes 300 question-answer pairs across\nthree categories, with baseline model accuracy ranging from 0.49 to 0.57.\nPost-integration of the RAG pipeline, we observed a significant improvement in\nmodel accuracy, achieving rates of 0.94, 0.87, and 0.89 for contraindications\nrelated to age groups, pregnancy, and concomitant drug use, respectively. The\nresults indicate that augmenting LLMs with a RAG framework can substantially\nreduce uncertainty in prescription and drug intake decisions by providing more\nprecise and reliable drug contraindication information."
                },
                "authors": [
                    {
                        "name": "Byeonghun Bang"
                    },
                    {
                        "name": "Jongsuk Yoon"
                    },
                    {
                        "name": "Dong-Jin Chang"
                    },
                    {
                        "name": "Seho Park"
                    },
                    {
                        "name": "Yong Oh Lee"
                    }
                ],
                "author_detail": {
                    "name": "Yong Oh Lee"
                },
                "author": "Yong Oh Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06137v1",
                "updated": "2025-08-08T08:59:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    59,
                    54,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T08:59:54Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    59,
                    54,
                    4,
                    220,
                    0
                ],
                "title": "Transformer-Based Explainable Deep Learning for Breast Cancer Detection\n  in Mammography: The MammoFormer Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-Based Explainable Deep Learning for Breast Cancer Detection\n  in Mammography: The MammoFormer Framework"
                },
                "summary": "Breast cancer detection through mammography interpretation remains difficult\nbecause of the minimal nature of abnormalities that experts need to identify\nalongside the variable interpretations between readers. The potential of CNNs\nfor medical image analysis faces two limitations: they fail to process both\nlocal information and wide contextual data adequately, and do not provide\nexplainable AI (XAI) operations that doctors need to accept them in clinics.\nThe researcher developed the MammoFormer framework, which unites\ntransformer-based architecture with multi-feature enhancement components and\nXAI functionalities within one framework. Seven different architectures\nconsisting of CNNs, Vision Transformer, Swin Transformer, and ConvNext were\ntested alongside four enhancement techniques, including original images,\nnegative transformation, adaptive histogram equalization, and histogram of\noriented gradients. The MammoFormer framework addresses critical clinical\nadoption barriers of AI mammography systems through: (1) systematic\noptimization of transformer architectures via architecture-specific feature\nenhancement, achieving up to 13% performance improvement, (2) comprehensive\nexplainable AI integration providing multi-perspective diagnostic\ninterpretability, and (3) a clinically deployable ensemble system combining CNN\nreliability with transformer global context modeling. The combination of\ntransformer models with suitable feature enhancements enables them to achieve\nequal or better results than CNN approaches. ViT achieves 98.3% accuracy\nalongside AHE while Swin Transformer gains a 13.0% advantage through HOG\nenhancements",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breast cancer detection through mammography interpretation remains difficult\nbecause of the minimal nature of abnormalities that experts need to identify\nalongside the variable interpretations between readers. The potential of CNNs\nfor medical image analysis faces two limitations: they fail to process both\nlocal information and wide contextual data adequately, and do not provide\nexplainable AI (XAI) operations that doctors need to accept them in clinics.\nThe researcher developed the MammoFormer framework, which unites\ntransformer-based architecture with multi-feature enhancement components and\nXAI functionalities within one framework. Seven different architectures\nconsisting of CNNs, Vision Transformer, Swin Transformer, and ConvNext were\ntested alongside four enhancement techniques, including original images,\nnegative transformation, adaptive histogram equalization, and histogram of\noriented gradients. The MammoFormer framework addresses critical clinical\nadoption barriers of AI mammography systems through: (1) systematic\noptimization of transformer architectures via architecture-specific feature\nenhancement, achieving up to 13% performance improvement, (2) comprehensive\nexplainable AI integration providing multi-perspective diagnostic\ninterpretability, and (3) a clinically deployable ensemble system combining CNN\nreliability with transformer global context modeling. The combination of\ntransformer models with suitable feature enhancements enables them to achieve\nequal or better results than CNN approaches. ViT achieves 98.3% accuracy\nalongside AHE while Swin Transformer gains a 13.0% advantage through HOG\nenhancements"
                },
                "authors": [
                    {
                        "name": "Ojonugwa Oluwafemi Ejiga Peter"
                    },
                    {
                        "name": "Daniel Emakporuena"
                    },
                    {
                        "name": "Bamidele Dayo Tunde"
                    },
                    {
                        "name": "Maryam Abdulkarim"
                    },
                    {
                        "name": "Abdullahi Bn Umar"
                    }
                ],
                "author_detail": {
                    "name": "Abdullahi Bn Umar"
                },
                "author": "Abdullahi Bn Umar",
                "arxiv_doi": "10.11648/j.ajcst.20250802.16",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.11648/j.ajcst.20250802.16",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.06137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proc. SPIE 13410, Medical Imaging 2025: Clinical and Biomedical\n  Imaging, 1341024 (2 April 2025)",
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06135v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06135v1",
                "updated": "2025-08-08T08:55:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    55,
                    53,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T08:55:53Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    55,
                    53,
                    4,
                    220,
                    0
                ],
                "title": "Less is More: Selective Reflection for Compatible and Efficient\n  Knowledge Distillation in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less is More: Selective Reflection for Compatible and Efficient\n  Knowledge Distillation in Large Language Models"
                },
                "summary": "Knowledge Distillation (KD) is a fundamental technique for compressing large\nlanguage models (LLMs) into compact, efficient student models. However,\nexisting white-box KD methods mainly focus on balancing ground truth and\nstudent-generated responses while overlooking two critical factors: training\ndata quality and student-model compatibility. To address these limitations, we\npropose Selective Reflection Distillation (SRD), a novel data curation\nframework that leverages reflections from student models to systematically\nrefine training data. SRD dynamically evaluates and selects prompt-response\npairs by comparing ground truth data with student model outputs, selectively\ncurating high-quality, student-compatible training instances through automated\nranking based on difficulty. Furthermore, after selecting the training data, a\ncurriculum scheduling strategy is employed to incrementally introduce these\ncurated subsets into the distillation process at fixed intervals. As a\nplug-and-play enhancement, SRD consistently improves distillation outcomes\nacross diverse white-box KD approaches and model architectures, as well as\ndecreases computational cost significantly during KD training. Experiments on a\nrange of language model benchmarks demonstrate SRD's consistent improvements in\ndistilled model performance, as well as a reduction in training runtime by up\nto 39%, under diverse KD methods and model families. Notably, SRD operates as a\nplug-and-play module, enhancing sample efficiency without modifying underlying\nKD algorithms. Our findings highlight that data quality and compatibility are\npivotal to effective and efficient distillation of LLMs, and SRD provides a\nprincipled framework to achieve both. This work advances the understanding of\ndata-centric factors in KD and offers practical insights for enhancing the\ncapability and efficiency of compressed LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Distillation (KD) is a fundamental technique for compressing large\nlanguage models (LLMs) into compact, efficient student models. However,\nexisting white-box KD methods mainly focus on balancing ground truth and\nstudent-generated responses while overlooking two critical factors: training\ndata quality and student-model compatibility. To address these limitations, we\npropose Selective Reflection Distillation (SRD), a novel data curation\nframework that leverages reflections from student models to systematically\nrefine training data. SRD dynamically evaluates and selects prompt-response\npairs by comparing ground truth data with student model outputs, selectively\ncurating high-quality, student-compatible training instances through automated\nranking based on difficulty. Furthermore, after selecting the training data, a\ncurriculum scheduling strategy is employed to incrementally introduce these\ncurated subsets into the distillation process at fixed intervals. As a\nplug-and-play enhancement, SRD consistently improves distillation outcomes\nacross diverse white-box KD approaches and model architectures, as well as\ndecreases computational cost significantly during KD training. Experiments on a\nrange of language model benchmarks demonstrate SRD's consistent improvements in\ndistilled model performance, as well as a reduction in training runtime by up\nto 39%, under diverse KD methods and model families. Notably, SRD operates as a\nplug-and-play module, enhancing sample efficiency without modifying underlying\nKD algorithms. Our findings highlight that data quality and compatibility are\npivotal to effective and efficient distillation of LLMs, and SRD provides a\nprincipled framework to achieve both. This work advances the understanding of\ndata-centric factors in KD and offers practical insights for enhancing the\ncapability and efficiency of compressed LLMs."
                },
                "authors": [
                    {
                        "name": "Lingyuan Liu"
                    },
                    {
                        "name": "Mengxiang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mengxiang Zhang"
                },
                "author": "Mengxiang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06135v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06135v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06133v1",
                "updated": "2025-08-08T08:54:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    54,
                    21,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T08:54:21Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    54,
                    21,
                    4,
                    220,
                    0
                ],
                "title": "LLM Serving Optimization with Variable Prefill and Decode Lengths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Serving Optimization with Variable Prefill and Decode Lengths"
                },
                "summary": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency."
                },
                "authors": [
                    {
                        "name": "Meixuan Wang"
                    },
                    {
                        "name": "Yinyu Ye"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06131v1",
                "updated": "2025-08-08T08:51:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    51,
                    1,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T08:51:01Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    51,
                    1,
                    4,
                    220,
                    0
                ],
                "title": "Enhancing the Scalability of Classical Surrogates for Real-World Quantum\n  Machine Learning Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing the Scalability of Classical Surrogates for Real-World Quantum\n  Machine Learning Applications"
                },
                "summary": "Quantum machine learning (QML) presents potential for early industrial\nadoption, yet limited access to quantum hardware remains a significant\nbottleneck for deployment of QML solutions. This work explores the use of\nclassical surrogates to bypass this restriction, which is a technique that\nallows to build a lightweight classical representation of a (trained) quantum\nmodel, enabling to perform inference on entirely classical devices. We reveal\nprohibiting high computational demand associated with previously proposed\nmethods for generating classical surrogates from quantum models, and propose an\nalternative pipeline enabling generation of classical surrogates at a larger\nscale than was previously possible. Previous methods required at least a\nhigh-performance computing (HPC) system for quantum models of below industrial\nscale (ca. 20 qubits), which raises questions about its practicality. We\ngreatly minimize the redundancies of the previous approach, utilizing only a\nminute fraction of the resources previously needed. We demonstrate the\neffectiveness of our method on a real-world energy demand forecasting problem,\nconducting rigorous testing of performance and computation demand in both\nsimulations and on quantum hardware. Our results indicate that our method\nachieves high accuracy on the testing dataset while its computational resource\nrequirements scale linearly rather than exponentially. This work presents a\nlightweight approach to transform quantum solutions into classically deployable\nversions, facilitating faster integration of quantum technology in industrial\nsettings. Furthermore, it can serve as a powerful research tool in search\npractical quantum advantage in an empirical setup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum machine learning (QML) presents potential for early industrial\nadoption, yet limited access to quantum hardware remains a significant\nbottleneck for deployment of QML solutions. This work explores the use of\nclassical surrogates to bypass this restriction, which is a technique that\nallows to build a lightweight classical representation of a (trained) quantum\nmodel, enabling to perform inference on entirely classical devices. We reveal\nprohibiting high computational demand associated with previously proposed\nmethods for generating classical surrogates from quantum models, and propose an\nalternative pipeline enabling generation of classical surrogates at a larger\nscale than was previously possible. Previous methods required at least a\nhigh-performance computing (HPC) system for quantum models of below industrial\nscale (ca. 20 qubits), which raises questions about its practicality. We\ngreatly minimize the redundancies of the previous approach, utilizing only a\nminute fraction of the resources previously needed. We demonstrate the\neffectiveness of our method on a real-world energy demand forecasting problem,\nconducting rigorous testing of performance and computation demand in both\nsimulations and on quantum hardware. Our results indicate that our method\nachieves high accuracy on the testing dataset while its computational resource\nrequirements scale linearly rather than exponentially. This work presents a\nlightweight approach to transform quantum solutions into classically deployable\nversions, facilitating faster integration of quantum technology in industrial\nsettings. Furthermore, it can serve as a powerful research tool in search\npractical quantum advantage in an empirical setup."
                },
                "authors": [
                    {
                        "name": "Philip Anton Hernicht"
                    },
                    {
                        "name": "Alona Sakhnenko"
                    },
                    {
                        "name": "Corey O'Meara"
                    },
                    {
                        "name": "Giorgio Cortiana"
                    },
                    {
                        "name": "Jeanette Miriam Lorenz"
                    }
                ],
                "author_detail": {
                    "name": "Jeanette Miriam Lorenz"
                },
                "author": "Jeanette Miriam Lorenz",
                "arxiv_comment": "9 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06124v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06124v1",
                "updated": "2025-08-08T08:43:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    43,
                    24,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T08:43:24Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    43,
                    24,
                    4,
                    220,
                    0
                ],
                "title": "AURA: Affordance-Understanding and Risk-aware Alignment Technique for\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AURA: Affordance-Understanding and Risk-aware Alignment Technique for\n  Large Language Models"
                },
                "summary": "Present day LLMs face the challenge of managing affordance-based safety\nrisks-situations where outputs inadvertently facilitate harmful actions due to\noverlooked logical implications. Traditional safety solutions, such as scalar\noutcome-based reward models, parameter tuning, or heuristic decoding\nstrategies, lack the granularity and proactive nature needed to reliably detect\nand intervene during subtle yet crucial reasoning steps. Addressing this\nfundamental gap, we introduce AURA, an innovative, multi-layered framework\ncentered around Process Reward Models (PRMs), providing comprehensive, step\nlevel evaluations across logical coherence and safety-awareness. Our framework\nseamlessly combines introspective self-critique, fine-grained PRM assessments,\nand adaptive safety-aware decoding to dynamically and proactively guide models\ntoward safer reasoning trajectories. Empirical evidence clearly demonstrates\nthat this approach significantly surpasses existing methods, significantly\nimproving the logical integrity and affordance-sensitive safety of model\noutputs. This research represents a pivotal step toward safer, more\nresponsible, and contextually aware AI, setting a new benchmark for\nalignment-sensitive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Present day LLMs face the challenge of managing affordance-based safety\nrisks-situations where outputs inadvertently facilitate harmful actions due to\noverlooked logical implications. Traditional safety solutions, such as scalar\noutcome-based reward models, parameter tuning, or heuristic decoding\nstrategies, lack the granularity and proactive nature needed to reliably detect\nand intervene during subtle yet crucial reasoning steps. Addressing this\nfundamental gap, we introduce AURA, an innovative, multi-layered framework\ncentered around Process Reward Models (PRMs), providing comprehensive, step\nlevel evaluations across logical coherence and safety-awareness. Our framework\nseamlessly combines introspective self-critique, fine-grained PRM assessments,\nand adaptive safety-aware decoding to dynamically and proactively guide models\ntoward safer reasoning trajectories. Empirical evidence clearly demonstrates\nthat this approach significantly surpasses existing methods, significantly\nimproving the logical integrity and affordance-sensitive safety of model\noutputs. This research represents a pivotal step toward safer, more\nresponsible, and contextually aware AI, setting a new benchmark for\nalignment-sensitive applications."
                },
                "authors": [
                    {
                        "name": "Sayantan Adak"
                    },
                    {
                        "name": "Pratyush Chatterjee"
                    },
                    {
                        "name": "Somnath Banerjee"
                    },
                    {
                        "name": "Rima Hazra"
                    },
                    {
                        "name": "Somak Aditya"
                    },
                    {
                        "name": "Animesh Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Animesh Mukherjee"
                },
                "author": "Animesh Mukherjee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06124v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06124v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]