[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.19341v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19341v2",
                "updated": "2025-10-06T13:23:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    13,
                    23,
                    4,
                    0,
                    279,
                    0
                ],
                "published": "2025-09-16T09:14:15Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    14,
                    15,
                    1,
                    259,
                    0
                ],
                "title": "Fine-Grained AI Model Caching and Downloading With Coordinated\n  Multipoint Broadcasting in Multi-Cell Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained AI Model Caching and Downloading With Coordinated\n  Multipoint Broadcasting in Multi-Cell Edge Networks"
                },
                "summary": "6G networks are envisioned to support on-demand AI model downloading to\naccommodate diverse inference requirements of end users. By proactively caching\nmodels at edge nodes, users can retrieve the requested models with low latency\nfor on-device AI inference. However, the substantial size of contemporary AI\nmodels poses significant challenges for edge caching under limited storage\ncapacity, as well as for the concurrent delivery of heterogeneous models over\nwireless channels. To address these challenges, we propose a fine-grained AI\nmodel caching and downloading system that exploits parameter reusability,\nstemming from the common practice of fine-tuning task-specific models from a\nshared pre-trained model with frozen parameters. This system selectively caches\nmodel parameter blocks (PBs) at edge nodes, eliminating redundant storage of\nreusable parameters across different cached models. Additionally, it\nincorporates coordinated multipoint (CoMP) broadcasting to simultaneously\ndeliver reusable PBs to multiple users, thereby enhancing downlink spectrum\nutilization. Under this arrangement, we formulate a model downloading delay\nminimization problem to jointly optimize PB caching, migration (among edge\nnodes), and broadcasting beamforming. To tackle this intractable problem, we\ndevelop a distributed multi-agent learning framework that enables edge nodes to\nexplicitly learn mutual influence among their actions, thereby facilitating\ncooperation. Furthermore, a data augmentation approach is proposed to\nadaptively generate synthetic training samples through a predictive model,\nboosting sample efficiency and accelerating policy learning. Both theoretical\nanalysis and simulation experiments validate the superior convergence\nperformance of the proposed learning framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6G networks are envisioned to support on-demand AI model downloading to\naccommodate diverse inference requirements of end users. By proactively caching\nmodels at edge nodes, users can retrieve the requested models with low latency\nfor on-device AI inference. However, the substantial size of contemporary AI\nmodels poses significant challenges for edge caching under limited storage\ncapacity, as well as for the concurrent delivery of heterogeneous models over\nwireless channels. To address these challenges, we propose a fine-grained AI\nmodel caching and downloading system that exploits parameter reusability,\nstemming from the common practice of fine-tuning task-specific models from a\nshared pre-trained model with frozen parameters. This system selectively caches\nmodel parameter blocks (PBs) at edge nodes, eliminating redundant storage of\nreusable parameters across different cached models. Additionally, it\nincorporates coordinated multipoint (CoMP) broadcasting to simultaneously\ndeliver reusable PBs to multiple users, thereby enhancing downlink spectrum\nutilization. Under this arrangement, we formulate a model downloading delay\nminimization problem to jointly optimize PB caching, migration (among edge\nnodes), and broadcasting beamforming. To tackle this intractable problem, we\ndevelop a distributed multi-agent learning framework that enables edge nodes to\nexplicitly learn mutual influence among their actions, thereby facilitating\ncooperation. Furthermore, a data augmentation approach is proposed to\nadaptively generate synthetic training samples through a predictive model,\nboosting sample efficiency and accelerating policy learning. Both theoretical\nanalysis and simulation experiments validate the superior convergence\nperformance of the proposed learning framework."
                },
                "authors": [
                    {
                        "name": "Yang Fu"
                    },
                    {
                        "name": "Peng Qin"
                    },
                    {
                        "name": "Yueyue Zhang"
                    },
                    {
                        "name": "Pao Cheng"
                    },
                    {
                        "name": "Jun Lu"
                    },
                    {
                        "name": "Yifei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yifei Wang"
                },
                "author": "Yifei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19341v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19341v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04646v1",
                "updated": "2025-10-06T09:49:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    9,
                    49,
                    14,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T09:49:14Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    9,
                    49,
                    14,
                    0,
                    279,
                    0
                ],
                "title": "Predictive Feature Caching for Training-free Acceleration of Molecular\n  Geometry Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictive Feature Caching for Training-free Acceleration of Molecular\n  Geometry Generation"
                },
                "summary": "Flow matching models generate high-fidelity molecular geometries but incur\nsignificant computational costs during inference, requiring hundreds of network\nevaluations. This inference overhead becomes the primary bottleneck when such\nmodels are employed in practice to sample large numbers of molecular\ncandidates. This work discusses a training-free caching strategy that\naccelerates molecular geometry generation by predicting intermediate hidden\nstates across solver steps. The proposed method operates directly on the\nSE(3)-equivariant backbone, is compatible with pretrained models, and is\northogonal to existing training-based accelerations and system-level\noptimizations. Experiments on the GEOM-Drugs dataset demonstrate that caching\nachieves a twofold reduction in wall-clock inference time at matched sample\nquality and a speedup of up to 3x compared to the base model with minimal\nsample quality degradation. Because these gains compound with other\noptimizations, applying caching alongside other general, lossless optimizations\nyield as much as a 7x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow matching models generate high-fidelity molecular geometries but incur\nsignificant computational costs during inference, requiring hundreds of network\nevaluations. This inference overhead becomes the primary bottleneck when such\nmodels are employed in practice to sample large numbers of molecular\ncandidates. This work discusses a training-free caching strategy that\naccelerates molecular geometry generation by predicting intermediate hidden\nstates across solver steps. The proposed method operates directly on the\nSE(3)-equivariant backbone, is compatible with pretrained models, and is\northogonal to existing training-based accelerations and system-level\noptimizations. Experiments on the GEOM-Drugs dataset demonstrate that caching\nachieves a twofold reduction in wall-clock inference time at matched sample\nquality and a speedup of up to 3x compared to the base model with minimal\nsample quality degradation. Because these gains compound with other\noptimizations, applying caching alongside other general, lossless optimizations\nyield as much as a 7x speedup."
                },
                "authors": [
                    {
                        "name": "Johanna Sommer"
                    },
                    {
                        "name": "John Rachwan"
                    },
                    {
                        "name": "Nils Fleischmann"
                    },
                    {
                        "name": "Stephan GÃ¼nnemann"
                    },
                    {
                        "name": "Bertrand Charpentier"
                    }
                ],
                "author_detail": {
                    "name": "Bertrand Charpentier"
                },
                "author": "Bertrand Charpentier",
                "arxiv_comment": "Accepted at the AI for Science Workshop @ NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04525v1",
                "updated": "2025-10-06T06:30:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    6,
                    30,
                    22,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T06:30:22Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    6,
                    30,
                    22,
                    0,
                    279,
                    0
                ],
                "title": "Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in\n  Masked Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in\n  Masked Diffusion"
                },
                "summary": "Masked diffusion models have shown promising performance in generating\nhigh-quality samples in a wide range of domains, but accelerating their\nsampling process remains relatively underexplored. To investigate efficient\nsamplers for masked diffusion, this paper theoretically analyzes the MaskGIT\nsampler for image modeling, revealing its implicit temperature sampling\nmechanism. Through this analysis, we introduce the \"moment sampler,\" an\nasymptotically equivalent but more tractable and interpretable alternative to\nMaskGIT, which employs a \"choose-then-sample\" approach by selecting unmasking\npositions before sampling tokens. In addition, we improve the efficiency of\nchoose-then-sample algorithms through two key innovations: a partial caching\ntechnique for transformers that approximates longer sampling trajectories\nwithout proportional computational cost, and a hybrid approach formalizing the\nexploration-exploitation trade-off in adaptive unmasking. Experiments in image\nand text domains demonstrate our theory as well as the efficiency of our\nproposed methods, advancing both theoretical understanding and practical\nimplementation of masked diffusion samplers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked diffusion models have shown promising performance in generating\nhigh-quality samples in a wide range of domains, but accelerating their\nsampling process remains relatively underexplored. To investigate efficient\nsamplers for masked diffusion, this paper theoretically analyzes the MaskGIT\nsampler for image modeling, revealing its implicit temperature sampling\nmechanism. Through this analysis, we introduce the \"moment sampler,\" an\nasymptotically equivalent but more tractable and interpretable alternative to\nMaskGIT, which employs a \"choose-then-sample\" approach by selecting unmasking\npositions before sampling tokens. In addition, we improve the efficiency of\nchoose-then-sample algorithms through two key innovations: a partial caching\ntechnique for transformers that approximates longer sampling trajectories\nwithout proportional computational cost, and a hybrid approach formalizing the\nexploration-exploitation trade-off in adaptive unmasking. Experiments in image\nand text domains demonstrate our theory as well as the efficiency of our\nproposed methods, advancing both theoretical understanding and practical\nimplementation of masked diffusion samplers."
                },
                "authors": [
                    {
                        "name": "Satoshi Hayakawa"
                    },
                    {
                        "name": "Yuhta Takida"
                    },
                    {
                        "name": "Masaaki Imaizumi"
                    },
                    {
                        "name": "Hiromi Wakaki"
                    },
                    {
                        "name": "Yuki Mitsufuji"
                    }
                ],
                "author_detail": {
                    "name": "Yuki Mitsufuji"
                },
                "author": "Yuki Mitsufuji",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04492v1",
                "updated": "2025-10-06T05:04:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    5,
                    4,
                    57,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T05:04:57Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    5,
                    4,
                    57,
                    0,
                    279,
                    0
                ],
                "title": "Joint Probing and Scheduling for Cache-Aided Hybrid\n  Satellite-Terrestrial Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Probing and Scheduling for Cache-Aided Hybrid\n  Satellite-Terrestrial Networks"
                },
                "summary": "Caching is crucial in hybrid satellite-terrestrial networks to reduce\nlatency, optimize throughput, and improve data availability by storing\nfrequently accessed content closer to users, especially in bandwidth-limited\nsatellite systems, requiring strategic Medium Access Control (MAC) layer. This\npaper addresses throughput optimization in satellite-terrestrial integrated\nnetworks through opportunistic cooperative caching. We propose a joint probing\nand scheduling strategy to enhance content retrieval efficiency. The strategy\nleverages the LEO satellite to probe satellite-to-ground links and cache states\nof multiple cooperative terrestrial stations, enabling dynamic user scheduling\nfor content delivery. Using an optimal stopping theoretic approach with two\nlevels of incomplete information, we make real-time decisions on\nsatellite-terrestrial hybrid links and caching probing. Our threshold-based\nstrategy optimizes probing and scheduling, significantly improving average\nsystem throughput by exploiting cooperative caching, satellite-terrestrial link\ntransmission, and time diversity from dynamic user requests. Simulation results\nvalidate the effectiveness and practicality of the proposed strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching is crucial in hybrid satellite-terrestrial networks to reduce\nlatency, optimize throughput, and improve data availability by storing\nfrequently accessed content closer to users, especially in bandwidth-limited\nsatellite systems, requiring strategic Medium Access Control (MAC) layer. This\npaper addresses throughput optimization in satellite-terrestrial integrated\nnetworks through opportunistic cooperative caching. We propose a joint probing\nand scheduling strategy to enhance content retrieval efficiency. The strategy\nleverages the LEO satellite to probe satellite-to-ground links and cache states\nof multiple cooperative terrestrial stations, enabling dynamic user scheduling\nfor content delivery. Using an optimal stopping theoretic approach with two\nlevels of incomplete information, we make real-time decisions on\nsatellite-terrestrial hybrid links and caching probing. Our threshold-based\nstrategy optimizes probing and scheduling, significantly improving average\nsystem throughput by exploiting cooperative caching, satellite-terrestrial link\ntransmission, and time diversity from dynamic user requests. Simulation results\nvalidate the effectiveness and practicality of the proposed strategies."
                },
                "authors": [
                    {
                        "name": "Zhou Zhang"
                    },
                    {
                        "name": "Yizhu Wang"
                    },
                    {
                        "name": "Saman Atapattu"
                    },
                    {
                        "name": "Sumei Sun"
                    }
                ],
                "author_detail": {
                    "name": "Sumei Sun"
                },
                "author": "Sumei Sun",
                "arxiv_comment": "6 pages, IEEE Global Communications Conference (GLOBECOM), December\n  2025, Taipei, Taiwan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07120v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07120v3",
                "updated": "2025-10-06T04:28:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    4,
                    28,
                    5,
                    0,
                    279,
                    0
                ],
                "published": "2025-03-10T09:49:18Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    49,
                    18,
                    0,
                    69,
                    0
                ],
                "title": "FEB-Cache: Frequency-Guided Exposure Bias Reduction for Enhancing\n  Diffusion Transformer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FEB-Cache: Frequency-Guided Exposure Bias Reduction for Enhancing\n  Diffusion Transformer Caching"
                },
                "summary": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis issue, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing why caching damage the generation processes. In this paper, we first\nconfirm that the cache greatly amplifies the exposure bias, resulting in a\ndecline in the generation quality. However, directly applying noise scaling is\nchallenging for this issue due to the non-smoothness of exposure bias. We found\nthat this phenomenon stems from the mismatch between its frequency response\ncharacteristics and the simple cache of Attention and MLP. Since these two\ncomponents exhibit unique preferences for frequency signals, which provides us\nwith a caching strategy to separate Attention and MLP to achieve an enhanced\nfit of exposure bias and reduce it. Based on this, we introduced FEB-Cache, a\njoint caching strategy that aligns with the non-exposed bias diffusion process\n(which gives us a higher performance cap) of caching Attention and MLP based on\nthe frequency-guided cache table. Our approach combines a comprehensive\nunderstanding of the caching mechanism and offers a new perspective on\nleveraging caching to accelerate the diffusion process. Empirical results\nindicate that FEB-Cache optimizes model performance while concurrently\nfacilitating acceleration. Code is available at\nhttps://github.com/aSleepyTree/EB-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis issue, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing why caching damage the generation processes. In this paper, we first\nconfirm that the cache greatly amplifies the exposure bias, resulting in a\ndecline in the generation quality. However, directly applying noise scaling is\nchallenging for this issue due to the non-smoothness of exposure bias. We found\nthat this phenomenon stems from the mismatch between its frequency response\ncharacteristics and the simple cache of Attention and MLP. Since these two\ncomponents exhibit unique preferences for frequency signals, which provides us\nwith a caching strategy to separate Attention and MLP to achieve an enhanced\nfit of exposure bias and reduce it. Based on this, we introduced FEB-Cache, a\njoint caching strategy that aligns with the non-exposed bias diffusion process\n(which gives us a higher performance cap) of caching Attention and MLP based on\nthe frequency-guided cache table. Our approach combines a comprehensive\nunderstanding of the caching mechanism and offers a new perspective on\nleveraging caching to accelerate the diffusion process. Empirical results\nindicate that FEB-Cache optimizes model performance while concurrently\nfacilitating acceleration. Code is available at\nhttps://github.com/aSleepyTree/EB-Cache."
                },
                "authors": [
                    {
                        "name": "Zhen Zou"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07120v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07120v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04476v1",
                "updated": "2025-10-06T04:24:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    4,
                    24,
                    23,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T04:24:23Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    4,
                    24,
                    23,
                    0,
                    279,
                    0
                ],
                "title": "Compressed Convolutional Attention: Efficient Attention in a Compressed\n  Latent Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressed Convolutional Attention: Efficient Attention in a Compressed\n  Latent Space"
                },
                "summary": "Multi-headed Attention's (MHA) quadratic compute and linearly growing\nKV-cache make long-context transformers expensive to train and serve. Prior\nworks such as Grouped Query Attention (GQA) and Multi-Latent Attention (MLA)\nshrink the cache, speeding decode, but leave compute, which determines prefill\nand training speed, largely unchanged. We introduce Compressed Convolutional\nAttention (CCA), a novel attention method which down-projects queries, keys,\nand values and performs the entire attention operation inside the shared latent\nspace. This simple design dramatically cuts parameters, KV-cache, and FLOPs all\nat once by the desired compression factor. Because CCA is orthogonal to\nhead-sharing, we combine the two to form Compressed Convolutional Grouped Query\nAttention (CCGQA), which further tightens the compute-bandwidth Pareto frontier\nso that users can tune compression toward either FLOP or memory limits without\nsacrificing quality. Experiments show that CCGQA consistently outperforms both\nGQA and MLA at equal KV-cache compression on dense and MoE models.\nAdditionally, we show that CCGQA outperforms all other attention methods on MoE\nmodels with half the KV-cache of GQA and MLA, achieving an 8x KV-cache\ncompression with no drop in performance compared to standard MHA. CCA and CCGQA\nalso dramatically reduce the FLOP cost of attention which leads to\nsubstantially faster training and prefill than existing methods. On H100 GPUs,\nour fused CCA/CCGQA kernel reduces prefill latency by about 1.7x at a sequence\nlength of 16k relative to MHA, and accelerates backward by about 1.3x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-headed Attention's (MHA) quadratic compute and linearly growing\nKV-cache make long-context transformers expensive to train and serve. Prior\nworks such as Grouped Query Attention (GQA) and Multi-Latent Attention (MLA)\nshrink the cache, speeding decode, but leave compute, which determines prefill\nand training speed, largely unchanged. We introduce Compressed Convolutional\nAttention (CCA), a novel attention method which down-projects queries, keys,\nand values and performs the entire attention operation inside the shared latent\nspace. This simple design dramatically cuts parameters, KV-cache, and FLOPs all\nat once by the desired compression factor. Because CCA is orthogonal to\nhead-sharing, we combine the two to form Compressed Convolutional Grouped Query\nAttention (CCGQA), which further tightens the compute-bandwidth Pareto frontier\nso that users can tune compression toward either FLOP or memory limits without\nsacrificing quality. Experiments show that CCGQA consistently outperforms both\nGQA and MLA at equal KV-cache compression on dense and MoE models.\nAdditionally, we show that CCGQA outperforms all other attention methods on MoE\nmodels with half the KV-cache of GQA and MLA, achieving an 8x KV-cache\ncompression with no drop in performance compared to standard MHA. CCA and CCGQA\nalso dramatically reduce the FLOP cost of attention which leads to\nsubstantially faster training and prefill than existing methods. On H100 GPUs,\nour fused CCA/CCGQA kernel reduces prefill latency by about 1.7x at a sequence\nlength of 16k relative to MHA, and accelerates backward by about 1.3x."
                },
                "authors": [
                    {
                        "name": "Tomas Figliolia"
                    },
                    {
                        "name": "Nicholas Alonso"
                    },
                    {
                        "name": "Rishi Iyer"
                    },
                    {
                        "name": "Quentin Anthony"
                    },
                    {
                        "name": "Beren Millidge"
                    }
                ],
                "author_detail": {
                    "name": "Beren Millidge"
                },
                "author": "Beren Millidge",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.18149v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.18149v2",
                "updated": "2025-10-06T02:46:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    2,
                    46,
                    1,
                    0,
                    279,
                    0
                ],
                "published": "2024-03-26T23:17:05Z",
                "published_parsed": [
                    2024,
                    3,
                    26,
                    23,
                    17,
                    5,
                    1,
                    86,
                    0
                ],
                "title": "Code Generation and Conic Constraints for Model-Predictive Control on\n  Microcontrollers with Conic-TinyMPC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Generation and Conic Constraints for Model-Predictive Control on\n  Microcontrollers with Conic-TinyMPC"
                },
                "summary": "Model-predictive control (MPC) is a powerful framework for controlling\ndynamic systems under constraints, but it remains challenging to deploy on\nresource-constrained platforms, especially for problems involving conic\nconstraints. To address this, we extend recent work developing fast,\nstructure-exploiting, cached ADMM solvers for embedded applications, to provide\nsupport for second-order cones, as well as C++ code generation from Python,\nMATLAB, and Julia for easy deployment. Microcontroller benchmarks show that our\nsolver provides up to a two-order-of-magnitude speedup, ranging from 10.6x to\n142.7x, over state-of-the-art embedded solvers on QP and SOCP problems, and\nenables us to fit order-of-magnitude larger problems in memory. We validate our\nsolver's deployed performance through simulation and hardware experiments,\nincluding conically-constrained trajectory tracking on a 27g Crazyflie\nquadrotor. To get started with Conic-TinyMPC, visit our documentation,\nexamples, and the open-source codebase at https://tinympc.org.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-predictive control (MPC) is a powerful framework for controlling\ndynamic systems under constraints, but it remains challenging to deploy on\nresource-constrained platforms, especially for problems involving conic\nconstraints. To address this, we extend recent work developing fast,\nstructure-exploiting, cached ADMM solvers for embedded applications, to provide\nsupport for second-order cones, as well as C++ code generation from Python,\nMATLAB, and Julia for easy deployment. Microcontroller benchmarks show that our\nsolver provides up to a two-order-of-magnitude speedup, ranging from 10.6x to\n142.7x, over state-of-the-art embedded solvers on QP and SOCP problems, and\nenables us to fit order-of-magnitude larger problems in memory. We validate our\nsolver's deployed performance through simulation and hardware experiments,\nincluding conically-constrained trajectory tracking on a 27g Crazyflie\nquadrotor. To get started with Conic-TinyMPC, visit our documentation,\nexamples, and the open-source codebase at https://tinympc.org."
                },
                "authors": [
                    {
                        "name": "Ishaan Mahajan"
                    },
                    {
                        "name": "Khai Nguyen"
                    },
                    {
                        "name": "Sam Schoedel"
                    },
                    {
                        "name": "Elakhya Nedumaran"
                    },
                    {
                        "name": "Moises Mata"
                    },
                    {
                        "name": "Brian Plancher"
                    },
                    {
                        "name": "Zachary Manchester"
                    }
                ],
                "author_detail": {
                    "name": "Zachary Manchester"
                },
                "author": "Zachary Manchester",
                "arxiv_comment": "First three authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.18149v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.18149v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v6",
                "updated": "2025-10-05T22:17:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    22,
                    17,
                    34,
                    6,
                    278,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Cache Selection for LLMs via Attention Output Error-Based\n  Token Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Cache Selection for LLMs via Attention Output Error-Based\n  Token Eviction"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value tokens on top of\nattention-based eviction scores in closed-form. Additionally, CAOTE can act as\na meta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value tokens on top of\nattention-based eviction scores in closed-form. Additionally, CAOTE can act as\na meta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "15 pages, 3 figures, 13 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00384v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00384v2",
                "updated": "2025-10-05T21:29:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    21,
                    29,
                    28,
                    6,
                    278,
                    0
                ],
                "published": "2025-05-31T04:27:22Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    4,
                    27,
                    22,
                    5,
                    151,
                    0
                ],
                "title": "Learning Semantics, Not Addresses: Runtime Neural Prefetching for Far\n  Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Semantics, Not Addresses: Runtime Neural Prefetching for Far\n  Memory"
                },
                "summary": "Memory prefetching has long boosted CPU caches and is increasingly vital for\nfar-memory systems, where large portions of memory are offloaded to cheaper,\nremote tiers. While effective prefetching requires accurate prediction of\nfuture accesses, prior ML approaches have been limited to simulation or\nsmall-scale hardware. We introduce FarSight, the first Linux-based far-memory\nsystem to leverage deep learning by decoupling application semantics from\nruntime memory layout. This separation enables offline-trained models to\npredict access patterns over a compact ordinal vocabulary, which are resolved\nat runtime through lightweight mappings. Across four data-intensive workloads,\nFarSight delivers up to 3.6x higher performance than the state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory prefetching has long boosted CPU caches and is increasingly vital for\nfar-memory systems, where large portions of memory are offloaded to cheaper,\nremote tiers. While effective prefetching requires accurate prediction of\nfuture accesses, prior ML approaches have been limited to simulation or\nsmall-scale hardware. We introduce FarSight, the first Linux-based far-memory\nsystem to leverage deep learning by decoupling application semantics from\nruntime memory layout. This separation enables offline-trained models to\npredict access patterns over a compact ordinal vocabulary, which are resolved\nat runtime through lightweight mappings. Across four data-intensive workloads,\nFarSight delivers up to 3.6x higher performance than the state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Yutong Huang"
                    },
                    {
                        "name": "Zhiyuan Guo"
                    },
                    {
                        "name": "Yiying Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiying Zhang"
                },
                "author": "Yiying Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00384v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00384v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09253v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09253v2",
                "updated": "2025-10-05T18:13:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    18,
                    13,
                    39,
                    6,
                    278,
                    0
                ],
                "published": "2025-01-16T02:40:07Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    2,
                    40,
                    7,
                    3,
                    16,
                    0
                ],
                "title": "PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid\n  Resolution Diffusion Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid\n  Resolution Diffusion Serving"
                },
                "summary": "The Text-to-Image (T2I) diffusion model has emerged as one of the most widely\nadopted generative models. However, serving diffusion models at the granularity\nof entire images introduces significant challenges, particularly under\nmulti-resolution workloads. First, image-level serving obstructs batching\nacross requests. Second, heterogeneous resolutions exhibit distinct locality\ncharacteristics, making it difficult to apply a uniform cache policy\neffectively.\n  To address these challenges, we present PatchedServe, a Patch Management\nFramework for SLO-Optimized Hybrid-Resolution Diffusion Serving. PatchedServe\nis the first SLO-optimized T2I diffusion serving framework designed to handle\nheterogeneous resolutions. Specifically, it incorporates a novel patch-based\nprocessing workflow that substantially improves throughput for\nhybrid-resolution inputs. Moreover, PatchedServe devises a patch-level cache\nreuse policy to fully exploit diffusion redundancies and integrates an\nSLO-aware scheduling algorithm with lightweight online latency prediction to\nimprove responsiveness. Our evaluation demonstrates that PatchedServe achieves\n30.1 % higher SLO satisfaction than the state-of-the-art diffusion serving\nsystem, while preserving image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Text-to-Image (T2I) diffusion model has emerged as one of the most widely\nadopted generative models. However, serving diffusion models at the granularity\nof entire images introduces significant challenges, particularly under\nmulti-resolution workloads. First, image-level serving obstructs batching\nacross requests. Second, heterogeneous resolutions exhibit distinct locality\ncharacteristics, making it difficult to apply a uniform cache policy\neffectively.\n  To address these challenges, we present PatchedServe, a Patch Management\nFramework for SLO-Optimized Hybrid-Resolution Diffusion Serving. PatchedServe\nis the first SLO-optimized T2I diffusion serving framework designed to handle\nheterogeneous resolutions. Specifically, it incorporates a novel patch-based\nprocessing workflow that substantially improves throughput for\nhybrid-resolution inputs. Moreover, PatchedServe devises a patch-level cache\nreuse policy to fully exploit diffusion redundancies and integrates an\nSLO-aware scheduling algorithm with lightweight online latency prediction to\nimprove responsiveness. Our evaluation demonstrates that PatchedServe achieves\n30.1 % higher SLO satisfaction than the state-of-the-art diffusion serving\nsystem, while preserving image quality."
                },
                "authors": [
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Zepeng Zhao"
                    },
                    {
                        "name": "Yuke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuke Wang"
                },
                "author": "Yuke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09253v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09253v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04188v1",
                "updated": "2025-10-05T13:01:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    13,
                    1,
                    8,
                    6,
                    278,
                    0
                ],
                "published": "2025-10-05T13:01:08Z",
                "published_parsed": [
                    2025,
                    10,
                    5,
                    13,
                    1,
                    8,
                    6,
                    278,
                    0
                ],
                "title": "Let Features Decide Their Own Solvers: Hybrid Feature Caching for\n  Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let Features Decide Their Own Solvers: Hybrid Feature Caching for\n  Diffusion Transformers"
                },
                "summary": "Diffusion Transformers offer state-of-the-art fidelity in image and video\nsynthesis, but their iterative sampling process remains a major bottleneck due\nto the high cost of transformer forward passes at each timestep. To mitigate\nthis, feature caching has emerged as a training-free acceleration technique\nthat reuses or forecasts hidden representations. However, existing methods\noften apply a uniform caching strategy across all feature dimensions, ignoring\ntheir heterogeneous dynamic behaviors. Therefore, we adopt a new perspective by\nmodeling hidden feature evolution as a mixture of ODEs across dimensions, and\nintroduce HyCa, a Hybrid ODE solver inspired caching framework that applies\ndimension-wise caching strategies. HyCa achieves near-lossless acceleration\nacross diverse domains and models, including 5.55 times speedup on FLUX, 5.56\ntimes speedup on HunyuanVideo, 6.24 times speedup on Qwen-Image and\nQwen-Image-Edit without retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers offer state-of-the-art fidelity in image and video\nsynthesis, but their iterative sampling process remains a major bottleneck due\nto the high cost of transformer forward passes at each timestep. To mitigate\nthis, feature caching has emerged as a training-free acceleration technique\nthat reuses or forecasts hidden representations. However, existing methods\noften apply a uniform caching strategy across all feature dimensions, ignoring\ntheir heterogeneous dynamic behaviors. Therefore, we adopt a new perspective by\nmodeling hidden feature evolution as a mixture of ODEs across dimensions, and\nintroduce HyCa, a Hybrid ODE solver inspired caching framework that applies\ndimension-wise caching strategies. HyCa achieves near-lossless acceleration\nacross diverse domains and models, including 5.55 times speedup on FLUX, 5.56\ntimes speedup on HunyuanVideo, 6.24 times speedup on Qwen-Image and\nQwen-Image-Edit without retraining."
                },
                "authors": [
                    {
                        "name": "Shikang Zheng"
                    },
                    {
                        "name": "Guantao Chen"
                    },
                    {
                        "name": "Qinming Zhou"
                    },
                    {
                        "name": "Yuqi Lin"
                    },
                    {
                        "name": "Lixuan He"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Peiliang Cai"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04153v1",
                "updated": "2025-10-05T11:09:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    11,
                    9,
                    10,
                    6,
                    278,
                    0
                ],
                "published": "2025-10-05T11:09:10Z",
                "published_parsed": [
                    2025,
                    10,
                    5,
                    11,
                    9,
                    10,
                    6,
                    278,
                    0
                ],
                "title": "ObCLIP: Oblivious CLoud-Device Hybrid Image Generation with Privacy\n  Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ObCLIP: Oblivious CLoud-Device Hybrid Image Generation with Privacy\n  Preservation"
                },
                "summary": "Diffusion Models have gained significant popularity due to their remarkable\ncapabilities in image generation, albeit at the cost of intensive computation\nrequirement. Meanwhile, despite their widespread deployment in inference\nservices such as Midjourney, concerns about the potential leakage of sensitive\ninformation in uploaded user prompts have arisen. Existing solutions either\nlack rigorous privacy guarantees or fail to strike an effective balance between\nutility and efficiency. To bridge this gap, we propose ObCLIP, a plug-and-play\nsafeguard that enables oblivious cloud-device hybrid generation. By oblivious,\neach input prompt is transformed into a set of semantically similar candidate\nprompts that differ only in sensitive attributes (e.g., gender, ethnicity). The\ncloud server processes all candidate prompts without knowing which one is the\nreal one, thus preventing any prompt leakage. To mitigate server cost, only a\nsmall portion of denoising steps is performed upon the large cloud model. The\nintermediate latents are then sent back to the client, which selects the\ntargeted latent and completes the remaining denoising using a small device\nmodel. Additionally, we analyze and incorporate several cache-based\naccelerations that leverage temporal and batch redundancy, effectively reducing\ncomputation cost with minimal utility degradation. Extensive experiments across\nmultiple datasets demonstrate that ObCLIP provides rigorous privacy and\ncomparable utility to cloud models with slightly increased server cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Models have gained significant popularity due to their remarkable\ncapabilities in image generation, albeit at the cost of intensive computation\nrequirement. Meanwhile, despite their widespread deployment in inference\nservices such as Midjourney, concerns about the potential leakage of sensitive\ninformation in uploaded user prompts have arisen. Existing solutions either\nlack rigorous privacy guarantees or fail to strike an effective balance between\nutility and efficiency. To bridge this gap, we propose ObCLIP, a plug-and-play\nsafeguard that enables oblivious cloud-device hybrid generation. By oblivious,\neach input prompt is transformed into a set of semantically similar candidate\nprompts that differ only in sensitive attributes (e.g., gender, ethnicity). The\ncloud server processes all candidate prompts without knowing which one is the\nreal one, thus preventing any prompt leakage. To mitigate server cost, only a\nsmall portion of denoising steps is performed upon the large cloud model. The\nintermediate latents are then sent back to the client, which selects the\ntargeted latent and completes the remaining denoising using a small device\nmodel. Additionally, we analyze and incorporate several cache-based\naccelerations that leverage temporal and batch redundancy, effectively reducing\ncomputation cost with minimal utility degradation. Extensive experiments across\nmultiple datasets demonstrate that ObCLIP provides rigorous privacy and\ncomparable utility to cloud models with slightly increased server cost."
                },
                "authors": [
                    {
                        "name": "Haoqi Wu"
                    },
                    {
                        "name": "Wei Dai"
                    },
                    {
                        "name": "Ming Xu"
                    },
                    {
                        "name": "Li Wang"
                    },
                    {
                        "name": "Qiang Yan"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Yan"
                },
                "author": "Qiang Yan",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10714v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10714v3",
                "updated": "2025-10-05T08:34:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    8,
                    34,
                    30,
                    6,
                    278,
                    0
                ],
                "published": "2025-03-13T03:36:03Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    36,
                    3,
                    3,
                    72,
                    0
                ],
                "title": "ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient\n  Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient\n  Long-Context LLMs"
                },
                "summary": "The linear growth of key-value (KV) cache memory and quadratic computational\nin attention mechanisms complexity pose significant bottlenecks for large\nlanguage models (LLMs) in long-context processing. While existing KV cache\noptimization methods address these challenges through token pruning or feature\nmerging, they often incur irreversible information loss or require costly\nparameter retraining. To this end, we propose ZSMerge, a dynamic KV cache\ncompression framework designed for efficient cache management, featuring three\nkey operations: (1) fine-grained memory allocation guided by multi-dimensional\ntoken importance metrics at head-level granularity, (2) a residual merging\nmechanism that preserves critical context through compensated attention\nscoring, and (3) a zero-shot adaptation mechanism compatible with diverse LLM\narchitectures without requiring retraining. ZSMerge significantly enhances\nmemory efficiency and inference speed with negligible performance degradation\nacross LLMs. When applied to LLaMA2-7B, it demonstrates a 20:1 compression\nratio for key-value cache retention (reducing memory footprint to 5\\% of\nbaseline) while sustaining comparable generation quality, coupled with triple\nthroughput gains at extreme 54k-token contexts that eliminate out-of-memory\nfailures. The code is available at https://github.com/SusCom-Lab/ZSMerge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The linear growth of key-value (KV) cache memory and quadratic computational\nin attention mechanisms complexity pose significant bottlenecks for large\nlanguage models (LLMs) in long-context processing. While existing KV cache\noptimization methods address these challenges through token pruning or feature\nmerging, they often incur irreversible information loss or require costly\nparameter retraining. To this end, we propose ZSMerge, a dynamic KV cache\ncompression framework designed for efficient cache management, featuring three\nkey operations: (1) fine-grained memory allocation guided by multi-dimensional\ntoken importance metrics at head-level granularity, (2) a residual merging\nmechanism that preserves critical context through compensated attention\nscoring, and (3) a zero-shot adaptation mechanism compatible with diverse LLM\narchitectures without requiring retraining. ZSMerge significantly enhances\nmemory efficiency and inference speed with negligible performance degradation\nacross LLMs. When applied to LLaMA2-7B, it demonstrates a 20:1 compression\nratio for key-value cache retention (reducing memory footprint to 5\\% of\nbaseline) while sustaining comparable generation quality, coupled with triple\nthroughput gains at extreme 54k-token contexts that eliminate out-of-memory\nfailures. The code is available at https://github.com/SusCom-Lab/ZSMerge."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Xudong Wang"
                    },
                    {
                        "name": "Pei Liu"
                    },
                    {
                        "name": "Guoming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Guoming Tang"
                },
                "author": "Guoming Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10714v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10714v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04033v1",
                "updated": "2025-10-05T04:52:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    4,
                    52,
                    26,
                    6,
                    278,
                    0
                ],
                "published": "2025-10-05T04:52:26Z",
                "published_parsed": [
                    2025,
                    10,
                    5,
                    4,
                    52,
                    26,
                    6,
                    278,
                    0
                ],
                "title": "A global log for medical AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A global log for medical AI"
                },
                "summary": "Modern computer systems often rely on syslog, a simple, universal protocol\nthat records every critical event across heterogeneous infrastructure. However,\nhealthcare's rapidly growing clinical AI stack has no equivalent. As hospitals\nrush to pilot large language models and other AI-based clinical decision\nsupport tools, we still lack a standard way to record how, when, by whom, and\nfor whom these AI models are used. Without that transparency and visibility, it\nis challenging to measure real-world performance and outcomes, detect adverse\nevents, or correct bias or dataset drift. In the spirit of syslog, we introduce\nMedLog, a protocol for event-level logging of clinical AI. Any time an AI model\nis invoked to interact with a human, interface with another algorithm, or act\nindependently, a MedLog record is created. This record consists of nine core\nfields: header, model, user, target, inputs, artifacts, outputs, outcomes, and\nfeedback, providing a structured and consistent record of model activity. To\nencourage early adoption, especially in low-resource settings, and minimize the\ndata footprint, MedLog supports risk-based sampling, lifecycle-aware retention\npolicies, and write-behind caching; detailed traces for complex, agentic, or\nmulti-stage workflows can also be captured under MedLog. MedLog can catalyze\nthe development of new databases and software to store and analyze MedLog\nrecords. Realizing this vision would enable continuous surveillance, auditing,\nand iterative improvement of medical AI, laying the foundation for a new form\nof digital epidemiology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern computer systems often rely on syslog, a simple, universal protocol\nthat records every critical event across heterogeneous infrastructure. However,\nhealthcare's rapidly growing clinical AI stack has no equivalent. As hospitals\nrush to pilot large language models and other AI-based clinical decision\nsupport tools, we still lack a standard way to record how, when, by whom, and\nfor whom these AI models are used. Without that transparency and visibility, it\nis challenging to measure real-world performance and outcomes, detect adverse\nevents, or correct bias or dataset drift. In the spirit of syslog, we introduce\nMedLog, a protocol for event-level logging of clinical AI. Any time an AI model\nis invoked to interact with a human, interface with another algorithm, or act\nindependently, a MedLog record is created. This record consists of nine core\nfields: header, model, user, target, inputs, artifacts, outputs, outcomes, and\nfeedback, providing a structured and consistent record of model activity. To\nencourage early adoption, especially in low-resource settings, and minimize the\ndata footprint, MedLog supports risk-based sampling, lifecycle-aware retention\npolicies, and write-behind caching; detailed traces for complex, agentic, or\nmulti-stage workflows can also be captured under MedLog. MedLog can catalyze\nthe development of new databases and software to store and analyze MedLog\nrecords. Realizing this vision would enable continuous surveillance, auditing,\nand iterative improvement of medical AI, laying the foundation for a new form\nof digital epidemiology."
                },
                "authors": [
                    {
                        "name": "Ayush Noori"
                    },
                    {
                        "name": "Adam Rodman"
                    },
                    {
                        "name": "Alan Karthikesalingam"
                    },
                    {
                        "name": "Bilal A. Mateen"
                    },
                    {
                        "name": "Christopher A. Longhurst"
                    },
                    {
                        "name": "Daniel Yang"
                    },
                    {
                        "name": "Dave deBronkart"
                    },
                    {
                        "name": "Gauden Galea"
                    },
                    {
                        "name": "Harold F. Wolf III"
                    },
                    {
                        "name": "Jacob Waxman"
                    },
                    {
                        "name": "Joshua C. Mandel"
                    },
                    {
                        "name": "Juliana Rotich"
                    },
                    {
                        "name": "Kenneth D. Mandl"
                    },
                    {
                        "name": "Maryam Mustafa"
                    },
                    {
                        "name": "Melissa Miles"
                    },
                    {
                        "name": "Nigam H. Shah"
                    },
                    {
                        "name": "Peter Lee"
                    },
                    {
                        "name": "Robert Korom"
                    },
                    {
                        "name": "Scott Mahoney"
                    },
                    {
                        "name": "Seth Hain"
                    },
                    {
                        "name": "Tien Yin Wong"
                    },
                    {
                        "name": "Trevor Mundel"
                    },
                    {
                        "name": "Vivek Natarajan"
                    },
                    {
                        "name": "Noa Dagan"
                    },
                    {
                        "name": "David A. Clifton"
                    },
                    {
                        "name": "Ran D. Balicer"
                    },
                    {
                        "name": "Isaac S. Kohane"
                    },
                    {
                        "name": "Marinka Zitnik"
                    }
                ],
                "author_detail": {
                    "name": "Marinka Zitnik"
                },
                "author": "Marinka Zitnik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03851v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03851v1",
                "updated": "2025-10-04T15:52:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    4,
                    15,
                    52,
                    31,
                    5,
                    277,
                    0
                ],
                "published": "2025-10-04T15:52:31Z",
                "published_parsed": [
                    2025,
                    10,
                    4,
                    15,
                    52,
                    31,
                    5,
                    277,
                    0
                ],
                "title": "Algorithm Generation via Creative Ideation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithm Generation via Creative Ideation"
                },
                "summary": "Designing system algorithms remains challenging, where the discontinuous\nnature of the solution space often forces system engineers to rely on generic\nheuristics at the expense of performance. We study whether LLMs can practically\ndrive algorithm generation, and find that they are biased towards well-known\ngeneric designs, rather than making the creative leaps needed to navigate the\ndiscontinuous solution space. To address this limitation, we introduce\nMetaMuse, a framework for creative ideation built on three self-reflection\nprinciples: (1) quantifying solution diversity and usefulness in measurable\nperformance space, rather than abstract idea space, (2) steering ideation\nthrough external stimuli, rather than internal randomness, and (3) constructing\nexecutable solutions using waypoint reasoning, rather than free-form\nchain-of-thought. Extensive evaluation shows that MetaMuse can generate\nhigh-performing solutions for two critical problems at a global cloud provider:\ncache replacement (reducing cache misses by up to 35.76%) and online bin\npacking (reducing bin usage by up to 30.93%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing system algorithms remains challenging, where the discontinuous\nnature of the solution space often forces system engineers to rely on generic\nheuristics at the expense of performance. We study whether LLMs can practically\ndrive algorithm generation, and find that they are biased towards well-known\ngeneric designs, rather than making the creative leaps needed to navigate the\ndiscontinuous solution space. To address this limitation, we introduce\nMetaMuse, a framework for creative ideation built on three self-reflection\nprinciples: (1) quantifying solution diversity and usefulness in measurable\nperformance space, rather than abstract idea space, (2) steering ideation\nthrough external stimuli, rather than internal randomness, and (3) constructing\nexecutable solutions using waypoint reasoning, rather than free-form\nchain-of-thought. Extensive evaluation shows that MetaMuse can generate\nhigh-performing solutions for two critical problems at a global cloud provider:\ncache replacement (reducing cache misses by up to 35.76%) and online bin\npacking (reducing bin usage by up to 30.93%)."
                },
                "authors": [
                    {
                        "name": "Ruiying Ma"
                    },
                    {
                        "name": "Chieh-Jan Mike Liang"
                    },
                    {
                        "name": "Yanjie Gao"
                    },
                    {
                        "name": "Francis Y. Yan"
                    }
                ],
                "author_detail": {
                    "name": "Francis Y. Yan"
                },
                "author": "Francis Y. Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03851v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03851v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03834v1",
                "updated": "2025-10-04T15:25:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    4,
                    15,
                    25,
                    4,
                    5,
                    277,
                    0
                ],
                "published": "2025-10-04T15:25:04Z",
                "published_parsed": [
                    2025,
                    10,
                    4,
                    15,
                    25,
                    4,
                    5,
                    277,
                    0
                ],
                "title": "Hybrid MBE Route to Adsorption-Controlled Growth of BaTiO3 Membranes\n  with Robust Polarization Switching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid MBE Route to Adsorption-Controlled Growth of BaTiO3 Membranes\n  with Robust Polarization Switching"
                },
                "summary": "Freestanding ferroelectric membranes are promising for flexible electronics,\nnonvolatile memory, photonics, and spintronics, but their synthesis is\nchallenged by the need for reproducibility with precise stoichiometric control.\nHere, we demonstrate the adsorption-controlled growth of single-crystalline,\nepitaxial BaTiO3 films by hybrid molecular beam epitaxy (MBE) on a binary oxide\nsacrificial layer. Using a simple water-droplet lift-off method, we obtained\nsubmillimeter- to millimeter-sized membranes that retained crystallinity, as\nconfirmed by high-resolution X-ray diffraction, and exhibited robust tetragonal\nsymmetry by Raman spectroscopy. Impedance spectroscopy confirmed a high\ndielectric constant of 1340, reflecting the robust dielectric response of the\nmembranes. Ferroelectric functionality was revealed by piezoresponse force\nmicroscopy (PFM) and further verified by polarization-electric field (P-E) loop\nmeasurements with Positive-Up-Negative-Down (PUND). The P-E loops exhibited a\nremnant polarization of 5 microC cm-2 and a coercive field of 63 kV cm-1. These\nresults were interpreted in relation to c- and a-domain configurations. These\nresults establish hybrid MBE as a generalizable route for producing\nstoichiometry-controlled ferroelectric membranes, enabling their integration\ninto next-generation flexible and multifunctional quantum oxide devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Freestanding ferroelectric membranes are promising for flexible electronics,\nnonvolatile memory, photonics, and spintronics, but their synthesis is\nchallenged by the need for reproducibility with precise stoichiometric control.\nHere, we demonstrate the adsorption-controlled growth of single-crystalline,\nepitaxial BaTiO3 films by hybrid molecular beam epitaxy (MBE) on a binary oxide\nsacrificial layer. Using a simple water-droplet lift-off method, we obtained\nsubmillimeter- to millimeter-sized membranes that retained crystallinity, as\nconfirmed by high-resolution X-ray diffraction, and exhibited robust tetragonal\nsymmetry by Raman spectroscopy. Impedance spectroscopy confirmed a high\ndielectric constant of 1340, reflecting the robust dielectric response of the\nmembranes. Ferroelectric functionality was revealed by piezoresponse force\nmicroscopy (PFM) and further verified by polarization-electric field (P-E) loop\nmeasurements with Positive-Up-Negative-Down (PUND). The P-E loops exhibited a\nremnant polarization of 5 microC cm-2 and a coercive field of 63 kV cm-1. These\nresults were interpreted in relation to c- and a-domain configurations. These\nresults establish hybrid MBE as a generalizable route for producing\nstoichiometry-controlled ferroelectric membranes, enabling their integration\ninto next-generation flexible and multifunctional quantum oxide devices."
                },
                "authors": [
                    {
                        "name": "S. Choo"
                    },
                    {
                        "name": "S. Varshney"
                    },
                    {
                        "name": "J. Shah"
                    },
                    {
                        "name": "A. K. Manjeshwar"
                    },
                    {
                        "name": "D. K. Lee"
                    },
                    {
                        "name": "K. A. Mkhoyan"
                    },
                    {
                        "name": "R. D. James"
                    },
                    {
                        "name": "B. Jalan"
                    }
                ],
                "author_detail": {
                    "name": "B. Jalan"
                },
                "author": "B. Jalan",
                "arxiv_comment": "22 pages 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03712v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03712v1",
                "updated": "2025-10-04T07:22:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    4,
                    7,
                    22,
                    39,
                    5,
                    277,
                    0
                ],
                "published": "2025-10-04T07:22:39Z",
                "published_parsed": [
                    2025,
                    10,
                    4,
                    7,
                    22,
                    39,
                    5,
                    277,
                    0
                ],
                "title": "Detecting and Preventing Latent Risk Accumulation in High-Performance\n  Software Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting and Preventing Latent Risk Accumulation in High-Performance\n  Software Systems"
                },
                "summary": "Modern distributed systems employ aggressive optimization strategies that\ncreate latent risks - hidden vulnerabilities where exceptional performance\nmasks catastrophic fragility when optimizations fail. Cache layers achieving\n99% hit rates can obscure database bottlenecks until cache failures trigger\n100x load amplification and cascading collapse. Current reliability engineering\nfocuses on reactive incident response rather than proactive detection of\noptimization-induced vulnerabilities. This paper presents the first\ncomprehensive framework for systematic latent risk detection, prevention, and\noptimization through integrated mathematical modeling, intelligent perturbation\ntesting, and risk-aware performance optimization. We introduce the Latent Risk\nIndex (LRI) that correlates strongly with incident severity (r=0.863, p<0.001),\nenabling predictive risk assessment. Our framework integrates three systems:\nHYDRA employing six optimization-aware perturbation strategies achieving 89.7%\nrisk discovery rates, RAVEN providing continuous production monitoring with\n92.9% precision and 93.8% recall across 1,748 scenarios, and APEX enabling\nrisk-aware optimization maintaining 96.6% baseline performance while reducing\nlatent risks by 59.2%. Evaluation across three testbed environments\ndemonstrates strong statistical validation with large effect sizes (Cohen\nd>2.0) and exceptional reproducibility (r>0.92). Production deployment over 24\nweeks shows 69.1% mean time to recovery reduction, 78.6% incident severity\nreduction, and 81 prevented incidents generating 1.44M USD average annual\nbenefits with 3.2-month ROI. Our approach transforms reliability engineering\nfrom reactive incident management to proactive risk-aware optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern distributed systems employ aggressive optimization strategies that\ncreate latent risks - hidden vulnerabilities where exceptional performance\nmasks catastrophic fragility when optimizations fail. Cache layers achieving\n99% hit rates can obscure database bottlenecks until cache failures trigger\n100x load amplification and cascading collapse. Current reliability engineering\nfocuses on reactive incident response rather than proactive detection of\noptimization-induced vulnerabilities. This paper presents the first\ncomprehensive framework for systematic latent risk detection, prevention, and\noptimization through integrated mathematical modeling, intelligent perturbation\ntesting, and risk-aware performance optimization. We introduce the Latent Risk\nIndex (LRI) that correlates strongly with incident severity (r=0.863, p<0.001),\nenabling predictive risk assessment. Our framework integrates three systems:\nHYDRA employing six optimization-aware perturbation strategies achieving 89.7%\nrisk discovery rates, RAVEN providing continuous production monitoring with\n92.9% precision and 93.8% recall across 1,748 scenarios, and APEX enabling\nrisk-aware optimization maintaining 96.6% baseline performance while reducing\nlatent risks by 59.2%. Evaluation across three testbed environments\ndemonstrates strong statistical validation with large effect sizes (Cohen\nd>2.0) and exceptional reproducibility (r>0.92). Production deployment over 24\nweeks shows 69.1% mean time to recovery reduction, 78.6% incident severity\nreduction, and 81 prevented incidents generating 1.44M USD average annual\nbenefits with 3.2-month ROI. Our approach transforms reliability engineering\nfrom reactive incident management to proactive risk-aware optimization."
                },
                "authors": [
                    {
                        "name": "Jahidul Arafat"
                    },
                    {
                        "name": "Kh. M. Moniruzzaman"
                    },
                    {
                        "name": "Shamim Hossain"
                    },
                    {
                        "name": "Fariha Tasmin"
                    },
                    {
                        "name": "Kamrujjaman"
                    },
                    {
                        "name": "Ahsan Habib Tareq"
                    }
                ],
                "author_detail": {
                    "name": "Ahsan Habib Tareq"
                },
                "author": "Ahsan Habib Tareq",
                "arxiv_comment": "26 pages, 12 tables, 4 figures. Academic-industry collaboration.\n  Framework (HYDRA, RAVEN, APEX) for optimization-induced vulnerabilities.\n  Evaluated: 2,160 configs, 12.7TB data, 1,748 scenarios",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03712v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03712v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M15, 90B25, 68T05, 90C29",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4; C.2.4; D.2.5; D.4.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16391v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16391v3",
                "updated": "2025-10-04T05:59:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    4,
                    5,
                    59,
                    1,
                    5,
                    277,
                    0
                ],
                "published": "2025-07-22T09:35:59Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    9,
                    35,
                    59,
                    1,
                    203,
                    0
                ],
                "title": "Ironman: Accelerating Oblivious Transfer Extension for\n  Privacy-Preserving AI with Near-Memory Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ironman: Accelerating Oblivious Transfer Extension for\n  Privacy-Preserving AI with Near-Memory Processing"
                },
                "summary": "With the wide application of machine learning (ML), privacy concerns arise\nwith user data as they may contain sensitive information. Privacy-preserving ML\n(PPML) based on cryptographic primitives has emerged as a promising solution in\nwhich an ML model is directly computed on the encrypted data to provide a\nformal privacy guarantee. However, PPML frameworks heavily rely on the\noblivious transfer (OT) primitive to compute nonlinear functions. OT mainly\ninvolves the computation of single-point correlated OT (SPCOT) and learning\nparity with noise (LPN) operations. As OT is still computed extensively on\ngeneral-purpose CPUs, it becomes the latency bottleneck of modern PPML\nframeworks.\n  In this paper, we propose a novel OT accelerator, dubbed Ironman, to\nsignificantly increase the efficiency of OT and the overall PPML framework. We\nobserve that SPCOT is computation-bounded, and thus propose a hardware-friendly\nSPCOT algorithm with a customized accelerator to improve SPCOT computation\nthroughput. In contrast, LPN is memory-bandwidth-bounded due to irregular\nmemory access patterns. Hence, we further leverage the near-memory processing\n(NMP) architecture equipped with memory-side cache and index sorting to improve\neffective memory bandwidth. With extensive experiments, we demonstrate Ironman\nachieves a 39.2-237.4 times improvement in OT throughput across different NMP\nconfigurations compared to the full-thread CPU implementation. For different\nPPML frameworks, Ironman demonstrates a 2.1-3.4 times reduction in end-to-end\nlatency for both CNN and Transformer models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the wide application of machine learning (ML), privacy concerns arise\nwith user data as they may contain sensitive information. Privacy-preserving ML\n(PPML) based on cryptographic primitives has emerged as a promising solution in\nwhich an ML model is directly computed on the encrypted data to provide a\nformal privacy guarantee. However, PPML frameworks heavily rely on the\noblivious transfer (OT) primitive to compute nonlinear functions. OT mainly\ninvolves the computation of single-point correlated OT (SPCOT) and learning\nparity with noise (LPN) operations. As OT is still computed extensively on\ngeneral-purpose CPUs, it becomes the latency bottleneck of modern PPML\nframeworks.\n  In this paper, we propose a novel OT accelerator, dubbed Ironman, to\nsignificantly increase the efficiency of OT and the overall PPML framework. We\nobserve that SPCOT is computation-bounded, and thus propose a hardware-friendly\nSPCOT algorithm with a customized accelerator to improve SPCOT computation\nthroughput. In contrast, LPN is memory-bandwidth-bounded due to irregular\nmemory access patterns. Hence, we further leverage the near-memory processing\n(NMP) architecture equipped with memory-side cache and index sorting to improve\neffective memory bandwidth. With extensive experiments, we demonstrate Ironman\nachieves a 39.2-237.4 times improvement in OT throughput across different NMP\nconfigurations compared to the full-thread CPU implementation. For different\nPPML frameworks, Ironman demonstrates a 2.1-3.4 times reduction in end-to-end\nlatency for both CNN and Transformer models."
                },
                "authors": [
                    {
                        "name": "Chenqi Lin"
                    },
                    {
                        "name": "Kang Yang"
                    },
                    {
                        "name": "Tianshi Xu"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Zhaohui Chen"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Mingyu Gao"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16391v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16391v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08134v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08134v3",
                "updated": "2025-10-04T05:28:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    4,
                    5,
                    28,
                    39,
                    5,
                    277,
                    0
                ],
                "published": "2025-08-11T16:10:00Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    10,
                    0,
                    0,
                    223,
                    0
                ],
                "title": "Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided\n  Region Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided\n  Region Control"
                },
                "summary": "While recent flow-based image editing models demonstrate general-purpose\ncapabilities across diverse tasks, they often struggle to specialize in\nchallenging scenarios -- particularly those involving large-scale shape\ntransformations. When performing such structural edits, these methods either\nfail to achieve the intended shape change or inadvertently alter non-target\nregions, resulting in degraded background quality. We propose\nFollow-Your-Shape, a training-free and mask-free framework that supports\nprecise and controllable editing of object shapes while strictly preserving\nnon-target content. Motivated by the divergence between inversion and editing\ntrajectories, we compute a Trajectory Divergence Map (TDM) by comparing\ntoken-wise velocity differences between the inversion and denoising paths. The\nTDM enables precise localization of editable regions and guides a Scheduled KV\nInjection mechanism that ensures stable and faithful editing. To facilitate a\nrigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120\nnew images and enriched prompt pairs specifically curated for shape-aware\nediting. Experiments demonstrate that our method achieves superior editability\nand visual fidelity, particularly in tasks requiring large-scale shape\nreplacement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While recent flow-based image editing models demonstrate general-purpose\ncapabilities across diverse tasks, they often struggle to specialize in\nchallenging scenarios -- particularly those involving large-scale shape\ntransformations. When performing such structural edits, these methods either\nfail to achieve the intended shape change or inadvertently alter non-target\nregions, resulting in degraded background quality. We propose\nFollow-Your-Shape, a training-free and mask-free framework that supports\nprecise and controllable editing of object shapes while strictly preserving\nnon-target content. Motivated by the divergence between inversion and editing\ntrajectories, we compute a Trajectory Divergence Map (TDM) by comparing\ntoken-wise velocity differences between the inversion and denoising paths. The\nTDM enables precise localization of editable regions and guides a Scheduled KV\nInjection mechanism that ensures stable and faithful editing. To facilitate a\nrigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120\nnew images and enriched prompt pairs specifically curated for shape-aware\nediting. Experiments demonstrate that our method achieves superior editability\nand visual fidelity, particularly in tasks requiring large-scale shape\nreplacement."
                },
                "authors": [
                    {
                        "name": "Zeqian Long"
                    },
                    {
                        "name": "Mingzhe Zheng"
                    },
                    {
                        "name": "Kunyu Feng"
                    },
                    {
                        "name": "Xinhua Zhang"
                    },
                    {
                        "name": "Hongyu Liu"
                    },
                    {
                        "name": "Harry Yang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Yue Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yue Ma"
                },
                "author": "Yue Ma",
                "arxiv_comment": "Project webpage is available at https://follow-your-shape.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08134v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08134v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05370v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05370v2",
                "updated": "2025-10-04T03:45:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    4,
                    3,
                    45,
                    40,
                    5,
                    277,
                    0
                ],
                "published": "2025-02-07T22:51:17Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    22,
                    51,
                    17,
                    4,
                    38,
                    0
                ],
                "title": "Taming Latency-Memory Trade-Off in MoE-Based LLM Serving via\n  Fine-Grained Expert Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taming Latency-Memory Trade-Off in MoE-Based LLM Serving via\n  Fine-Grained Expert Offloading"
                },
                "summary": "Large Language Models (LLMs) have gained immense success in revolutionizing\nvarious applications, including content generation, search and recommendation,\nand AI-assisted operation. To reduce high training costs, Mixture-of-Experts\n(MoE) architecture has become a popular backbone for modern LLMs. However,\ndespite the benefits, serving MoE-based LLMs experience severe memory\ninefficiency due to sparsely activated experts. Recent studies propose to\noffload inactive experts from GPU memory to CPU memory to improve the serving\nefficiency of MoE models. However, they either incur high inference latency or\nhigh model memory footprints due to coarse-grained designs.\n  To tame the latency-memory trade-off in MoE serving, we present FineMoE, a\nfine-grained expert offloading system for MoE serving that achieves low\ninference latency with memory efficiency. We design FineMoE to extract\nfine-grained expert selection patterns from MoE models and semantic hints from\ninput prompts to efficiently guide expert prefetching, caching, and offloading\ndecisions. FineMoE is prototyped on top of HuggingFace Transformers and\ndeployed on a six-GPU testbed. Experiments with open-source MoE models and\nreal-world workloads show that FineMoE reduces inference latency by 47% and\nimproves expert hit rate by 39% over state-of-the-art solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained immense success in revolutionizing\nvarious applications, including content generation, search and recommendation,\nand AI-assisted operation. To reduce high training costs, Mixture-of-Experts\n(MoE) architecture has become a popular backbone for modern LLMs. However,\ndespite the benefits, serving MoE-based LLMs experience severe memory\ninefficiency due to sparsely activated experts. Recent studies propose to\noffload inactive experts from GPU memory to CPU memory to improve the serving\nefficiency of MoE models. However, they either incur high inference latency or\nhigh model memory footprints due to coarse-grained designs.\n  To tame the latency-memory trade-off in MoE serving, we present FineMoE, a\nfine-grained expert offloading system for MoE serving that achieves low\ninference latency with memory efficiency. We design FineMoE to extract\nfine-grained expert selection patterns from MoE models and semantic hints from\ninput prompts to efficiently guide expert prefetching, caching, and offloading\ndecisions. FineMoE is prototyped on top of HuggingFace Transformers and\ndeployed on a six-GPU testbed. Experiments with open-source MoE models and\nreal-world workloads show that FineMoE reduces inference latency by 47% and\nimproves expert hit rate by 39% over state-of-the-art solutions."
                },
                "authors": [
                    {
                        "name": "Hanfei Yu"
                    },
                    {
                        "name": "Xingqi Cui"
                    },
                    {
                        "name": "Hong Zhang"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05370v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05370v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03215v1",
                "updated": "2025-10-03T17:52:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    52,
                    32,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T17:52:32Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    52,
                    32,
                    4,
                    276,
                    0
                ],
                "title": "Cache-to-Cache: Direct Semantic Communication Between Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-to-Cache: Direct Semantic Communication Between Large Language\n  Models"
                },
                "summary": "Multi-LLM systems harness the complementary strengths of diverse Large\nLanguage Models, achieving performance and efficiency gains unattainable by a\nsingle model. In existing designs, LLMs communicate through text, forcing\ninternal representations to be transformed into output token sequences. This\nprocess both loses rich semantic information and incurs token-by-token\ngeneration latency. Motivated by these limitations, we ask: Can LLMs\ncommunicate beyond text? Oracle experiments show that enriching the KV-Cache\nsemantics can improve response quality without increasing cache size,\nsupporting KV-Cache as an effective medium for inter-model communication. Thus,\nwe propose Cache-to-Cache (C2C), a new paradigm for direct semantic\ncommunication between LLMs. C2C uses a neural network to project and fuse the\nsource model's KV-cache with that of the target model to enable direct semantic\ntransfer. A learnable gating mechanism selects the target layers that benefit\nfrom cache communication. Compared with text communication, C2C utilizes the\ndeep, specialized semantics from both models, while avoiding explicit\nintermediate text generation. Experiments show that C2C achieves 8.5-10.5%\nhigher average accuracy than individual models. It further outperforms the text\ncommunication paradigm by approximately 3.0-5.0%, while delivering an average\n2.0x speedup in latency. Our code is available at\nhttps://github.com/thu-nics/C2C.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-LLM systems harness the complementary strengths of diverse Large\nLanguage Models, achieving performance and efficiency gains unattainable by a\nsingle model. In existing designs, LLMs communicate through text, forcing\ninternal representations to be transformed into output token sequences. This\nprocess both loses rich semantic information and incurs token-by-token\ngeneration latency. Motivated by these limitations, we ask: Can LLMs\ncommunicate beyond text? Oracle experiments show that enriching the KV-Cache\nsemantics can improve response quality without increasing cache size,\nsupporting KV-Cache as an effective medium for inter-model communication. Thus,\nwe propose Cache-to-Cache (C2C), a new paradigm for direct semantic\ncommunication between LLMs. C2C uses a neural network to project and fuse the\nsource model's KV-cache with that of the target model to enable direct semantic\ntransfer. A learnable gating mechanism selects the target layers that benefit\nfrom cache communication. Compared with text communication, C2C utilizes the\ndeep, specialized semantics from both models, while avoiding explicit\nintermediate text generation. Experiments show that C2C achieves 8.5-10.5%\nhigher average accuracy than individual models. It further outperforms the text\ncommunication paradigm by approximately 3.0-5.0%, while delivering an average\n2.0x speedup in latency. Our code is available at\nhttps://github.com/thu-nics/C2C."
                },
                "authors": [
                    {
                        "name": "Tianyu Fu"
                    },
                    {
                        "name": "Zihan Min"
                    },
                    {
                        "name": "Hanling Zhang"
                    },
                    {
                        "name": "Jichao Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03198v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03198v1",
                "updated": "2025-10-03T17:35:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    35,
                    16,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T17:35:16Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    35,
                    16,
                    4,
                    276,
                    0
                ],
                "title": "Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation\n  on Minecraft",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation\n  on Minecraft"
                },
                "summary": "Autoregressive video diffusion models have proved effective for world\nmodeling and interactive scene generation, with Minecraft gameplay as a\nrepresentative application. To faithfully simulate play, a model must generate\nnatural content while exploring new scenes and preserve spatial consistency\nwhen revisiting explored areas. Under limited computation budgets, it must\ncompress and exploit historical cues within a finite context window, which\nexposes a trade-off: Temporal-only memory lacks long-term spatial consistency,\nwhereas adding spatial memory strengthens consistency but may degrade new scene\ngeneration quality when the model over-relies on insufficient spatial context.\nWe present Memory Forcing, a learning framework that pairs training protocols\nwith a geometry-indexed spatial memory. Hybrid Training exposes distinct\ngameplay regimes, guiding the model to rely on temporal memory during\nexploration and incorporate spatial memory for revisits. Chained Forward\nTraining extends autoregressive training with model rollouts, where chained\npredictions create larger pose variations and encourage reliance on spatial\nmemory for maintaining consistency. Point-to-Frame Retrieval efficiently\nretrieves history by mapping currently visible points to their source frames,\nwhile Incremental 3D Reconstruction maintains and updates an explicit 3D cache.\nExtensive experiments demonstrate that Memory Forcing achieves superior\nlong-term spatial consistency and generative quality across diverse\nenvironments, while maintaining computational efficiency for extended\nsequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive video diffusion models have proved effective for world\nmodeling and interactive scene generation, with Minecraft gameplay as a\nrepresentative application. To faithfully simulate play, a model must generate\nnatural content while exploring new scenes and preserve spatial consistency\nwhen revisiting explored areas. Under limited computation budgets, it must\ncompress and exploit historical cues within a finite context window, which\nexposes a trade-off: Temporal-only memory lacks long-term spatial consistency,\nwhereas adding spatial memory strengthens consistency but may degrade new scene\ngeneration quality when the model over-relies on insufficient spatial context.\nWe present Memory Forcing, a learning framework that pairs training protocols\nwith a geometry-indexed spatial memory. Hybrid Training exposes distinct\ngameplay regimes, guiding the model to rely on temporal memory during\nexploration and incorporate spatial memory for revisits. Chained Forward\nTraining extends autoregressive training with model rollouts, where chained\npredictions create larger pose variations and encourage reliance on spatial\nmemory for maintaining consistency. Point-to-Frame Retrieval efficiently\nretrieves history by mapping currently visible points to their source frames,\nwhile Incremental 3D Reconstruction maintains and updates an explicit 3D cache.\nExtensive experiments demonstrate that Memory Forcing achieves superior\nlong-term spatial consistency and generative quality across diverse\nenvironments, while maintaining computational efficiency for extended\nsequences."
                },
                "authors": [
                    {
                        "name": "Junchao Huang"
                    },
                    {
                        "name": "Xinting Hu"
                    },
                    {
                        "name": "Boyao Han"
                    },
                    {
                        "name": "Shaoshuai Shi"
                    },
                    {
                        "name": "Zhuotao Tian"
                    },
                    {
                        "name": "Tianyu He"
                    },
                    {
                        "name": "Li Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Li Jiang"
                },
                "author": "Li Jiang",
                "arxiv_comment": "19 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03198v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03198v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14837v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14837v2",
                "updated": "2025-10-03T15:37:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    37,
                    19,
                    4,
                    276,
                    0
                ],
                "published": "2025-02-20T18:50:42Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    50,
                    42,
                    3,
                    51,
                    0
                ],
                "title": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs"
                },
                "summary": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance."
                },
                "authors": [
                    {
                        "name": "Tao Ji"
                    },
                    {
                        "name": "Bin Guo"
                    },
                    {
                        "name": "Yuanbin Wu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Lixing Shen"
                    },
                    {
                        "name": "Zhan Chen"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Tao Gui"
                    }
                ],
                "author_detail": {
                    "name": "Tao Gui"
                },
                "author": "Tao Gui",
                "arxiv_comment": "16 pages, 8 figures; Accepted to ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14837v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14837v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02866v1",
                "updated": "2025-10-03T10:06:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    10,
                    6,
                    44,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T10:06:44Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    10,
                    6,
                    44,
                    4,
                    276,
                    0
                ],
                "title": "Life Estimation of HVDC Cable Insulation under Load Cycles: from\n  Macroscopic to Microscopic Charge Conduction Modelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Life Estimation of HVDC Cable Insulation under Load Cycles: from\n  Macroscopic to Microscopic Charge Conduction Modelling"
                },
                "summary": "This paper goes one step forward in the life estimation of HVDC cable\ninsulation under load cycles by introducing for the first time a microscopic\nmodel of charge conduction and transport i.e., Bipolar Charge Transport BCT\nmodel for electric field calculation inside the insulation thickness. The paper\nfirstly includes the development and the validation of BCT model with that\nfound in literature. Then, the parameters of the developed BCT model are\noptimized using Pulsed Electro-Acoustic PEA space charge measurements. Followed\nby the integration of the developed, validated and optimized model into the\nelectric field calculation for life estimation of a 500 kV DC-XLPE insulated\ncable subjected to Type Test load cycles according to Cigre Techical Brochure\n852. The developed microscopic model is compared to the macroscopic models\nalready found in the literature. The microscopic model shows a comparable\nelectric field inversion similarly to macroscopic models. However, the behavior\nof the microscopic model is noticed to be different under heating and cooling\nload cycles. In hot cable, the maximum electric field stabilizes at different\namplitude and position inside the insulation thickness in both models. This\ninvestigation has been carried out in the framework of the HEU-NEWGEN research\nproject.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper goes one step forward in the life estimation of HVDC cable\ninsulation under load cycles by introducing for the first time a microscopic\nmodel of charge conduction and transport i.e., Bipolar Charge Transport BCT\nmodel for electric field calculation inside the insulation thickness. The paper\nfirstly includes the development and the validation of BCT model with that\nfound in literature. Then, the parameters of the developed BCT model are\noptimized using Pulsed Electro-Acoustic PEA space charge measurements. Followed\nby the integration of the developed, validated and optimized model into the\nelectric field calculation for life estimation of a 500 kV DC-XLPE insulated\ncable subjected to Type Test load cycles according to Cigre Techical Brochure\n852. The developed microscopic model is compared to the macroscopic models\nalready found in the literature. The microscopic model shows a comparable\nelectric field inversion similarly to macroscopic models. However, the behavior\nof the microscopic model is noticed to be different under heating and cooling\nload cycles. In hot cable, the maximum electric field stabilizes at different\namplitude and position inside the insulation thickness in both models. This\ninvestigation has been carried out in the framework of the HEU-NEWGEN research\nproject."
                },
                "authors": [
                    {
                        "name": "Bassel Diban"
                    },
                    {
                        "name": "Giovanni Mazzanti"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Mazzanti"
                },
                "author": "Giovanni Mazzanti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02758v1",
                "updated": "2025-10-03T06:43:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    6,
                    43,
                    24,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T06:43:24Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    6,
                    43,
                    24,
                    4,
                    276,
                    0
                ],
                "title": "TokenFlow: Responsive LLM Text Streaming Serving under Request Burst via\n  Preemptive Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenFlow: Responsive LLM Text Streaming Serving under Request Burst via\n  Preemptive Scheduling"
                },
                "summary": "Real-time LLM interactions demand streamed token generations, where text\ntokens are progressively generated and delivered to users while balancing two\nobjectives: responsiveness (i.e., low time-to-first-token) and steady\ngeneration (i.e.,required time-between-tokens). Standard LLM serving systems\nsuffer from the inflexibility caused by non-preemptive request scheduling and\nreactive memory management, leading to poor resource utilization and low\nrequest processing parallelism under request bursts. Therefore, we present\nTokenFlow, a novel LLM serving system with enhanced text streaming performance\nvia preemptive request scheduling and proactive key-value (KV) cache\nmanagement. TokenFlow dynamically prioritizes requests based on real-time token\nbuffer occupancy and token consumption rate, while actively transferring KV\ncache between GPU and CPU memory in the background and overlapping I/O with\ncomputation to minimize request preemption overhead. Extensive experiments on\nLlama3-8B and Qwen2.5-32B across multiple GPUs (RTX 4090, A6000, H200)\ndemonstrate that TokenFlow achieves up to 82.5% higher effective throughput\n(accounting for actual user consumption) while reducing P99 TTFT by up to\n80.2%, without degrading overall token throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time LLM interactions demand streamed token generations, where text\ntokens are progressively generated and delivered to users while balancing two\nobjectives: responsiveness (i.e., low time-to-first-token) and steady\ngeneration (i.e.,required time-between-tokens). Standard LLM serving systems\nsuffer from the inflexibility caused by non-preemptive request scheduling and\nreactive memory management, leading to poor resource utilization and low\nrequest processing parallelism under request bursts. Therefore, we present\nTokenFlow, a novel LLM serving system with enhanced text streaming performance\nvia preemptive request scheduling and proactive key-value (KV) cache\nmanagement. TokenFlow dynamically prioritizes requests based on real-time token\nbuffer occupancy and token consumption rate, while actively transferring KV\ncache between GPU and CPU memory in the background and overlapping I/O with\ncomputation to minimize request preemption overhead. Extensive experiments on\nLlama3-8B and Qwen2.5-32B across multiple GPUs (RTX 4090, A6000, H200)\ndemonstrate that TokenFlow achieves up to 82.5% higher effective throughput\n(accounting for actual user consumption) while reducing P99 TTFT by up to\n80.2%, without degrading overall token throughput."
                },
                "authors": [
                    {
                        "name": "Junyi Chen"
                    },
                    {
                        "name": "Chuheng Du"
                    },
                    {
                        "name": "Renyuan Liu"
                    },
                    {
                        "name": "Shuochao Yao"
                    },
                    {
                        "name": "Dingtian Yan"
                    },
                    {
                        "name": "Jiang Liao"
                    },
                    {
                        "name": "Shengzhong Liu"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_comment": "Accepted by EuroSys 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02750v1",
                "updated": "2025-10-03T06:27:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    6,
                    27,
                    33,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T06:27:33Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    6,
                    27,
                    33,
                    4,
                    276,
                    0
                ],
                "title": "Bayesian Test-time Adaptation for Object Recognition and Detection with\n  Vision-language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Test-time Adaptation for Object Recognition and Detection with\n  Vision-language Models"
                },
                "summary": "Vision-language models (VLMs) such as CLIP and Grounding DINO have achieved\nremarkable success in object recognition and detection. However, their\nperformance often degrades under real-world distribution shifts. Test-time\nadaptation (TTA) aims to mitigate this issue by adapting models during\ninference. Existing methods either rely on computationally expensive\nbackpropagation, which hinders real-time deployment, or focus solely on\nlikelihood adaptation, which overlooks the critical role of the prior. Our\nprior work, Bayesian Class Adaptation (BCA), addressed these shortcomings for\nobject recognition by introducing a training-free framework that incorporates\nadaptive priors. Building upon this foundation, we now present Bayesian Class\nAdaptation plus (BCA+), a unified, training-free framework for TTA for both\nobject recognition and detection. BCA+ introduces a dynamic cache that\nadaptively stores and updates class embeddings, spatial scales (for detection),\nand, crucially, adaptive class priors derived from historical predictions. We\nformulate adaptation as a Bayesian inference problem, where final predictions\nare generated by fusing the initial VLM output with a cache-based prediction.\nThis cache-based prediction combines a dynamically updated likelihood\n(measuring feature and scale similarity) and a prior (reflecting the evolving\nclass distribution). This dual-adaptation mechanism, coupled with\nuncertainty-guided fusion, enables BCA+ to correct both the model's semantic\nunderstanding and its contextual confidence. As a training-free method\nrequiring no backpropagation, BCA+ is highly efficient. Extensive experiments\ndemonstrate that BCA+ achieves state-of-the-art performance on both recognition\nand detection benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) such as CLIP and Grounding DINO have achieved\nremarkable success in object recognition and detection. However, their\nperformance often degrades under real-world distribution shifts. Test-time\nadaptation (TTA) aims to mitigate this issue by adapting models during\ninference. Existing methods either rely on computationally expensive\nbackpropagation, which hinders real-time deployment, or focus solely on\nlikelihood adaptation, which overlooks the critical role of the prior. Our\nprior work, Bayesian Class Adaptation (BCA), addressed these shortcomings for\nobject recognition by introducing a training-free framework that incorporates\nadaptive priors. Building upon this foundation, we now present Bayesian Class\nAdaptation plus (BCA+), a unified, training-free framework for TTA for both\nobject recognition and detection. BCA+ introduces a dynamic cache that\nadaptively stores and updates class embeddings, spatial scales (for detection),\nand, crucially, adaptive class priors derived from historical predictions. We\nformulate adaptation as a Bayesian inference problem, where final predictions\nare generated by fusing the initial VLM output with a cache-based prediction.\nThis cache-based prediction combines a dynamically updated likelihood\n(measuring feature and scale similarity) and a prior (reflecting the evolving\nclass distribution). This dual-adaptation mechanism, coupled with\nuncertainty-guided fusion, enables BCA+ to correct both the model's semantic\nunderstanding and its contextual confidence. As a training-free method\nrequiring no backpropagation, BCA+ is highly efficient. Extensive experiments\ndemonstrate that BCA+ achieves state-of-the-art performance on both recognition\nand detection benchmarks."
                },
                "authors": [
                    {
                        "name": "Lihua Zhou"
                    },
                    {
                        "name": "Mao Ye"
                    },
                    {
                        "name": "Shuaifeng Li"
                    },
                    {
                        "name": "Nianxin Li"
                    },
                    {
                        "name": "Jinlin Wu"
                    },
                    {
                        "name": "Xiatian Zhu"
                    },
                    {
                        "name": "Lei Deng"
                    },
                    {
                        "name": "Hongbin Liu"
                    },
                    {
                        "name": "Jiebo Luo"
                    },
                    {
                        "name": "Zhen Lei"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Lei"
                },
                "author": "Zhen Lei",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02084v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02084v2",
                "updated": "2025-10-03T05:10:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    5,
                    10,
                    2,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-02T14:50:50Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    50,
                    50,
                    3,
                    275,
                    0
                ],
                "title": "KAIROS: Unified Training for Universal Non-Autoregressive Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KAIROS: Unified Training for Universal Non-Autoregressive Time Series\n  Forecasting"
                },
                "summary": "In the World Wide Web, reliable time series forecasts provide the\nforward-looking signals that drive resource planning, cache placement, and\nanomaly response, enabling platforms to operate efficiently as user behavior\nand content distributions evolve. Compared with other domains, time series\nforecasting for Web applications requires much faster responsiveness to support\nreal-time decision making. We present KAIROS, a non-autoregressive time series\nforecasting framework that directly models segment-level multi-peak\ndistributions. Unlike autoregressive approaches, KAIROS avoids error\naccumulation and achieves just-in-time inference, while improving over existing\nnon-autoregressive models that collapse to over-smoothed predictions. Trained\non the large-scale corpus, KAIROS demonstrates strong zero-shot generalization\non six widely used benchmarks, delivering forecasting performance comparable to\nstate-of-the-art foundation models with similar scale, at a fraction of their\ninference cost. Beyond empirical results, KAIROS highlights the importance of\nnon-autoregressive design as a scalable paradigm for foundation models in time\nseries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the World Wide Web, reliable time series forecasts provide the\nforward-looking signals that drive resource planning, cache placement, and\nanomaly response, enabling platforms to operate efficiently as user behavior\nand content distributions evolve. Compared with other domains, time series\nforecasting for Web applications requires much faster responsiveness to support\nreal-time decision making. We present KAIROS, a non-autoregressive time series\nforecasting framework that directly models segment-level multi-peak\ndistributions. Unlike autoregressive approaches, KAIROS avoids error\naccumulation and achieves just-in-time inference, while improving over existing\nnon-autoregressive models that collapse to over-smoothed predictions. Trained\non the large-scale corpus, KAIROS demonstrates strong zero-shot generalization\non six widely used benchmarks, delivering forecasting performance comparable to\nstate-of-the-art foundation models with similar scale, at a fraction of their\ninference cost. Beyond empirical results, KAIROS highlights the importance of\nnon-autoregressive design as a scalable paradigm for foundation models in time\nseries."
                },
                "authors": [
                    {
                        "name": "Kuiye Ding"
                    },
                    {
                        "name": "Fanda Fan"
                    },
                    {
                        "name": "Zheya Wang"
                    },
                    {
                        "name": "Hongxiao Li"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Chunjie Luo"
                    },
                    {
                        "name": "Jianfeng Zhan"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Zhan"
                },
                "author": "Jianfeng Zhan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02084v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02084v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25188v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25188v2",
                "updated": "2025-10-03T00:40:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    0,
                    40,
                    49,
                    4,
                    276,
                    0
                ],
                "published": "2025-09-29T17:59:54Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    59,
                    54,
                    0,
                    272,
                    0
                ],
                "title": "Learning to Parallel: Accelerating Diffusion Large Language Models via\n  Learnable Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Parallel: Accelerating Diffusion Large Language Models via\n  Learnable Parallel Decoding"
                },
                "summary": "Autoregressive decoding in large language models (LLMs) requires\n$\\mathcal{O}(n)$ sequential steps for $n$ tokens, fundamentally limiting\ninference throughput. Recent diffusion-based LLMs (dLLMs) enable parallel token\ngeneration through iterative denoising. However, current parallel decoding\nstrategies rely on fixed, input-agnostic heuristics (e.g., confidence\nthresholds), which fail to adapt to input-specific characteristics, resulting\nin suboptimal speed-quality trade-offs across diverse NLP tasks. In this work,\nwe explore a more flexible and dynamic approach to parallel decoding. We\npropose Learning to Parallel Decode (Learn2PD), a framework that trains a\nlightweight and adaptive filter model to predict, for each token position,\nwhether the current prediction matches the final output. This learned filter\napproximates an oracle parallel decoding strategy that unmasks tokens only when\ncorrectly predicted. Importantly, the filter model is learned in a\npost-training manner, requiring only a small amount of computation to optimize\nit (minute-level GPU time). Additionally, we introduce End-of-Text Prediction\n(EoTP) to detect decoding completion at the end of sequence, avoiding redundant\ndecoding of padding tokens. Experiments on the LLaDA benchmark demonstrate that\nour method achieves up to 22.58$\\times$ speedup without any performance drop,\nand up to 57.51$\\times$ when combined with KV-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive decoding in large language models (LLMs) requires\n$\\mathcal{O}(n)$ sequential steps for $n$ tokens, fundamentally limiting\ninference throughput. Recent diffusion-based LLMs (dLLMs) enable parallel token\ngeneration through iterative denoising. However, current parallel decoding\nstrategies rely on fixed, input-agnostic heuristics (e.g., confidence\nthresholds), which fail to adapt to input-specific characteristics, resulting\nin suboptimal speed-quality trade-offs across diverse NLP tasks. In this work,\nwe explore a more flexible and dynamic approach to parallel decoding. We\npropose Learning to Parallel Decode (Learn2PD), a framework that trains a\nlightweight and adaptive filter model to predict, for each token position,\nwhether the current prediction matches the final output. This learned filter\napproximates an oracle parallel decoding strategy that unmasks tokens only when\ncorrectly predicted. Importantly, the filter model is learned in a\npost-training manner, requiring only a small amount of computation to optimize\nit (minute-level GPU time). Additionally, we introduce End-of-Text Prediction\n(EoTP) to detect decoding completion at the end of sequence, avoiding redundant\ndecoding of padding tokens. Experiments on the LLaDA benchmark demonstrate that\nour method achieves up to 22.58$\\times$ speedup without any performance drop,\nand up to 57.51$\\times$ when combined with KV-Cache."
                },
                "authors": [
                    {
                        "name": "Wenrui Bao"
                    },
                    {
                        "name": "Zhiben Chen"
                    },
                    {
                        "name": "Dan Xu"
                    },
                    {
                        "name": "Yuzhang Shang"
                    }
                ],
                "author_detail": {
                    "name": "Yuzhang Shang"
                },
                "author": "Yuzhang Shang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25188v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25188v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02613v1",
                "updated": "2025-10-02T23:16:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    23,
                    16,
                    35,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T23:16:35Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    23,
                    16,
                    35,
                    3,
                    275,
                    0
                ],
                "title": "ElasticMoE: An Efficient Auto Scaling Method for Mixture-of-Experts\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ElasticMoE: An Efficient Auto Scaling Method for Mixture-of-Experts\n  Models"
                },
                "summary": "Mixture-of-Experts (MoE) models promise efficient scaling of large language\nmodels (LLMs) by activating only a small subset of experts per token, but their\nparallelized inference pipelines make elastic serving challenging. Existing\nstrategies fall short: horizontal scaling provisions entire replicas of the\ncurrent configuration, often tens to hundreds of accelerators, leading to\ncoarse granularity, long provisioning delays, and costly overprovisioning.\nVertical scaling offers finer adjustments but typically requires instance\nrestarts, incurring downtime. These limitations make current approaches\nill-suited for the bursty, short-lived traffic patterns common in cloud\ndeployments.\n  We present ElasticMoE, an elastic scaling framework for MoE LLMs that\nachieves fine-grained, low-latency, and zero-downtime scaling. ElasticMoE\ndecouples inference execution from memory operations, enabling scaling steps to\nproceed concurrently with serving. An HBM Management Module (HMM) reuses\nweights and KV caches via zero-copy remapping, while high-bandwidth\npeer-to-peer transfers bring newly added accelerators online without\ninterrupting service. A virtual memory based expert redistribution mechanism\nmigrates MoE experts without costly buffer reallocations, reducing peak memory\nusage during expert parallelism reconfiguration.\n  Our evaluation on Ascend NPUs with three popular MoE LLMs shows that\nElasticMoE achieves up to 9x lower scale-up latency, up to 2x better throughput\nduring scaling, and significantly improves SLO attainment compared to\nbaselines. By enabling fine-grained, concurrent scaling with minimal\ndisruption, ElasticMoE advances the practicality of deploying massive MoE LLMs\nin dynamic cloud environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models promise efficient scaling of large language\nmodels (LLMs) by activating only a small subset of experts per token, but their\nparallelized inference pipelines make elastic serving challenging. Existing\nstrategies fall short: horizontal scaling provisions entire replicas of the\ncurrent configuration, often tens to hundreds of accelerators, leading to\ncoarse granularity, long provisioning delays, and costly overprovisioning.\nVertical scaling offers finer adjustments but typically requires instance\nrestarts, incurring downtime. These limitations make current approaches\nill-suited for the bursty, short-lived traffic patterns common in cloud\ndeployments.\n  We present ElasticMoE, an elastic scaling framework for MoE LLMs that\nachieves fine-grained, low-latency, and zero-downtime scaling. ElasticMoE\ndecouples inference execution from memory operations, enabling scaling steps to\nproceed concurrently with serving. An HBM Management Module (HMM) reuses\nweights and KV caches via zero-copy remapping, while high-bandwidth\npeer-to-peer transfers bring newly added accelerators online without\ninterrupting service. A virtual memory based expert redistribution mechanism\nmigrates MoE experts without costly buffer reallocations, reducing peak memory\nusage during expert parallelism reconfiguration.\n  Our evaluation on Ascend NPUs with three popular MoE LLMs shows that\nElasticMoE achieves up to 9x lower scale-up latency, up to 2x better throughput\nduring scaling, and significantly improves SLO attainment compared to\nbaselines. By enabling fine-grained, concurrent scaling with minimal\ndisruption, ElasticMoE advances the practicality of deploying massive MoE LLMs\nin dynamic cloud environments."
                },
                "authors": [
                    {
                        "name": "Gursimran Singh"
                    },
                    {
                        "name": "Timothy Yu"
                    },
                    {
                        "name": "Haley Li"
                    },
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Hanieh Sadri"
                    },
                    {
                        "name": "Qintao Zhang"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "arxiv_affiliation": "Huawei Technologies Canada",
                "author": "Zhenan Fan",
                "arxiv_comment": "19 pages, 15 figures, Under Submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12397v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12397v5",
                "updated": "2025-10-02T19:25:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    19,
                    25,
                    29,
                    3,
                    275,
                    0
                ],
                "published": "2025-04-16T18:03:21Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activated LoRA: Fine-tuned LLMs for Intrinsics"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is inefficient, as the key-value (KV) cache of the entire\nturn history must be recomputed with the LoRA weights before generation can\nbegin. To address this problem, we propose Activated LoRA (aLoRA), an adapter\narchitecture which modifies the LoRA framework to only adapt weights for the\ntokens in the sequence after the aLoRA is invoked. This change crucially allows\naLoRA to accept the base model's KV cache of the input string, meaning that\naLoRA can be instantly activated whenever needed in a chain without recomputing\nthe prior keys and values. This enables building what we call intrinsics, i.e.\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\ntrain a set of aLoRA-based intrinsics models, demonstrating competitive\naccuracy with standard LoRA while significantly improving inference efficiency.\nWe contributed our Activated LoRA implementation to the Huggingface PEFT\nlibrary https://github.com/huggingface/peft.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is inefficient, as the key-value (KV) cache of the entire\nturn history must be recomputed with the LoRA weights before generation can\nbegin. To address this problem, we propose Activated LoRA (aLoRA), an adapter\narchitecture which modifies the LoRA framework to only adapt weights for the\ntokens in the sequence after the aLoRA is invoked. This change crucially allows\naLoRA to accept the base model's KV cache of the input string, meaning that\naLoRA can be instantly activated whenever needed in a chain without recomputing\nthe prior keys and values. This enables building what we call intrinsics, i.e.\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\ntrain a set of aLoRA-based intrinsics models, demonstrating competitive\naccuracy with standard LoRA while significantly improving inference efficiency.\nWe contributed our Activated LoRA implementation to the Huggingface PEFT\nlibrary https://github.com/huggingface/peft."
                },
                "authors": [
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Vraj Shah"
                    },
                    {
                        "name": "Lucian Popa"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "David Cox"
                    }
                ],
                "author_detail": {
                    "name": "David Cox"
                },
                "author": "David Cox",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12397v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12397v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00299v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00299v4",
                "updated": "2025-10-02T19:09:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    19,
                    9,
                    19,
                    3,
                    275,
                    0
                ],
                "published": "2025-02-01T03:49:47Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem. The code is available at\n\\href{https://github.com/NVIDIA/kvpress}{link}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem. The code is available at\n\\href{https://github.com/NVIDIA/kvpress}{link}."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00299v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00299v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17650v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17650v2",
                "updated": "2025-10-02T18:38:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    18,
                    38,
                    0,
                    3,
                    275,
                    0
                ],
                "published": "2025-09-22T11:54:58Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    11,
                    54,
                    58,
                    0,
                    265,
                    0
                ],
                "title": "Evict3R: Training-Free Token Eviction for Memory-Bounded Streaming\n  Visual Geometry Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evict3R: Training-Free Token Eviction for Memory-Bounded Streaming\n  Visual Geometry Transformers"
                },
                "summary": "Streaming visual transformers like StreamVGGT achieve strong 3D perception\nbut suffer from unbounded growth of key value (KV) memory, which limits\nscalability. We propose a training-free, inference-time token eviction policy\nthat bounds memory by discarding redundant tokens while keeping the most\ninformative ones. Our method uses significantly less memory with little to no\ndrop in accuracy: on 7-Scenes with long sequences it reduces peak memory from\n18.63 GB to 9.39 GB while accuracy and completeness drop by only 0.003. Under\nstrict memory budgets, eviction enables denser frame sampling, which improves\nreconstruction accuracy compared to the baseline. Experiments across video\ndepth estimation (Sintel, KITTI), 3D reconstruction (7-Scenes, NRGBD), and\ncamera pose estimation (Sintel, TUM-dynamics) show that our approach closely\nmatches StreamVGGT at a fraction of the memory and makes long-horizon streaming\ninference more practical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming visual transformers like StreamVGGT achieve strong 3D perception\nbut suffer from unbounded growth of key value (KV) memory, which limits\nscalability. We propose a training-free, inference-time token eviction policy\nthat bounds memory by discarding redundant tokens while keeping the most\ninformative ones. Our method uses significantly less memory with little to no\ndrop in accuracy: on 7-Scenes with long sequences it reduces peak memory from\n18.63 GB to 9.39 GB while accuracy and completeness drop by only 0.003. Under\nstrict memory budgets, eviction enables denser frame sampling, which improves\nreconstruction accuracy compared to the baseline. Experiments across video\ndepth estimation (Sintel, KITTI), 3D reconstruction (7-Scenes, NRGBD), and\ncamera pose estimation (Sintel, TUM-dynamics) show that our approach closely\nmatches StreamVGGT at a fraction of the memory and makes long-horizon streaming\ninference more practical."
                },
                "authors": [
                    {
                        "name": "Soroush Mahdi"
                    },
                    {
                        "name": "Fardin Ayar"
                    },
                    {
                        "name": "Ehsan Javanmardi"
                    },
                    {
                        "name": "Manabu Tsukada"
                    },
                    {
                        "name": "Mahdi Javanmardi"
                    }
                ],
                "author_detail": {
                    "name": "Mahdi Javanmardi"
                },
                "author": "Mahdi Javanmardi",
                "arxiv_comment": "project page: https://soroush-mim.github.io/projects/evict3r/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17650v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17650v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17033v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17033v3",
                "updated": "2025-10-02T18:20:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    18,
                    20,
                    18,
                    3,
                    275,
                    0
                ],
                "published": "2025-07-22T21:41:43Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    41,
                    43,
                    1,
                    203,
                    0
                ],
                "title": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI"
                },
                "summary": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy. To our knowledge, this is the first\nside-channel attack on AI privacy that exploits hardware optimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy. To our knowledge, this is the first\nside-channel attack on AI privacy that exploits hardware optimizations."
                },
                "authors": [
                    {
                        "name": "Joshua Kalyanapu"
                    },
                    {
                        "name": "Farshad Dizani"
                    },
                    {
                        "name": "Darsh Asher"
                    },
                    {
                        "name": "Azam Ghanbari"
                    },
                    {
                        "name": "Rosario Cammarota"
                    },
                    {
                        "name": "Aydin Aysu"
                    },
                    {
                        "name": "Samira Mirbagher Ajorpaz"
                    }
                ],
                "author_detail": {
                    "name": "Samira Mirbagher Ajorpaz"
                },
                "author": "Samira Mirbagher Ajorpaz",
                "arxiv_comment": "Accepted at MICRO 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17033v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17033v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02312v1",
                "updated": "2025-10-02T17:59:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    59,
                    51,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:59:51Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    59,
                    51,
                    3,
                    275,
                    0
                ],
                "title": "KaVa: Latent Reasoning via Compressed KV-Cache Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KaVa: Latent Reasoning via Compressed KV-Cache Distillation"
                },
                "summary": "Large Language Models (LLMs) excel at multi-step reasoning problems with\nexplicit chain-of-thought (CoT), but verbose traces incur significant\ncomputational costs and memory overhead, and often carry redundant, stylistic\nartifacts. Latent reasoning has emerged as an efficient alternative that\ninternalizes the thought process, but it suffers from a critical lack of\nsupervision, limiting its effectiveness on complex, natural-language reasoning\ntraces. In this work, we propose KaVa, the first framework that bridges this\ngap by distilling knowledge directly from a compressed KV-cache of the teacher\ninto a latent-reasoning student via self-distillation, leveraging the\nrepresentational flexibility of continuous latent tokens to align stepwise KV\ntrajectories. We show that the abstract, unstructured knowledge within\ncompressed KV-cache, which lacks direct token correspondence, can serve as a\nrich supervisory signal for a latent reasoning student. Empirically, the\napproach consistently outperforms strong latent baselines, exhibits markedly\nsmaller degradation from equation-only to natural-language traces, and scales\nto larger backbones while preserving efficiency. These results establish\ncompressed KV-cache distillation as a scalable supervision signal for latent\nreasoning, combining the accuracy of CoT-trained teachers with the efficiency\nand deployability of latent inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at multi-step reasoning problems with\nexplicit chain-of-thought (CoT), but verbose traces incur significant\ncomputational costs and memory overhead, and often carry redundant, stylistic\nartifacts. Latent reasoning has emerged as an efficient alternative that\ninternalizes the thought process, but it suffers from a critical lack of\nsupervision, limiting its effectiveness on complex, natural-language reasoning\ntraces. In this work, we propose KaVa, the first framework that bridges this\ngap by distilling knowledge directly from a compressed KV-cache of the teacher\ninto a latent-reasoning student via self-distillation, leveraging the\nrepresentational flexibility of continuous latent tokens to align stepwise KV\ntrajectories. We show that the abstract, unstructured knowledge within\ncompressed KV-cache, which lacks direct token correspondence, can serve as a\nrich supervisory signal for a latent reasoning student. Empirically, the\napproach consistently outperforms strong latent baselines, exhibits markedly\nsmaller degradation from equation-only to natural-language traces, and scales\nto larger backbones while preserving efficiency. These results establish\ncompressed KV-cache distillation as a scalable supervision signal for latent\nreasoning, combining the accuracy of CoT-trained teachers with the efficiency\nand deployability of latent inference."
                },
                "authors": [
                    {
                        "name": "Anna Kuzina"
                    },
                    {
                        "name": "Maciej Pioro"
                    },
                    {
                        "name": "Paul N. Whatmough"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    }
                ],
                "author_detail": {
                    "name": "Babak Ehteshami Bejnordi"
                },
                "author": "Babak Ehteshami Bejnordi",
                "arxiv_comment": "Preprint. Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03346v1",
                "updated": "2025-10-02T16:01:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    1,
                    54,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T16:01:54Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    1,
                    54,
                    3,
                    275,
                    0
                ],
                "title": "KVComm: Enabling Efficient LLM Communication through Selective KV\n  Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVComm: Enabling Efficient LLM Communication through Selective KV\n  Sharing"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in multi-agent\nsystems, where effective inter-model communication is crucial. Existing\ncommunication protocols either rely on natural language, incurring high\ninference costs and information loss, or on hidden states, which suffer from\ninformation concentration bias and inefficiency. To address these limitations,\nwe propose KVComm, a novel communication framework that enables efficient\ncommunication between LLMs through selective sharing of KV pairs. KVComm\nleverages the rich information encoded in the KV pairs while avoiding the\npitfalls of hidden states. We introduce a KV layer-wise selection strategy\nbased on attention importance scores with a Gaussian prior to identify the most\ninformative KV pairs for communication. Extensive experiments across diverse\ntasks and model pairs demonstrate that KVComm achieves comparable performance\nto the upper-bound method, which directly merges inputs to one model without\nany communication, while transmitting as few as 30\\% of layers' KV pairs. Our\nstudy highlights the potential of KV pairs as an effective medium for inter-LLM\ncommunication, paving the way for scalable and efficient multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in multi-agent\nsystems, where effective inter-model communication is crucial. Existing\ncommunication protocols either rely on natural language, incurring high\ninference costs and information loss, or on hidden states, which suffer from\ninformation concentration bias and inefficiency. To address these limitations,\nwe propose KVComm, a novel communication framework that enables efficient\ncommunication between LLMs through selective sharing of KV pairs. KVComm\nleverages the rich information encoded in the KV pairs while avoiding the\npitfalls of hidden states. We introduce a KV layer-wise selection strategy\nbased on attention importance scores with a Gaussian prior to identify the most\ninformative KV pairs for communication. Extensive experiments across diverse\ntasks and model pairs demonstrate that KVComm achieves comparable performance\nto the upper-bound method, which directly merges inputs to one model without\nany communication, while transmitting as few as 30\\% of layers' KV pairs. Our\nstudy highlights the potential of KV pairs as an effective medium for inter-LLM\ncommunication, paving the way for scalable and efficient multi-agent systems."
                },
                "authors": [
                    {
                        "name": "Xiangyu Shi"
                    },
                    {
                        "name": "Marco Chiesa"
                    },
                    {
                        "name": "Gerald Q. Maguire Jr."
                    },
                    {
                        "name": "Dejan Kostic"
                    }
                ],
                "author_detail": {
                    "name": "Dejan Kostic"
                },
                "author": "Dejan Kostic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17356v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17356v2",
                "updated": "2025-10-02T14:42:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    42,
                    41,
                    3,
                    275,
                    0
                ],
                "published": "2025-08-24T13:30:00Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    13,
                    30,
                    0,
                    6,
                    236,
                    0
                ],
                "title": "DiCache: Let Diffusion Model Determine Its Own Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiCache: Let Diffusion Model Determine Its Own Cache"
                },
                "summary": "Recent years have witnessed the rapid development of acceleration techniques\nfor diffusion models, especially caching-based acceleration methods. These\nstudies seek to answer two fundamental questions: \"When to cache\" and \"How to\nuse cache\", typically relying on predefined empirical laws or dataset-level\npriors to determine caching timings and adopting handcrafted rules for\nmulti-step cache utilization. However, given the highly dynamic nature of the\ndiffusion process, they often exhibit limited generalizability and fail to cope\nwith diverse samples. In this paper, a strong sample-specific correlation is\nrevealed between the variation patterns of the shallow-layer feature\ndifferences in the diffusion model and those of deep-layer features. Moreover,\nwe have observed that the features from different model layers form similar\ntrajectories. Based on these observations, we present DiCache, a novel\ntraining-free adaptive caching strategy for accelerating diffusion models at\nruntime, answering both when and how to cache within a unified framework.\nSpecifically, DiCache is composed of two principal components: (1) Online Probe\nProfiling Scheme leverages a shallow-layer online probe to obtain an on-the-fly\nindicator for the caching error in real time, enabling the model to dynamically\ncustomize the caching schedule for each sample. (2) Dynamic Cache Trajectory\nAlignment adaptively approximates the deep-layer feature output from multi-step\nhistorical caches based on the shallow-layer feature trajectory, facilitating\nhigher visual quality. Extensive experiments validate DiCache's capability in\nachieving higher efficiency and improved fidelity over state-of-the-art\napproaches on various leading diffusion models including WAN 2.1, HunyuanVideo\nand Flux.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have witnessed the rapid development of acceleration techniques\nfor diffusion models, especially caching-based acceleration methods. These\nstudies seek to answer two fundamental questions: \"When to cache\" and \"How to\nuse cache\", typically relying on predefined empirical laws or dataset-level\npriors to determine caching timings and adopting handcrafted rules for\nmulti-step cache utilization. However, given the highly dynamic nature of the\ndiffusion process, they often exhibit limited generalizability and fail to cope\nwith diverse samples. In this paper, a strong sample-specific correlation is\nrevealed between the variation patterns of the shallow-layer feature\ndifferences in the diffusion model and those of deep-layer features. Moreover,\nwe have observed that the features from different model layers form similar\ntrajectories. Based on these observations, we present DiCache, a novel\ntraining-free adaptive caching strategy for accelerating diffusion models at\nruntime, answering both when and how to cache within a unified framework.\nSpecifically, DiCache is composed of two principal components: (1) Online Probe\nProfiling Scheme leverages a shallow-layer online probe to obtain an on-the-fly\nindicator for the caching error in real time, enabling the model to dynamically\ncustomize the caching schedule for each sample. (2) Dynamic Cache Trajectory\nAlignment adaptively approximates the deep-layer feature output from multi-step\nhistorical caches based on the shallow-layer feature trajectory, facilitating\nhigher visual quality. Extensive experiments validate DiCache's capability in\nachieving higher efficiency and improved fidelity over state-of-the-art\napproaches on various leading diffusion models including WAN 2.1, HunyuanVideo\nand Flux."
                },
                "authors": [
                    {
                        "name": "Jiazi Bu"
                    },
                    {
                        "name": "Pengyang Ling"
                    },
                    {
                        "name": "Yujie Zhou"
                    },
                    {
                        "name": "Yibin Wang"
                    },
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiaqi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi Wang"
                },
                "author": "Jiaqi Wang",
                "arxiv_comment": "Project Page: https://bujiazi.github.io/dicache.github.io/ Code:\n  https://github.com/Bujiazi/DiCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17356v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17356v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11305v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11305v3",
                "updated": "2025-10-02T14:09:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    9,
                    3,
                    3,
                    275,
                    0
                ],
                "published": "2024-10-15T05:57:51Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    57,
                    51,
                    1,
                    289,
                    0
                ],
                "title": "QSpec: Speculative Decoding with Complementary Quantization Schemes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QSpec: Speculative Decoding with Complementary Quantization Schemes"
                },
                "summary": "Quantization is widely adopted to accelerate inference and reduce memory\nconsumption in large language models (LLMs). While activation-weight joint\nquantization enables efficient low-precision decoding, it suffers from\nsubstantial performance degradation on multi-step reasoning tasks. We propose\nQSpec, a novel quantization paradigm that decouples efficiency from quality by\nintegrating two complementary schemes via speculative decoding: low-precision\njoint quantization for fast drafting and high-precision weight-only\nquantization for accurate verification. QSpec reuses both weights and KV cache\nacross stages, enabling near-zero-cost switching without retraining or\nauxiliary models. Compared to high-precision baselines, QSpec achieves up to\n1.64x speedup without quality degradation, and outperforms state-of-the-art\nspeculative decoding methods by up to 1.55x in batched settings. Furthermore,\nQSpec supports plug-and-play deployment and generalizes well across model\nscales, quantization methods, and workloads. These properties make QSpec a\npractical and scalable solution for high-fidelity quantized LLM serving under\nmemory-constrained scenarios. Our code is available at\nhttps://github.com/hku-netexplo-lab/QSpec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is widely adopted to accelerate inference and reduce memory\nconsumption in large language models (LLMs). While activation-weight joint\nquantization enables efficient low-precision decoding, it suffers from\nsubstantial performance degradation on multi-step reasoning tasks. We propose\nQSpec, a novel quantization paradigm that decouples efficiency from quality by\nintegrating two complementary schemes via speculative decoding: low-precision\njoint quantization for fast drafting and high-precision weight-only\nquantization for accurate verification. QSpec reuses both weights and KV cache\nacross stages, enabling near-zero-cost switching without retraining or\nauxiliary models. Compared to high-precision baselines, QSpec achieves up to\n1.64x speedup without quality degradation, and outperforms state-of-the-art\nspeculative decoding methods by up to 1.55x in batched settings. Furthermore,\nQSpec supports plug-and-play deployment and generalizes well across model\nscales, quantization methods, and workloads. These properties make QSpec a\npractical and scalable solution for high-fidelity quantized LLM serving under\nmemory-constrained scenarios. Our code is available at\nhttps://github.com/hku-netexplo-lab/QSpec."
                },
                "authors": [
                    {
                        "name": "Juntao Zhao"
                    },
                    {
                        "name": "Wenhao Lu"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "arxiv_journal_ref": "Proceedings of the 2025 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11305v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11305v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01884v1",
                "updated": "2025-10-02T10:49:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    10,
                    49,
                    54,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T10:49:54Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    10,
                    49,
                    54,
                    3,
                    275,
                    0
                ],
                "title": "Study of the $^{20}$Ne($p,Î³$)$^{21}$Na reaction at LUNA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Study of the $^{20}$Ne($p,Î³$)$^{21}$Na reaction at LUNA"
                },
                "summary": "The NeNa-MgAl cycles are involved in the synthesis of Ne, Na, Mg, and Al\nisotopes. The $^{20}$Ne($p,\\gamma$)$^{21}$Na (Q = 2431.68 keV) reaction is the\nfirst and slowest reaction of the NeNa cycle and it controls the speed at which\nthe entire cycle proceeds. At the state of the art, the uncertainty on the\n20Ne(p,{\\gamma})21Na reaction rate affects the production of the elements in\nthe NeNa cycle. In particular, in the temperature range from 0.1 GK to 1 GK,\nthe rate is dominated by the 366 keV resonance corresponding to the excited\nstate of EX = 2797.5 keV and by the direct capture component. The present study\nfocus on the study of the 366 keV resonance and the direct capture below 400\nkeV. At LUNA (Laboratory for Underground Nuclear Astrophysics) the\n$^{20}$Ne($p,\\gamma$)$^{21}$Na reaction has been measured using the intense\nproton beam delivered by the LUNA 400 kV accelerator and a windowless\ndifferential-pumping gas target. The products of the reaction are detected with\ntwo high-purity germanium detectors. The experimental details and preliminary\nresults on the 366 keV resonance and on the direct capture component at very\nlow energies will be shown, together with their possible impact on the\n$^{20}$Ne($p,\\gamma$)$^{21}$Na reaction rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The NeNa-MgAl cycles are involved in the synthesis of Ne, Na, Mg, and Al\nisotopes. The $^{20}$Ne($p,\\gamma$)$^{21}$Na (Q = 2431.68 keV) reaction is the\nfirst and slowest reaction of the NeNa cycle and it controls the speed at which\nthe entire cycle proceeds. At the state of the art, the uncertainty on the\n20Ne(p,{\\gamma})21Na reaction rate affects the production of the elements in\nthe NeNa cycle. In particular, in the temperature range from 0.1 GK to 1 GK,\nthe rate is dominated by the 366 keV resonance corresponding to the excited\nstate of EX = 2797.5 keV and by the direct capture component. The present study\nfocus on the study of the 366 keV resonance and the direct capture below 400\nkeV. At LUNA (Laboratory for Underground Nuclear Astrophysics) the\n$^{20}$Ne($p,\\gamma$)$^{21}$Na reaction has been measured using the intense\nproton beam delivered by the LUNA 400 kV accelerator and a windowless\ndifferential-pumping gas target. The products of the reaction are detected with\ntwo high-purity germanium detectors. The experimental details and preliminary\nresults on the 366 keV resonance and on the direct capture component at very\nlow energies will be shown, together with their possible impact on the\n$^{20}$Ne($p,\\gamma$)$^{21}$Na reaction rate."
                },
                "authors": [
                    {
                        "name": "A. Caciolli"
                    }
                ],
                "author_detail": {
                    "name": "A. Caciolli"
                },
                "arxiv_affiliation": "on behalf of the LUNA collaboration",
                "author": "A. Caciolli",
                "arxiv_doi": "10.1051/epjconf/202429207005",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/epjconf/202429207005",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.01884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "EPJ Web Conf., 292 (2024) 07005",
                "arxiv_primary_category": {
                    "term": "nucl-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20211v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20211v2",
                "updated": "2025-10-02T04:11:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    4,
                    11,
                    7,
                    3,
                    275,
                    0
                ],
                "published": "2025-05-26T16:52:40Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    16,
                    52,
                    40,
                    0,
                    146,
                    0
                ],
                "title": "PiCa: Parameter-Efficient Fine-Tuning with Column Space Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PiCa: Parameter-Efficient Fine-Tuning with Column Space Projection"
                },
                "summary": "Fine-tuning large foundation models is essential for building expert models\ntailored to specialized tasks and domains, but fully updating billions of\nparameters is computationally prohibitive. Reducing the number of trainable\nparameters using parameter-efficient fine-tuning is therefore crucial not only\nto reduce training costs but also to mitigate storage, caching, and serving\noverheads during deployment. Prior works, such as Singular Vectors-guided\nFine-Tuning, have shown that exploiting the geometry of pre-trained weights can\nsignificantly improve parameter-efficiency, but they lack a solid theoretical\nfoundation. In this paper, we introduce Parameter-efficient Fine-tuning with\nColumn Space Projection (PiCa), a novel theoretically grounded PEFT method. We\nprove that projecting gradients onto the principal column space of pre-trained\nweights provides an effective inductive bias for adaptation and further enhance\nparameter efficiency through a novel weight-sharing strategy. Across diverse\nNLP and vision tasks, PiCa consistently outperforms state-of-the-art baselines\nunder comparable or smaller parameter budgets, demonstrating both theoretical\nrigor and practical effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large foundation models is essential for building expert models\ntailored to specialized tasks and domains, but fully updating billions of\nparameters is computationally prohibitive. Reducing the number of trainable\nparameters using parameter-efficient fine-tuning is therefore crucial not only\nto reduce training costs but also to mitigate storage, caching, and serving\noverheads during deployment. Prior works, such as Singular Vectors-guided\nFine-Tuning, have shown that exploiting the geometry of pre-trained weights can\nsignificantly improve parameter-efficiency, but they lack a solid theoretical\nfoundation. In this paper, we introduce Parameter-efficient Fine-tuning with\nColumn Space Projection (PiCa), a novel theoretically grounded PEFT method. We\nprove that projecting gradients onto the principal column space of pre-trained\nweights provides an effective inductive bias for adaptation and further enhance\nparameter efficiency through a novel weight-sharing strategy. Across diverse\nNLP and vision tasks, PiCa consistently outperforms state-of-the-art baselines\nunder comparable or smaller parameter budgets, demonstrating both theoretical\nrigor and practical effectiveness."
                },
                "authors": [
                    {
                        "name": "Junseo Hwang"
                    },
                    {
                        "name": "Wonguk Cho"
                    },
                    {
                        "name": "Taesup Kim"
                    }
                ],
                "author_detail": {
                    "name": "Taesup Kim"
                },
                "author": "Taesup Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20211v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20211v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07447v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07447v4",
                "updated": "2025-10-01T20:30:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    20,
                    30,
                    18,
                    2,
                    274,
                    0
                ],
                "published": "2024-11-12T00:10:34Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    0,
                    10,
                    34,
                    1,
                    317,
                    0
                ],
                "title": "Faster LLM Inference using DBMS-Inspired Preemption and Cache\n  Replacement Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster LLM Inference using DBMS-Inspired Preemption and Cache\n  Replacement Policies"
                },
                "summary": "LLMs are increasingly used world-wide from daily tasks to agentic systems and\ndata analytics, requiring significant GPU resources. LLM inference systems,\nhowever, are slow compared to database systems, and inference performance and\nmechanism have been often regarded as a black box, limiting the expansion of\nthe use of LLMs inside databases and other performance-critical applications.\nThis paper first analyzes the LLM inference performance and focuses on a data\nmanagement issue inside LLM inference. We find that inference systems lack an\nadequate resource cost model and optimization strategy to schedule requests\nwith their intermediate results in a cache reside in GPU memory when executing\nmultiple concurrent inference requests. We adapt classic database techniques by\nbuilding cost models for concurrent inference requests and a new cache\nreplacement policy tailored for LLM inference, which can substantially save GPU\ncosts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are increasingly used world-wide from daily tasks to agentic systems and\ndata analytics, requiring significant GPU resources. LLM inference systems,\nhowever, are slow compared to database systems, and inference performance and\nmechanism have been often regarded as a black box, limiting the expansion of\nthe use of LLMs inside databases and other performance-critical applications.\nThis paper first analyzes the LLM inference performance and focuses on a data\nmanagement issue inside LLM inference. We find that inference systems lack an\nadequate resource cost model and optimization strategy to schedule requests\nwith their intermediate results in a cache reside in GPU memory when executing\nmultiple concurrent inference requests. We adapt classic database techniques by\nbuilding cost models for concurrent inference requests and a new cache\nreplacement policy tailored for LLM inference, which can substantially save GPU\ncosts."
                },
                "authors": [
                    {
                        "name": "Kyoungmin Kim"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Kijae Hong"
                    },
                    {
                        "name": "Anastasia Ailamaki"
                    }
                ],
                "author_detail": {
                    "name": "Anastasia Ailamaki"
                },
                "author": "Anastasia Ailamaki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07447v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07447v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01875v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01875v2",
                "updated": "2025-10-01T19:06:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    19,
                    6,
                    10,
                    2,
                    274,
                    0
                ],
                "published": "2025-08-03T18:15:42Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    18,
                    15,
                    42,
                    6,
                    215,
                    0
                ],
                "title": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding"
                },
                "summary": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios."
                },
                "authors": [
                    {
                        "name": "Haolin Yang"
                    },
                    {
                        "name": "Feilong Tang"
                    },
                    {
                        "name": "Linxiao Zhao"
                    },
                    {
                        "name": "Xiang An"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Huifa Li"
                    },
                    {
                        "name": "Xinlin Zhuang"
                    },
                    {
                        "name": "Boqian Wang"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Xiaofeng Zhang"
                    },
                    {
                        "name": "Abdalla Swikir"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Zongyuan Ge"
                    },
                    {
                        "name": "Imran Razzak"
                    }
                ],
                "author_detail": {
                    "name": "Imran Razzak"
                },
                "author": "Imran Razzak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01875v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01875v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09350v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09350v2",
                "updated": "2025-10-01T18:55:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    18,
                    55,
                    20,
                    2,
                    274,
                    0
                ],
                "published": "2025-06-11T03:04:23Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    3,
                    4,
                    23,
                    2,
                    162,
                    0
                ],
                "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation"
                },
                "summary": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2"
                },
                "authors": [
                    {
                        "name": "Shanchuan Lin"
                    },
                    {
                        "name": "Ceyuan Yang"
                    },
                    {
                        "name": "Hao He"
                    },
                    {
                        "name": "Jianwen Jiang"
                    },
                    {
                        "name": "Yuxi Ren"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Xuefeng Xiao"
                    },
                    {
                        "name": "Lu Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Jiang"
                },
                "author": "Lu Jiang",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09350v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09350v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01336v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01336v1",
                "updated": "2025-10-01T18:04:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    18,
                    4,
                    14,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T18:04:14Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    18,
                    4,
                    14,
                    2,
                    274,
                    0
                ],
                "title": "HiSpec: Hierarchical Speculative Decoding for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiSpec: Hierarchical Speculative Decoding for LLMs"
                },
                "summary": "Speculative decoding accelerates LLM inference by using a smaller draft model\nto speculate tokens that a larger target model verifies. Verification is often\nthe bottleneck (e.g. verification is $4\\times$ slower than token generation\nwhen a 3B model speculates for a 70B target model), but most prior works focus\nonly on accelerating drafting. $\\textit{``Intermediate\"}$ verification reduces\nverification time by discarding inaccurate draft tokens early, but existing\nmethods incur substantial training overheads in incorporating the intermediate\nverifier, increase the memory footprint to orchestrate the intermediate\nverification step, and compromise accuracy by relying on approximate\nheuristics.\n  We propose $\\underline{\\textit{Hi}}\\textit{erarchical\n}\\underline{\\textit{Spec}}\\textit{ulative Decoding (HiSpec)}$, a framework for\nhigh-throughput speculative decoding that exploits $\\textit{early-exit (EE)\nmodels}$ for low-overhead intermediate verification. EE models allow tokens to\nexit early by skipping layer traversal and are explicitly trained so that\nhidden states at selected layers can be interpreted, making them uniquely\nsuited for intermediate verification without drastically increasing compute and\nmemory overheads. To improve resource-efficiency even further, we design a\nmethodology that enables HiSpec to re-use key-value caches and hidden states\nbetween the draft, intermediate verifier, and target models. To maintain\naccuracy, HiSpec periodically validates the draft tokens accepted by the\nintermediate verifier against the target model. Our evaluations using various\nrepresentative benchmarks and models show that HiSpec improves throughput by\n1.28$\\times$ on average and by up to 2.01$\\times$ compared to the baseline\nsingle-layer speculation without compromising accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding accelerates LLM inference by using a smaller draft model\nto speculate tokens that a larger target model verifies. Verification is often\nthe bottleneck (e.g. verification is $4\\times$ slower than token generation\nwhen a 3B model speculates for a 70B target model), but most prior works focus\nonly on accelerating drafting. $\\textit{``Intermediate\"}$ verification reduces\nverification time by discarding inaccurate draft tokens early, but existing\nmethods incur substantial training overheads in incorporating the intermediate\nverifier, increase the memory footprint to orchestrate the intermediate\nverification step, and compromise accuracy by relying on approximate\nheuristics.\n  We propose $\\underline{\\textit{Hi}}\\textit{erarchical\n}\\underline{\\textit{Spec}}\\textit{ulative Decoding (HiSpec)}$, a framework for\nhigh-throughput speculative decoding that exploits $\\textit{early-exit (EE)\nmodels}$ for low-overhead intermediate verification. EE models allow tokens to\nexit early by skipping layer traversal and are explicitly trained so that\nhidden states at selected layers can be interpreted, making them uniquely\nsuited for intermediate verification without drastically increasing compute and\nmemory overheads. To improve resource-efficiency even further, we design a\nmethodology that enables HiSpec to re-use key-value caches and hidden states\nbetween the draft, intermediate verifier, and target models. To maintain\naccuracy, HiSpec periodically validates the draft tokens accepted by the\nintermediate verifier against the target model. Our evaluations using various\nrepresentative benchmarks and models show that HiSpec improves throughput by\n1.28$\\times$ on average and by up to 2.01$\\times$ compared to the baseline\nsingle-layer speculation without compromising accuracy."
                },
                "authors": [
                    {
                        "name": "Avinash Kumar"
                    },
                    {
                        "name": "Sujay Sanghavi"
                    },
                    {
                        "name": "Poulami Das"
                    }
                ],
                "author_detail": {
                    "name": "Poulami Das"
                },
                "author": "Poulami Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01336v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00948v1",
                "updated": "2025-10-01T14:21:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    14,
                    21,
                    45,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T14:21:45Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    14,
                    21,
                    45,
                    2,
                    274,
                    0
                ],
                "title": "InfVSR: Breaking Length Limits of Generic Video Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfVSR: Breaking Length Limits of Generic Video Super-Resolution"
                },
                "summary": "Real-world videos often extend over thousands of frames. Existing video\nsuper-resolution (VSR) approaches, however, face two persistent challenges when\nprocessing long sequences: (1) inefficiency due to the heavy cost of multi-step\ndenoising for full-length sequences; and (2) poor scalability hindered by\ntemporal decomposition that causes artifacts and discontinuities. To break\nthese limits, we propose InfVSR, which novelly reformulates VSR as an\nautoregressive-one-step-diffusion paradigm. This enables streaming inference\nwhile fully leveraging pre-trained video diffusion priors. First, we adapt the\npre-trained DiT into a causal structure, maintaining both local and global\ncoherence via rolling KV-cache and joint visual guidance. Second, we distill\nthe diffusion process into a single step efficiently, with patch-wise pixel\nsupervision and cross-chunk distribution matching. Together, these designs\nenable efficient and scalable VSR for unbounded-length videos. To fill the gap\nin long-form video evaluation, we build a new benchmark tailored for extended\nsequences and further introduce semantic-level metrics to comprehensively\nassess temporal consistency. Our method pushes the frontier of long-form VSR,\nachieves state-of-the-art quality with enhanced semantic consistency, and\ndelivers up to 58x speed-up over existing methods such as MGLD-VSR. Code will\nbe available at https://github.com/Kai-Liu001/InfVSR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world videos often extend over thousands of frames. Existing video\nsuper-resolution (VSR) approaches, however, face two persistent challenges when\nprocessing long sequences: (1) inefficiency due to the heavy cost of multi-step\ndenoising for full-length sequences; and (2) poor scalability hindered by\ntemporal decomposition that causes artifacts and discontinuities. To break\nthese limits, we propose InfVSR, which novelly reformulates VSR as an\nautoregressive-one-step-diffusion paradigm. This enables streaming inference\nwhile fully leveraging pre-trained video diffusion priors. First, we adapt the\npre-trained DiT into a causal structure, maintaining both local and global\ncoherence via rolling KV-cache and joint visual guidance. Second, we distill\nthe diffusion process into a single step efficiently, with patch-wise pixel\nsupervision and cross-chunk distribution matching. Together, these designs\nenable efficient and scalable VSR for unbounded-length videos. To fill the gap\nin long-form video evaluation, we build a new benchmark tailored for extended\nsequences and further introduce semantic-level metrics to comprehensively\nassess temporal consistency. Our method pushes the frontier of long-form VSR,\nachieves state-of-the-art quality with enhanced semantic consistency, and\ndelivers up to 58x speed-up over existing methods such as MGLD-VSR. Code will\nbe available at https://github.com/Kai-Liu001/InfVSR."
                },
                "authors": [
                    {
                        "name": "Ziqing Zhang"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Zheng Chen"
                    },
                    {
                        "name": "Xi Li"
                    },
                    {
                        "name": "Yucong Chen"
                    },
                    {
                        "name": "Bingnan Duan"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Yulun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yulun Zhang"
                },
                "author": "Yulun Zhang",
                "arxiv_comment": "Code will be available at https://github.com/Kai-Liu001/InfVSR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26432v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26432v2",
                "updated": "2025-10-01T11:26:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    11,
                    26,
                    36,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-30T15:53:56Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    53,
                    56,
                    1,
                    273,
                    0
                ],
                "title": "AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block\n  Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block\n  Size"
                },
                "summary": "Diffusion-based large language models (dLLMs) are gaining attention for their\ninherent capacity for parallel decoding, offering a compelling alternative to\nautoregressive LLMs. Among various decoding strategies, blockwise\nsemi-autoregressive (semi-AR) approaches are widely adopted due to their\nnatural support for KV caching and their favorable accuracy-speed trade-off.\nHowever, this paper identifies two fundamental limitations in the conventional\nsemi-AR decoding approach that applies a fixed block size: i) late decoding\noverhead, where the unmasking of high-confidence tokens outside the current\nblock is unnecessarily delayed, and ii) premature decoding error, where\nlow-confidence tokens inside the current block are committed too early, leading\nto incorrect tokens. This paper presents the first systematic investigation\nchallenging the fixed block size assumption in semi-AR decoding. Through a\nstatistical analysis of confidence dynamics during the denoising process, we\nidentify a volatility band (VB) region during dLLM decoding, which encodes\nlocal semantic structure and can be used to guide adaptive block sizing.\nLeveraging these insights, we introduce AdaBlock-dLLM, a training-free,\nplug-and-play scheduler that adaptively aligns block boundaries with semantic\nsteps by adjusting block size during runtime. Extensive experiments across\ndiverse benchmarks show that AdaBlock-dLLM achieves up to 5.3% accuracy\nimprovement under the same throughput budget. Beyond inference-time\noptimization, we hope our semantics-aware adaptive scheduling approach and\nconfidence-based analysis will inspire future training strategies for dLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs) are gaining attention for their\ninherent capacity for parallel decoding, offering a compelling alternative to\nautoregressive LLMs. Among various decoding strategies, blockwise\nsemi-autoregressive (semi-AR) approaches are widely adopted due to their\nnatural support for KV caching and their favorable accuracy-speed trade-off.\nHowever, this paper identifies two fundamental limitations in the conventional\nsemi-AR decoding approach that applies a fixed block size: i) late decoding\noverhead, where the unmasking of high-confidence tokens outside the current\nblock is unnecessarily delayed, and ii) premature decoding error, where\nlow-confidence tokens inside the current block are committed too early, leading\nto incorrect tokens. This paper presents the first systematic investigation\nchallenging the fixed block size assumption in semi-AR decoding. Through a\nstatistical analysis of confidence dynamics during the denoising process, we\nidentify a volatility band (VB) region during dLLM decoding, which encodes\nlocal semantic structure and can be used to guide adaptive block sizing.\nLeveraging these insights, we introduce AdaBlock-dLLM, a training-free,\nplug-and-play scheduler that adaptively aligns block boundaries with semantic\nsteps by adjusting block size during runtime. Extensive experiments across\ndiverse benchmarks show that AdaBlock-dLLM achieves up to 5.3% accuracy\nimprovement under the same throughput budget. Beyond inference-time\noptimization, we hope our semantics-aware adaptive scheduling approach and\nconfidence-based analysis will inspire future training strategies for dLLMs."
                },
                "authors": [
                    {
                        "name": "Guanxi Lu"
                    },
                    {
                        "name": "Hao Mark Chen"
                    },
                    {
                        "name": "Yuto Karashima"
                    },
                    {
                        "name": "Zhican Wang"
                    },
                    {
                        "name": "Daichi Fujiki"
                    },
                    {
                        "name": "Hongxiang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Hongxiang Fan"
                },
                "author": "Hongxiang Fan",
                "arxiv_comment": "Preprint. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26432v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26432v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00636v1",
                "updated": "2025-10-01T08:12:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    8,
                    12,
                    14,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T08:12:14Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    8,
                    12,
                    14,
                    2,
                    274,
                    0
                ],
                "title": "Expected Attention: KV Cache Compression by Estimating Attention from\n  Future Queries Distribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expected Attention: KV Cache Compression by Estimating Attention from\n  Future Queries Distribution"
                },
                "summary": "Memory consumption of the Key-Value (KV) cache represents a major bottleneck\nfor efficient large language model inference. While attention-score-based KV\ncache pruning shows promise, it faces critical practical limitations: attention\nscores from future tokens are unavailable during compression, and modern\nimplementations like Flash Attention do not materialize the full attention\nmatrix, making past scores inaccessible. To overcome these challenges, we\nintroduce $\\textbf{Expected Attention, a training-free compression method}$\nthat estimates KV pairs importance by predicting how future queries will attend\nto them. Our approach leverages the distributional properties of LLM\nactivations to compute expected attention scores in closed form for each KV\npair. These scores enable principled ranking and pruning of KV pairs with\nminimal impact on the residual stream, achieving effective compression without\nperformance degradation. Importantly, our method operates seamlessly across\nboth prefilling and decoding phases, consistently outperforming\nstate-of-the-art baselines in both scenarios. Finally, $\\textbf{we release\nKVPress, a comprehensive library to enable researchers to implement and\nbenchmark KV cache compression methods, already including more than 20\ntechniques}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory consumption of the Key-Value (KV) cache represents a major bottleneck\nfor efficient large language model inference. While attention-score-based KV\ncache pruning shows promise, it faces critical practical limitations: attention\nscores from future tokens are unavailable during compression, and modern\nimplementations like Flash Attention do not materialize the full attention\nmatrix, making past scores inaccessible. To overcome these challenges, we\nintroduce $\\textbf{Expected Attention, a training-free compression method}$\nthat estimates KV pairs importance by predicting how future queries will attend\nto them. Our approach leverages the distributional properties of LLM\nactivations to compute expected attention scores in closed form for each KV\npair. These scores enable principled ranking and pruning of KV pairs with\nminimal impact on the residual stream, achieving effective compression without\nperformance degradation. Importantly, our method operates seamlessly across\nboth prefilling and decoding phases, consistently outperforming\nstate-of-the-art baselines in both scenarios. Finally, $\\textbf{we release\nKVPress, a comprehensive library to enable researchers to implement and\nbenchmark KV cache compression methods, already including more than 20\ntechniques}$."
                },
                "authors": [
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Maximilian Jeblick"
                    },
                    {
                        "name": "Simon JÃ©gou"
                    }
                ],
                "author_detail": {
                    "name": "Simon JÃ©gou"
                },
                "author": "Simon JÃ©gou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00566v1",
                "updated": "2025-10-01T06:38:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    6,
                    38,
                    45,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T06:38:45Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    6,
                    38,
                    45,
                    2,
                    274,
                    0
                ],
                "title": "Panorama: Fast-Track Nearest Neighbors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panorama: Fast-Track Nearest Neighbors"
                },
                "summary": "Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose\nembeddings are close to that of a given query in a high-dimensional space,\naiming to balance accuracy with speed. Used in recommendation systems, image\nand video retrieval, natural language processing, and retrieval-augmented\ngeneration (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT\nutilize graph, tree, clustering, and quantization techniques to navigate large\nvector spaces. Despite this progress, ANNS systems spend up to 99\\% of query\ntime to compute distances in their final refinement phase. In this paper, we\npresent PANORAMA, a machine learning-driven approach that tackles the ANNS\nverification bottleneck through data-adaptive learned orthogonal transforms\nthat facilitate the accretive refinement of distance bounds. Such transforms\ncompact over 90\\% of signal energy into the first half of dimensions, enabling\nearly candidate pruning with partial distance computations. We integrate\nPANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and\nAnnoy, without index modification, using level-major memory layouts,\nSIMD-vectorized partial distance computations, and cache-aware access patterns.\nExperiments across diverse datasets -- from image-based CIFAR-10 and GIST to\nmodern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate\nthat PANORAMA affords a 2--30$\\times$ end-to-end speedup with no recall loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose\nembeddings are close to that of a given query in a high-dimensional space,\naiming to balance accuracy with speed. Used in recommendation systems, image\nand video retrieval, natural language processing, and retrieval-augmented\ngeneration (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT\nutilize graph, tree, clustering, and quantization techniques to navigate large\nvector spaces. Despite this progress, ANNS systems spend up to 99\\% of query\ntime to compute distances in their final refinement phase. In this paper, we\npresent PANORAMA, a machine learning-driven approach that tackles the ANNS\nverification bottleneck through data-adaptive learned orthogonal transforms\nthat facilitate the accretive refinement of distance bounds. Such transforms\ncompact over 90\\% of signal energy into the first half of dimensions, enabling\nearly candidate pruning with partial distance computations. We integrate\nPANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and\nAnnoy, without index modification, using level-major memory layouts,\nSIMD-vectorized partial distance computations, and cache-aware access patterns.\nExperiments across diverse datasets -- from image-based CIFAR-10 and GIST to\nmodern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate\nthat PANORAMA affords a 2--30$\\times$ end-to-end speedup with no recall loss."
                },
                "authors": [
                    {
                        "name": "Vansh Ramani"
                    },
                    {
                        "name": "Alexis Schlomer"
                    },
                    {
                        "name": "Akash Nayar"
                    },
                    {
                        "name": "Panagiotis Karras"
                    },
                    {
                        "name": "Sayan Ranu"
                    },
                    {
                        "name": "Jignesh M. Patel"
                    }
                ],
                "author_detail": {
                    "name": "Jignesh M. Patel"
                },
                "author": "Jignesh M. Patel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00536v1",
                "updated": "2025-10-01T05:37:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    5,
                    37,
                    54,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T05:37:54Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    5,
                    37,
                    54,
                    2,
                    274,
                    0
                ],
                "title": "GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness"
                },
                "summary": "Graphical user interface (GUI) agents built on vision-language models have\nemerged as a promising approach to automate human-computer workflows. However,\nthey also face the inefficiency challenge as they process long sequences of\nhigh-resolution screenshots and solving long-horizon tasks, making inference\nslow, costly and memory-bound. While key-value (KV) caching can mitigate this,\nstoring the full cache is prohibitive for image-heavy contexts. Existing\ncache-compression methods are sub-optimal as they do not account for the\nspatial and temporal redundancy of GUIs. In this work, we first analyze\nattention patterns in GUI agent workloads and find that, unlike in natural\nimages, attention sparsity is uniformly high across all transformer layers.\nThis insight motivates a simple uniform budget allocation strategy, which we\nshow empirically outperforms more complex layer-varying schemes. Building on\nthis, we introduce GUI-KV, a plug-and-play KV cache compression method for GUI\nagents that requires no retraining. GUI-KV combines two novel techniques: (i)\nspatial saliency guidance, which augments attention scores with the L2 norm of\nhidden states to better preserve semantically important visual tokens, and (ii)\ntemporal redundancy scoring, which projects previous frames' keys onto the\ncurrent frame's key subspace to preferentially prune redundant history. Across\nstandard GUI agent benchmarks and models, GUI-KV outperforms competitive KV\ncompression baselines, closely matching full-cache accuracy at modest budgets.\nNotably, in a 5-screenshot setting on the AgentNetBench benchmark, GUI-KV\nreduces decoding FLOPs by 38.9% while increasing step accuracy by 4.1% over the\nfull-cache baseline. These results demonstrate that exploiting GUI-specific\nredundancies enables efficient and reliable agent performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphical user interface (GUI) agents built on vision-language models have\nemerged as a promising approach to automate human-computer workflows. However,\nthey also face the inefficiency challenge as they process long sequences of\nhigh-resolution screenshots and solving long-horizon tasks, making inference\nslow, costly and memory-bound. While key-value (KV) caching can mitigate this,\nstoring the full cache is prohibitive for image-heavy contexts. Existing\ncache-compression methods are sub-optimal as they do not account for the\nspatial and temporal redundancy of GUIs. In this work, we first analyze\nattention patterns in GUI agent workloads and find that, unlike in natural\nimages, attention sparsity is uniformly high across all transformer layers.\nThis insight motivates a simple uniform budget allocation strategy, which we\nshow empirically outperforms more complex layer-varying schemes. Building on\nthis, we introduce GUI-KV, a plug-and-play KV cache compression method for GUI\nagents that requires no retraining. GUI-KV combines two novel techniques: (i)\nspatial saliency guidance, which augments attention scores with the L2 norm of\nhidden states to better preserve semantically important visual tokens, and (ii)\ntemporal redundancy scoring, which projects previous frames' keys onto the\ncurrent frame's key subspace to preferentially prune redundant history. Across\nstandard GUI agent benchmarks and models, GUI-KV outperforms competitive KV\ncompression baselines, closely matching full-cache accuracy at modest budgets.\nNotably, in a 5-screenshot setting on the AgentNetBench benchmark, GUI-KV\nreduces decoding FLOPs by 38.9% while increasing step accuracy by 4.1% over the\nfull-cache baseline. These results demonstrate that exploiting GUI-specific\nredundancies enables efficient and reliable agent performance."
                },
                "authors": [
                    {
                        "name": "Kung-Hsiang Huang"
                    },
                    {
                        "name": "Haoyi Qiu"
                    },
                    {
                        "name": "Yutong Dai"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Chien-Sheng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chien-Sheng Wu"
                },
                "author": "Chien-Sheng Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25454v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25454v2",
                "updated": "2025-10-01T05:09:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    5,
                    9,
                    42,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-29T20:00:29Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    20,
                    0,
                    29,
                    0,
                    272,
                    0
                ],
                "title": "DeepSearch: Overcome the Bottleneck of Reinforcement Learning with\n  Verifiable Rewards via Monte Carlo Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSearch: Overcome the Bottleneck of Reinforcement Learning with\n  Verifiable Rewards via Monte Carlo Tree Search"
                },
                "summary": "Although RLVR has become an essential component for developing advanced\nreasoning skills in LLMs, contemporary studies have documented training\nplateaus that emerge following thousands of optimization steps, demonstrating\nnotable decreases in performance gains despite increased computational\ninvestment. This limitation stems from the sparse exploration patterns inherent\nin current RLVR practices, where models rely on limited rollouts that often\nmiss critical reasoning paths and fail to provide systematic coverage of the\nsolution space. We present DeepSearch, a framework that integrates Monte Carlo\nTree Search directly into RLVR training. In contrast to existing methods that\nrely on tree search only at inference, DeepSearch embeds structured search into\nthe training loop, enabling systematic exploration and fine-grained credit\nassignment across reasoning steps. Through training-time exploration,\nDeepSearch addresses the fundamental bottleneck of insufficient exploration,\nwhich leads to diminishing performance improvements over prolonged training\nsteps. Our contributions include: (1) a global frontier selection strategy that\nprioritizes promising nodes across the search tree, (2) selection with\nentropy-based guidance that identifies confident paths for supervision, and (3)\nadaptive replay buffer training with solution caching for efficiency.\nExperiments on mathematical reasoning benchmarks show that DeepSearch achieves\n62.95% average accuracy and establishes a new state-of-the-art for 1.5B\nreasoning models - using 5.7x fewer GPU hours than extended training\napproaches. These results highlight the importance of strategic exploration\nover brute-force scaling and demonstrate the promise of algorithmic innovation\nfor advancing RLVR methodologies. DeepSearch establishes a new direction for\nscaling reasoning capabilities through systematic search rather than prolonged\ncomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although RLVR has become an essential component for developing advanced\nreasoning skills in LLMs, contemporary studies have documented training\nplateaus that emerge following thousands of optimization steps, demonstrating\nnotable decreases in performance gains despite increased computational\ninvestment. This limitation stems from the sparse exploration patterns inherent\nin current RLVR practices, where models rely on limited rollouts that often\nmiss critical reasoning paths and fail to provide systematic coverage of the\nsolution space. We present DeepSearch, a framework that integrates Monte Carlo\nTree Search directly into RLVR training. In contrast to existing methods that\nrely on tree search only at inference, DeepSearch embeds structured search into\nthe training loop, enabling systematic exploration and fine-grained credit\nassignment across reasoning steps. Through training-time exploration,\nDeepSearch addresses the fundamental bottleneck of insufficient exploration,\nwhich leads to diminishing performance improvements over prolonged training\nsteps. Our contributions include: (1) a global frontier selection strategy that\nprioritizes promising nodes across the search tree, (2) selection with\nentropy-based guidance that identifies confident paths for supervision, and (3)\nadaptive replay buffer training with solution caching for efficiency.\nExperiments on mathematical reasoning benchmarks show that DeepSearch achieves\n62.95% average accuracy and establishes a new state-of-the-art for 1.5B\nreasoning models - using 5.7x fewer GPU hours than extended training\napproaches. These results highlight the importance of strategic exploration\nover brute-force scaling and demonstrate the promise of algorithmic innovation\nfor advancing RLVR methodologies. DeepSearch establishes a new direction for\nscaling reasoning capabilities through systematic search rather than prolonged\ncomputation."
                },
                "authors": [
                    {
                        "name": "Fang Wu"
                    },
                    {
                        "name": "Weihao Xuan"
                    },
                    {
                        "name": "Heli Qi"
                    },
                    {
                        "name": "Ximing Lu"
                    },
                    {
                        "name": "Aaron Tu"
                    },
                    {
                        "name": "Li Erran Li"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25454v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25454v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01290v1",
                "updated": "2025-10-01T04:09:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    4,
                    9,
                    2,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T04:09:02Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    4,
                    9,
                    2,
                    2,
                    274,
                    0
                ],
                "title": "ThinKV: Thought-Adaptive KV Cache Compression for Efficient Reasoning\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinKV: Thought-Adaptive KV Cache Compression for Efficient Reasoning\n  Models"
                },
                "summary": "The long-output context generation of large reasoning models enables extended\nchain of thought (CoT) but also drives rapid growth of the key-value (KV)\ncache, quickly overwhelming GPU memory. To address this challenge, we propose\nThinKV, a thought-adaptive KV cache compression framework. ThinKV is based on\nthe observation that attention sparsity reveals distinct thought types with\nvarying importance within the CoT. It applies a hybrid quantization-eviction\nstrategy, assigning token precision by thought importance and progressively\nevicting tokens from less critical thoughts as reasoning trajectories evolve.\nFurthermore, to implement ThinKV, we design a kernel that extends\nPagedAttention to enable efficient reuse of evicted tokens' memory slots,\neliminating compaction overheads. Extensive experiments on DeepSeek-R1-Distill,\nGPT-OSS, and NVIDIA AceReason across mathematics and coding benchmarks show\nthat ThinKV achieves near-lossless accuracy with less than 5% of the original\nKV cache, while improving performance with up to 5.8x higher inference\nthroughput over state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The long-output context generation of large reasoning models enables extended\nchain of thought (CoT) but also drives rapid growth of the key-value (KV)\ncache, quickly overwhelming GPU memory. To address this challenge, we propose\nThinKV, a thought-adaptive KV cache compression framework. ThinKV is based on\nthe observation that attention sparsity reveals distinct thought types with\nvarying importance within the CoT. It applies a hybrid quantization-eviction\nstrategy, assigning token precision by thought importance and progressively\nevicting tokens from less critical thoughts as reasoning trajectories evolve.\nFurthermore, to implement ThinKV, we design a kernel that extends\nPagedAttention to enable efficient reuse of evicted tokens' memory slots,\neliminating compaction overheads. Extensive experiments on DeepSeek-R1-Distill,\nGPT-OSS, and NVIDIA AceReason across mathematics and coding benchmarks show\nthat ThinKV achieves near-lossless accuracy with less than 5% of the original\nKV cache, while improving performance with up to 5.8x higher inference\nthroughput over state-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Akshat Ramachandran"
                    },
                    {
                        "name": "Marina Neseem"
                    },
                    {
                        "name": "Charbel Sakr"
                    },
                    {
                        "name": "Rangharajan Venkatesan"
                    },
                    {
                        "name": "Brucek Khailany"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01289v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01289v1",
                "updated": "2025-10-01T02:56:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    2,
                    56,
                    59,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T02:56:59Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    2,
                    56,
                    59,
                    2,
                    274,
                    0
                ],
                "title": "Detailed Derivation of the Scalar Explicit Expressions Governing the\n  Electric Field, Current Density, and Volumetric Power Density in the Four\n  Types of Linear Divergent MHD Channels Under a Unidirectional Applied\n  Magnetic Field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detailed Derivation of the Scalar Explicit Expressions Governing the\n  Electric Field, Current Density, and Volumetric Power Density in the Four\n  Types of Linear Divergent MHD Channels Under a Unidirectional Applied\n  Magnetic Field"
                },
                "summary": "The current study belongs to the field of applied mathematics in plasma\nphysics and electric power, where mathematical analysis of the algebraic\nequations governing the electric field vector, and the electric-current density\nfield vector within a Magnetohydrodynamic (MHD) linear two-dimensional\ndivergent supersonic channel is utilized to derive analytical expressions for\nthese important fields, as well as closed-form equations for the volumetric\npower density (output electric power per unit volume of the plasma channel).\nThe expressions presented here describe analytically the operation of the MHD\nchannel as an electric power source within an Open-Cycle Magnetohydrodynamic\n(OCMHD) generator. The four common types of the MHD linear channels are covered\nhere: namely, (1) continuous-electrode Faraday channel, (2) linear Hall\nchannel, (3) segmented-electrode Faraday channel, and (4) diagonal-electrode\nchannel. The mathematical results, their detailed derivation, and the companion\ngraphical illustrations aid in making a proper decision regarding which channel\ntype is the most suitable for a given application.Under typical operational\nconditions of 5 S/m plasma electric conductivity, 5 T magnetic field, and 2,000\nm/s plasma speed, as well as an optimized load factor of 0.5, we estimate the\nfollowing numerical values (unsigned magnitudes) for the continuous-electrode\nFaraday channel (with a Hall parameter of 1): useful electric field (across the\nexternal electric load): 5 kV/m, useful electric current-density (between the\nterminal electrodes within the channel): 12.5 kA/m2 , volumetric power density\n(dissipated by the load per unit volume of plasma): 62.5 MW/m3 , and electric\nefficiency (for the electric field or voltage): 50%. For the Halllinear channel\n(with a Hall parameter of 5), these quantitative performance values become25\nkV/m, 4.808 kA/m2, 120.19 MW/m3, and 46.30%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current study belongs to the field of applied mathematics in plasma\nphysics and electric power, where mathematical analysis of the algebraic\nequations governing the electric field vector, and the electric-current density\nfield vector within a Magnetohydrodynamic (MHD) linear two-dimensional\ndivergent supersonic channel is utilized to derive analytical expressions for\nthese important fields, as well as closed-form equations for the volumetric\npower density (output electric power per unit volume of the plasma channel).\nThe expressions presented here describe analytically the operation of the MHD\nchannel as an electric power source within an Open-Cycle Magnetohydrodynamic\n(OCMHD) generator. The four common types of the MHD linear channels are covered\nhere: namely, (1) continuous-electrode Faraday channel, (2) linear Hall\nchannel, (3) segmented-electrode Faraday channel, and (4) diagonal-electrode\nchannel. The mathematical results, their detailed derivation, and the companion\ngraphical illustrations aid in making a proper decision regarding which channel\ntype is the most suitable for a given application.Under typical operational\nconditions of 5 S/m plasma electric conductivity, 5 T magnetic field, and 2,000\nm/s plasma speed, as well as an optimized load factor of 0.5, we estimate the\nfollowing numerical values (unsigned magnitudes) for the continuous-electrode\nFaraday channel (with a Hall parameter of 1): useful electric field (across the\nexternal electric load): 5 kV/m, useful electric current-density (between the\nterminal electrodes within the channel): 12.5 kA/m2 , volumetric power density\n(dissipated by the load per unit volume of plasma): 62.5 MW/m3 , and electric\nefficiency (for the electric field or voltage): 50%. For the Halllinear channel\n(with a Hall parameter of 5), these quantitative performance values become25\nkV/m, 4.808 kA/m2, 120.19 MW/m3, and 46.30%."
                },
                "authors": [
                    {
                        "name": "Osama A. Marzouk"
                    }
                ],
                "author_detail": {
                    "name": "Osama A. Marzouk"
                },
                "author": "Osama A. Marzouk",
                "arxiv_doi": "10.37256/cm.6420256918",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.37256/cm.6420256918",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.01289v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01289v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "41 pages, 8 figures, 4 tables, published journal article,\n  peer-reviewed, open access",
                "arxiv_journal_ref": "Contemporary Mathematics. volume 6, issue 4, pages 4060-4100,\n  https://ojs.wiserpub.com/index.php/CM/article/view/6918 (2025)",
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "00A79, 03H10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02388v1",
                "updated": "2025-09-30T22:19:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    22,
                    19,
                    44,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T22:19:44Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    22,
                    19,
                    44,
                    1,
                    273,
                    0
                ],
                "title": "Learning to Route: A Rule-Driven Agent Framework for Hybrid-Source\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Route: A Rule-Driven Agent Framework for Hybrid-Source\n  Retrieval-Augmented Generation"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable performance on general\nQuestion Answering (QA), yet they often struggle in domain-specific scenarios\nwhere accurate and up-to-date information is required. Retrieval-Augmented\nGeneration (RAG) addresses this limitation by enriching LLMs with external\nknowledge, but existing systems primarily rely on unstructured documents, while\nlargely overlooking relational databases, which provide precise, timely, and\nefficiently queryable factual information, serving as indispensable\ninfrastructure in domains such as finance, healthcare, and scientific research.\nMotivated by this gap, we conduct a systematic analysis that reveals three\ncentral observations: (i) databases and documents offer complementary strengths\nacross queries, (ii) naively combining both sources introduces noise and cost\nwithout consistent accuracy gains, and (iii) selecting the most suitable source\nfor each query is crucial to balance effectiveness and efficiency. We further\nobserve that query types show consistent regularities in their alignment with\nretrieval paths, suggesting that routing decisions can be effectively guided by\nsystematic rules that capture these patterns. Building on these insights, we\npropose a rule-driven routing framework. A routing agent scores candidate\naugmentation paths based on explicit rules and selects the most suitable one; a\nrule-making expert agent refines the rules over time using QA feedback to\nmaintain adaptability; and a path-level meta-cache reuses past routing\ndecisions for semantically similar queries to reduce latency and cost.\nExperiments on three QA benchmarks demonstrate that our framework consistently\noutperforms static strategies and learned routing baselines, achieving higher\naccuracy while maintaining moderate computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable performance on general\nQuestion Answering (QA), yet they often struggle in domain-specific scenarios\nwhere accurate and up-to-date information is required. Retrieval-Augmented\nGeneration (RAG) addresses this limitation by enriching LLMs with external\nknowledge, but existing systems primarily rely on unstructured documents, while\nlargely overlooking relational databases, which provide precise, timely, and\nefficiently queryable factual information, serving as indispensable\ninfrastructure in domains such as finance, healthcare, and scientific research.\nMotivated by this gap, we conduct a systematic analysis that reveals three\ncentral observations: (i) databases and documents offer complementary strengths\nacross queries, (ii) naively combining both sources introduces noise and cost\nwithout consistent accuracy gains, and (iii) selecting the most suitable source\nfor each query is crucial to balance effectiveness and efficiency. We further\nobserve that query types show consistent regularities in their alignment with\nretrieval paths, suggesting that routing decisions can be effectively guided by\nsystematic rules that capture these patterns. Building on these insights, we\npropose a rule-driven routing framework. A routing agent scores candidate\naugmentation paths based on explicit rules and selects the most suitable one; a\nrule-making expert agent refines the rules over time using QA feedback to\nmaintain adaptability; and a path-level meta-cache reuses past routing\ndecisions for semantically similar queries to reduce latency and cost.\nExperiments on three QA benchmarks demonstrate that our framework consistently\noutperforms static strategies and learned routing baselines, achieving higher\naccuracy while maintaining moderate computational cost."
                },
                "authors": [
                    {
                        "name": "Haoyue Bai"
                    },
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Shengyu Chen"
                    },
                    {
                        "name": "Zhengzhang Chen"
                    },
                    {
                        "name": "Lu-An Tang"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Haifeng Chen"
                    },
                    {
                        "name": "Yanjie Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yanjie Fu"
                },
                "author": "Yanjie Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00294v1",
                "updated": "2025-09-30T21:28:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    21,
                    28,
                    4,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T21:28:04Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    21,
                    28,
                    4,
                    1,
                    273,
                    0
                ],
                "title": "Free Draft-and-Verification: Toward Lossless Parallel Decoding for\n  Diffusion Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Free Draft-and-Verification: Toward Lossless Parallel Decoding for\n  Diffusion Large Language Models"
                },
                "summary": "Diffusion Large Language Models (DLLMs) have emerged as a new paradigm of\nlanguage modeling beyond autoregressive next-token prediction. Thanks to their\nbidirectional attention mechanism, DLLMs are more capable of capturing the\nconnection of context, and thus show unique advantages in challenges like the\nfamous \"reversal curse\" or learning under data-constrained scenarios. However,\nthis bidirectional nature also brings an obstacle that DLLMs are not inherently\ncompatible with KV Cache, and consequently, the inference efficiency is not\ncompetitive compared with autoregressive models. Taking advantage of their\ninherent capability of multi-token prediction, existing parallel decoding\nalgorithms can speed up the DLLM inference, but at the cost of non-negligible\nperformance degradation. To overcome this challenge, we introduce Free\nDraft-and-Verification (Freedave), a novel fast sampling algorithm tailored for\nDLLMs that achieves lossless parallel decoding. Specifically, we propose a\npipeline of parallel-decoded candidate generation and verification, which is\nguaranteed to reproduce the same sequence generated by static sampling, without\nintroducing extra model forward calls. By applying Freedave, the throughput of\nDLLMs can be boosted up to $2.8\\times$ without performance degradation on math\nreasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Large Language Models (DLLMs) have emerged as a new paradigm of\nlanguage modeling beyond autoregressive next-token prediction. Thanks to their\nbidirectional attention mechanism, DLLMs are more capable of capturing the\nconnection of context, and thus show unique advantages in challenges like the\nfamous \"reversal curse\" or learning under data-constrained scenarios. However,\nthis bidirectional nature also brings an obstacle that DLLMs are not inherently\ncompatible with KV Cache, and consequently, the inference efficiency is not\ncompetitive compared with autoregressive models. Taking advantage of their\ninherent capability of multi-token prediction, existing parallel decoding\nalgorithms can speed up the DLLM inference, but at the cost of non-negligible\nperformance degradation. To overcome this challenge, we introduce Free\nDraft-and-Verification (Freedave), a novel fast sampling algorithm tailored for\nDLLMs that achieves lossless parallel decoding. Specifically, we propose a\npipeline of parallel-decoded candidate generation and verification, which is\nguaranteed to reproduce the same sequence generated by static sampling, without\nintroducing extra model forward calls. By applying Freedave, the throughput of\nDLLMs can be boosted up to $2.8\\times$ without performance degradation on math\nreasoning tasks."
                },
                "authors": [
                    {
                        "name": "Shutong Wu"
                    },
                    {
                        "name": "Jiawei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Zhang"
                },
                "author": "Jiawei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00231v1",
                "updated": "2025-09-30T19:55:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    19,
                    55,
                    26,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T19:55:26Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    19,
                    55,
                    26,
                    1,
                    273,
                    0
                ],
                "title": "The Pitfalls of KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Pitfalls of KV Cache Compression"
                },
                "summary": "KV cache compression promises increased throughput and efficiency with\nnegligible loss in performance. While the gains in throughput are indisputable\nand recent literature has indeed shown minimal degradation on particular\nbenchmarks, in general the consequences of compression in realistic scenarios\nsuch as multi-instruction prompting have been insufficiently studied. In this\npaper, we identify several pitfalls practitioners should be aware of when\ndeploying KV cache compressed LLMs. Importantly, we show that certain\ninstructions degrade much more rapidly with compression, effectively causing\nthem to be completely ignored by the LLM. As a practical example of that, we\nhighlight system prompt leakage as a case study, empirically showing the impact\nof compression on leakage and general instruction following. We show several\nfactors that play a role in prompt leakage: compression method, instruction\norder, and KV eviction bias. We then propose simple changes to KV cache\neviction policies that can reduce the impact of these factors and improve the\noverall performance in multi-instruction tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache compression promises increased throughput and efficiency with\nnegligible loss in performance. While the gains in throughput are indisputable\nand recent literature has indeed shown minimal degradation on particular\nbenchmarks, in general the consequences of compression in realistic scenarios\nsuch as multi-instruction prompting have been insufficiently studied. In this\npaper, we identify several pitfalls practitioners should be aware of when\ndeploying KV cache compressed LLMs. Importantly, we show that certain\ninstructions degrade much more rapidly with compression, effectively causing\nthem to be completely ignored by the LLM. As a practical example of that, we\nhighlight system prompt leakage as a case study, empirically showing the impact\nof compression on leakage and general instruction following. We show several\nfactors that play a role in prompt leakage: compression method, instruction\norder, and KV eviction bias. We then propose simple changes to KV cache\neviction policies that can reduce the impact of these factors and improve the\noverall performance in multi-instruction tasks."
                },
                "authors": [
                    {
                        "name": "Alex Chen"
                    },
                    {
                        "name": "Renato Geh"
                    },
                    {
                        "name": "Aditya Grover"
                    },
                    {
                        "name": "Guy Van den Broeck"
                    },
                    {
                        "name": "Daniel Israel"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Israel"
                },
                "author": "Daniel Israel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00184v1",
                "updated": "2025-09-30T19:03:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    19,
                    3,
                    26,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T19:03:26Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    19,
                    3,
                    26,
                    1,
                    273,
                    0
                ],
                "title": "Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals\n  Long-Range Dependency Pitfalls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals\n  Long-Range Dependency Pitfalls"
                },
                "summary": "Language models are increasingly capable, yet still fail at a seemingly\nsimple task of multi-digit multiplication. In this work, we study why, by\nreverse-engineering a model that successfully learns multiplication via\n\\emph{implicit chain-of-thought}, and report three findings: (1) Evidence of\nlong-range structure: Logit attributions and linear probes indicate that the\nmodel encodes the necessary long-range dependencies for multi-digit\nmultiplication. (2) Mechanism: the model encodes long-range dependencies using\nattention to construct a directed acyclic graph to ``cache'' and ``retrieve''\npairwise partial products. (3) Geometry: the model implements partial products\nin attention heads by forming Minkowski sums between pairs of digits, and\ndigits are represented using a Fourier basis, both of which are intuitive and\nefficient representations that the standard fine-tuning model lacks. With these\ninsights, we revisit the learning dynamics of standard fine-tuning and find\nthat the model converges to a local optimum that lacks the required long-range\ndependencies. We further validate this understanding by introducing an\nauxiliary loss that predicts the ``running sum'' via a linear regression probe,\nwhich provides an inductive bias that enables the model to successfully learn\nmulti-digit multiplication. In summary, by reverse-engineering the mechanisms\nof an implicit chain-of-thought model we uncover a pitfall for learning\nlong-range dependencies in Transformers and provide an example of how the\ncorrect inductive bias can address this issue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models are increasingly capable, yet still fail at a seemingly\nsimple task of multi-digit multiplication. In this work, we study why, by\nreverse-engineering a model that successfully learns multiplication via\n\\emph{implicit chain-of-thought}, and report three findings: (1) Evidence of\nlong-range structure: Logit attributions and linear probes indicate that the\nmodel encodes the necessary long-range dependencies for multi-digit\nmultiplication. (2) Mechanism: the model encodes long-range dependencies using\nattention to construct a directed acyclic graph to ``cache'' and ``retrieve''\npairwise partial products. (3) Geometry: the model implements partial products\nin attention heads by forming Minkowski sums between pairs of digits, and\ndigits are represented using a Fourier basis, both of which are intuitive and\nefficient representations that the standard fine-tuning model lacks. With these\ninsights, we revisit the learning dynamics of standard fine-tuning and find\nthat the model converges to a local optimum that lacks the required long-range\ndependencies. We further validate this understanding by introducing an\nauxiliary loss that predicts the ``running sum'' via a linear regression probe,\nwhich provides an inductive bias that enables the model to successfully learn\nmulti-digit multiplication. In summary, by reverse-engineering the mechanisms\nof an implicit chain-of-thought model we uncover a pitfall for learning\nlong-range dependencies in Transformers and provide an example of how the\ncorrect inductive bias can address this issue."
                },
                "authors": [
                    {
                        "name": "Xiaoyan Bai"
                    },
                    {
                        "name": "Itamar Pres"
                    },
                    {
                        "name": "Yuntian Deng"
                    },
                    {
                        "name": "Chenhao Tan"
                    },
                    {
                        "name": "Stuart Shieber"
                    },
                    {
                        "name": "Fernanda ViÃ©gas"
                    },
                    {
                        "name": "Martin Wattenberg"
                    },
                    {
                        "name": "Andrew Lee"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Lee"
                },
                "author": "Andrew Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26541v1",
                "updated": "2025-09-30T17:15:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    15,
                    27,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:15:27Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    15,
                    27,
                    1,
                    273,
                    0
                ],
                "title": "TASP: Topology-aware Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TASP: Topology-aware Sequence Parallelism"
                },
                "summary": "Long-context large language models (LLMs) face constraints due to the\nquadratic complexity of the self-attention mechanism. The mainstream sequence\nparallelism (SP) method, Ring Attention, attempts to solve this by distributing\nthe query into multiple query chunks across accelerators and enable each Q\ntensor to access all KV tensors from other accelerators via the Ring AllGather\ncommunication primitive. However, it exhibits low communication efficiency,\nrestricting its practical applicability. This inefficiency stems from the\nmismatch between the Ring AllGather communication primitive it adopts and the\nAlltoAll topology of modern accelerators. A Ring AllGather primitive is\ncomposed of iterations of ring-styled data transfer, which can only utilize a\nvery limited fraction of an AlltoAll topology.\n  Inspired by the Hamiltonian decomposition of complete directed graphs, we\nidentify that modern accelerator topology can be decomposed into multiple\northogonal ring datapaths which can concurrently transfer data without\ninterference. Based on this, we further observe that the Ring AllGather\nprimitive can also be decomposed into the same number of concurrent ring-styled\ndata transfer at every iteration. Based on these insights, we propose TASP, a\ntopology-aware SP method for long-context LLMs that fully utilizes the\ncommunication capacity of modern accelerators via topology decomposition and\nprimitive decomposition. Experimental results on both single-node and\nmulti-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate\nthat TASP achieves higher communication efficiency than Ring Attention on these\nmodern accelerator topologies and achieves up to 3.58 speedup than Ring\nAttention and its variant Zigzag-Ring Attention. The code is available at\nhttps://github.com/infinigence/HamiltonAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context large language models (LLMs) face constraints due to the\nquadratic complexity of the self-attention mechanism. The mainstream sequence\nparallelism (SP) method, Ring Attention, attempts to solve this by distributing\nthe query into multiple query chunks across accelerators and enable each Q\ntensor to access all KV tensors from other accelerators via the Ring AllGather\ncommunication primitive. However, it exhibits low communication efficiency,\nrestricting its practical applicability. This inefficiency stems from the\nmismatch between the Ring AllGather communication primitive it adopts and the\nAlltoAll topology of modern accelerators. A Ring AllGather primitive is\ncomposed of iterations of ring-styled data transfer, which can only utilize a\nvery limited fraction of an AlltoAll topology.\n  Inspired by the Hamiltonian decomposition of complete directed graphs, we\nidentify that modern accelerator topology can be decomposed into multiple\northogonal ring datapaths which can concurrently transfer data without\ninterference. Based on this, we further observe that the Ring AllGather\nprimitive can also be decomposed into the same number of concurrent ring-styled\ndata transfer at every iteration. Based on these insights, we propose TASP, a\ntopology-aware SP method for long-context LLMs that fully utilizes the\ncommunication capacity of modern accelerators via topology decomposition and\nprimitive decomposition. Experimental results on both single-node and\nmulti-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate\nthat TASP achieves higher communication efficiency than Ring Attention on these\nmodern accelerator topologies and achieves up to 3.58 speedup than Ring\nAttention and its variant Zigzag-Ring Attention. The code is available at\nhttps://github.com/infinigence/HamiltonAttention."
                },
                "authors": [
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ke Hong"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Wenxun Wang"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "arxiv_affiliation": "Tsinghua University",
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23666v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23666v2",
                "updated": "2025-09-30T16:42:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    42,
                    50,
                    1,
                    273,
                    0
                ],
                "published": "2025-05-29T17:12:42Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    12,
                    42,
                    3,
                    149,
                    0
                ],
                "title": "LoLA: Low-Rank Linear Attention With Sparse Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoLA: Low-Rank Linear Attention With Sparse Caching"
                },
                "summary": "The per-token cost of transformer inference scales with context length,\npreventing its application to lifelong in-context learning. Linear attention is\nan efficient alternative that maintains a constant memory footprint, even on\ninfinite context lengths. While this is a potential candidate for lifelong\nlearning, it falls short in memory capacity. In this paper, we propose LoLA, a\ntraining-free augmentation to linear attention that boosts associative recall.\nLoLA distributes past key-value pairs from context into three memory systems:\n(i) recent pairs in a local sliding window cache; (ii) difficult-to-memorize\npairs in a sparse, global cache; and (iii) generic pairs in the recurrent\nhidden state of linear attention. We show through ablations that our\nself-recall error metric is crucial to efficiently manage long-term associative\nmemories. On pass-key retrieval tasks, LoLA improves the base model's\nperformance from 0.6% to 97.4% accuracy. This is achieved with a 4.6x smaller\ncache than Llama-3.1 8B on 4K context length. LoLA also outperforms other 1B\nand 8B parameter subquadratic models on zero-shot commonsense reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The per-token cost of transformer inference scales with context length,\npreventing its application to lifelong in-context learning. Linear attention is\nan efficient alternative that maintains a constant memory footprint, even on\ninfinite context lengths. While this is a potential candidate for lifelong\nlearning, it falls short in memory capacity. In this paper, we propose LoLA, a\ntraining-free augmentation to linear attention that boosts associative recall.\nLoLA distributes past key-value pairs from context into three memory systems:\n(i) recent pairs in a local sliding window cache; (ii) difficult-to-memorize\npairs in a sparse, global cache; and (iii) generic pairs in the recurrent\nhidden state of linear attention. We show through ablations that our\nself-recall error metric is crucial to efficiently manage long-term associative\nmemories. On pass-key retrieval tasks, LoLA improves the base model's\nperformance from 0.6% to 97.4% accuracy. This is achieved with a 4.6x smaller\ncache than Llama-3.1 8B on 4K context length. LoLA also outperforms other 1B\nand 8B parameter subquadratic models on zero-shot commonsense reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Luke McDermott"
                    },
                    {
                        "name": "Robert W. Heath Jr."
                    },
                    {
                        "name": "Rahul Parhi"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Parhi"
                },
                "author": "Rahul Parhi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23666v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23666v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18250v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18250v3",
                "updated": "2025-09-30T15:44:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    44,
                    29,
                    1,
                    273,
                    0
                ],
                "published": "2025-08-25T17:41:13Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    41,
                    13,
                    0,
                    237,
                    0
                ],
                "title": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study"
                },
                "summary": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM."
                },
                "authors": [
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Fernando GarcÃ­a-Redondo"
                    },
                    {
                        "name": "Arvind Sharma"
                    },
                    {
                        "name": "Van Dai Nguyen"
                    },
                    {
                        "name": "Andrea Fantini"
                    },
                    {
                        "name": "Philippe Matagne"
                    },
                    {
                        "name": "Siddharth Rao"
                    },
                    {
                        "name": "Subhali Subhechha"
                    },
                    {
                        "name": "Lynn Verschueren"
                    },
                    {
                        "name": "Mohammed Aftab Baig"
                    },
                    {
                        "name": "Marie Garcia Bardon"
                    },
                    {
                        "name": "Geert Hellings"
                    }
                ],
                "author_detail": {
                    "name": "Geert Hellings"
                },
                "author": "Geert Hellings",
                "arxiv_doi": "10.1109/TED.2025.3617043",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TED.2025.3617043",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.18250v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18250v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to IEEE Trans. Elec. Dev. Work enabled in part by NanoIC\n  pilot line; acquisition and operation jointly funded by Chips Joint\n  Undertaking, through EU's Digital Europe (101183266) and Horizon Europe\n  programs (101183277), as well as by the participating states\n  (Belgium-Flanders, France, Germany, Finland, Ireland, Romania)",
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26328v1",
                "updated": "2025-09-30T14:40:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    40,
                    18,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T14:40:18Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    40,
                    18,
                    1,
                    273,
                    0
                ],
                "title": "Fast-dLLM v2: Efficient Block-Diffusion LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast-dLLM v2: Efficient Block-Diffusion LLM"
                },
                "summary": "Autoregressive (AR) large language models (LLMs) have achieved remarkable\nperformance across a wide range of natural language tasks, yet their inherent\nsequential decoding limits inference efficiency. In this work, we propose\nFast-dLLM v2, a carefully designed block diffusion language model (dLLM) that\nefficiently adapts pretrained AR models into dLLMs for parallel text\ngeneration, requiring only approximately 1B tokens of fine-tuning. This\nrepresents a 500x reduction in training data compared to full-attention\ndiffusion LLMs such as Dream (580B tokens), while preserving the original\nmodel's performance. Our approach introduces a novel training recipe that\ncombines a block diffusion mechanism with a complementary attention mask,\nenabling blockwise bidirectional context modeling without sacrificing AR\ntraining objectives. To further accelerate decoding, we design a hierarchical\ncaching mechanism: a block-level cache that stores historical context\nrepresentations across blocks, and a sub-block cache that enables efficient\nparallel generation within partially decoded blocks. Coupled with our parallel\ndecoding pipeline, Fast-dLLM v2 achieves up to 2.5x speedup over standard AR\ndecoding without compromising generation quality. Extensive experiments across\ndiverse benchmarks demonstrate that Fast-dLLM v2 matches or surpasses AR\nbaselines in accuracy, while delivering state-of-the-art efficiency among dLLMs\n- marking a significant step toward the practical deployment of fast and\naccurate LLMs. Code and model will be publicly released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) large language models (LLMs) have achieved remarkable\nperformance across a wide range of natural language tasks, yet their inherent\nsequential decoding limits inference efficiency. In this work, we propose\nFast-dLLM v2, a carefully designed block diffusion language model (dLLM) that\nefficiently adapts pretrained AR models into dLLMs for parallel text\ngeneration, requiring only approximately 1B tokens of fine-tuning. This\nrepresents a 500x reduction in training data compared to full-attention\ndiffusion LLMs such as Dream (580B tokens), while preserving the original\nmodel's performance. Our approach introduces a novel training recipe that\ncombines a block diffusion mechanism with a complementary attention mask,\nenabling blockwise bidirectional context modeling without sacrificing AR\ntraining objectives. To further accelerate decoding, we design a hierarchical\ncaching mechanism: a block-level cache that stores historical context\nrepresentations across blocks, and a sub-block cache that enables efficient\nparallel generation within partially decoded blocks. Coupled with our parallel\ndecoding pipeline, Fast-dLLM v2 achieves up to 2.5x speedup over standard AR\ndecoding without compromising generation quality. Extensive experiments across\ndiverse benchmarks demonstrate that Fast-dLLM v2 matches or surpasses AR\nbaselines in accuracy, while delivering state-of-the-art efficiency among dLLMs\n- marking a significant step toward the practical deployment of fast and\naccurate LLMs. Code and model will be publicly released."
                },
                "authors": [
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Shuchen Xue"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07966v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07966v4",
                "updated": "2025-09-30T14:13:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    13,
                    20,
                    1,
                    273,
                    0
                ],
                "published": "2025-07-10T17:47:40Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    47,
                    40,
                    3,
                    191,
                    0
                ],
                "title": "Scaling RL to Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling RL to Long Videos"
                },
                "summary": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B\nsupports processing up to 8,192 video frames per video, and configurable FPS\nsettings. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B\nsupports processing up to 8,192 video frames per video, and configurable FPS\nsettings. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames)."
                },
                "authors": [
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Baifeng Shi"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Hanrong Ye"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Xiaojuan Qi"
                    },
                    {
                        "name": "Sifei Liu"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Accepted by NeurIPS 2025. Code at https://github.com/NVlabs/Long-RL\n  and model at https://huggingface.co/Efficient-Large-Model/LongVILA-R1-7B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07966v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07966v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17139v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17139v2",
                "updated": "2025-09-30T09:10:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    9,
                    10,
                    26,
                    1,
                    273,
                    0
                ],
                "published": "2025-02-24T13:30:30Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    30,
                    30,
                    0,
                    55,
                    0
                ],
                "title": "FastCoder: Accelerating Repository-level Code Generation via Efficient\n  Retrieval and Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCoder: Accelerating Repository-level Code Generation via Efficient\n  Retrieval and Verification"
                },
                "summary": "Code generation is a latency-sensitive task that demands high timeliness.\nHowever, with the growing interest and inherent difficulty in repository-level\ncode generation, most existing code generation studies focus on improving the\ncorrectness of generated code while overlooking the inference efficiency, which\nis substantially affected by the overhead during LLM generation. Although there\nhas been work on accelerating LLM inference, these approaches are not tailored\nto the specific characteristics of code generation; instead, they treat code\nthe same as natural language sequences and ignore its unique syntax and\nsemantic characteristics, which are also crucial for improving efficiency.\nConsequently, these approaches exhibit limited effectiveness in code generation\ntasks, particularly for repository-level scenarios with considerable complexity\nand difficulty. To alleviate this issue, following draft-verification paradigm,\nwe propose FastCoder, a simple yet highly efficient inference acceleration\napproach specifically designed for code generation, without compromising the\nquality of the output. FastCoder constructs a multi-source datastore, providing\naccess to both general and project-specific knowledge, facilitating the\nretrieval of high-quality draft sequences. Moreover, FastCoder reduces the\nretrieval cost by controlling retrieval timing, and enhances efficiency through\nparallel retrieval and a context- and LLM preference-aware cache. Experimental\nresults show that FastCoder can reach up to 2.53x and 2.54x speedup compared to\nautoregressive decoding in repository-level and standalone code generation\ntasks, respectively, outperforming state-of-the-art inference acceleration\napproaches by up to 88%. FastCoder can also be integrated with existing\ncorrectness-focused code generation approaches to accelerate the LLM generation\nprocess, and reach a speedup exceeding 2.6x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code generation is a latency-sensitive task that demands high timeliness.\nHowever, with the growing interest and inherent difficulty in repository-level\ncode generation, most existing code generation studies focus on improving the\ncorrectness of generated code while overlooking the inference efficiency, which\nis substantially affected by the overhead during LLM generation. Although there\nhas been work on accelerating LLM inference, these approaches are not tailored\nto the specific characteristics of code generation; instead, they treat code\nthe same as natural language sequences and ignore its unique syntax and\nsemantic characteristics, which are also crucial for improving efficiency.\nConsequently, these approaches exhibit limited effectiveness in code generation\ntasks, particularly for repository-level scenarios with considerable complexity\nand difficulty. To alleviate this issue, following draft-verification paradigm,\nwe propose FastCoder, a simple yet highly efficient inference acceleration\napproach specifically designed for code generation, without compromising the\nquality of the output. FastCoder constructs a multi-source datastore, providing\naccess to both general and project-specific knowledge, facilitating the\nretrieval of high-quality draft sequences. Moreover, FastCoder reduces the\nretrieval cost by controlling retrieval timing, and enhances efficiency through\nparallel retrieval and a context- and LLM preference-aware cache. Experimental\nresults show that FastCoder can reach up to 2.53x and 2.54x speedup compared to\nautoregressive decoding in repository-level and standalone code generation\ntasks, respectively, outperforming state-of-the-art inference acceleration\napproaches by up to 88%. FastCoder can also be integrated with existing\ncorrectness-focused code generation approaches to accelerate the LLM generation\nprocess, and reach a speedup exceeding 2.6x."
                },
                "authors": [
                    {
                        "name": "Qianhui Zhao"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Xiaoli Lian"
                    },
                    {
                        "name": "Qiaoyuanhe Meng"
                    },
                    {
                        "name": "Ziqian Jiao"
                    },
                    {
                        "name": "Zetong Zhou"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Lin Shi"
                    }
                ],
                "author_detail": {
                    "name": "Lin Shi"
                },
                "author": "Lin Shi",
                "arxiv_comment": "Accepted by ASE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17139v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17139v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23416v2",
                "updated": "2025-09-30T02:51:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    2,
                    51,
                    5,
                    1,
                    273,
                    0
                ],
                "published": "2025-05-29T13:05:47Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    13,
                    5,
                    47,
                    3,
                    149,
                    0
                ],
                "title": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction"
                },
                "summary": "Transformer-based large language models (LLMs) cache context as key-value\n(KV) pairs during inference. As context length grows, KV cache sizes expand,\nleading to substantial memory overhead and increased attention latency. This\npaper introduces KVzip, a query-agnostic KV cache eviction method enabling\neffective reuse of compressed KV caches across diverse queries. KVzip\nquantifies the importance of a KV pair using the underlying LLM to reconstruct\noriginal contexts from cached KV pairs, subsequently evicting pairs with lower\nimportance. Extensive empirical evaluations demonstrate that KVzip reduces KV\ncache size by $3$-$4\\times$ and FlashAttention decoding latency by\napproximately $2\\times$, with negligible performance loss in\nquestion-answering, retrieval, reasoning, and code comprehension tasks.\nEvaluations include various models such as LLaMA3.1, Qwen2.5, and Gemma3, with\ncontext lengths reaching up to 170K tokens. KVzip significantly outperforms\nexisting query-aware KV eviction methods, which suffer from performance\ndegradation even at a 90% cache budget ratio under multi-query scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) cache context as key-value\n(KV) pairs during inference. As context length grows, KV cache sizes expand,\nleading to substantial memory overhead and increased attention latency. This\npaper introduces KVzip, a query-agnostic KV cache eviction method enabling\neffective reuse of compressed KV caches across diverse queries. KVzip\nquantifies the importance of a KV pair using the underlying LLM to reconstruct\noriginal contexts from cached KV pairs, subsequently evicting pairs with lower\nimportance. Extensive empirical evaluations demonstrate that KVzip reduces KV\ncache size by $3$-$4\\times$ and FlashAttention decoding latency by\napproximately $2\\times$, with negligible performance loss in\nquestion-answering, retrieval, reasoning, and code comprehension tasks.\nEvaluations include various models such as LLaMA3.1, Qwen2.5, and Gemma3, with\ncontext lengths reaching up to 170K tokens. KVzip significantly outperforms\nexisting query-aware KV eviction methods, which suffer from performance\ndegradation even at a 90% cache budget ratio under multi-query scenarios."
                },
                "authors": [
                    {
                        "name": "Jang-Hyun Kim"
                    },
                    {
                        "name": "Jinuk Kim"
                    },
                    {
                        "name": "Sangwoo Kwon"
                    },
                    {
                        "name": "Jae W. Lee"
                    },
                    {
                        "name": "Sangdoo Yun"
                    },
                    {
                        "name": "Hyun Oh Song"
                    }
                ],
                "author_detail": {
                    "name": "Hyun Oh Song"
                },
                "author": "Hyun Oh Song",
                "arxiv_comment": "NeurIPS 2025 Oral. Code: https://github.com/snu-mllab/KVzip",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25681v1",
                "updated": "2025-09-30T02:36:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    2,
                    36,
                    11,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T02:36:11Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    2,
                    36,
                    11,
                    1,
                    273,
                    0
                ],
                "title": "dVLA: Diffusion Vision-Language-Action Model with Multimodal\n  Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "dVLA: Diffusion Vision-Language-Action Model with Multimodal\n  Chain-of-Thought"
                },
                "summary": "Vision-Language-Action (VLA) models are emerging as a next-generation\nparadigm for robotics. We introduce dVLA, a diffusion-based VLA that leverages\na multimodal chain-of-thought to unify visual perception, language reasoning,\nand robotic control in a single system. dVLA jointly optimizes perception,\nlanguage understanding, and action under a single diffusion objective, enabling\nstronger cross-modal reasoning and better generalization to novel instructions\nand objects. For practical deployment, we mitigate inference latency by\nincorporating two acceleration strategies, a prefix attention mask and KV\ncaching, yielding up to around times speedup at test-time inference. We\nevaluate dVLA in both simulation and the real world: on the LIBERO benchmark,\nit achieves state-of-the-art performance with a 96.4% average success rate,\nconsistently surpassing both discrete and continuous action policies; on a real\nFranka robot, it succeeds across a diverse task suite, including a challenging\nbin-picking task that requires multi-step planning, demonstrating robust\nreal-world performance. Together, these results underscore the promise of\nunified diffusion frameworks for practical, high-performance VLA robotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models are emerging as a next-generation\nparadigm for robotics. We introduce dVLA, a diffusion-based VLA that leverages\na multimodal chain-of-thought to unify visual perception, language reasoning,\nand robotic control in a single system. dVLA jointly optimizes perception,\nlanguage understanding, and action under a single diffusion objective, enabling\nstronger cross-modal reasoning and better generalization to novel instructions\nand objects. For practical deployment, we mitigate inference latency by\nincorporating two acceleration strategies, a prefix attention mask and KV\ncaching, yielding up to around times speedup at test-time inference. We\nevaluate dVLA in both simulation and the real world: on the LIBERO benchmark,\nit achieves state-of-the-art performance with a 96.4% average success rate,\nconsistently surpassing both discrete and continuous action policies; on a real\nFranka robot, it succeeds across a diverse task suite, including a challenging\nbin-picking task that requires multi-step planning, demonstrating robust\nreal-world performance. Together, these results underscore the promise of\nunified diffusion frameworks for practical, high-performance VLA robotics."
                },
                "authors": [
                    {
                        "name": "Junjie Wen"
                    },
                    {
                        "name": "Minjie Zhu"
                    },
                    {
                        "name": "Jiaming Liu"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Yicun Yang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Shanghang Zhang"
                    },
                    {
                        "name": "Yichen Zhu"
                    },
                    {
                        "name": "Yi Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Xu"
                },
                "author": "Yi Xu",
                "arxiv_comment": "technique report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25401v1",
                "updated": "2025-09-29T18:57:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    18,
                    57,
                    14,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T18:57:14Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    18,
                    57,
                    14,
                    0,
                    272,
                    0
                ],
                "title": "FlashOmni: A Unified Sparse Attention Engine for Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashOmni: A Unified Sparse Attention Engine for Diffusion Transformers"
                },
                "summary": "Multi-Modal Diffusion Transformers (DiTs) demonstrate exceptional\ncapabilities in visual synthesis, yet their deployment remains constrained by\nsubstantial computational demands. To alleviate this bottleneck, many\nsparsity-based acceleration methods have been proposed. However, their diverse\nsparsity patterns often require customized kernels for high-performance\ninference, limiting universality. We propose FlashOmni, a unified sparse\nattention engine compatible with arbitrary DiT architectures. FlashOmni\nintroduces flexible sparse symbols to standardize the representation of a wide\nrange of sparsity strategies, such as feature caching and block-sparse\nskipping. This unified abstraction enables the execution of diverse sparse\ncomputations within a single attention kernel. In addition, FlashOmni designs\noptimized sparse GEMMs for attention blocks, leveraging sparse symbols to\neliminate redundant computations and further improve efficiency. Experiments\ndemonstrate that FlashOmni delivers near-linear, closely matching the sparsity\nratio speedup (1:1) in attention and GEMM-$Q$, and achieves\n2.5$\\times$-3.8$\\times$ acceleration in GEMM-$O$ (max peaking at about 87.5% of\nthe theoretical limit). Applied with a multi-granularity sparsity strategy, it\nenables the Hunyuan model (33K) to achieve about 1.5$\\times$ end-to-end\nacceleration without degrading visual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Modal Diffusion Transformers (DiTs) demonstrate exceptional\ncapabilities in visual synthesis, yet their deployment remains constrained by\nsubstantial computational demands. To alleviate this bottleneck, many\nsparsity-based acceleration methods have been proposed. However, their diverse\nsparsity patterns often require customized kernels for high-performance\ninference, limiting universality. We propose FlashOmni, a unified sparse\nattention engine compatible with arbitrary DiT architectures. FlashOmni\nintroduces flexible sparse symbols to standardize the representation of a wide\nrange of sparsity strategies, such as feature caching and block-sparse\nskipping. This unified abstraction enables the execution of diverse sparse\ncomputations within a single attention kernel. In addition, FlashOmni designs\noptimized sparse GEMMs for attention blocks, leveraging sparse symbols to\neliminate redundant computations and further improve efficiency. Experiments\ndemonstrate that FlashOmni delivers near-linear, closely matching the sparsity\nratio speedup (1:1) in attention and GEMM-$Q$, and achieves\n2.5$\\times$-3.8$\\times$ acceleration in GEMM-$O$ (max peaking at about 87.5% of\nthe theoretical limit). Applied with a multi-granularity sparsity strategy, it\nenables the Hunyuan model (33K) to achieve about 1.5$\\times$ end-to-end\nacceleration without degrading visual quality."
                },
                "authors": [
                    {
                        "name": "Liang Qiao"
                    },
                    {
                        "name": "Yue Dai"
                    },
                    {
                        "name": "Yeqi Huang"
                    },
                    {
                        "name": "Hongyu Kan"
                    },
                    {
                        "name": "Jun Shi"
                    },
                    {
                        "name": "Hong An"
                    }
                ],
                "author_detail": {
                    "name": "Hong An"
                },
                "author": "Hong An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25155v1",
                "updated": "2025-09-29T17:55:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    55,
                    43,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:55:43Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    55,
                    43,
                    0,
                    272,
                    0
                ],
                "title": "Context-Driven Performance Modeling for Causal Inference Operators on\n  Neural Processing Units",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Driven Performance Modeling for Causal Inference Operators on\n  Neural Processing Units"
                },
                "summary": "The proliferation of large language models (LLMs) has driven demand for long\ncontext inference on resource constrained edge devices. However, deploying\nthese models on Neural Processing Units (NPUs) presents significant challenges\ndue to the architectural mismatch: quadratic complexity of standard attention\nmechanisms conflicts with memory and compute patterns of edge accelerators.\nThis paper presents a comprehensive performance analysis of various causal\ninference operators on a modern NPU. We benchmark standard quadratic attention\nagainst several sub-quadratic alternatives, including structured state-space\nand linear attention models. Our analysis reveals that while sub-quadratic\nmethods offer superior scalability, they introduce distinct computational\nbottlenecks on the NPU's specialized execution units. We identify that\nquadratic attention becomes severely memory-bound, suffering from cache\ninefficiency and pipeline stalls exceeding 95% at long contexts. In contrast,\nsub-quadratic models can become compute-bound on programmable vector cores.\nThese findings provide critical insights for the co-design of hardware-aware\nmodels and optimization strategies to enable on-device AI inference with\nlong-contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of large language models (LLMs) has driven demand for long\ncontext inference on resource constrained edge devices. However, deploying\nthese models on Neural Processing Units (NPUs) presents significant challenges\ndue to the architectural mismatch: quadratic complexity of standard attention\nmechanisms conflicts with memory and compute patterns of edge accelerators.\nThis paper presents a comprehensive performance analysis of various causal\ninference operators on a modern NPU. We benchmark standard quadratic attention\nagainst several sub-quadratic alternatives, including structured state-space\nand linear attention models. Our analysis reveals that while sub-quadratic\nmethods offer superior scalability, they introduce distinct computational\nbottlenecks on the NPU's specialized execution units. We identify that\nquadratic attention becomes severely memory-bound, suffering from cache\ninefficiency and pipeline stalls exceeding 95% at long contexts. In contrast,\nsub-quadratic models can become compute-bound on programmable vector cores.\nThese findings provide critical insights for the co-design of hardware-aware\nmodels and optimization strategies to enable on-device AI inference with\nlong-contexts."
                },
                "authors": [
                    {
                        "name": "Neelesh Gupta"
                    },
                    {
                        "name": "Rakshith Jayanth"
                    },
                    {
                        "name": "Dhruv Parikh"
                    },
                    {
                        "name": "Viktor Prasanna"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Prasanna"
                },
                "author": "Viktor Prasanna",
                "arxiv_comment": "IEEE HiPC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02850v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02850v2",
                "updated": "2025-09-29T15:20:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    20,
                    29,
                    0,
                    272,
                    0
                ],
                "published": "2025-06-03T13:19:41Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    19,
                    41,
                    1,
                    154,
                    0
                ],
                "title": "METok: Multi-Stage Event-based Token Compression for Efficient Long\n  Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "METok: Multi-Stage Event-based Token Compression for Efficient Long\n  Video Understanding"
                },
                "summary": "Recent advances in Video Large Language Models (VLLMs) have significantly\nenhanced their ability to understand video content. Nonetheless, processing\nlong videos remains challenging due to high computational demands and the\nredundancy present in the visual data. In this work, we propose METok, a\ntraining-free, Multi-stage Event-based Token compression framework designed to\naccelerate VLLMs' inference while preserving accuracy. METok progressively\neliminates redundant visual tokens across three critical stages: (1)\nevent-aware compression during vision encoding, (2) hierarchical token pruning\nin the prefilling stage based on semantic alignment and event importance, and\n(3) a decoding-stage KV Cache optimization that further reduces memory\nconsumption. Our experiments on diverse video benchmarks demonstrate that METok\nachieves an optimal trade-off between efficiency and accuracy by dynamically\nselecting informative visual tokens. For instance, equipping LongVA-7B with\nMETok realizes an 80.6% FLOPs reduction and 93.5% KV Cache memory savings, all\nwhile maintaining comparable or even superior accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Video Large Language Models (VLLMs) have significantly\nenhanced their ability to understand video content. Nonetheless, processing\nlong videos remains challenging due to high computational demands and the\nredundancy present in the visual data. In this work, we propose METok, a\ntraining-free, Multi-stage Event-based Token compression framework designed to\naccelerate VLLMs' inference while preserving accuracy. METok progressively\neliminates redundant visual tokens across three critical stages: (1)\nevent-aware compression during vision encoding, (2) hierarchical token pruning\nin the prefilling stage based on semantic alignment and event importance, and\n(3) a decoding-stage KV Cache optimization that further reduces memory\nconsumption. Our experiments on diverse video benchmarks demonstrate that METok\nachieves an optimal trade-off between efficiency and accuracy by dynamically\nselecting informative visual tokens. For instance, equipping LongVA-7B with\nMETok realizes an 80.6% FLOPs reduction and 93.5% KV Cache memory savings, all\nwhile maintaining comparable or even superior accuracy."
                },
                "authors": [
                    {
                        "name": "Mengyue Wang"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Kristian Kersting"
                    },
                    {
                        "name": "Volker Tresp"
                    },
                    {
                        "name": "Yunpu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yunpu Ma"
                },
                "author": "Yunpu Ma",
                "arxiv_comment": "EMNLP 2025; 15 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02850v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02850v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16056v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16056v2",
                "updated": "2025-09-29T15:15:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    15,
                    49,
                    0,
                    272,
                    0
                ],
                "published": "2025-05-21T22:13:09Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    22,
                    13,
                    9,
                    2,
                    141,
                    0
                ],
                "title": "Not All Models Suit Expert Offloading: On Local Routing Consistency of\n  Mixture-of-Expert Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Models Suit Expert Offloading: On Local Routing Consistency of\n  Mixture-of-Expert Models"
                },
                "summary": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\n(LLMs) with sparsely activated experts during inference. To effectively deploy\nlarge MoE models on memory-constrained devices, many systems introduce *expert\noffloading* that caches a subset of experts in fast memory, leaving others on\nslow memory to run on CPU or load on demand. While some research has exploited\nthe locality of expert activations, where consecutive tokens activate similar\nexperts, the degree of this **local routing consistency** varies across models\nand remains understudied. In this paper, we propose two metrics to measure\nlocal routing consistency of MoE models: (1) **Segment Routing Best Performance\n(SRP)**, which evaluates how well a fixed group of experts can cover the needs\nof a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which\nmeasures the optimal segment-level cache hit rate under a given cache size\nlimit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found\nthat models that apply MoE on every layer and do not use shared experts exhibit\nthe highest local routing consistency. We further showed that\ndomain-specialized experts contribute more to routing consistency than\nvocabulary-specialized ones, and that most models can balance between cache\neffectiveness and efficiency with cache sizes approximately 2x the active\nexperts. These findings pave the way for memory-efficient MoE design and\ndeployment without compromising inference speed. We publish the code for\nreplicating experiments at https://github.com/ljcleo/moe-lrc .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\n(LLMs) with sparsely activated experts during inference. To effectively deploy\nlarge MoE models on memory-constrained devices, many systems introduce *expert\noffloading* that caches a subset of experts in fast memory, leaving others on\nslow memory to run on CPU or load on demand. While some research has exploited\nthe locality of expert activations, where consecutive tokens activate similar\nexperts, the degree of this **local routing consistency** varies across models\nand remains understudied. In this paper, we propose two metrics to measure\nlocal routing consistency of MoE models: (1) **Segment Routing Best Performance\n(SRP)**, which evaluates how well a fixed group of experts can cover the needs\nof a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which\nmeasures the optimal segment-level cache hit rate under a given cache size\nlimit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found\nthat models that apply MoE on every layer and do not use shared experts exhibit\nthe highest local routing consistency. We further showed that\ndomain-specialized experts contribute more to routing consistency than\nvocabulary-specialized ones, and that most models can balance between cache\neffectiveness and efficiency with cache sizes approximately 2x the active\nexperts. These findings pave the way for memory-efficient MoE design and\ndeployment without compromising inference speed. We publish the code for\nreplicating experiments at https://github.com/ljcleo/moe-lrc ."
                },
                "authors": [
                    {
                        "name": "Jingcong Liang"
                    },
                    {
                        "name": "Siyuan Wang"
                    },
                    {
                        "name": "Miren Tian"
                    },
                    {
                        "name": "Yitong Li"
                    },
                    {
                        "name": "Duyu Tang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16056v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16056v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24832v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24832v1",
                "updated": "2025-09-29T14:16:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    16,
                    13,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T14:16:13Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    16,
                    13,
                    0,
                    272,
                    0
                ],
                "title": "SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts\n  via Token-Level LSH Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts\n  via Token-Level LSH Matching"
                },
                "summary": "As large language models (LLMs) continue to scale, the memory footprint of\nkey-value (KV) caches during inference has become a significant bottleneck.\nExisting approaches primarily focus on compressing KV caches within a single\nprompt or reusing shared prefixes or frequently ocurred text segments across\nprompts. However, such strategies are limited in scenarios where prompts are\nsemantically similar but lexically different, which frequently occurs in tasks\nsuch as multi-document summarization and conversational agents. We propose\n\\textit{SemShareKV}, a KV cache sharing and compression framework that\naccelerates LLM inference by reusing KVCache in semantically similar prompts.\nInstead of relying on exact token matches, SemShareKV applies fuzzy token\nmatching using locality-sensitive hashing (LSH) on token embeddings and\nincorporates Rotary Position Embedding (RoPE) to better preserve positional\ninformation. By selectively reusing relevant key-value pairs from a reference\nprompt's cache, SemShareKV reduces redundant computation while maintaining\noutput quality. Experiments on diverse summarization datasets show up to\n6.25$\\times$ speedup and 42\\% lower GPU memory usage with 5k tokens input, with\nnegligible quality degradation. These results highlight the potential of\nsemantic-aware cache sharing for efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to scale, the memory footprint of\nkey-value (KV) caches during inference has become a significant bottleneck.\nExisting approaches primarily focus on compressing KV caches within a single\nprompt or reusing shared prefixes or frequently ocurred text segments across\nprompts. However, such strategies are limited in scenarios where prompts are\nsemantically similar but lexically different, which frequently occurs in tasks\nsuch as multi-document summarization and conversational agents. We propose\n\\textit{SemShareKV}, a KV cache sharing and compression framework that\naccelerates LLM inference by reusing KVCache in semantically similar prompts.\nInstead of relying on exact token matches, SemShareKV applies fuzzy token\nmatching using locality-sensitive hashing (LSH) on token embeddings and\nincorporates Rotary Position Embedding (RoPE) to better preserve positional\ninformation. By selectively reusing relevant key-value pairs from a reference\nprompt's cache, SemShareKV reduces redundant computation while maintaining\noutput quality. Experiments on diverse summarization datasets show up to\n6.25$\\times$ speedup and 42\\% lower GPU memory usage with 5k tokens input, with\nnegligible quality degradation. These results highlight the potential of\nsemantic-aware cache sharing for efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "Xinye Zhao"
                    },
                    {
                        "name": "Spyridon Mastorakis"
                    }
                ],
                "author_detail": {
                    "name": "Spyridon Mastorakis"
                },
                "author": "Spyridon Mastorakis",
                "arxiv_comment": "11 figures, 14pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24832v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24832v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24791v1",
                "updated": "2025-09-29T13:45:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    45,
                    35,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T13:45:35Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    45,
                    35,
                    0,
                    272,
                    0
                ],
                "title": "Vision Function Layer in Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Function Layer in Multimodal LLMs"
                },
                "summary": "This study identifies that visual-related functional decoding is distributed\nacross different decoder layers in Multimodal Large Language Models (MLLMs).\nTypically, each function, such as counting, grounding, or OCR recognition,\nnarrows down to two or three layers, which we define as Vision Function Layers\n(VFL). Additionally, the depth and its order of different VFLs exhibits a\nconsistent pattern across different MLLMs, which is well-aligned with human\nbehaviors (e.g., recognition occurs first, followed by counting, and then\ngrounding). These findings are derived from Visual Token Swapping, our novel\nanalytical framework that modifies targeted KV cache entries to precisely\nelucidate layer-specific functions during decoding. Furthermore, these insights\noffer substantial utility in tailoring MLLMs for real-world downstream\napplications. For instance, when LoRA training is selectively applied to VFLs\nwhose functions align with the training data, VFL-LoRA not only outperform\nfull-LoRA but also prevent out-of-domain function forgetting. Moreover, by\nanalyzing the performance differential on training data when particular VFLs\nare ablated, VFL-select automatically classifies data by function, enabling\nhighly efficient data selection to directly bolster corresponding capabilities.\nConsequently, VFL-select surpasses human experts in data selection, and\nachieves 98% of full-data performance with only 20% of the original dataset.\nThis study delivers deeper comprehension of MLLM visual processing, fostering\nthe creation of more efficient, interpretable, and robust models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study identifies that visual-related functional decoding is distributed\nacross different decoder layers in Multimodal Large Language Models (MLLMs).\nTypically, each function, such as counting, grounding, or OCR recognition,\nnarrows down to two or three layers, which we define as Vision Function Layers\n(VFL). Additionally, the depth and its order of different VFLs exhibits a\nconsistent pattern across different MLLMs, which is well-aligned with human\nbehaviors (e.g., recognition occurs first, followed by counting, and then\ngrounding). These findings are derived from Visual Token Swapping, our novel\nanalytical framework that modifies targeted KV cache entries to precisely\nelucidate layer-specific functions during decoding. Furthermore, these insights\noffer substantial utility in tailoring MLLMs for real-world downstream\napplications. For instance, when LoRA training is selectively applied to VFLs\nwhose functions align with the training data, VFL-LoRA not only outperform\nfull-LoRA but also prevent out-of-domain function forgetting. Moreover, by\nanalyzing the performance differential on training data when particular VFLs\nare ablated, VFL-select automatically classifies data by function, enabling\nhighly efficient data selection to directly bolster corresponding capabilities.\nConsequently, VFL-select surpasses human experts in data selection, and\nachieves 98% of full-data performance with only 20% of the original dataset.\nThis study delivers deeper comprehension of MLLM visual processing, fostering\nthe creation of more efficient, interpretable, and robust models."
                },
                "authors": [
                    {
                        "name": "Cheng Shi"
                    },
                    {
                        "name": "Yizhou Yu"
                    },
                    {
                        "name": "Sibei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Sibei Yang"
                },
                "author": "Sibei Yang",
                "arxiv_comment": "Accepted at NeurIPS 2025 (preview; camera-ready in preparation)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20776v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20776v3",
                "updated": "2025-09-29T12:34:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    12,
                    34,
                    50,
                    0,
                    272,
                    0
                ],
                "published": "2025-05-27T06:30:00Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    6,
                    30,
                    0,
                    1,
                    147,
                    0
                ],
                "title": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences"
                },
                "summary": "Speculative decoding is a widely used technique for accelerating inference in\nlarge language models (LLMs), but its performance degrades as input length\ngrows, with significant drops even at moderate lengths. Yet, this early\ndegradation has remained largely underexplored. We introduce SpecExtend, a\ndrop-in enhancement that improves speculative decoding on long sequences\nwithout additional training. SpecExtend integrates efficient attention\nmechanisms such as FlashAttention and Hybrid Tree Attention to accelerate\nprefill and verification steps. To improve both draft accuracy and speed on\nlong inputs without retraining, we propose Cross-model Retrieval, a novel KV\ncache eviction strategy that leverages the target model's attention scores to\ndynamically select relevant context for the smaller draft model. Extensive\nevaluations show that SpecExtend accelerates speculative decoding by up to\n2.84x on 16K-token long summarization and up to 3.86x on long reasoning, while\npreserving the short-input performance of state-of-the-art frameworks. Our code\nis available at https://github.com/jycha98/SpecExtend .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a widely used technique for accelerating inference in\nlarge language models (LLMs), but its performance degrades as input length\ngrows, with significant drops even at moderate lengths. Yet, this early\ndegradation has remained largely underexplored. We introduce SpecExtend, a\ndrop-in enhancement that improves speculative decoding on long sequences\nwithout additional training. SpecExtend integrates efficient attention\nmechanisms such as FlashAttention and Hybrid Tree Attention to accelerate\nprefill and verification steps. To improve both draft accuracy and speed on\nlong inputs without retraining, we propose Cross-model Retrieval, a novel KV\ncache eviction strategy that leverages the target model's attention scores to\ndynamically select relevant context for the smaller draft model. Extensive\nevaluations show that SpecExtend accelerates speculative decoding by up to\n2.84x on 16K-token long summarization and up to 3.86x on long reasoning, while\npreserving the short-input performance of state-of-the-art frameworks. Our code\nis available at https://github.com/jycha98/SpecExtend ."
                },
                "authors": [
                    {
                        "name": "Jungyoub Cha"
                    },
                    {
                        "name": "Hyunjong Kim"
                    },
                    {
                        "name": "Sungzoon Cho"
                    }
                ],
                "author_detail": {
                    "name": "Sungzoon Cho"
                },
                "author": "Sungzoon Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20776v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20776v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24695v1",
                "updated": "2025-09-29T12:28:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    12,
                    28,
                    9,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T12:28:09Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    12,
                    28,
                    9,
                    0,
                    272,
                    0
                ],
                "title": "SANA-Video: Efficient Video Generation with Block Linear Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SANA-Video: Efficient Video Generation with Block Linear Diffusion\n  Transformer"
                },
                "summary": "We introduce SANA-Video, a small diffusion model that can efficiently\ngenerate videos up to 720x1280 resolution and minute-length duration.\nSANA-Video synthesizes high-resolution, high-quality and long videos with\nstrong text-video alignment at a remarkably fast speed, deployable on RTX 5090\nGPU. Two core designs ensure our efficient, effective and long video\ngeneration: (1) Linear DiT: We leverage linear attention as the core operation,\nwhich is more efficient than vanilla attention given the large number of tokens\nprocessed in video generation. (2) Constant-Memory KV cache for Block Linear\nAttention: we design block-wise autoregressive approach for long video\ngeneration by employing a constant-memory state, derived from the cumulative\nproperties of linear attention. This KV cache provides the Linear DiT with\nglobal context at a fixed memory cost, eliminating the need for a traditional\nKV cache and enabling efficient, minute-long video generation. In addition, we\nexplore effective data filters and model training strategies, narrowing the\ntraining cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of\nMovieGen. Given its low cost, SANA-Video achieves competitive performance\ncompared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B\nand SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover,\nSANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating\nthe inference speed of generating a 5-second 720p video from 71s to 29s (2.4x\nspeedup). In summary, SANA-Video enables low-cost, high-quality video\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SANA-Video, a small diffusion model that can efficiently\ngenerate videos up to 720x1280 resolution and minute-length duration.\nSANA-Video synthesizes high-resolution, high-quality and long videos with\nstrong text-video alignment at a remarkably fast speed, deployable on RTX 5090\nGPU. Two core designs ensure our efficient, effective and long video\ngeneration: (1) Linear DiT: We leverage linear attention as the core operation,\nwhich is more efficient than vanilla attention given the large number of tokens\nprocessed in video generation. (2) Constant-Memory KV cache for Block Linear\nAttention: we design block-wise autoregressive approach for long video\ngeneration by employing a constant-memory state, derived from the cumulative\nproperties of linear attention. This KV cache provides the Linear DiT with\nglobal context at a fixed memory cost, eliminating the need for a traditional\nKV cache and enabling efficient, minute-long video generation. In addition, we\nexplore effective data filters and model training strategies, narrowing the\ntraining cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of\nMovieGen. Given its low cost, SANA-Video achieves competitive performance\ncompared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B\nand SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover,\nSANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating\nthe inference speed of generating a 5-second 720p video from 71s to 29s (2.4x\nspeedup). In summary, SANA-Video enables low-cost, high-quality video\ngeneration."
                },
                "authors": [
                    {
                        "name": "Junsong Chen"
                    },
                    {
                        "name": "Yuyang Zhao"
                    },
                    {
                        "name": "Jincheng Yu"
                    },
                    {
                        "name": "Ruihang Chu"
                    },
                    {
                        "name": "Junyu Chen"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Xianbang Wang"
                    },
                    {
                        "name": "Yicheng Pan"
                    },
                    {
                        "name": "Daquan Zhou"
                    },
                    {
                        "name": "Huan Ling"
                    },
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Hongwei Yi"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Han Cai"
                    },
                    {
                        "name": "Sanja Fidler"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "arxiv_comment": "21 pages, 15 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24626v1",
                "updated": "2025-09-29T11:35:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    11,
                    35,
                    55,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T11:35:55Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    11,
                    35,
                    55,
                    0,
                    272,
                    0
                ],
                "title": "SparseServe: Unlocking Parallelism for Dynamic Sparse Attention in\n  Long-Context LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseServe: Unlocking Parallelism for Dynamic Sparse Attention in\n  Long-Context LLM Serving"
                },
                "summary": "Serving long-context LLMs is costly because attention computation grows\nlinearly with context length. Dynamic sparse attention algorithms (DSAs)\nmitigate this by attending only to the key-value (KV) cache of critical tokens.\nHowever, with DSAs, the main performance bottleneck shifts from HBM bandwidth\nto HBM capacity: KV caches for unselected tokens must remain in HBM for\nlow-latency decoding, constraining parallel batch size and stalling further\nthroughput gains. Offloading these underutilized KV caches to DRAM could free\nHBM capacity, allowing larger parallel batch sizes. Yet, achieving such\nhierarchical HBM-DRAM storage raises new challenges, including fragmented KV\ncache access, HBM cache contention, and high HBM demands of hybrid batching,\nthat remain unresolved in prior work.\n  This paper proposes SparseServe, an LLM serving system that unlocks the\nparallel potential of DSAs through efficient hierarchical HBM-DRAM management.\nSparseServe introduces three key innovations to address the challenges\nmentioned above: (1) fragmentation-aware KV cache transfer, which accelerates\nHBM-DRAM data movement through GPU-direct loading (FlashH2D) and CPU-assisted\nsaving (FlashD2H); (2) working-set-aware batch size control that adjusts batch\nsizes based on real-time working set estimation to minimize HBM cache\nthrashing; (3) layer-segmented prefill that bounds HBM use during prefill to a\nsingle layer, enabling efficient execution even for long prompts. Extensive\nexperimental results demonstrate that SparseServe achieves up to 9.26x lower\nmean time-to-first-token (TTFT) latency and up to 3.14x higher token generation\nthroughput compared to state-of-the-art LLM serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving long-context LLMs is costly because attention computation grows\nlinearly with context length. Dynamic sparse attention algorithms (DSAs)\nmitigate this by attending only to the key-value (KV) cache of critical tokens.\nHowever, with DSAs, the main performance bottleneck shifts from HBM bandwidth\nto HBM capacity: KV caches for unselected tokens must remain in HBM for\nlow-latency decoding, constraining parallel batch size and stalling further\nthroughput gains. Offloading these underutilized KV caches to DRAM could free\nHBM capacity, allowing larger parallel batch sizes. Yet, achieving such\nhierarchical HBM-DRAM storage raises new challenges, including fragmented KV\ncache access, HBM cache contention, and high HBM demands of hybrid batching,\nthat remain unresolved in prior work.\n  This paper proposes SparseServe, an LLM serving system that unlocks the\nparallel potential of DSAs through efficient hierarchical HBM-DRAM management.\nSparseServe introduces three key innovations to address the challenges\nmentioned above: (1) fragmentation-aware KV cache transfer, which accelerates\nHBM-DRAM data movement through GPU-direct loading (FlashH2D) and CPU-assisted\nsaving (FlashD2H); (2) working-set-aware batch size control that adjusts batch\nsizes based on real-time working set estimation to minimize HBM cache\nthrashing; (3) layer-segmented prefill that bounds HBM use during prefill to a\nsingle layer, enabling efficient execution even for long prompts. Extensive\nexperimental results demonstrate that SparseServe achieves up to 9.26x lower\nmean time-to-first-token (TTFT) latency and up to 3.14x higher token generation\nthroughput compared to state-of-the-art LLM serving systems."
                },
                "authors": [
                    {
                        "name": "Qihui Zhou"
                    },
                    {
                        "name": "Peiqi Yin"
                    },
                    {
                        "name": "Pengfei Zuo"
                    },
                    {
                        "name": "James Cheng"
                    }
                ],
                "author_detail": {
                    "name": "James Cheng"
                },
                "author": "James Cheng",
                "arxiv_comment": "14 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24407v1",
                "updated": "2025-09-29T07:54:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    7,
                    54,
                    44,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T07:54:44Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    7,
                    54,
                    44,
                    0,
                    272,
                    0
                ],
                "title": "Q-REACH: Quantum information Repetition, Error Analysis and Correction\n  using Caching Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-REACH: Quantum information Repetition, Error Analysis and Correction\n  using Caching Network"
                },
                "summary": "Quantum repeaters incorporating quantum memory play a pivotal role in\nmitigating loss in transmitted quantum information (photons) due to link\nattenuation over a long-distance quantum communication network. However,\nlimited availability of available storage in such quantum repeaters and the\nimpact on the time spent within the memory unit presents a trade-off between\nquantum information fidelity (a metric that quantifies the degree of similarity\nbetween a pair of quantum states) and qubit transmission rate. Thus, effective\nmanagement of storage time for qubits becomes a key consideration in multi-hop\nquantum networks. To address these challenges, we propose Q-REACH, which\nleverages queuing theory in caching networks to tune qubit transmission rate\nwhile considering fidelity as the cost metric. Our contributions in this work\ninclude (i) utilizing a method of repetition that encodes and broadcasts\nmultiple qubits through different quantum paths, (ii) analytically estimating\nthe time spent by these emitted qubits as a function of the number of paths and\nrepeaters, as well as memory units within a repeater, and (iii) formulating\noptimization problem that leverages this analysis to correct the transmitted\nlogic qubit and select the optimum repetition rate at the transmitter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum repeaters incorporating quantum memory play a pivotal role in\nmitigating loss in transmitted quantum information (photons) due to link\nattenuation over a long-distance quantum communication network. However,\nlimited availability of available storage in such quantum repeaters and the\nimpact on the time spent within the memory unit presents a trade-off between\nquantum information fidelity (a metric that quantifies the degree of similarity\nbetween a pair of quantum states) and qubit transmission rate. Thus, effective\nmanagement of storage time for qubits becomes a key consideration in multi-hop\nquantum networks. To address these challenges, we propose Q-REACH, which\nleverages queuing theory in caching networks to tune qubit transmission rate\nwhile considering fidelity as the cost metric. Our contributions in this work\ninclude (i) utilizing a method of repetition that encodes and broadcasts\nmultiple qubits through different quantum paths, (ii) analytically estimating\nthe time spent by these emitted qubits as a function of the number of paths and\nrepeaters, as well as memory units within a repeater, and (iii) formulating\noptimization problem that leverages this analysis to correct the transmitted\nlogic qubit and select the optimum repetition rate at the transmitter."
                },
                "authors": [
                    {
                        "name": "Karl C. Linne"
                    },
                    {
                        "name": "Yuanyuan Li"
                    },
                    {
                        "name": "Debashri Roy"
                    },
                    {
                        "name": "Kaushik Chowdhury"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Chowdhury"
                },
                "arxiv_affiliation": "Kai Li",
                "author": "Kaushik Chowdhury",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00970v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00970v2",
                "updated": "2025-09-29T05:12:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    5,
                    12,
                    51,
                    0,
                    272,
                    0
                ],
                "published": "2025-04-01T17:08:57Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    8,
                    57,
                    1,
                    91,
                    0
                ],
                "title": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching"
                },
                "summary": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "Ali Falahati"
                    },
                    {
                        "name": "David H. Yang"
                    },
                    {
                        "name": "Mohammad Mohammadi Amiri"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Mohammadi Amiri"
                },
                "author": "Mohammad Mohammadi Amiri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00970v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00970v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16257v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16257v2",
                "updated": "2025-09-29T02:46:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    2,
                    46,
                    45,
                    0,
                    272,
                    0
                ],
                "published": "2025-03-20T15:52:43Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    52,
                    43,
                    3,
                    79,
                    0
                ],
                "title": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models"
                },
                "summary": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, the key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, the key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16257v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16257v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24178v1",
                "updated": "2025-09-29T01:52:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    1,
                    52,
                    10,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T01:52:10Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    1,
                    52,
                    10,
                    0,
                    272,
                    0
                ],
                "title": "BladderFormer: A Streaming Transformer for Real-Time Urological State\n  Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BladderFormer: A Streaming Transformer for Real-Time Urological State\n  Monitoring"
                },
                "summary": "Bladder pressure monitoring systems are increasingly vital in diagnosing and\nmanaging urinary tract dysfunction. Existing solutions rely heavily on\nhand-crafted features and shallow classifiers, limiting their adaptability to\ncomplex signal dynamics. We propose a one-layer streaming transformer model for\nreal-time classification of bladder pressure states, operating on\nwavelet-transformed representations of raw time-series data. Our model\nincorporates temporal multi-head self-attention and state caching, enabling\nefficient online inference with high adaptability. Trained on a dataset of 91\npatients with 20,000-80,000 samples each, our method demonstrates improved\naccuracy, higher energy- and latency-efficiency. Implementation considerations\nfor edge deployment on low-power hardware, such as edge graphical processing\nunits (GPU) and micro-controllers, are also discussed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bladder pressure monitoring systems are increasingly vital in diagnosing and\nmanaging urinary tract dysfunction. Existing solutions rely heavily on\nhand-crafted features and shallow classifiers, limiting their adaptability to\ncomplex signal dynamics. We propose a one-layer streaming transformer model for\nreal-time classification of bladder pressure states, operating on\nwavelet-transformed representations of raw time-series data. Our model\nincorporates temporal multi-head self-attention and state caching, enabling\nefficient online inference with high adaptability. Trained on a dataset of 91\npatients with 20,000-80,000 samples each, our method demonstrates improved\naccuracy, higher energy- and latency-efficiency. Implementation considerations\nfor edge deployment on low-power hardware, such as edge graphical processing\nunits (GPU) and micro-controllers, are also discussed."
                },
                "authors": [
                    {
                        "name": "Chengwei Zhou"
                    },
                    {
                        "name": "Steve Majerus"
                    },
                    {
                        "name": "Gourav Datta"
                    }
                ],
                "author_detail": {
                    "name": "Gourav Datta"
                },
                "author": "Gourav Datta",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24088v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24088v1",
                "updated": "2025-09-28T21:47:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    21,
                    47,
                    20,
                    6,
                    271,
                    0
                ],
                "published": "2025-09-28T21:47:20Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    21,
                    47,
                    20,
                    6,
                    271,
                    0
                ],
                "title": "CORRECT: COndensed eRror RECognition via knowledge Transfer in\n  multi-agent systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CORRECT: COndensed eRror RECognition via knowledge Transfer in\n  multi-agent systems"
                },
                "summary": "Multi-agent systems (MAS) are increasingly capable of tackling complex\nreal-world tasks, yet their reliance on inter-agent coordination, tool use, and\nlong-horizon reasoning makes error recognition particularly challenging. Minor\nerrors can propagate across agents, escalating into task failures while\nproducing long, intertwined execution trajectories that impose significant\ncosts for both human developers and automated systems to debug and analyze. Our\nkey insight is that, despite surface differences in failure trajectories (e.g.,\nlogs), MAS errors often recur with similar structural patterns. This paper\npresents CORRECT, the first lightweight, training-free framework that leverages\nan online cache of distilled error schemata to recognize and transfer knowledge\nof failure structures across new requests. This cache-based reuse allows LLMs\nto perform targeted error localization at inference time, avoiding the need for\nexpensive retraining while adapting to dynamic MAS deployments in subseconds.\nTo support rigorous study in this domain, we also introduce CORRECT-Error, a\nlarge-scale dataset of over 2,000 annotated trajectories collected through a\nnovel error-injection pipeline guided by real-world distributions, and further\nvalidated through human evaluation to ensure alignment with natural failure\npatterns. Experiments across seven diverse MAS applications show that CORRECT\nimproves step-level error localization up to 19.8% over existing advances while\nat near-zero overhead, substantially narrowing the gap between automated and\nhuman-level error recognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems (MAS) are increasingly capable of tackling complex\nreal-world tasks, yet their reliance on inter-agent coordination, tool use, and\nlong-horizon reasoning makes error recognition particularly challenging. Minor\nerrors can propagate across agents, escalating into task failures while\nproducing long, intertwined execution trajectories that impose significant\ncosts for both human developers and automated systems to debug and analyze. Our\nkey insight is that, despite surface differences in failure trajectories (e.g.,\nlogs), MAS errors often recur with similar structural patterns. This paper\npresents CORRECT, the first lightweight, training-free framework that leverages\nan online cache of distilled error schemata to recognize and transfer knowledge\nof failure structures across new requests. This cache-based reuse allows LLMs\nto perform targeted error localization at inference time, avoiding the need for\nexpensive retraining while adapting to dynamic MAS deployments in subseconds.\nTo support rigorous study in this domain, we also introduce CORRECT-Error, a\nlarge-scale dataset of over 2,000 annotated trajectories collected through a\nnovel error-injection pipeline guided by real-world distributions, and further\nvalidated through human evaluation to ensure alignment with natural failure\npatterns. Experiments across seven diverse MAS applications show that CORRECT\nimproves step-level error localization up to 19.8% over existing advances while\nat near-zero overhead, substantially narrowing the gap between automated and\nhuman-level error recognition."
                },
                "authors": [
                    {
                        "name": "Yifan Yu"
                    },
                    {
                        "name": "Moyan Li"
                    },
                    {
                        "name": "Shaoyuan Xu"
                    },
                    {
                        "name": "Jinmiao Fu"
                    },
                    {
                        "name": "Xinhai Hou"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Bryan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Wang"
                },
                "author": "Bryan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24088v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24088v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24007v1",
                "updated": "2025-09-28T17:59:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    17,
                    59,
                    15,
                    6,
                    271,
                    0
                ],
                "published": "2025-09-28T17:59:15Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    17,
                    59,
                    15,
                    6,
                    271,
                    0
                ],
                "title": "Sequential Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Diffusion Language Models"
                },
                "summary": "Diffusion language models (DLMs) have strong theoretical efficiency but are\nlimited by fixed-length decoding and incompatibility with key-value (KV)\ncaches. Block diffusion mitigates these issues, yet still enforces a fixed\nblock size and requires expensive training. We introduce Next Sequence\nPrediction (NSP), which unifies next-token and next-block prediction, enabling\nthe model to adaptively determine the generation length at each step. When the\nlength is fixed to 1, NSP reduces to standard next-token prediction. Building\non NSP, we propose Sequential Diffusion Language Model (SDLM), which can\nretrofit pre-trained autoregressive language models (ALMs) at minimal cost.\nSpecifically, SDLM performs diffusion inference within fixed-size mask blocks,\nbut dynamically decodes consecutive subsequences based on model confidence,\nthereby preserving KV-cache compatibility and improving robustness to varying\nuncertainty and semantics across the sequence. Experiments show that SDLM\nmatches or surpasses strong autoregressive baselines using only 3.5M training\nsamples, while achieving 2.1 higher throughput than Qwen-2.5. Notably, the\nSDLM-32B model delivers even more pronounced efficiency gains, demonstrating\nthe strong scalability potential of our modeling paradigm. Project page and\ncodes: https://github.com/OpenGVLab/SDLM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models (DLMs) have strong theoretical efficiency but are\nlimited by fixed-length decoding and incompatibility with key-value (KV)\ncaches. Block diffusion mitigates these issues, yet still enforces a fixed\nblock size and requires expensive training. We introduce Next Sequence\nPrediction (NSP), which unifies next-token and next-block prediction, enabling\nthe model to adaptively determine the generation length at each step. When the\nlength is fixed to 1, NSP reduces to standard next-token prediction. Building\non NSP, we propose Sequential Diffusion Language Model (SDLM), which can\nretrofit pre-trained autoregressive language models (ALMs) at minimal cost.\nSpecifically, SDLM performs diffusion inference within fixed-size mask blocks,\nbut dynamically decodes consecutive subsequences based on model confidence,\nthereby preserving KV-cache compatibility and improving robustness to varying\nuncertainty and semantics across the sequence. Experiments show that SDLM\nmatches or surpasses strong autoregressive baselines using only 3.5M training\nsamples, while achieving 2.1 higher throughput than Qwen-2.5. Notably, the\nSDLM-32B model delivers even more pronounced efficiency gains, demonstrating\nthe strong scalability potential of our modeling paradigm. Project page and\ncodes: https://github.com/OpenGVLab/SDLM"
                },
                "authors": [
                    {
                        "name": "Yangzhou Liu"
                    },
                    {
                        "name": "Yue Cao"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Weiyun Wang"
                    },
                    {
                        "name": "Xiaobo Liang"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Lijun Wu"
                    },
                    {
                        "name": "Changyao Tian"
                    },
                    {
                        "name": "Yanting Zhang"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Tong Lu"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Jifeng Dai"
                    },
                    {
                        "name": "Wenhai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenhai Wang"
                },
                "author": "Wenhai Wang",
                "arxiv_comment": "14 pages, 5 figures, technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23928v1",
                "updated": "2025-09-28T15:05:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    15,
                    5,
                    21,
                    6,
                    271,
                    0
                ],
                "published": "2025-09-28T15:05:21Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    15,
                    5,
                    21,
                    6,
                    271,
                    0
                ],
                "title": "HiViS: Hiding Visual Tokens from the Drafter for Speculative Decoding in\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiViS: Hiding Visual Tokens from the Drafter for Speculative Decoding in\n  Vision-Language Models"
                },
                "summary": "Speculative decoding is an effective approach for accelerating inference in\nLarge Language models (LLMs), but its adaptation to Vision-Language models\n(VLMs) remains challenging for additional visual tokens in multimodal inputs.\nFirst, owing to the fact that the drafter and the target VLM may derived from\ndifferent families, the semantic representations of visual tokens in the target\nVLM are misaligned with those in the drafter, introducing bias into the\nKV-cache during the prefill stage. Second, the large number of visual tokens\nsubstantially slows down the drafter's self-attention during the decoding\nstage. We propose Hiding Visual Tokens from the Drafter for Speculative\nDecoding in Vision-Language Models (HiViS), an explicit-implicit input\ndecomposition framework that alleviates the above inefficiency. All visual\ntokens are removed from the drafter's input, retaining only textual tokens as\nexplicit inputs, while directly reusing the target VLM's corresponding\nlast-layer hidden states as implicit visual information without additional\nprocessing. To train the drafter efficiently, we introduces multi-step\nself-feedback training strategy with dynamic data selection and sequential\nembedding supervision to simulate reasoning during training. Our approach\ncompresses the prefill sequence length of the drafter to only 0.7%-1.3% of the\ntarget VLM's input, while maintaining lossless generation quality. Extensive\nexperiments across diverse models and tasks demonstrate up to 2.65x speedup,\nconfirming the effectiveness of HiViS in accelerating VLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is an effective approach for accelerating inference in\nLarge Language models (LLMs), but its adaptation to Vision-Language models\n(VLMs) remains challenging for additional visual tokens in multimodal inputs.\nFirst, owing to the fact that the drafter and the target VLM may derived from\ndifferent families, the semantic representations of visual tokens in the target\nVLM are misaligned with those in the drafter, introducing bias into the\nKV-cache during the prefill stage. Second, the large number of visual tokens\nsubstantially slows down the drafter's self-attention during the decoding\nstage. We propose Hiding Visual Tokens from the Drafter for Speculative\nDecoding in Vision-Language Models (HiViS), an explicit-implicit input\ndecomposition framework that alleviates the above inefficiency. All visual\ntokens are removed from the drafter's input, retaining only textual tokens as\nexplicit inputs, while directly reusing the target VLM's corresponding\nlast-layer hidden states as implicit visual information without additional\nprocessing. To train the drafter efficiently, we introduces multi-step\nself-feedback training strategy with dynamic data selection and sequential\nembedding supervision to simulate reasoning during training. Our approach\ncompresses the prefill sequence length of the drafter to only 0.7%-1.3% of the\ntarget VLM's input, while maintaining lossless generation quality. Extensive\nexperiments across diverse models and tasks demonstrate up to 2.65x speedup,\nconfirming the effectiveness of HiViS in accelerating VLM inference."
                },
                "authors": [
                    {
                        "name": "Zhinan Xie"
                    },
                    {
                        "name": "Peisong Wang"
                    },
                    {
                        "name": "Jian Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Jian Cheng"
                },
                "author": "Jian Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02361v1",
                "updated": "2025-09-28T11:04:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    11,
                    4,
                    0,
                    6,
                    271,
                    0
                ],
                "published": "2025-09-28T11:04:00Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    11,
                    4,
                    0,
                    6,
                    271,
                    0
                ],
                "title": "ChunkLLM: A Lightweight Pluggable Framework for Accelerating LLMs\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkLLM: A Lightweight Pluggable Framework for Accelerating LLMs\n  Inference"
                },
                "summary": "Transformer-based large models excel in natural language processing and\ncomputer vision, but face severe computational inefficiencies due to the\nself-attention's quadratic complexity with input tokens. Recently, researchers\nhave proposed a series of methods based on block selection and compression to\nalleviate this problem, but they either have issues with semantic\nincompleteness or poor training-inference efficiency. To comprehensively\naddress these challenges, we propose ChunkLLM, a lightweight and pluggable\ntraining framework. Specifically, we introduce two components: QK Adapter\n(Q-Adapter and K-Adapter) and Chunk Adapter. The former is attached to each\nTransformer layer, serving dual purposes of feature compression and chunk\nattention acquisition. The latter operates at the bottommost layer of the\nmodel, functioning to detect chunk boundaries by leveraging contextual semantic\ninformation. During the training phase, the parameters of the backbone remain\nfrozen, with only the QK Adapter and Chunk Adapter undergoing training.\nNotably, we design an attention distillation method for training the QK\nAdapter, which enhances the recall rate of key chunks. During the inference\nphase, chunk selection is triggered exclusively when the current token is\ndetected as a chunk boundary, thereby accelerating model inference.\nExperimental evaluations are conducted on a diverse set of long-text and\nshort-text benchmark datasets spanning multiple tasks. ChunkLLM not only\nattains comparable performance on short-text benchmarks but also maintains\n98.64% of the performance on long-context benchmarks while preserving a 48.58%\nkey-value cache retention rate. Particularly, ChunkLLM attains a maximum\nspeedup of 4.48x in comparison to the vanilla Transformer in the processing of\n120K long texts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large models excel in natural language processing and\ncomputer vision, but face severe computational inefficiencies due to the\nself-attention's quadratic complexity with input tokens. Recently, researchers\nhave proposed a series of methods based on block selection and compression to\nalleviate this problem, but they either have issues with semantic\nincompleteness or poor training-inference efficiency. To comprehensively\naddress these challenges, we propose ChunkLLM, a lightweight and pluggable\ntraining framework. Specifically, we introduce two components: QK Adapter\n(Q-Adapter and K-Adapter) and Chunk Adapter. The former is attached to each\nTransformer layer, serving dual purposes of feature compression and chunk\nattention acquisition. The latter operates at the bottommost layer of the\nmodel, functioning to detect chunk boundaries by leveraging contextual semantic\ninformation. During the training phase, the parameters of the backbone remain\nfrozen, with only the QK Adapter and Chunk Adapter undergoing training.\nNotably, we design an attention distillation method for training the QK\nAdapter, which enhances the recall rate of key chunks. During the inference\nphase, chunk selection is triggered exclusively when the current token is\ndetected as a chunk boundary, thereby accelerating model inference.\nExperimental evaluations are conducted on a diverse set of long-text and\nshort-text benchmark datasets spanning multiple tasks. ChunkLLM not only\nattains comparable performance on short-text benchmarks but also maintains\n98.64% of the performance on long-context benchmarks while preserving a 48.58%\nkey-value cache retention rate. Particularly, ChunkLLM attains a maximum\nspeedup of 4.48x in comparison to the vanilla Transformer in the processing of\n120K long texts."
                },
                "authors": [
                    {
                        "name": "Haojie Ouyang"
                    },
                    {
                        "name": "Jianwei Lv"
                    },
                    {
                        "name": "Lei Ren"
                    },
                    {
                        "name": "Chen Wei"
                    },
                    {
                        "name": "Xiaojie Wang"
                    },
                    {
                        "name": "Fangxiang Feng"
                    }
                ],
                "author_detail": {
                    "name": "Fangxiang Feng"
                },
                "author": "Fangxiang Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09081v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09081v2",
                "updated": "2025-09-28T08:32:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    8,
                    32,
                    26,
                    6,
                    271,
                    0
                ],
                "published": "2025-05-14T02:29:46Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    29,
                    46,
                    2,
                    134,
                    0
                ],
                "title": "SALM: A Multi-Agent Framework for Language Model-Driven Social Network\n  Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALM: A Multi-Agent Framework for Language Model-Driven Social Network\n  Simulation"
                },
                "summary": "Contemporary approaches to agent-based modeling (ABM) of social systems have\ntraditionally emphasized rule-based behaviors, limiting their ability to\ncapture nuanced dynamics by moving beyond predefined rules and leveraging\ncontextual understanding from LMs of human social interaction. This paper\npresents SALM (Social Agent LM Framework), a novel approach for integrating\nlanguage models (LMs) into social network simulation that achieves\nunprecedented temporal stability in multi-agent scenarios. Our primary\ncontributions include: (1) a hierarchical prompting architecture enabling\nstable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)\nan attention-based memory system achieving 80% cache hit rates (95% CI [78%,\n82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on\npersonality stability. Through extensive validation against SNAP ego networks,\nwe demonstrate the first LLM-based framework capable of modeling long-term\nsocial phenomena while maintaining empirically validated behavioral fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary approaches to agent-based modeling (ABM) of social systems have\ntraditionally emphasized rule-based behaviors, limiting their ability to\ncapture nuanced dynamics by moving beyond predefined rules and leveraging\ncontextual understanding from LMs of human social interaction. This paper\npresents SALM (Social Agent LM Framework), a novel approach for integrating\nlanguage models (LMs) into social network simulation that achieves\nunprecedented temporal stability in multi-agent scenarios. Our primary\ncontributions include: (1) a hierarchical prompting architecture enabling\nstable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)\nan attention-based memory system achieving 80% cache hit rates (95% CI [78%,\n82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on\npersonality stability. Through extensive validation against SNAP ego networks,\nwe demonstrate the first LLM-based framework capable of modeling long-term\nsocial phenomena while maintaining empirically validated behavioral fidelity."
                },
                "authors": [
                    {
                        "name": "Gaurav Koley"
                    }
                ],
                "author_detail": {
                    "name": "Gaurav Koley"
                },
                "author": "Gaurav Koley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09081v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09081v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23601v1",
                "updated": "2025-09-28T03:12:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    3,
                    12,
                    43,
                    6,
                    271,
                    0
                ],
                "published": "2025-09-28T03:12:43Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    3,
                    12,
                    43,
                    6,
                    271,
                    0
                ],
                "title": "VAMamba: An Efficient Visual Adaptive Mamba for Image Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VAMamba: An Efficient Visual Adaptive Mamba for Image Restoration"
                },
                "summary": "Recent Mamba-based image restoration methods have achieved promising results\nbut remain\n  limited by fixed scanning patterns and inefficient feature utilization.\nConventional Mamba\n  architectures rely on predetermined paths that cannot adapt to diverse\ndegradations, constraining\n  both restoration performance and computational efficiency. To overcome these\nlimitations, we\n  propose VAMamba, a Visual Adaptive Mamba framework with two key innovations.\nFirst,\n  QCLAM(Queue-basedCacheLow-rankAdaptiveMemory)enhancesfeaturelearningthrougha\n  FIFO cache that stores historical representations. Similarity between current\nLoRA-adapted and\n  cached features guides intelligent fusion, enabling dynamic reuse while\neffectively controlling\n  memorygrowth.Second, GPS-SS2D(GreedyPathScanSS2D)introducesadaptive scanning.\nA\n  Vision Transformer generates score maps to estimate pixel importance, and a\ngreedy strategy de termines optimal forward and backward scanning paths. These\nlearned trajectories replace rigid\n  patterns, enabling SS2D to perform targeted feature extraction. The\nintegration of QCLAM and\n  GPS-SS2D allows VAMamba to adaptively focus on degraded regions while\nmaintaining high\n  computational efficiency. Extensive experiments across diverse restoration\ntasks demonstrate\n  that VAMamba consistently outperforms existing approaches in both restoration\nquality and\n  efficiency, establishing new benchmarks for adaptive image restoration. Our\ncode is available\n  at https://github.com/WaterHQH/VAMamba.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Mamba-based image restoration methods have achieved promising results\nbut remain\n  limited by fixed scanning patterns and inefficient feature utilization.\nConventional Mamba\n  architectures rely on predetermined paths that cannot adapt to diverse\ndegradations, constraining\n  both restoration performance and computational efficiency. To overcome these\nlimitations, we\n  propose VAMamba, a Visual Adaptive Mamba framework with two key innovations.\nFirst,\n  QCLAM(Queue-basedCacheLow-rankAdaptiveMemory)enhancesfeaturelearningthrougha\n  FIFO cache that stores historical representations. Similarity between current\nLoRA-adapted and\n  cached features guides intelligent fusion, enabling dynamic reuse while\neffectively controlling\n  memorygrowth.Second, GPS-SS2D(GreedyPathScanSS2D)introducesadaptive scanning.\nA\n  Vision Transformer generates score maps to estimate pixel importance, and a\ngreedy strategy de termines optimal forward and backward scanning paths. These\nlearned trajectories replace rigid\n  patterns, enabling SS2D to perform targeted feature extraction. The\nintegration of QCLAM and\n  GPS-SS2D allows VAMamba to adaptively focus on degraded regions while\nmaintaining high\n  computational efficiency. Extensive experiments across diverse restoration\ntasks demonstrate\n  that VAMamba consistently outperforms existing approaches in both restoration\nquality and\n  efficiency, establishing new benchmarks for adaptive image restoration. Our\ncode is available\n  at https://github.com/WaterHQH/VAMamba."
                },
                "authors": [
                    {
                        "name": "Han Hu"
                    },
                    {
                        "name": "Zhuoran Zheng"
                    },
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Chen Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Chen Lyu"
                },
                "author": "Chen Lyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09072v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09072v2",
                "updated": "2025-09-27T20:13:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    27,
                    20,
                    13,
                    25,
                    5,
                    270,
                    0
                ],
                "published": "2025-08-12T16:47:48Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    47,
                    48,
                    1,
                    224,
                    0
                ],
                "title": "READER: Retrieval-Assisted Drafter for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "READER: Retrieval-Assisted Drafter for Efficient LLM Inference"
                },
                "summary": "Autoregressive Language Models instantiate a factorized likelihood over token\nsequences, yet their strictly sequential decoding process imposes an intrinsic\nlower bound on inference latency. This bottleneck has emerged as a central\nobstacle to the scalable deployment of large-scale generative models. Existing\nacceleration techniques partially mitigate token-level latency by relying on\nauxiliary draft models or introducing an additional training phase, but fail to\naddress the dominant memory and communication costs. We present READER, a\nprovably lossless speculative decoding framework that bypasses the training of\nthe auxiliary draft model. READER formalizes speculative decoding as a\nstochastic tree construction problem and exploits the empirical redundancy\nstructure of natural language to generate high-probability candidate\ncontinuations. Our method revisits the problem of constructing draft trees,\nestablishing substantial statistical improvements over stochastic draft-tree\nmethods and providing a complexity-theoretic analysis that characterizes the\noptimality frontier of speculative decoding under bounded computation and\nmemory resources. Beyond the single-sequence regime traditionally considered in\nprior work, we introduce a memory-optimal key-value cache-serving strategy that\nguarantees amortized sublinear overhead in the batch dimension, allowing READER\nto scale to realistic inference workloads. Comprehensive experiments\ndemonstrate up to 6.13x wall-clock speedup on single-prompt inference and up to\n5.92x on batched inference, consistently surpassing prior speculative decoding\nbaselines, while preserving exact output equivalence, with even more pronounced\ngains in retrieval-augmented generation pipelines. Our results close a key gap\nbetween theoretical parallelism limits and practical LLM inference, suggesting\na new standard for efficient deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Language Models instantiate a factorized likelihood over token\nsequences, yet their strictly sequential decoding process imposes an intrinsic\nlower bound on inference latency. This bottleneck has emerged as a central\nobstacle to the scalable deployment of large-scale generative models. Existing\nacceleration techniques partially mitigate token-level latency by relying on\nauxiliary draft models or introducing an additional training phase, but fail to\naddress the dominant memory and communication costs. We present READER, a\nprovably lossless speculative decoding framework that bypasses the training of\nthe auxiliary draft model. READER formalizes speculative decoding as a\nstochastic tree construction problem and exploits the empirical redundancy\nstructure of natural language to generate high-probability candidate\ncontinuations. Our method revisits the problem of constructing draft trees,\nestablishing substantial statistical improvements over stochastic draft-tree\nmethods and providing a complexity-theoretic analysis that characterizes the\noptimality frontier of speculative decoding under bounded computation and\nmemory resources. Beyond the single-sequence regime traditionally considered in\nprior work, we introduce a memory-optimal key-value cache-serving strategy that\nguarantees amortized sublinear overhead in the batch dimension, allowing READER\nto scale to realistic inference workloads. Comprehensive experiments\ndemonstrate up to 6.13x wall-clock speedup on single-prompt inference and up to\n5.92x on batched inference, consistently surpassing prior speculative decoding\nbaselines, while preserving exact output equivalence, with even more pronounced\ngains in retrieval-augmented generation pipelines. Our results close a key gap\nbetween theoretical parallelism limits and practical LLM inference, suggesting\na new standard for efficient deployment."
                },
                "authors": [
                    {
                        "name": "Maxim Divilkovskiy"
                    },
                    {
                        "name": "Vitaly Malygin"
                    },
                    {
                        "name": "Sergey Zlobin"
                    },
                    {
                        "name": "Stanislav Ilyushin"
                    },
                    {
                        "name": "Sultan Isali"
                    },
                    {
                        "name": "Vasily Kalugin"
                    },
                    {
                        "name": "Nuriza Aitassova"
                    },
                    {
                        "name": "Fei Yi"
                    },
                    {
                        "name": "Weidi Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Weidi Zeng"
                },
                "author": "Weidi Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09072v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09072v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23179v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23179v1",
                "updated": "2025-09-27T08:15:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    27,
                    8,
                    15,
                    17,
                    5,
                    270,
                    0
                ],
                "published": "2025-09-27T08:15:17Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    8,
                    15,
                    17,
                    5,
                    270,
                    0
                ],
                "title": "A Near-Cache Architectural Framework for Cryptographic Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Near-Cache Architectural Framework for Cryptographic Computing"
                },
                "summary": "Recent advancements in post-quantum cryptographic algorithms have led to\ntheir standardization by the National Institute of Standards and Technology\n(NIST) to safeguard information security in the post-quantum era. These\nalgorithms, however, employ public keys and signatures that are 3 to 9$\\times$\nlonger than those used in pre-quantum cryptography, resulting in significant\nperformance and energy efficiency overheads. A critical bottleneck identified\nin our analysis is the cache bandwidth. This limitation motivates the adoption\nof on-chip in-/near-cache computing, a computing paradigm that offers\nhigh-performance, exceptional energy efficiency, and flexibility to accelerate\npost-quantum cryptographic algorithms. Our analysis of existing works reveals\nchallenges in integrating in-/near-cache computing into modern computer systems\nand performance limitations due to external bandwidth limitation, highlighting\nthe need for innovative solutions that can seamlessly integrate into existing\nsystems without performance and energy efficiency issues. In this paper, we\nintroduce a near-cache-slice computing paradigm with support of customization\nand virtual address, named Crypto-Near-Cache (CNC), designed to accelerate\npost-quantum cryptographic algorithms and other applications. By placing SRAM\narrays with bitline computing capability near cache slices, high internal\nbandwidth and short data movement are achieved with native support of virtual\naddressing. An ISA extension to facilitate CNC is also proposed, with detailed\ndiscussion on the implementation aspects of the core/cache datapath.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in post-quantum cryptographic algorithms have led to\ntheir standardization by the National Institute of Standards and Technology\n(NIST) to safeguard information security in the post-quantum era. These\nalgorithms, however, employ public keys and signatures that are 3 to 9$\\times$\nlonger than those used in pre-quantum cryptography, resulting in significant\nperformance and energy efficiency overheads. A critical bottleneck identified\nin our analysis is the cache bandwidth. This limitation motivates the adoption\nof on-chip in-/near-cache computing, a computing paradigm that offers\nhigh-performance, exceptional energy efficiency, and flexibility to accelerate\npost-quantum cryptographic algorithms. Our analysis of existing works reveals\nchallenges in integrating in-/near-cache computing into modern computer systems\nand performance limitations due to external bandwidth limitation, highlighting\nthe need for innovative solutions that can seamlessly integrate into existing\nsystems without performance and energy efficiency issues. In this paper, we\nintroduce a near-cache-slice computing paradigm with support of customization\nand virtual address, named Crypto-Near-Cache (CNC), designed to accelerate\npost-quantum cryptographic algorithms and other applications. By placing SRAM\narrays with bitline computing capability near cache slices, high internal\nbandwidth and short data movement are achieved with native support of virtual\naddressing. An ISA extension to facilitate CNC is also proposed, with detailed\ndiscussion on the implementation aspects of the core/cache datapath."
                },
                "authors": [
                    {
                        "name": "Jingyao Zhang"
                    },
                    {
                        "name": "Elaheh Sadredini"
                    }
                ],
                "author_detail": {
                    "name": "Elaheh Sadredini"
                },
                "author": "Elaheh Sadredini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23179v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17138v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17138v4",
                "updated": "2025-09-27T07:41:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    27,
                    7,
                    41,
                    38,
                    5,
                    270,
                    0
                ],
                "published": "2025-05-22T06:12:42Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    6,
                    12,
                    42,
                    3,
                    142,
                    0
                ],
                "title": "Runtime Adaptive Pruning for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Runtime Adaptive Pruning for LLM Inference"
                },
                "summary": "Large language models (LLMs) excel at language understanding and generation,\nbut their enormous computational and memory requirements hinder deployment.\nCompression offers a potential solution to mitigate these constraints. However,\nmost existing methods rely on fixed heuristics and thus fail to adapt to\nruntime memory variations or heterogeneous KV-cache demands arising from\ndiverse user requests. To address these limitations, we propose RAP, an elastic\npruning framework driven by reinforcement learning (RL) that dynamically\nadjusts compression strategies in a runtime-aware manner. Specifically, RAP\ndynamically tracks the evolving ratio between model parameters and KV-cache\nacross practical execution. Recognizing that FFNs house most parameters,\nwhereas parameter -light attention layers dominate KV-cache formation, the RL\nagent retains only those components that maximize utility within the current\nmemory budget, conditioned on instantaneous workload and device state.\nExtensive experiments results demonstrate that RAP outperforms state-of-the-art\nbaselines, marking the first time to jointly consider model weights and\nKV-cache on the fly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at language understanding and generation,\nbut their enormous computational and memory requirements hinder deployment.\nCompression offers a potential solution to mitigate these constraints. However,\nmost existing methods rely on fixed heuristics and thus fail to adapt to\nruntime memory variations or heterogeneous KV-cache demands arising from\ndiverse user requests. To address these limitations, we propose RAP, an elastic\npruning framework driven by reinforcement learning (RL) that dynamically\nadjusts compression strategies in a runtime-aware manner. Specifically, RAP\ndynamically tracks the evolving ratio between model parameters and KV-cache\nacross practical execution. Recognizing that FFNs house most parameters,\nwhereas parameter -light attention layers dominate KV-cache formation, the RL\nagent retains only those components that maximize utility within the current\nmemory budget, conditioned on instantaneous workload and device state.\nExtensive experiments results demonstrate that RAP outperforms state-of-the-art\nbaselines, marking the first time to jointly consider model weights and\nKV-cache on the fly."
                },
                "authors": [
                    {
                        "name": "Huanrong Liu"
                    },
                    {
                        "name": "Chunlin Tian"
                    },
                    {
                        "name": "Xuyang Wei"
                    },
                    {
                        "name": "Qingbiao Li"
                    },
                    {
                        "name": "Li Li"
                    }
                ],
                "author_detail": {
                    "name": "Li Li"
                },
                "author": "Li Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17138v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17138v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23094v1",
                "updated": "2025-09-27T04:07:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    27,
                    4,
                    7,
                    23,
                    5,
                    270,
                    0
                ],
                "published": "2025-09-27T04:07:23Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    4,
                    7,
                    23,
                    5,
                    270,
                    0
                ],
                "title": "d$^2$Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "d$^2$Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching"
                },
                "summary": "Diffusion-based large language models (dLLMs), despite their promising\nperformance, still suffer from inferior inference efficiency. This is because\ndLLMs rely on bidirectional attention and cannot directly benefit from the\nstandard key-value (KV) cache as autoregressive models (ARMs) do. To tackle\nthis issue, we introduce \\textit{Dual aDaptive Cache} (d$^2$Cache), which is a\ntraining-free approximate KV cache framework for accelerating dLLM inference.\nd$^2$Cache features a two-stage fine-grained selection strategy to identify\ntokens and adaptively update their KV states at each decoding step, while\ncaching the KV states of the remaining tokens for reuse. Furthermore,\nd$^2$Cache naturally offers a more reliable decoding alternative, which can\nenable quasi left-to-right generation and mitigate premature overconfidence in\ntokens at the end of the sequence. Extensive experimental results on two\nrepresentative dLLMs (\\ie, LLaDA and Dream) demonstrate that d$^2$Cache not\nonly achieves substantial inference speedups, but also yields consistent\nimprovements in generation quality. The code is available at\nhttps://github.com/Kamichanw/d2Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs), despite their promising\nperformance, still suffer from inferior inference efficiency. This is because\ndLLMs rely on bidirectional attention and cannot directly benefit from the\nstandard key-value (KV) cache as autoregressive models (ARMs) do. To tackle\nthis issue, we introduce \\textit{Dual aDaptive Cache} (d$^2$Cache), which is a\ntraining-free approximate KV cache framework for accelerating dLLM inference.\nd$^2$Cache features a two-stage fine-grained selection strategy to identify\ntokens and adaptively update their KV states at each decoding step, while\ncaching the KV states of the remaining tokens for reuse. Furthermore,\nd$^2$Cache naturally offers a more reliable decoding alternative, which can\nenable quasi left-to-right generation and mitigate premature overconfidence in\ntokens at the end of the sequence. Extensive experimental results on two\nrepresentative dLLMs (\\ie, LLaDA and Dream) demonstrate that d$^2$Cache not\nonly achieves substantial inference speedups, but also yields consistent\nimprovements in generation quality. The code is available at\nhttps://github.com/Kamichanw/d2Cache."
                },
                "authors": [
                    {
                        "name": "Yuchu Jiang"
                    },
                    {
                        "name": "Yue Cai"
                    },
                    {
                        "name": "Xiangzhong Luo"
                    },
                    {
                        "name": "Jiale Fu"
                    },
                    {
                        "name": "Jiarui Wang"
                    },
                    {
                        "name": "Chonghan Liu"
                    },
                    {
                        "name": "Xu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xu Yang"
                },
                "author": "Xu Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24357v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24357v3",
                "updated": "2025-09-27T03:37:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    27,
                    3,
                    37,
                    40,
                    5,
                    270,
                    0
                ],
                "published": "2025-05-30T08:49:27Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    8,
                    49,
                    27,
                    4,
                    150,
                    0
                ],
                "title": "ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline\n  Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline\n  Calibration"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance, but\ntheir long-context reasoning remains constrained by the excessive memory\nrequired for the Key-Value (KV) cache. This makes KV cache compression a\ncritical step toward efficient long-context inference. Recent methods have\nexplored low-rank techniques to reduce the hidden size of the KV cache.\nHowever, they neglect the distinct roles and varying importance of Keys and\nValues, leading to significant performance drops under high compression. To\naddress this, we propose ReCalKV, a post-training low-rank KV cache compression\napproach with tailored strategies for Keys and Values. For Keys, we propose\nHead-wise Similarity aware Reordering (HSR), which clusters structurally\nsimilar heads into groups, enabling more accurate low-rank approximation via\ngrouped SVD. For Values, we propose Offline Value Calibration (OVC), which\nefficiently calibrates the value projection matrix using calibration data\nwithout training, ensuring an accurate representation of contextual\ninformation. Extensive experiments show that ReCalKV consistently outperforms\nexisting low-rank compression methods, achieving high compression ratios with\nminimal performance loss. The code and models will be available\nat:https://github.com/XIANGLONGYAN/ReCalKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance, but\ntheir long-context reasoning remains constrained by the excessive memory\nrequired for the Key-Value (KV) cache. This makes KV cache compression a\ncritical step toward efficient long-context inference. Recent methods have\nexplored low-rank techniques to reduce the hidden size of the KV cache.\nHowever, they neglect the distinct roles and varying importance of Keys and\nValues, leading to significant performance drops under high compression. To\naddress this, we propose ReCalKV, a post-training low-rank KV cache compression\napproach with tailored strategies for Keys and Values. For Keys, we propose\nHead-wise Similarity aware Reordering (HSR), which clusters structurally\nsimilar heads into groups, enabling more accurate low-rank approximation via\ngrouped SVD. For Values, we propose Offline Value Calibration (OVC), which\nefficiently calibrates the value projection matrix using calibration data\nwithout training, ensuring an accurate representation of contextual\ninformation. Extensive experiments show that ReCalKV consistently outperforms\nexisting low-rank compression methods, achieving high compression ratios with\nminimal performance loss. The code and models will be available\nat:https://github.com/XIANGLONGYAN/ReCalKV."
                },
                "authors": [
                    {
                        "name": "Xianglong Yan"
                    },
                    {
                        "name": "Zhiteng Li"
                    },
                    {
                        "name": "Tianao Zhang"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Yulun Zhang"
                    },
                    {
                        "name": "Xiaokang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokang Yang"
                },
                "author": "Xiaokang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24357v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24357v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03771v4",
                "updated": "2025-09-26T21:40:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    21,
                    40,
                    58,
                    4,
                    269,
                    0
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "vCache: Verified Semantic Prompt Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vCache: Verified Semantic Prompt Caching"
                },
                "summary": "Semantic caches return cached responses for semantically similar prompts to\nreduce LLM inference latency and cost. They embed cached prompts and store them\nalongside their response in a vector database. Embedding similarity metrics\nassign a numerical score to quantify the similarity between a request and its\nnearest neighbor prompt from the cache. Existing systems use the same static\nsimilarity threshold across all requests to determine whether two prompts can\nshare similar responses. However, we observe that static thresholds do not give\nformal correctness guarantees, can result in unexpected error rates, and lead\nto suboptimal cache hit rates. This paper proposes vCache, the first verified\nsemantic cache with user-defined error rate guarantees. It employs an online\nlearning algorithm to estimate an optimal threshold for each cached prompt,\nenabling reliable cache responses without additional training. Our experiments\nshow that vCache consistently meets the specified error bounds while\noutperforming state-of-the-art static-threshold and fine-tuned embedding\nbaselines. We release the vCache implementation and three benchmarks to support\nfuture research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caches return cached responses for semantically similar prompts to\nreduce LLM inference latency and cost. They embed cached prompts and store them\nalongside their response in a vector database. Embedding similarity metrics\nassign a numerical score to quantify the similarity between a request and its\nnearest neighbor prompt from the cache. Existing systems use the same static\nsimilarity threshold across all requests to determine whether two prompts can\nshare similar responses. However, we observe that static thresholds do not give\nformal correctness guarantees, can result in unexpected error rates, and lead\nto suboptimal cache hit rates. This paper proposes vCache, the first verified\nsemantic cache with user-defined error rate guarantees. It employs an online\nlearning algorithm to estimate an optimal threshold for each cached prompt,\nenabling reliable cache responses without additional training. Our experiments\nshow that vCache consistently meets the specified error bounds while\noutperforming state-of-the-art static-threshold and fine-tuned embedding\nbaselines. We release the vCache implementation and three benchmarks to support\nfuture research."
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Aditya Desai"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Kyle Chu"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03771v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03771v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22875v1",
                "updated": "2025-09-26T19:40:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    19,
                    40,
                    33,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T19:40:33Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    19,
                    40,
                    33,
                    4,
                    269,
                    0
                ],
                "title": "On KV-Poisson Structure and related invariants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On KV-Poisson Structure and related invariants"
                },
                "summary": "We propose an deepened analysis of KV-Poisson structures of on IR^2. We\npresent their classification their properties an their possible applications in\ndifferent domains. We prove that these structure give rise to a new\nCohomological invariant. We explicitly compute the Cohomological groups of some\nof these structures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose an deepened analysis of KV-Poisson structures of on IR^2. We\npresent their classification their properties an their possible applications in\ndifferent domains. We prove that these structure give rise to a new\nCohomological invariant. We explicitly compute the Cohomological groups of some\nof these structures."
                },
                "authors": [
                    {
                        "name": "Prosper Rosaire Mama Assandje"
                    },
                    {
                        "name": "Herguey Mopeng"
                    },
                    {
                        "name": "Joseph Dongho"
                    }
                ],
                "author_detail": {
                    "name": "Joseph Dongho"
                },
                "author": "Joseph Dongho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.DG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.DG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08799v2",
                "updated": "2025-09-26T17:59:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    59,
                    54,
                    4,
                    269,
                    0
                ],
                "published": "2025-07-11T17:59:36Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    59,
                    36,
                    4,
                    192,
                    0
                ],
                "title": "KV Cache Steering for Controlling Frozen LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Steering for Controlling Frozen LLMs"
                },
                "summary": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach constructs\nsteering vectors from reasoning traces, obtained either from teacher models\n(e.g., GPT-4o) or existing human annotations, that shift model behavior toward\nmore explicit, multi-step reasoning without fine-tuning or prompt\nmodifications. Experimental evaluations on diverse reasoning benchmarks\ndemonstrate that cache steering improves both the qualitative structure of\nmodel reasoning and quantitative task performance. Additional experiments show\nthat the method also scales to larger models and yields further gains on\nchallenging datasets such as GPQA and MATH. Compared to prior activation\nsteering techniques that require continuous interventions, our one-shot cache\nsteering offers substantial advantages in terms of inference latency,\nhyperparameter stability, and ease of integration with existing inference APIs.\nBeyond mere reasoning induction, we show that cache steering enables\ncontrollable transfer of reasoning styles (e.g., stepwise, causal, analogical),\nmaking it a practical tool for behavior-level guidance of language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach constructs\nsteering vectors from reasoning traces, obtained either from teacher models\n(e.g., GPT-4o) or existing human annotations, that shift model behavior toward\nmore explicit, multi-step reasoning without fine-tuning or prompt\nmodifications. Experimental evaluations on diverse reasoning benchmarks\ndemonstrate that cache steering improves both the qualitative structure of\nmodel reasoning and quantitative task performance. Additional experiments show\nthat the method also scales to larger models and yields further gains on\nchallenging datasets such as GPQA and MATH. Compared to prior activation\nsteering techniques that require continuous interventions, our one-shot cache\nsteering offers substantial advantages in terms of inference latency,\nhyperparameter stability, and ease of integration with existing inference APIs.\nBeyond mere reasoning induction, we show that cache steering enables\ncontrollable transfer of reasoning styles (e.g., stepwise, causal, analogical),\nmaking it a practical tool for behavior-level guidance of language models."
                },
                "authors": [
                    {
                        "name": "Max Belitsky"
                    },
                    {
                        "name": "Dawid J. Kopiczko"
                    },
                    {
                        "name": "Michael Dorkenwald"
                    },
                    {
                        "name": "M. Jehanzeb Mirza"
                    },
                    {
                        "name": "James R. Glass"
                    },
                    {
                        "name": "Cees G. M. Snoek"
                    },
                    {
                        "name": "Yuki M. Asano"
                    }
                ],
                "author_detail": {
                    "name": "Yuki M. Asano"
                },
                "author": "Yuki M. Asano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22622v1",
                "updated": "2025-09-26T17:48:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    48,
                    24,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T17:48:24Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    48,
                    24,
                    4,
                    269,
                    0
                ],
                "title": "LongLive: Real-time Interactive Long Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongLive: Real-time Interactive Long Video Generation"
                },
                "summary": "We present LongLive, a frame-level autoregressive (AR) framework for\nreal-time and interactive long video generation. Long video generation presents\nchallenges in both efficiency and quality. Diffusion and Diffusion-Forcing\nmodels can produce high-quality videos but suffer from low efficiency due to\nbidirectional attention. Causal attention AR models support KV caching for\nfaster inference, but often degrade in quality on long videos due to memory\nchallenges during long-video training. In addition, beyond static prompt-based\ngeneration, interactive capabilities, such as streaming prompt inputs, are\ncritical for dynamic content creation, enabling users to guide narratives in\nreal time. This interactive requirement significantly increases complexity,\nespecially in ensuring visual consistency and semantic coherence during prompt\ntransitions. To address these challenges, LongLive adopts a causal, frame-level\nAR design that integrates a KV-recache mechanism that refreshes cached states\nwith new prompts for smooth, adherent switches; streaming long tuning to enable\nlong video training and to align training and inference (train-long-test-long);\nand short window attention paired with a frame-level attention sink, shorten as\nframe sink, preserving long-range consistency while enabling faster generation.\nWith these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model\nto minute-long generation in just 32 GPU-days. At inference, LongLive sustains\n20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both\nshort and long videos. LongLive supports up to 240-second videos on a single\nH100 GPU. LongLive further supports INT8-quantized inference with only marginal\nquality loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LongLive, a frame-level autoregressive (AR) framework for\nreal-time and interactive long video generation. Long video generation presents\nchallenges in both efficiency and quality. Diffusion and Diffusion-Forcing\nmodels can produce high-quality videos but suffer from low efficiency due to\nbidirectional attention. Causal attention AR models support KV caching for\nfaster inference, but often degrade in quality on long videos due to memory\nchallenges during long-video training. In addition, beyond static prompt-based\ngeneration, interactive capabilities, such as streaming prompt inputs, are\ncritical for dynamic content creation, enabling users to guide narratives in\nreal time. This interactive requirement significantly increases complexity,\nespecially in ensuring visual consistency and semantic coherence during prompt\ntransitions. To address these challenges, LongLive adopts a causal, frame-level\nAR design that integrates a KV-recache mechanism that refreshes cached states\nwith new prompts for smooth, adherent switches; streaming long tuning to enable\nlong video training and to align training and inference (train-long-test-long);\nand short window attention paired with a frame-level attention sink, shorten as\nframe sink, preserving long-range consistency while enabling faster generation.\nWith these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model\nto minute-long generation in just 32 GPU-days. At inference, LongLive sustains\n20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both\nshort and long videos. LongLive supports up to 240-second videos on a single\nH100 GPU. LongLive further supports INT8-quantized inference with only marginal\nquality loss."
                },
                "authors": [
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Ruihang Chu"
                    },
                    {
                        "name": "Yicheng Xiao"
                    },
                    {
                        "name": "Yuyang Zhao"
                    },
                    {
                        "name": "Xianbang Wang"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Enze Xie"
                    },
                    {
                        "name": "Yingcong Chen"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Yukang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yukang Chen"
                },
                "author": "Yukang Chen",
                "arxiv_comment": "Code, model, and demos are available at\n  https://github.com/NVlabs/LongLive",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22548v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22548v1",
                "updated": "2025-09-26T16:29:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    29,
                    37,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:29:37Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    29,
                    37,
                    4,
                    269,
                    0
                ],
                "title": "JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory\n  for Vision-Language Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory\n  for Vision-Language Navigation"
                },
                "summary": "Vision-and-Language Navigation requires an embodied agent to navigate through\nunseen environments, guided by natural language instructions and a continuous\nvideo stream. Recent advances in VLN have been driven by the powerful semantic\nunderstanding of Multimodal Large Language Models. However, these methods\ntypically rely on explicit semantic memory, such as building textual cognitive\nmaps or storing historical visual frames. This type of method suffers from\nspatial information loss, computational redundancy, and memory bloat, which\nimpede efficient navigation. Inspired by the implicit scene representation in\nhuman navigation, analogous to the left brain's semantic understanding and the\nright brain's spatial cognition, we propose JanusVLN, a novel VLN framework\nfeaturing a dual implicit neural memory that models spatial-geometric and\nvisual-semantic memory as separate, compact, and fixed-size neural\nrepresentations. This framework first extends the MLLM to incorporate 3D prior\nknowledge from the spatial-geometric encoder, thereby enhancing the spatial\nreasoning capabilities of models based solely on RGB input. Then, the\nhistorical key-value caches from the spatial-geometric and visual-semantic\nencoders are constructed into a dual implicit memory. By retaining only the KVs\nof tokens in the initial and sliding window, redundant computation is avoided,\nenabling efficient incremental updates. Extensive experiments demonstrate that\nJanusVLN outperforms over 20 recent methods to achieve SOTA performance. For\nexample, the success rate improves by 10.5-35.5 compared to methods using\nmultiple data types as input and by 3.6-10.8 compared to methods using more RGB\ntraining data. This indicates that the proposed dual implicit neural memory, as\na novel paradigm, explores promising new directions for future VLN research.\nOurs project page: https://miv-xjtu.github.io/JanusVLN.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation requires an embodied agent to navigate through\nunseen environments, guided by natural language instructions and a continuous\nvideo stream. Recent advances in VLN have been driven by the powerful semantic\nunderstanding of Multimodal Large Language Models. However, these methods\ntypically rely on explicit semantic memory, such as building textual cognitive\nmaps or storing historical visual frames. This type of method suffers from\nspatial information loss, computational redundancy, and memory bloat, which\nimpede efficient navigation. Inspired by the implicit scene representation in\nhuman navigation, analogous to the left brain's semantic understanding and the\nright brain's spatial cognition, we propose JanusVLN, a novel VLN framework\nfeaturing a dual implicit neural memory that models spatial-geometric and\nvisual-semantic memory as separate, compact, and fixed-size neural\nrepresentations. This framework first extends the MLLM to incorporate 3D prior\nknowledge from the spatial-geometric encoder, thereby enhancing the spatial\nreasoning capabilities of models based solely on RGB input. Then, the\nhistorical key-value caches from the spatial-geometric and visual-semantic\nencoders are constructed into a dual implicit memory. By retaining only the KVs\nof tokens in the initial and sliding window, redundant computation is avoided,\nenabling efficient incremental updates. Extensive experiments demonstrate that\nJanusVLN outperforms over 20 recent methods to achieve SOTA performance. For\nexample, the success rate improves by 10.5-35.5 compared to methods using\nmultiple data types as input and by 3.6-10.8 compared to methods using more RGB\ntraining data. This indicates that the proposed dual implicit neural memory, as\na novel paradigm, explores promising new directions for future VLN research.\nOurs project page: https://miv-xjtu.github.io/JanusVLN.github.io/."
                },
                "authors": [
                    {
                        "name": "Shuang Zeng"
                    },
                    {
                        "name": "Dekang Qi"
                    },
                    {
                        "name": "Xinyuan Chang"
                    },
                    {
                        "name": "Feng Xiong"
                    },
                    {
                        "name": "Shichao Xie"
                    },
                    {
                        "name": "Xiaolong Wu"
                    },
                    {
                        "name": "Shiyi Liang"
                    },
                    {
                        "name": "Mu Xu"
                    },
                    {
                        "name": "Xing Wei"
                    }
                ],
                "author_detail": {
                    "name": "Xing Wei"
                },
                "author": "Xing Wei",
                "arxiv_comment": "Project page: https://miv-xjtu.github.io/JanusVLN.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22548v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22548v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22516v1",
                "updated": "2025-09-26T16:00:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    0,
                    36,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:00:36Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    0,
                    36,
                    4,
                    269,
                    0
                ],
                "title": "TrueGradeAI: Retrieval-Augmented and Bias-Resistant AI for Transparent\n  and Explainable Digital Assessments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrueGradeAI: Retrieval-Augmented and Bias-Resistant AI for Transparent\n  and Explainable Digital Assessments"
                },
                "summary": "This paper introduces TrueGradeAI, an AI-driven digital examination framework\ndesigned to overcome the shortcomings of traditional paper-based assessments,\nincluding excessive paper usage, logistical complexity, grading delays, and\nevaluator bias. The system preserves natural handwriting by capturing stylus\ninput on secure tablets and applying transformer-based optical character\nrecognition for transcription. Evaluation is conducted through a\nretrieval-augmented pipeline that integrates faculty solutions, cache layers,\nand external references, enabling a large language model to assign scores with\nexplicit, evidence-linked reasoning. Unlike prior tablet-based exam systems\nthat primarily digitize responses, TrueGradeAI advances the field by\nincorporating explainable automation, bias mitigation, and auditable grading\ntrails. By uniting handwriting preservation with scalable and transparent\nevaluation, the framework reduces environmental costs, accelerates feedback\ncycles, and progressively builds a reusable knowledge base, while actively\nworking to mitigate grading bias and ensure fairness in assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces TrueGradeAI, an AI-driven digital examination framework\ndesigned to overcome the shortcomings of traditional paper-based assessments,\nincluding excessive paper usage, logistical complexity, grading delays, and\nevaluator bias. The system preserves natural handwriting by capturing stylus\ninput on secure tablets and applying transformer-based optical character\nrecognition for transcription. Evaluation is conducted through a\nretrieval-augmented pipeline that integrates faculty solutions, cache layers,\nand external references, enabling a large language model to assign scores with\nexplicit, evidence-linked reasoning. Unlike prior tablet-based exam systems\nthat primarily digitize responses, TrueGradeAI advances the field by\nincorporating explainable automation, bias mitigation, and auditable grading\ntrails. By uniting handwriting preservation with scalable and transparent\nevaluation, the framework reduces environmental costs, accelerates feedback\ncycles, and progressively builds a reusable knowledge base, while actively\nworking to mitigate grading bias and ensure fairness in assessment."
                },
                "authors": [
                    {
                        "name": "Rakesh Thakur"
                    },
                    {
                        "name": "Shivaansh Kaushik"
                    },
                    {
                        "name": "Gauri Chopra"
                    },
                    {
                        "name": "Harsh Rohilla"
                    }
                ],
                "author_detail": {
                    "name": "Harsh Rohilla"
                },
                "author": "Harsh Rohilla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22512v1",
                "updated": "2025-09-26T15:54:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    54,
                    50,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T15:54:50Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    54,
                    50,
                    4,
                    269,
                    0
                ],
                "title": "AxLLM: accelerator architecture for large language models with\n  computation reuse capability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AxLLM: accelerator architecture for large language models with\n  computation reuse capability"
                },
                "summary": "Large language models demand massive computational power and memory\nresources, posing significant challenges for efficient deployment. While\nquantization has been widely explored to reduce model size and computation,\nthis paper demonstrates an additional benefit: quantization increases parameter\nlocality, creating opportunities for computation reuse. Building on this\ninsight, we propose AxLLM, a hardware accelerator architecture designed for\nquantized models. Axllm introduces a novel redundancy elimination technique\nthat caches and reuses multiplication results for repeated weight values,\nsubstantially reducing redundant operations. The architecture features dual\nmultiply and reuse pipelines, efficiently supporting both base models and LoRA\nfine-tuned models without altering parameters, retraining, or requiring offline\npreprocessing. Experimental results show that AxLLM achieves up to 90%\nreduction in computations, delivering 28% lower energy consumption and a 1.7x\nspeedup over baseline execution. These results highlight Axllm as a scalable\nand efficient solution for accelerating LLMs on specialized hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models demand massive computational power and memory\nresources, posing significant challenges for efficient deployment. While\nquantization has been widely explored to reduce model size and computation,\nthis paper demonstrates an additional benefit: quantization increases parameter\nlocality, creating opportunities for computation reuse. Building on this\ninsight, we propose AxLLM, a hardware accelerator architecture designed for\nquantized models. Axllm introduces a novel redundancy elimination technique\nthat caches and reuses multiplication results for repeated weight values,\nsubstantially reducing redundant operations. The architecture features dual\nmultiply and reuse pipelines, efficiently supporting both base models and LoRA\nfine-tuned models without altering parameters, retraining, or requiring offline\npreprocessing. Experimental results show that AxLLM achieves up to 90%\nreduction in computations, delivering 28% lower energy consumption and a 1.7x\nspeedup over baseline execution. These results highlight Axllm as a scalable\nand efficient solution for accelerating LLMs on specialized hardware."
                },
                "authors": [
                    {
                        "name": "Soroush Ahadi"
                    },
                    {
                        "name": "Mehdi Modarressi"
                    },
                    {
                        "name": "Masoud Daneshtalab"
                    }
                ],
                "author_detail": {
                    "name": "Masoud Daneshtalab"
                },
                "author": "Masoud Daneshtalab",
                "arxiv_comment": "7 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "n/a",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22488v1",
                "updated": "2025-09-26T15:35:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    35,
                    5,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T15:35:05Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    35,
                    5,
                    4,
                    269,
                    0
                ],
                "title": "Organ dose optimization for a point-of-care forearm X-ray\n  photon-counting CT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Organ dose optimization for a point-of-care forearm X-ray\n  photon-counting CT"
                },
                "summary": "Background: Spectral shaping is a computed tomography (CT) dose optimization\ntechnique that adjusts source voltage and filtration to reduce patient\nradiation exposure without compromising image quality. Traditionally, radiation\ndose has been assessed using the computed tomography dose index (CTDI).\nHowever, emerging dosimetric approaches aim to enable patient-specific\nevaluations by estimating organ absorbed doses, providing a more accurate\nrepresentation of the biological impact. This study investigates spectral\nshaping for an extremity photon-counting detector (PCD) CT, through organ\nabsorbed dose estimation and image quality evaluation. Method: Monte Carlo\nsimulations were conducted to evaluate various combinations of source voltage\nand filtration. Tube voltage ranged from 80 to 140 kV, combined with three\ndistinct filtration material and thicknesses. Simulations included three\nstages: a standardized phantom for CTDI assessment, an adult forearm phantom\nfor organ dose measurement, and an image quality phantom for evaluation of an\nadvanced image quality metric: the detectability index. Results: In a wrist\nPCD-CT imaging protocol, operating the source at 80 kV can reduce the radiation\ndose by up to 50%. This reduction is achieved while maintaining the same\ndetectability index value as the standard 120 kV protocol. However, the optimal\nfiltration depends on the organ targeted for dose reduction, as bone and skin\nbenefit from opposing filtration approaches. While CTDI provides a useful\ninitial estimate, it may lead to suboptimal optimization compared to\norgan-specific dose evaluation. Conclusions: Patient-specific dosimetry based\non organ absorbed dose estimation offers a more accurate framework for\noptimizing CT protocols through spectral shaping than conventional CTDI-based\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Spectral shaping is a computed tomography (CT) dose optimization\ntechnique that adjusts source voltage and filtration to reduce patient\nradiation exposure without compromising image quality. Traditionally, radiation\ndose has been assessed using the computed tomography dose index (CTDI).\nHowever, emerging dosimetric approaches aim to enable patient-specific\nevaluations by estimating organ absorbed doses, providing a more accurate\nrepresentation of the biological impact. This study investigates spectral\nshaping for an extremity photon-counting detector (PCD) CT, through organ\nabsorbed dose estimation and image quality evaluation. Method: Monte Carlo\nsimulations were conducted to evaluate various combinations of source voltage\nand filtration. Tube voltage ranged from 80 to 140 kV, combined with three\ndistinct filtration material and thicknesses. Simulations included three\nstages: a standardized phantom for CTDI assessment, an adult forearm phantom\nfor organ dose measurement, and an image quality phantom for evaluation of an\nadvanced image quality metric: the detectability index. Results: In a wrist\nPCD-CT imaging protocol, operating the source at 80 kV can reduce the radiation\ndose by up to 50%. This reduction is achieved while maintaining the same\ndetectability index value as the standard 120 kV protocol. However, the optimal\nfiltration depends on the organ targeted for dose reduction, as bone and skin\nbenefit from opposing filtration approaches. While CTDI provides a useful\ninitial estimate, it may lead to suboptimal optimization compared to\norgan-specific dose evaluation. Conclusions: Patient-specific dosimetry based\non organ absorbed dose estimation offers a more accurate framework for\noptimizing CT protocols through spectral shaping than conventional CTDI-based\napproaches."
                },
                "authors": [
                    {
                        "name": "Pierre-Antoine Rodesch"
                    },
                    {
                        "name": "AnaÃ¯s Viry"
                    },
                    {
                        "name": "Mouad Khorsi"
                    },
                    {
                        "name": "Fabio Becce"
                    },
                    {
                        "name": "JÃ©rÃ´me Damet"
                    },
                    {
                        "name": "LucÃ­a Gallego Manzano"
                    }
                ],
                "author_detail": {
                    "name": "LucÃ­a Gallego Manzano"
                },
                "author": "LucÃ­a Gallego Manzano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16950v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16950v3",
                "updated": "2025-09-26T14:35:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    35,
                    4,
                    4,
                    269,
                    0
                ],
                "published": "2025-05-22T17:33:49Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    33,
                    49,
                    3,
                    142,
                    0
                ],
                "title": "Bottlenecked Transformers: Periodic KV Cache Consolidation for\n  Generalised Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bottlenecked Transformers: Periodic KV Cache Consolidation for\n  Generalised Reasoning"
                },
                "summary": "Transformer LLMs have been shown to exhibit strong reasoning ability that\nscales with inference-time compute, most prominently through token-space\n\"thinking\" chains of thought. A growing line of work pushes extra computation\ninto the model's latent space, which we term Auxiliary Latent-Space Computation\n(ALSC). Existing ALSC methods largely fall into three buckets: (i)\ntoken-mediated latent rollouts, (ii) residual/activation steering, and (iii)\nmemory (KV) compression. An underexplored alternative is memory\nconsolidation/reconsolidation, two processes in the brain that are responsible\nfor stabilising newly formed memory traces, and, upon recall, transiently\nrendering established traces plastic such they can integrate new contextual\ninformation before restabilising. In Transformer LLMs, this can be seen as\nanalogous to performing in-place rewrites of new KV segments, and rewrites of\nrecalled past segments. In this work, we give a theoretical justification as to\nwhy memory (re)consolidation via KV cache rewrites is beneficial for improved\nreasoning. We do this through the lens of Information Bottleneck (IB) theory,\nwhich posits that model generalisation emerges from an optimal balance between\ninput information compression and retention of predictive information in latent\nrepresentations. We then introduce the Bottlenecked Transformer, which augments\na backbone LLM with a Cache Processor, an auxiliary Transformer that performs\nperiodic, non-causal, in-place KV rewrites at newline-delimited reasoning step\nboundaries. The Processor consolidates recently written KV entries and\nreconsolidates a small, top-k attention-selected set of prior entries. We\nevaluate our Bottlenecked Transformer architecture on math reasoning\nbenchmarks. Our model sees consistent performance gains over vanilla\nTransformers and pause-token augmented baselines, with gains of up to +6.6pp\nfor selected tasks/backbones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer LLMs have been shown to exhibit strong reasoning ability that\nscales with inference-time compute, most prominently through token-space\n\"thinking\" chains of thought. A growing line of work pushes extra computation\ninto the model's latent space, which we term Auxiliary Latent-Space Computation\n(ALSC). Existing ALSC methods largely fall into three buckets: (i)\ntoken-mediated latent rollouts, (ii) residual/activation steering, and (iii)\nmemory (KV) compression. An underexplored alternative is memory\nconsolidation/reconsolidation, two processes in the brain that are responsible\nfor stabilising newly formed memory traces, and, upon recall, transiently\nrendering established traces plastic such they can integrate new contextual\ninformation before restabilising. In Transformer LLMs, this can be seen as\nanalogous to performing in-place rewrites of new KV segments, and rewrites of\nrecalled past segments. In this work, we give a theoretical justification as to\nwhy memory (re)consolidation via KV cache rewrites is beneficial for improved\nreasoning. We do this through the lens of Information Bottleneck (IB) theory,\nwhich posits that model generalisation emerges from an optimal balance between\ninput information compression and retention of predictive information in latent\nrepresentations. We then introduce the Bottlenecked Transformer, which augments\na backbone LLM with a Cache Processor, an auxiliary Transformer that performs\nperiodic, non-causal, in-place KV rewrites at newline-delimited reasoning step\nboundaries. The Processor consolidates recently written KV entries and\nreconsolidates a small, top-k attention-selected set of prior entries. We\nevaluate our Bottlenecked Transformer architecture on math reasoning\nbenchmarks. Our model sees consistent performance gains over vanilla\nTransformers and pause-token augmented baselines, with gains of up to +6.6pp\nfor selected tasks/backbones."
                },
                "authors": [
                    {
                        "name": "Adnan Oomerjee"
                    },
                    {
                        "name": "Zafeirios Fountas"
                    },
                    {
                        "name": "Haitham Bou-Ammar"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16950v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16950v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22323v1",
                "updated": "2025-09-26T13:20:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    20,
                    52,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T13:20:52Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    20,
                    52,
                    4,
                    269,
                    0
                ],
                "title": "RAPID^3: Tri-Level Reinforced Acceleration Policies for Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAPID^3: Tri-Level Reinforced Acceleration Policies for Diffusion\n  Transformer"
                },
                "summary": "Diffusion Transformers (DiTs) excel at visual generation yet remain hampered\nby slow sampling. Existing training-free accelerators - step reduction, feature\ncaching, and sparse attention - enhance inference speed but typically rely on a\nuniform heuristic or a manually designed adaptive strategy for all images,\nleaving quality on the table. Alternatively, dynamic neural networks offer\nper-image adaptive acceleration, but their high fine-tuning costs limit broader\napplicability. To address these limitations, we introduce RAPID3: Tri-Level\nReinforced Acceleration Policies for Diffusion Transformers, a framework that\ndelivers image-wise acceleration with zero updates to the base generator.\nSpecifically, three lightweight policy heads - Step-Skip, Cache-Reuse, and\nSparse-Attention - observe the current denoising state and independently decide\ntheir corresponding speed-up at each timestep. All policy parameters are\ntrained online via Group Relative Policy Optimization (GRPO) while the\ngenerator remains frozen. Meanwhile, an adversarially learned discriminator\naugments the reward signal, discouraging reward hacking by boosting returns\nonly when generated samples stay close to the original model's distribution.\nAcross state-of-the-art DiT backbones, including Stable Diffusion 3 and FLUX,\nRAPID3 achieves nearly 3x faster sampling with competitive generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) excel at visual generation yet remain hampered\nby slow sampling. Existing training-free accelerators - step reduction, feature\ncaching, and sparse attention - enhance inference speed but typically rely on a\nuniform heuristic or a manually designed adaptive strategy for all images,\nleaving quality on the table. Alternatively, dynamic neural networks offer\nper-image adaptive acceleration, but their high fine-tuning costs limit broader\napplicability. To address these limitations, we introduce RAPID3: Tri-Level\nReinforced Acceleration Policies for Diffusion Transformers, a framework that\ndelivers image-wise acceleration with zero updates to the base generator.\nSpecifically, three lightweight policy heads - Step-Skip, Cache-Reuse, and\nSparse-Attention - observe the current denoising state and independently decide\ntheir corresponding speed-up at each timestep. All policy parameters are\ntrained online via Group Relative Policy Optimization (GRPO) while the\ngenerator remains frozen. Meanwhile, an adversarially learned discriminator\naugments the reward signal, discouraging reward hacking by boosting returns\nonly when generated samples stay close to the original model's distribution.\nAcross state-of-the-art DiT backbones, including Stable Diffusion 3 and FLUX,\nRAPID3 achieves nearly 3x faster sampling with competitive generation quality."
                },
                "authors": [
                    {
                        "name": "Wangbo Zhao"
                    },
                    {
                        "name": "Yizeng Han"
                    },
                    {
                        "name": "Zhiwei Tang"
                    },
                    {
                        "name": "Jiasheng Tang"
                    },
                    {
                        "name": "Pengfei Zhou"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Yang You"
                    }
                ],
                "author_detail": {
                    "name": "Yang You"
                },
                "author": "Yang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v8",
                "updated": "2025-09-26T10:00:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    10,
                    0,
                    54,
                    4,
                    269,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty, and only those\nelements are processed to draw the next frame, dramatically reducing latency.\nHowever, the standard incremental layout algorithm must search the page for\ndirty elements, accessing auxiliary elements in the process. These auxiliary\nelements add cache misses and stalled cycles, and are responsible for a sizable\nfraction of all layout latency. We introduce a new, faster incremental layout\nalgorithm called Spineless Traversal. Spineless Traversal uses a\ncache-friendlier priority queue algorithm that avoids accessing auxiliary nodes\nand thus reduces cache traffic and stalls. This leads to dramatic speedups on\nthe most latency-critical interactions such as hovering, typing, and animation.\nMoreover, thanks to numerous low-level optimizations, Spineless Traversal is\ncompetitive across the whole spectrum of incremental layout workloads.\nSpineless Traversal is faster than the standard approach on 83.0% of 2216\nbenchmarks, with a mean speedup of 1.80x concentrated in the most\nlatency-critical interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty, and only those\nelements are processed to draw the next frame, dramatically reducing latency.\nHowever, the standard incremental layout algorithm must search the page for\ndirty elements, accessing auxiliary elements in the process. These auxiliary\nelements add cache misses and stalled cycles, and are responsible for a sizable\nfraction of all layout latency. We introduce a new, faster incremental layout\nalgorithm called Spineless Traversal. Spineless Traversal uses a\ncache-friendlier priority queue algorithm that avoids accessing auxiliary nodes\nand thus reduces cache traffic and stalls. This leads to dramatic speedups on\nthe most latency-critical interactions such as hovering, typing, and animation.\nMoreover, thanks to numerous low-level optimizations, Spineless Traversal is\ncompetitive across the whole spectrum of incremental layout workloads.\nSpineless Traversal is faster than the standard approach on 83.0% of 2216\nbenchmarks, with a mean speedup of 1.80x concentrated in the most\nlatency-critical interactions."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22756v1",
                "updated": "2025-09-26T09:33:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    9,
                    33,
                    36,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T09:33:36Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    9,
                    33,
                    36,
                    4,
                    269,
                    0
                ],
                "title": "Persistent Autoregressive Mapping with Traffic Rules for Autonomous\n  Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Autoregressive Mapping with Traffic Rules for Autonomous\n  Driving"
                },
                "summary": "Safe autonomous driving requires both accurate HD map construction and\npersistent awareness of traffic rules, even when their associated signs are no\nlonger visible. However, existing methods either focus solely on geometric\nelements or treat rules as temporary classifications, failing to capture their\npersistent effectiveness across extended driving sequences. In this paper, we\npresent PAMR (Persistent Autoregressive Mapping with Traffic Rules), a novel\nframework that performs autoregressive co-construction of lane vectors and\ntraffic rules from visual observations. Our approach introduces two key\nmechanisms: Map-Rule Co-Construction for processing driving scenes in temporal\nsegments, and Map-Rule Cache for maintaining rule consistency across these\nsegments. To properly evaluate continuous and consistent map generation, we\ndevelop MapDRv2, featuring improved lane geometry annotations. Extensive\nexperiments demonstrate that PAMR achieves superior performance in joint\nvector-rule mapping tasks, while maintaining persistent rule effectiveness\nthroughout extended driving sequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safe autonomous driving requires both accurate HD map construction and\npersistent awareness of traffic rules, even when their associated signs are no\nlonger visible. However, existing methods either focus solely on geometric\nelements or treat rules as temporary classifications, failing to capture their\npersistent effectiveness across extended driving sequences. In this paper, we\npresent PAMR (Persistent Autoregressive Mapping with Traffic Rules), a novel\nframework that performs autoregressive co-construction of lane vectors and\ntraffic rules from visual observations. Our approach introduces two key\nmechanisms: Map-Rule Co-Construction for processing driving scenes in temporal\nsegments, and Map-Rule Cache for maintaining rule consistency across these\nsegments. To properly evaluate continuous and consistent map generation, we\ndevelop MapDRv2, featuring improved lane geometry annotations. Extensive\nexperiments demonstrate that PAMR achieves superior performance in joint\nvector-rule mapping tasks, while maintaining persistent rule effectiveness\nthroughout extended driving sequences."
                },
                "authors": [
                    {
                        "name": "Shiyi Liang"
                    },
                    {
                        "name": "Xinyuan Chang"
                    },
                    {
                        "name": "Changjie Wu"
                    },
                    {
                        "name": "Huiyuan Yan"
                    },
                    {
                        "name": "Yifan Bai"
                    },
                    {
                        "name": "Xinran Liu"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Yujian Yuan"
                    },
                    {
                        "name": "Shuang Zeng"
                    },
                    {
                        "name": "Mu Xu"
                    },
                    {
                        "name": "Xing Wei"
                    }
                ],
                "author_detail": {
                    "name": "Xing Wei"
                },
                "author": "Xing Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13681v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13681v2",
                "updated": "2025-09-26T07:14:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    7,
                    14,
                    44,
                    4,
                    269,
                    0
                ],
                "published": "2025-07-18T06:12:08Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    6,
                    12,
                    8,
                    4,
                    199,
                    0
                ],
                "title": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for\n  Multi-Turn Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for\n  Multi-Turn Dialogues"
                },
                "summary": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. As a result,\nthese models cannot accurately identify and prioritize the most relevant\ncontext, leading to degraded response quality. In this paper, we present\nLoopServe, an adaptive dual-phase inference acceleration framework for large\nlanguage models in multi-turn dialogues. LoopServe introduces two main\ninnovations. First, it performs online sparsification during the prefilling\nphase by dynamically selecting the most important parts of the attention matrix\nfor each new input. Second, it uses progressive key value compression during\ndecoding by adaptively maintaining a relevant and efficient cache based on the\nmost recently generated output tokens. We also propose a new benchmark with\neleven multi-turn datasets that reflect realistic query positions and\nconversational dependencies. Extensive experiments demonstrate that LoopServe\nconsistently achieves superior effectiveness compared to existing baselines and\nsignificantly accelerates LLM inference across a wide range of long-context\ndialogue tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. As a result,\nthese models cannot accurately identify and prioritize the most relevant\ncontext, leading to degraded response quality. In this paper, we present\nLoopServe, an adaptive dual-phase inference acceleration framework for large\nlanguage models in multi-turn dialogues. LoopServe introduces two main\ninnovations. First, it performs online sparsification during the prefilling\nphase by dynamically selecting the most important parts of the attention matrix\nfor each new input. Second, it uses progressive key value compression during\ndecoding by adaptively maintaining a relevant and efficient cache based on the\nmost recently generated output tokens. We also propose a new benchmark with\neleven multi-turn datasets that reflect realistic query positions and\nconversational dependencies. Extensive experiments demonstrate that LoopServe\nconsistently achieves superior effectiveness compared to existing baselines and\nsignificantly accelerates LLM inference across a wide range of long-context\ndialogue tasks."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Darian Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Qingfa Xiao"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13681v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13681v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.05094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05094v1",
                "updated": "2025-10-06T17:57:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    57,
                    59,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T17:57:59Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    57,
                    59,
                    0,
                    279,
                    0
                ],
                "title": "VChain: Chain-of-Visual-Thought for Reasoning in Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VChain: Chain-of-Visual-Thought for Reasoning in Video Generation"
                },
                "summary": "Recent video generation models can produce smooth and visually appealing\nclips, but they often struggle to synthesize complex dynamics with a coherent\nchain of consequences. Accurately modeling visual outcomes and state\ntransitions over time remains a core challenge. In contrast, large language and\nmultimodal models (e.g., GPT-4o) exhibit strong visual state reasoning and\nfuture prediction capabilities. To bridge these strengths, we introduce VChain,\na novel inference-time chain-of-visual-thought framework that injects visual\nreasoning signals from multimodal models into video generation. Specifically,\nVChain contains a dedicated pipeline that leverages large multimodal models to\ngenerate a sparse set of critical keyframes as snapshots, which are then used\nto guide the sparse inference-time tuning of a pre-trained video generator only\nat these key moments. Our approach is tuning-efficient, introduces minimal\noverhead and avoids dense supervision. Extensive experiments on complex,\nmulti-step scenarios show that VChain significantly enhances the quality of\ngenerated videos.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent video generation models can produce smooth and visually appealing\nclips, but they often struggle to synthesize complex dynamics with a coherent\nchain of consequences. Accurately modeling visual outcomes and state\ntransitions over time remains a core challenge. In contrast, large language and\nmultimodal models (e.g., GPT-4o) exhibit strong visual state reasoning and\nfuture prediction capabilities. To bridge these strengths, we introduce VChain,\na novel inference-time chain-of-visual-thought framework that injects visual\nreasoning signals from multimodal models into video generation. Specifically,\nVChain contains a dedicated pipeline that leverages large multimodal models to\ngenerate a sparse set of critical keyframes as snapshots, which are then used\nto guide the sparse inference-time tuning of a pre-trained video generator only\nat these key moments. Our approach is tuning-efficient, introduces minimal\noverhead and avoids dense supervision. Extensive experiments on complex,\nmulti-step scenarios show that VChain significantly enhances the quality of\ngenerated videos."
                },
                "authors": [
                    {
                        "name": "Ziqi Huang"
                    },
                    {
                        "name": "Ning Yu"
                    },
                    {
                        "name": "Gordon Chen"
                    },
                    {
                        "name": "Haonan Qiu"
                    },
                    {
                        "name": "Paul Debevec"
                    },
                    {
                        "name": "Ziwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ziwei Liu"
                },
                "author": "Ziwei Liu",
                "arxiv_comment": "Project page: https://eyeline-labs.github.io/VChain Code:\n  https://github.com/Eyeline-Labs/VChain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01928v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01928v3",
                "updated": "2025-10-06T17:57:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    57,
                    15,
                    0,
                    279,
                    0
                ],
                "published": "2024-12-02T19:30:36Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    19,
                    30,
                    36,
                    0,
                    337,
                    0
                ],
                "title": "MALT: Improving Reasoning with Multi-Agent LLM Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MALT: Improving Reasoning with Multi-Agent LLM Training"
                },
                "summary": "Large Language Models (LLMs) often produce answers with a single\nchain-of-thought, which restricts their ability to explore reasoning paths or\nself-correct flawed outputs in complex tasks. In this paper, we introduce MALT\n(Multi-Agent LLM Training), a novel post-training strategy that divides the\nreasoning process into generation, verification, and refinement steps using a\nsequential pipeline of heterogeneous agents. During data generation, each agent\nis repeatedly sampled to form a multi-agent search tree, where final outputs\nare graded against ground-truth data. We then apply value iteration to\npropagate reward signals back to each role-conditioned model, automatically\nproducing multi-agent post-training data without human or teacher-model\nsupervision. Our off-policy approach allows each agent to specialize by\nlearning from correct and incorrect trajectories, ultimately improving the\nend-to-end reasoning chain. On MATH, GSM8K, and CSQA, MALT surpasses the same\nbaseline LLM with a relative improvement of 15.66%, 7.42%, and 9.40%\nrespectively, making it an important advance towards multi-agent cooperative\ntraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often produce answers with a single\nchain-of-thought, which restricts their ability to explore reasoning paths or\nself-correct flawed outputs in complex tasks. In this paper, we introduce MALT\n(Multi-Agent LLM Training), a novel post-training strategy that divides the\nreasoning process into generation, verification, and refinement steps using a\nsequential pipeline of heterogeneous agents. During data generation, each agent\nis repeatedly sampled to form a multi-agent search tree, where final outputs\nare graded against ground-truth data. We then apply value iteration to\npropagate reward signals back to each role-conditioned model, automatically\nproducing multi-agent post-training data without human or teacher-model\nsupervision. Our off-policy approach allows each agent to specialize by\nlearning from correct and incorrect trajectories, ultimately improving the\nend-to-end reasoning chain. On MATH, GSM8K, and CSQA, MALT surpasses the same\nbaseline LLM with a relative improvement of 15.66%, 7.42%, and 9.40%\nrespectively, making it an important advance towards multi-agent cooperative\ntraining."
                },
                "authors": [
                    {
                        "name": "Sumeet Ramesh Motwani"
                    },
                    {
                        "name": "Chandler Smith"
                    },
                    {
                        "name": "Rocktim Jyoti Das"
                    },
                    {
                        "name": "Rafael Rafailov"
                    },
                    {
                        "name": "Ivan Laptev"
                    },
                    {
                        "name": "Philip H. S. Torr"
                    },
                    {
                        "name": "Fabio Pizzati"
                    },
                    {
                        "name": "Ronald Clark"
                    },
                    {
                        "name": "Christian Schroeder de Witt"
                    }
                ],
                "author_detail": {
                    "name": "Christian Schroeder de Witt"
                },
                "author": "Christian Schroeder de Witt",
                "arxiv_comment": "Published at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01928v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01928v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05091v1",
                "updated": "2025-10-06T17:56:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    56,
                    55,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T17:56:55Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    56,
                    55,
                    0,
                    279,
                    0
                ],
                "title": "Factuality Matters: When Image Generation and Editing Meet Structured\n  Visuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Factuality Matters: When Image Generation and Editing Meet Structured\n  Visuals"
                },
                "summary": "While modern visual generation models excel at creating aesthetically\npleasing natural images, they struggle with producing or editing structured\nvisuals like charts, diagrams, and mathematical figures, which demand\ncomposition planning, text rendering, and multimodal reasoning for factual\nfidelity. To address this, we present the first comprehensive, systematic\ninvestigation of this domain, encompassing data construction, model training,\nand an evaluation benchmark. First, we construct a large-scale dataset of 1.3\nmillion high-quality structured image pairs derived from executable drawing\nprograms and augmented with chain-of-thought reasoning annotations. Building on\nit, we train a unified model that integrates a VLM with FLUX.1 Kontext via a\nlightweight connector for enhanced multimodal understanding. A three-stage\ntraining curriculum enables progressive feature alignment, knowledge infusion,\nand reasoning-augmented generation, further boosted by an external reasoner at\ninference time. Finally, we introduce StructBench, a novel benchmark for\ngeneration and editing with over 1,700 challenging instances, and an\naccompanying evaluation metric, StructScore, which employs a multi-round Q\\&A\nprotocol to assess fine-grained factual accuracy. Evaluations of 15 models\nreveal that even leading closed-source systems remain far from satisfactory.\nOur model attains strong editing performance, and inference-time reasoning\nyields consistent gains across diverse architectures. By releasing the dataset,\nmodel, and benchmark, we aim to advance unified multimodal foundations for\nstructured visuals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While modern visual generation models excel at creating aesthetically\npleasing natural images, they struggle with producing or editing structured\nvisuals like charts, diagrams, and mathematical figures, which demand\ncomposition planning, text rendering, and multimodal reasoning for factual\nfidelity. To address this, we present the first comprehensive, systematic\ninvestigation of this domain, encompassing data construction, model training,\nand an evaluation benchmark. First, we construct a large-scale dataset of 1.3\nmillion high-quality structured image pairs derived from executable drawing\nprograms and augmented with chain-of-thought reasoning annotations. Building on\nit, we train a unified model that integrates a VLM with FLUX.1 Kontext via a\nlightweight connector for enhanced multimodal understanding. A three-stage\ntraining curriculum enables progressive feature alignment, knowledge infusion,\nand reasoning-augmented generation, further boosted by an external reasoner at\ninference time. Finally, we introduce StructBench, a novel benchmark for\ngeneration and editing with over 1,700 challenging instances, and an\naccompanying evaluation metric, StructScore, which employs a multi-round Q\\&A\nprotocol to assess fine-grained factual accuracy. Evaluations of 15 models\nreveal that even leading closed-source systems remain far from satisfactory.\nOur model attains strong editing performance, and inference-time reasoning\nyields consistent gains across diverse architectures. By releasing the dataset,\nmodel, and benchmark, we aim to advance unified multimodal foundations for\nstructured visuals."
                },
                "authors": [
                    {
                        "name": "Le Zhuo"
                    },
                    {
                        "name": "Songhao Han"
                    },
                    {
                        "name": "Yuandong Pu"
                    },
                    {
                        "name": "Boxiang Qiu"
                    },
                    {
                        "name": "Sayak Paul"
                    },
                    {
                        "name": "Yue Liao"
                    },
                    {
                        "name": "Yihao Liu"
                    },
                    {
                        "name": "Jie Shao"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Si Liu"
                    },
                    {
                        "name": "Hongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongsheng Li"
                },
                "author": "Hongsheng Li",
                "arxiv_comment": "Project page: https://structvisuals.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05087v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05087v1",
                "updated": "2025-10-06T17:55:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    55,
                    4,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T17:55:04Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    55,
                    4,
                    0,
                    279,
                    0
                ],
                "title": "TeachLM: Post-Training LLMs for Education Using Authentic Learning Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TeachLM: Post-Training LLMs for Education Using Authentic Learning Data"
                },
                "summary": "The promise of generative AI to revolutionize education is constrained by the\npedagogical limits of large language models (LLMs). A major issue is the lack\nof access to high-quality training data that reflect the learning of actual\nstudents. Prompt engineering has emerged as a stopgap, but the ability of\nprompts to encode complex pedagogical strategies in rule-based natural language\nis inherently limited. To address this gap we introduce TeachLM - an LLM\noptimized for teaching through parameter-efficient fine-tuning of\nstate-of-the-art models. TeachLM is trained on a dataset comprised of 100,000\nhours of one-on-one, longitudinal student-tutor interactions maintained by\nPolygence, which underwent a rigorous anonymization process to protect privacy.\nWe use parameter-efficient fine-tuning to develop an authentic student model\nthat enables the generation of high-fidelity synthetic student-tutor dialogues.\nBuilding on this capability, we propose a novel multi-turn evaluation protocol\nthat leverages synthetic dialogue generation to provide fast, scalable, and\nreproducible assessments of the dialogical capabilities of LLMs. Our\nevaluations demonstrate that fine-tuning on authentic learning data\nsignificantly improves conversational and pedagogical performance - doubling\nstudent talk time, improving questioning style, increasing dialogue turns by\n50%, and greater personalization of instruction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promise of generative AI to revolutionize education is constrained by the\npedagogical limits of large language models (LLMs). A major issue is the lack\nof access to high-quality training data that reflect the learning of actual\nstudents. Prompt engineering has emerged as a stopgap, but the ability of\nprompts to encode complex pedagogical strategies in rule-based natural language\nis inherently limited. To address this gap we introduce TeachLM - an LLM\noptimized for teaching through parameter-efficient fine-tuning of\nstate-of-the-art models. TeachLM is trained on a dataset comprised of 100,000\nhours of one-on-one, longitudinal student-tutor interactions maintained by\nPolygence, which underwent a rigorous anonymization process to protect privacy.\nWe use parameter-efficient fine-tuning to develop an authentic student model\nthat enables the generation of high-fidelity synthetic student-tutor dialogues.\nBuilding on this capability, we propose a novel multi-turn evaluation protocol\nthat leverages synthetic dialogue generation to provide fast, scalable, and\nreproducible assessments of the dialogical capabilities of LLMs. Our\nevaluations demonstrate that fine-tuning on authentic learning data\nsignificantly improves conversational and pedagogical performance - doubling\nstudent talk time, improving questioning style, increasing dialogue turns by\n50%, and greater personalization of instruction."
                },
                "authors": [
                    {
                        "name": "Janos Perczel"
                    },
                    {
                        "name": "Jin Chow"
                    },
                    {
                        "name": "Dorottya Demszky"
                    }
                ],
                "author_detail": {
                    "name": "Dorottya Demszky"
                },
                "author": "Dorottya Demszky",
                "arxiv_comment": "28 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05087v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05087v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12284v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12284v5",
                "updated": "2025-10-06T17:54:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    54,
                    9,
                    0,
                    279,
                    0
                ],
                "published": "2025-09-14T19:40:41Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    19,
                    40,
                    41,
                    6,
                    257,
                    0
                ],
                "title": "New HDE models with higher derivatives of the Hubble parameter $H$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New HDE models with higher derivatives of the Hubble parameter $H$"
                },
                "summary": "In this work, we investigate two Dark Energy (DE) models characterized by\nhigher-order derivatives of the Hubble parameter $H$, which generalize\npreviously proposed DE scenarios. Assuming a power-law form of the scale factor\n$a(t)$ given by $a(t)=b_0t^n$, we derive analytical expressions for the DE\nenergy density, pressure, the Equation of State (EoS) parameter, the\ndeceleration parameter and the evolutionary form of the fractional DE density.\nBoth non-interacting and interacting dark sector frameworks are examined, with\nthe interaction modeled through a coupling term proportional to the Dark Matter\n(DM) energy density. For specific parameter sets corresponding to power-law\nindices $n=2$, $n=3$, and $n=4$, we compute the present age of the Universe.\nThe values obtained slightly deviate from the observationally inferred age of\n$\\approx 13.8$ Gyr; moreover, a systematic trend is identified, with larger $n$\nleading to higher ages. Furthermore, interacting scenarios consistently predict\nlarger ages compared to their non-interacting counterparts. These results\nhighlight the phenomenological viability and limitations of higher-derivative\nDE models in describing the cosmic evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we investigate two Dark Energy (DE) models characterized by\nhigher-order derivatives of the Hubble parameter $H$, which generalize\npreviously proposed DE scenarios. Assuming a power-law form of the scale factor\n$a(t)$ given by $a(t)=b_0t^n$, we derive analytical expressions for the DE\nenergy density, pressure, the Equation of State (EoS) parameter, the\ndeceleration parameter and the evolutionary form of the fractional DE density.\nBoth non-interacting and interacting dark sector frameworks are examined, with\nthe interaction modeled through a coupling term proportional to the Dark Matter\n(DM) energy density. For specific parameter sets corresponding to power-law\nindices $n=2$, $n=3$, and $n=4$, we compute the present age of the Universe.\nThe values obtained slightly deviate from the observationally inferred age of\n$\\approx 13.8$ Gyr; moreover, a systematic trend is identified, with larger $n$\nleading to higher ages. Furthermore, interacting scenarios consistently predict\nlarger ages compared to their non-interacting counterparts. These results\nhighlight the phenomenological viability and limitations of higher-derivative\nDE models in describing the cosmic evolution."
                },
                "authors": [
                    {
                        "name": "Antonio Pasqua"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Pasqua"
                },
                "author": "Antonio Pasqua",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2509.08029",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12284v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12284v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05085v1",
                "updated": "2025-10-06T17:53:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    53,
                    5,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T17:53:05Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    53,
                    5,
                    0,
                    279,
                    0
                ],
                "title": "WOW: WAIC-Optimized Gating of Mixture Priors for External Data Borrowing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WOW: WAIC-Optimized Gating of Mixture Priors for External Data Borrowing"
                },
                "summary": "The integration of external data using Bayesian mixture priors has become a\npowerful approach in clinical trials, offering significant potential to improve\ntrial efficiency. Despite their strengths in analytical tractability and\npractical flexibility, existing methods such as the robust\nmeta-analytic-predictive (rMAP) and self-adapting mixture (SAM) often presume\nborrowing without rigorously assessing whether, how, or when integration is\nappropriate. When external and concurrent data are discordant, excessive\nborrowing can bias estimates and lead to misleading conclusions. To address\nthis, we introduce WOW, a Kullback-Leibler-based gating strategy guided by the\nwidely applicable information criterion (WAIC). WOW conducts a preliminary\ncompatibility assessment between external and concurrent trial data and gates\nthe level of borrowing accordingly. The approach is prior-agnostic and can be\nseamlessly integrated with any mixture prior method, whether using fixed or\nadaptive weighting schemes, after the WOW step. Simulation studies demonstrate\nthat incorporating the WOW strategy before Bayesian mixture prior borrowing\nmethods effectively mitigates excessive borrowing and improves estimation\naccuracy. By providing robust and reliable inference, WOW strengthens the\nperformance of mixture-prior methods and supports better decision-making in\nclinical trials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of external data using Bayesian mixture priors has become a\npowerful approach in clinical trials, offering significant potential to improve\ntrial efficiency. Despite their strengths in analytical tractability and\npractical flexibility, existing methods such as the robust\nmeta-analytic-predictive (rMAP) and self-adapting mixture (SAM) often presume\nborrowing without rigorously assessing whether, how, or when integration is\nappropriate. When external and concurrent data are discordant, excessive\nborrowing can bias estimates and lead to misleading conclusions. To address\nthis, we introduce WOW, a Kullback-Leibler-based gating strategy guided by the\nwidely applicable information criterion (WAIC). WOW conducts a preliminary\ncompatibility assessment between external and concurrent trial data and gates\nthe level of borrowing accordingly. The approach is prior-agnostic and can be\nseamlessly integrated with any mixture prior method, whether using fixed or\nadaptive weighting schemes, after the WOW step. Simulation studies demonstrate\nthat incorporating the WOW strategy before Bayesian mixture prior borrowing\nmethods effectively mitigates excessive borrowing and improves estimation\naccuracy. By providing robust and reliable inference, WOW strengthens the\nperformance of mixture-prior methods and supports better decision-making in\nclinical trials."
                },
                "authors": [
                    {
                        "name": "Shouhao Zhou"
                    },
                    {
                        "name": "Qiuxin Gao"
                    },
                    {
                        "name": "Chenqi Fu"
                    },
                    {
                        "name": "Yanxun Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yanxun Xu"
                },
                "author": "Yanxun Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20666v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20666v3",
                "updated": "2025-10-06T17:52:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    52,
                    34,
                    0,
                    279,
                    0
                ],
                "published": "2025-06-25T17:58:12Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    58,
                    12,
                    2,
                    176,
                    0
                ],
                "title": "Using cognitive models to reveal value trade-offs in language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using cognitive models to reveal value trade-offs in language models"
                },
                "summary": "Value trade-offs are an integral part of human decision-making and language\nuse, however, current tools for interpreting such dynamic and multi-faceted\nnotions of values in LLMs are limited. In cognitive science, so-called\n\"cognitive models\" provide formal accounts of such trade-offs in humans, by\nmodeling the weighting of a speaker's competing utility functions in choosing\nan action or utterance. Here we use a leading cognitive model of polite speech\nto systematically evaluate value trade-offs in two encompassing model settings:\ndegrees of reasoning \"effort\" in frontier black-box models, and RL\npost-training dynamics of open-source models. Our results highlight patterns of\nhigher informational utility than social utility in reasoning models' default\nbehavior, and demonstrate that these patterns shift in predictable ways when\nmodels are prompted to prioritize certain goals over others. Our findings from\nLLMs' training dynamics suggest large shifts in utility values early on in\ntraining with persistent effects of the choice of base model and pretraining\ndata, compared to feedback dataset or alignment method. Our framework offers a\nflexible tool for probing value trade-offs across diverse model types,\nproviding insights for generating hypotheses about other social behaviors such\nas sycophancy and for shaping training regimes that better control trade-offs\nbetween values during model development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value trade-offs are an integral part of human decision-making and language\nuse, however, current tools for interpreting such dynamic and multi-faceted\nnotions of values in LLMs are limited. In cognitive science, so-called\n\"cognitive models\" provide formal accounts of such trade-offs in humans, by\nmodeling the weighting of a speaker's competing utility functions in choosing\nan action or utterance. Here we use a leading cognitive model of polite speech\nto systematically evaluate value trade-offs in two encompassing model settings:\ndegrees of reasoning \"effort\" in frontier black-box models, and RL\npost-training dynamics of open-source models. Our results highlight patterns of\nhigher informational utility than social utility in reasoning models' default\nbehavior, and demonstrate that these patterns shift in predictable ways when\nmodels are prompted to prioritize certain goals over others. Our findings from\nLLMs' training dynamics suggest large shifts in utility values early on in\ntraining with persistent effects of the choice of base model and pretraining\ndata, compared to feedback dataset or alignment method. Our framework offers a\nflexible tool for probing value trade-offs across diverse model types,\nproviding insights for generating hypotheses about other social behaviors such\nas sycophancy and for shaping training regimes that better control trade-offs\nbetween values during model development."
                },
                "authors": [
                    {
                        "name": "Sonia K. Murthy"
                    },
                    {
                        "name": "Rosie Zhao"
                    },
                    {
                        "name": "Jennifer Hu"
                    },
                    {
                        "name": "Sham Kakade"
                    },
                    {
                        "name": "Markus Wulfmeier"
                    },
                    {
                        "name": "Peng Qian"
                    },
                    {
                        "name": "Tomer Ullman"
                    }
                ],
                "author_detail": {
                    "name": "Tomer Ullman"
                },
                "author": "Tomer Ullman",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20666v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20666v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10924v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10924v6",
                "updated": "2025-10-06T17:52:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    52,
                    33,
                    0,
                    279,
                    0
                ],
                "published": "2024-12-14T18:18:52Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    18,
                    18,
                    52,
                    5,
                    349,
                    0
                ],
                "title": "Tokens, the oft-overlooked appetizer: Large language models, the\n  distributional hypothesis, and meaning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokens, the oft-overlooked appetizer: Large language models, the\n  distributional hypothesis, and meaning"
                },
                "summary": "Tokenization is a necessary component within the current architecture of many\nlanguage models, including the transformer-based large language models (LLMs)\nof Generative AI, yet its impact on the model's cognition is often overlooked.\nWe argue that LLMs demonstrate that the Distributional Hypothesis (DH) is\nsufficient for reasonably human-like language performance, and that the\nemergence of human-meaningful linguistic units among tokens and current\nstructural constraints motivate changes to existing, linguistically-agnostic\ntokenization techniques, particularly with respect to their roles as (1)\nsemantic primitives and as (2) vehicles for conveying salient distributional\npatterns from human language to the model. We explore tokenizations from a BPE\ntokenizer; extant model vocabularies obtained from Hugging Face and tiktoken;\nand the information in exemplar token vectors as they move through the layers\nof a RoBERTa (large) model. Besides creating sub-optimal semantic building\nblocks and obscuring the model's access to the necessary distributional\npatterns, we describe how tokens and pretraining can act as a backdoor for bias\nand other unwanted content, which current alignment practices may not\nremediate. Additionally, we relay evidence that the tokenization algorithm's\nobjective function impacts the LLM's cognition, despite being arguably\nmeaningfully insulated from the main system intelligence. [First uploaded to\narXiv in December, 2024.]",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization is a necessary component within the current architecture of many\nlanguage models, including the transformer-based large language models (LLMs)\nof Generative AI, yet its impact on the model's cognition is often overlooked.\nWe argue that LLMs demonstrate that the Distributional Hypothesis (DH) is\nsufficient for reasonably human-like language performance, and that the\nemergence of human-meaningful linguistic units among tokens and current\nstructural constraints motivate changes to existing, linguistically-agnostic\ntokenization techniques, particularly with respect to their roles as (1)\nsemantic primitives and as (2) vehicles for conveying salient distributional\npatterns from human language to the model. We explore tokenizations from a BPE\ntokenizer; extant model vocabularies obtained from Hugging Face and tiktoken;\nand the information in exemplar token vectors as they move through the layers\nof a RoBERTa (large) model. Besides creating sub-optimal semantic building\nblocks and obscuring the model's access to the necessary distributional\npatterns, we describe how tokens and pretraining can act as a backdoor for bias\nand other unwanted content, which current alignment practices may not\nremediate. Additionally, we relay evidence that the tokenization algorithm's\nobjective function impacts the LLM's cognition, despite being arguably\nmeaningfully insulated from the main system intelligence. [First uploaded to\narXiv in December, 2024.]"
                },
                "authors": [
                    {
                        "name": "Julia Witte Zimmerman"
                    },
                    {
                        "name": "Denis Hudon"
                    },
                    {
                        "name": "Kathryn Cramer"
                    },
                    {
                        "name": "Alejandro J. Ruiz"
                    },
                    {
                        "name": "Calla Beauregard"
                    },
                    {
                        "name": "Ashley Fehr"
                    },
                    {
                        "name": "Mikaela Irene Fudolig"
                    },
                    {
                        "name": "Bradford Demarest"
                    },
                    {
                        "name": "Yoshi Meke Bird"
                    },
                    {
                        "name": "Milo Z. Trujillo"
                    },
                    {
                        "name": "Christopher M. Danforth"
                    },
                    {
                        "name": "Peter Sheridan Dodds"
                    }
                ],
                "author_detail": {
                    "name": "Peter Sheridan Dodds"
                },
                "author": "Peter Sheridan Dodds",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10924v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10924v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05069v1",
                "updated": "2025-10-06T17:46:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    46,
                    34,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T17:46:34Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    46,
                    34,
                    0,
                    279,
                    0
                ],
                "title": "SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior\n  Reasoning LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior\n  Reasoning LLMs"
                },
                "summary": "Recent work shows that, beyond discrete reasoning through explicit\nchain-of-thought steps, which are limited by the boundaries of natural\nlanguages, large language models (LLMs) can also reason continuously in latent\nspace, allowing richer information per step and thereby improving token\nefficiency. Despite this promise, latent reasoning still faces two challenges,\nespecially in training-free settings: 1) purely latent reasoning broadens the\nsearch distribution by maintaining multiple implicit paths, which diffuses\nprobability mass, introduces noise, and impedes convergence to a single\nhigh-confidence solution, thereby hurting accuracy; and 2) overthinking\npersists even without explicit text, wasting tokens and degrading efficiency.\nTo address these issues, we introduce SwiReasoning, a training-free framework\nfor LLM reasoning which features two key innovations: 1) SwiReasoning\ndynamically switches between explicit and latent reasoning, guided by\nblock-wise confidence estimated from entropy trends in next-token\ndistributions, to balance exploration and exploitation and promote timely\nconvergence. 2) By limiting the maximum number of thinking-block switches,\nSwiReasoning curbs overthinking and improves token efficiency across varying\nproblem difficulties. On widely used mathematics and STEM benchmarks,\nSwiReasoning consistently improves average accuracy by 1.5%-2.8% across\nreasoning LLMs of different model families and scales. Furthermore, under\nconstrained budgets, SwiReasoning improves average token efficiency by 56%-79%,\nwith larger gains as budgets tighten.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work shows that, beyond discrete reasoning through explicit\nchain-of-thought steps, which are limited by the boundaries of natural\nlanguages, large language models (LLMs) can also reason continuously in latent\nspace, allowing richer information per step and thereby improving token\nefficiency. Despite this promise, latent reasoning still faces two challenges,\nespecially in training-free settings: 1) purely latent reasoning broadens the\nsearch distribution by maintaining multiple implicit paths, which diffuses\nprobability mass, introduces noise, and impedes convergence to a single\nhigh-confidence solution, thereby hurting accuracy; and 2) overthinking\npersists even without explicit text, wasting tokens and degrading efficiency.\nTo address these issues, we introduce SwiReasoning, a training-free framework\nfor LLM reasoning which features two key innovations: 1) SwiReasoning\ndynamically switches between explicit and latent reasoning, guided by\nblock-wise confidence estimated from entropy trends in next-token\ndistributions, to balance exploration and exploitation and promote timely\nconvergence. 2) By limiting the maximum number of thinking-block switches,\nSwiReasoning curbs overthinking and improves token efficiency across varying\nproblem difficulties. On widely used mathematics and STEM benchmarks,\nSwiReasoning consistently improves average accuracy by 1.5%-2.8% across\nreasoning LLMs of different model families and scales. Furthermore, under\nconstrained budgets, SwiReasoning improves average token efficiency by 56%-79%,\nwith larger gains as budgets tighten."
                },
                "authors": [
                    {
                        "name": "Dachuan Shi"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Keying Li"
                    },
                    {
                        "name": "Xiangchi Yuan"
                    },
                    {
                        "name": "Leyan Pan"
                    },
                    {
                        "name": "Wenke Lee"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "arxiv_comment": "Code: https://github.com/sdc17/SwiReasoning, Website:\n  https://swireasoning.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05064v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05064v1",
                "updated": "2025-10-06T17:41:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    41,
                    20,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T17:41:20Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    41,
                    20,
                    0,
                    279,
                    0
                ],
                "title": "Boomerang Distillation Enables Zero-Shot Model Size Interpolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boomerang Distillation Enables Zero-Shot Model Size Interpolation"
                },
                "summary": "Large language models (LLMs) are typically deployed under diverse memory and\ncompute constraints. Existing approaches build model families by training each\nsize independently, which is prohibitively expensive and provides only\ncoarse-grained size options. In this work, we identify a novel phenomenon that\nwe call boomerang distillation: starting from a large base model (the teacher),\none first distills down to a small student and then progressively reconstructs\nintermediate-sized models by re-incorporating blocks of teacher layers into the\nstudent without any additional training. This process produces zero-shot\ninterpolated models of many intermediate sizes whose performance scales\nsmoothly between the student and teacher, often matching or surpassing\npretrained or distilled models of the same size. We further analyze when this\ntype of interpolation succeeds, showing that alignment between teacher and\nstudent through pruning and distillation is essential. Boomerang distillation\nthus provides a simple and efficient way to generate fine-grained model\nfamilies, dramatically reducing training cost while enabling flexible\nadaptation across deployment environments. The code and models are available at\nhttps://github.com/dcml-lab/boomerang-distillation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are typically deployed under diverse memory and\ncompute constraints. Existing approaches build model families by training each\nsize independently, which is prohibitively expensive and provides only\ncoarse-grained size options. In this work, we identify a novel phenomenon that\nwe call boomerang distillation: starting from a large base model (the teacher),\none first distills down to a small student and then progressively reconstructs\nintermediate-sized models by re-incorporating blocks of teacher layers into the\nstudent without any additional training. This process produces zero-shot\ninterpolated models of many intermediate sizes whose performance scales\nsmoothly between the student and teacher, often matching or surpassing\npretrained or distilled models of the same size. We further analyze when this\ntype of interpolation succeeds, showing that alignment between teacher and\nstudent through pruning and distillation is essential. Boomerang distillation\nthus provides a simple and efficient way to generate fine-grained model\nfamilies, dramatically reducing training cost while enabling flexible\nadaptation across deployment environments. The code and models are available at\nhttps://github.com/dcml-lab/boomerang-distillation."
                },
                "authors": [
                    {
                        "name": "Sara Kangaslahti"
                    },
                    {
                        "name": "Nihal V. Nayak"
                    },
                    {
                        "name": "Jonathan Geuter"
                    },
                    {
                        "name": "Marco Fumero"
                    },
                    {
                        "name": "Francesco Locatello"
                    },
                    {
                        "name": "David Alvarez-Melis"
                    }
                ],
                "author_detail": {
                    "name": "David Alvarez-Melis"
                },
                "author": "David Alvarez-Melis",
                "arxiv_comment": "10 pages, 7 figures in main text",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05064v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05064v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05059v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05059v1",
                "updated": "2025-10-06T17:37:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    37,
                    35,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T17:37:35Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    37,
                    35,
                    0,
                    279,
                    0
                ],
                "title": "Staircase Streaming for Low-Latency Multi-Agent Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Staircase Streaming for Low-Latency Multi-Agent Inference"
                },
                "summary": "Recent advances in large language models (LLMs) opened up new directions for\nleveraging the collective expertise of multiple LLMs. These methods, such as\nMixture-of-Agents, typically employ additional inference steps to generate\nintermediate outputs, which are then used to produce the final response. While\nmulti-agent inference can enhance response quality, it can significantly\nincrease the time to first token (TTFT), posing a challenge for\nlatency-sensitive applications and hurting user experience. To address this\nissue, we propose staircase streaming for low-latency multi-agent inference.\nInstead of waiting for the complete intermediate outputs from previous steps,\nwe begin generating the final response as soon as we receive partial outputs\nfrom these steps. Experimental results demonstrate that staircase streaming\nreduces TTFT by up to 93% while maintaining response quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) opened up new directions for\nleveraging the collective expertise of multiple LLMs. These methods, such as\nMixture-of-Agents, typically employ additional inference steps to generate\nintermediate outputs, which are then used to produce the final response. While\nmulti-agent inference can enhance response quality, it can significantly\nincrease the time to first token (TTFT), posing a challenge for\nlatency-sensitive applications and hurting user experience. To address this\nissue, we propose staircase streaming for low-latency multi-agent inference.\nInstead of waiting for the complete intermediate outputs from previous steps,\nwe begin generating the final response as soon as we receive partial outputs\nfrom these steps. Experimental results demonstrate that staircase streaming\nreduces TTFT by up to 93% while maintaining response quality."
                },
                "authors": [
                    {
                        "name": "Junlin Wang"
                    },
                    {
                        "name": "Jue Wang"
                    },
                    {
                        "name": "Zhen"
                    },
                    {
                        "name": "Xu"
                    },
                    {
                        "name": "Ben Athiwaratkun"
                    },
                    {
                        "name": "Bhuwan Dhingra"
                    },
                    {
                        "name": "Ce Zhang"
                    },
                    {
                        "name": "James Zou"
                    }
                ],
                "author_detail": {
                    "name": "James Zou"
                },
                "arxiv_affiliation": "Zach",
                "author": "James Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05059v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05059v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05057v1",
                "updated": "2025-10-06T17:37:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    37,
                    24,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T17:37:24Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    37,
                    24,
                    0,
                    279,
                    0
                ],
                "title": "StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact\n  State Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact\n  State Representation"
                },
                "summary": "A fundamental challenge in embodied intelligence is developing expressive and\ncompact state representations for efficient world modeling and decision making.\nHowever, existing methods often fail to achieve this balance, yielding\nrepresentations that are either overly redundant or lacking in task-critical\ninformation. We propose an unsupervised approach that learns a highly\ncompressed two-token state representation using a lightweight encoder and a\npre-trained Diffusion Transformer (DiT) decoder, capitalizing on its strong\ngenerative prior. Our representation is efficient, interpretable, and\nintegrates seamlessly into existing VLA-based models, improving performance by\n14.3% on LIBERO and 30% in real-world task success with minimal inference\noverhead. More importantly, we find that the difference between these tokens,\nobtained via latent interpolation, naturally serves as a highly effective\nlatent action, which can be further decoded into executable robot actions. This\nemergent capability reveals that our representation captures structured\ndynamics without explicit supervision. We name our method StaMo for its ability\nto learn generalizable robotic Motion from compact State representation, which\nis encoded from static images, challenging the prevalent dependence to learning\nlatent action on complex architectures and video data. The resulting latent\nactions also enhance policy co-training, outperforming prior methods by 10.4%\nwith improved interpretability. Moreover, our approach scales effectively\nacross diverse data sources, including real-world robot data, simulation, and\nhuman egocentric video.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fundamental challenge in embodied intelligence is developing expressive and\ncompact state representations for efficient world modeling and decision making.\nHowever, existing methods often fail to achieve this balance, yielding\nrepresentations that are either overly redundant or lacking in task-critical\ninformation. We propose an unsupervised approach that learns a highly\ncompressed two-token state representation using a lightweight encoder and a\npre-trained Diffusion Transformer (DiT) decoder, capitalizing on its strong\ngenerative prior. Our representation is efficient, interpretable, and\nintegrates seamlessly into existing VLA-based models, improving performance by\n14.3% on LIBERO and 30% in real-world task success with minimal inference\noverhead. More importantly, we find that the difference between these tokens,\nobtained via latent interpolation, naturally serves as a highly effective\nlatent action, which can be further decoded into executable robot actions. This\nemergent capability reveals that our representation captures structured\ndynamics without explicit supervision. We name our method StaMo for its ability\nto learn generalizable robotic Motion from compact State representation, which\nis encoded from static images, challenging the prevalent dependence to learning\nlatent action on complex architectures and video data. The resulting latent\nactions also enhance policy co-training, outperforming prior methods by 10.4%\nwith improved interpretability. Moreover, our approach scales effectively\nacross diverse data sources, including real-world robot data, simulation, and\nhuman egocentric video."
                },
                "authors": [
                    {
                        "name": "Mingyu Liu"
                    },
                    {
                        "name": "Jiuhe Shu"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Zeju Li"
                    },
                    {
                        "name": "Canyu Zhao"
                    },
                    {
                        "name": "Jiange Yang"
                    },
                    {
                        "name": "Shenyuan Gao"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Chunhua Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chunhua Shen"
                },
                "author": "Chunhua Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05052v1",
                "updated": "2025-10-06T17:32:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    32,
                    40,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T17:32:40Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    32,
                    40,
                    0,
                    279,
                    0
                ],
                "title": "Proactive defense against LLM Jailbreak",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proactive defense against LLM Jailbreak"
                },
                "summary": "The proliferation of powerful large language models (LLMs) has necessitated\nrobust safety alignment, yet these models remain vulnerable to evolving\nadversarial attacks, including multi-turn jailbreaks that iteratively search\nfor successful queries. Current defenses, primarily reactive and static, often\nfail to counter these search-based attacks. In this paper, we introduce ProAct,\na novel proactive defense framework designed to disrupt and mislead autonomous\njailbreaking processes. Our core idea is to intentionally provide adversaries\nwith \"spurious responses\" that appear to be results of successful jailbreak\nattacks but contain no actual harmful content. These misleading responses\nprovide false signals to the attacker's internal optimization loop, causing the\nadversarial search to terminate prematurely and effectively jailbreaking the\njailbreak. By conducting extensive experiments across state-of-the-art LLMs,\njailbreaking frameworks, and safety benchmarks, our method consistently and\nsignificantly reduces attack success rates by up to 92\\%. When combined with\nother defense frameworks, it further reduces the success rate of the latest\nattack strategies to 0\\%. ProAct represents an orthogonal defense strategy that\ncan serve as an additional guardrail to enhance LLM safety against the most\neffective jailbreaking attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of powerful large language models (LLMs) has necessitated\nrobust safety alignment, yet these models remain vulnerable to evolving\nadversarial attacks, including multi-turn jailbreaks that iteratively search\nfor successful queries. Current defenses, primarily reactive and static, often\nfail to counter these search-based attacks. In this paper, we introduce ProAct,\na novel proactive defense framework designed to disrupt and mislead autonomous\njailbreaking processes. Our core idea is to intentionally provide adversaries\nwith \"spurious responses\" that appear to be results of successful jailbreak\nattacks but contain no actual harmful content. These misleading responses\nprovide false signals to the attacker's internal optimization loop, causing the\nadversarial search to terminate prematurely and effectively jailbreaking the\njailbreak. By conducting extensive experiments across state-of-the-art LLMs,\njailbreaking frameworks, and safety benchmarks, our method consistently and\nsignificantly reduces attack success rates by up to 92\\%. When combined with\nother defense frameworks, it further reduces the success rate of the latest\nattack strategies to 0\\%. ProAct represents an orthogonal defense strategy that\ncan serve as an additional guardrail to enhance LLM safety against the most\neffective jailbreaking attacks."
                },
                "authors": [
                    {
                        "name": "Weiliang Zhao"
                    },
                    {
                        "name": "Jinjun Peng"
                    },
                    {
                        "name": "Daniel Ben-Levi"
                    },
                    {
                        "name": "Zhou Yu"
                    },
                    {
                        "name": "Junfeng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Junfeng Yang"
                },
                "author": "Junfeng Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04289v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04289v2",
                "updated": "2025-10-06T17:29:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    29,
                    15,
                    0,
                    279,
                    0
                ],
                "published": "2025-03-06T10:17:11Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    10,
                    17,
                    11,
                    3,
                    65,
                    0
                ],
                "title": "Machine learning in top quark physics at ATLAS and CMS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning in top quark physics at ATLAS and CMS"
                },
                "summary": "This note presents an overview of current and potential future applications\nof machine-learning-based techniques in the study of the top quark. The\nresearch community has developed a diverse set of ideas and tools, including\nalgorithms for the efficient reconstruction of recorded collision events and\ninnovative methods for statistical inference. Recent applications of some\ntechniques by the ATLAS and CMS collaborations are also highlighted.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This note presents an overview of current and potential future applications\nof machine-learning-based techniques in the study of the top quark. The\nresearch community has developed a diverse set of ideas and tools, including\nalgorithms for the efficient reconstruction of recorded collision events and\ninnovative methods for statistical inference. Recent applications of some\ntechniques by the ATLAS and CMS collaborations are also highlighted."
                },
                "authors": [
                    {
                        "name": "Matthias Komm"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Komm"
                },
                "arxiv_affiliation": "for the ATLAS and CMS collaborations",
                "author": "Matthias Komm",
                "arxiv_comment": "Talk at the 17th International Workshop on Top Quark Physics\n  (Top2024), 22-27 September 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04289v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04289v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05046v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05046v2",
                "updated": "2025-10-07T02:23:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    2,
                    23,
                    31,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-06T17:26:41Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    26,
                    41,
                    0,
                    279,
                    0
                ],
                "title": "COLE: a Comprehensive Benchmark for French Language Understanding\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COLE: a Comprehensive Benchmark for French Language Understanding\n  Evaluation"
                },
                "summary": "To address the need for a more comprehensive evaluation of French Natural\nLanguage Understanding (NLU), we introduce COLE, a new benchmark composed of 23\ndiverse task covering a broad range of NLU capabilities, including sentiment\nanalysis, paraphrase detection, grammatical judgment, and reasoning, with a\nparticular focus on linguistic phenomena relevant to the French language. We\nbenchmark 94 large language models (LLM), providing an extensive analysis of\nthe current state of French NLU. Our results highlight a significant\nperformance gap between closed- and open-weights models and identify key\nchallenging frontiers for current LLMs, such as zero-shot extractive\nquestion-answering (QA), fine-grained word sense disambiguation, and\nunderstanding of regional language variations. We release COLE as a public\nresource to foster further progress in French language modelling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To address the need for a more comprehensive evaluation of French Natural\nLanguage Understanding (NLU), we introduce COLE, a new benchmark composed of 23\ndiverse task covering a broad range of NLU capabilities, including sentiment\nanalysis, paraphrase detection, grammatical judgment, and reasoning, with a\nparticular focus on linguistic phenomena relevant to the French language. We\nbenchmark 94 large language models (LLM), providing an extensive analysis of\nthe current state of French NLU. Our results highlight a significant\nperformance gap between closed- and open-weights models and identify key\nchallenging frontiers for current LLMs, such as zero-shot extractive\nquestion-answering (QA), fine-grained word sense disambiguation, and\nunderstanding of regional language variations. We release COLE as a public\nresource to foster further progress in French language modelling."
                },
                "authors": [
                    {
                        "name": "David Beauchemin"
                    },
                    {
                        "name": "Yan Tremblay"
                    },
                    {
                        "name": "Mohamed Amine Youssef"
                    },
                    {
                        "name": "Richard Khoury"
                    }
                ],
                "author_detail": {
                    "name": "Richard Khoury"
                },
                "author": "Richard Khoury",
                "arxiv_comment": "Submitted to ACL Rolling Review of October",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05046v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05046v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12491v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12491v3",
                "updated": "2025-10-06T17:25:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    25,
                    58,
                    0,
                    279,
                    0
                ],
                "published": "2024-10-16T12:14:25Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    14,
                    25,
                    2,
                    290,
                    0
                ],
                "title": "Insights from the Inverse: Reconstructing LLM Training Goals Through\n  Inverse Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Insights from the Inverse: Reconstructing LLM Training Goals Through\n  Inverse Reinforcement Learning"
                },
                "summary": "Large language models (LLMs) trained with Reinforcement Learning from Human\nFeedback (RLHF) have demonstrated remarkable capabilities, but their underlying\nreward functions and decision-making processes remain opaque. This paper\nintroduces a novel approach to interpreting LLMs by applying inverse\nreinforcement learning (IRL) to recover their implicit reward functions. We\nconduct experiments on toxicity-aligned LLMs of varying sizes, extracting\nreward models that achieve up to 85% accuracy in predicting human preferences.\nOur analysis reveals key insights into the non-identifiability of reward\nfunctions, the relationship between model size and interpretability, and\npotential pitfalls in the RLHF process. We demonstrate that IRL-derived reward\nmodels can be used to fine-tune new LLMs, resulting in comparable or improved\nperformance on toxicity benchmarks. This work provides a new lens for\nunderstanding and improving LLM alignment, with implications for the\nresponsible development and deployment of these powerful systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) trained with Reinforcement Learning from Human\nFeedback (RLHF) have demonstrated remarkable capabilities, but their underlying\nreward functions and decision-making processes remain opaque. This paper\nintroduces a novel approach to interpreting LLMs by applying inverse\nreinforcement learning (IRL) to recover their implicit reward functions. We\nconduct experiments on toxicity-aligned LLMs of varying sizes, extracting\nreward models that achieve up to 85% accuracy in predicting human preferences.\nOur analysis reveals key insights into the non-identifiability of reward\nfunctions, the relationship between model size and interpretability, and\npotential pitfalls in the RLHF process. We demonstrate that IRL-derived reward\nmodels can be used to fine-tune new LLMs, resulting in comparable or improved\nperformance on toxicity benchmarks. This work provides a new lens for\nunderstanding and improving LLM alignment, with implications for the\nresponsible development and deployment of these powerful systems."
                },
                "authors": [
                    {
                        "name": "Jared Joselowitz"
                    },
                    {
                        "name": "Ritam Majumdar"
                    },
                    {
                        "name": "Arjun Jagota"
                    },
                    {
                        "name": "Matthieu Bou"
                    },
                    {
                        "name": "Nyal Patel"
                    },
                    {
                        "name": "Satyapriya Krishna"
                    },
                    {
                        "name": "Sonali Parbhoo"
                    }
                ],
                "author_detail": {
                    "name": "Sonali Parbhoo"
                },
                "author": "Sonali Parbhoo",
                "arxiv_comment": "Published as a conference paper at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12491v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12491v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05040v1",
                "updated": "2025-10-06T17:16:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    16,
                    41,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T17:16:41Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    16,
                    41,
                    0,
                    279,
                    0
                ],
                "title": "Test-Time Scaling in Diffusion LLMs via Hidden Semi-Autoregressive\n  Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Scaling in Diffusion LLMs via Hidden Semi-Autoregressive\n  Experts"
                },
                "summary": "Diffusion-based large language models (dLLMs) are trained flexibly to model\nextreme dependence in the data distribution; however, how to best utilize this\ninformation at inference time remains an open problem. In this work, we uncover\nan interesting property of these models: dLLMs trained on textual data\nimplicitly learn a mixture of semi-autoregressive experts, where different\ngeneration orders reveal different specialized behaviors. We show that\ncommitting to any single, fixed inference time schedule, a common practice,\ncollapses performance by failing to leverage this latent ensemble. To address\nthis, we introduce HEX (Hidden semiautoregressive EXperts for test-time\nscaling), a training-free inference method that ensembles across heterogeneous\nblock schedules. By doing a majority vote over diverse block-sized generation\npaths, HEX robustly avoids failure modes associated with any single fixed\nschedule. On reasoning benchmarks such as GSM8K, it boosts accuracy by up to\n3.56X (from 24.72% to 88.10%), outperforming top-K margin inference and\nspecialized fine-tuned methods like GRPO, without additional training. HEX even\nyields significant gains on MATH benchmark from 16.40% to 40.00%, scientific\nreasoning on ARC-C from 54.18% to 87.80%, and TruthfulQA from 28.36% to 57.46%.\nOur results establish a new paradigm for test-time scaling in diffusion-based\nLLMs (dLLMs), revealing that the sequence in which masking is performed plays a\ncritical role in determining performance during inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs) are trained flexibly to model\nextreme dependence in the data distribution; however, how to best utilize this\ninformation at inference time remains an open problem. In this work, we uncover\nan interesting property of these models: dLLMs trained on textual data\nimplicitly learn a mixture of semi-autoregressive experts, where different\ngeneration orders reveal different specialized behaviors. We show that\ncommitting to any single, fixed inference time schedule, a common practice,\ncollapses performance by failing to leverage this latent ensemble. To address\nthis, we introduce HEX (Hidden semiautoregressive EXperts for test-time\nscaling), a training-free inference method that ensembles across heterogeneous\nblock schedules. By doing a majority vote over diverse block-sized generation\npaths, HEX robustly avoids failure modes associated with any single fixed\nschedule. On reasoning benchmarks such as GSM8K, it boosts accuracy by up to\n3.56X (from 24.72% to 88.10%), outperforming top-K margin inference and\nspecialized fine-tuned methods like GRPO, without additional training. HEX even\nyields significant gains on MATH benchmark from 16.40% to 40.00%, scientific\nreasoning on ARC-C from 54.18% to 87.80%, and TruthfulQA from 28.36% to 57.46%.\nOur results establish a new paradigm for test-time scaling in diffusion-based\nLLMs (dLLMs), revealing that the sequence in which masking is performed plays a\ncritical role in determining performance during inference."
                },
                "authors": [
                    {
                        "name": "Jihoon Lee"
                    },
                    {
                        "name": "Hoyeon Moon"
                    },
                    {
                        "name": "Kevin Zhai"
                    },
                    {
                        "name": "Arun Kumar Chithanar"
                    },
                    {
                        "name": "Anit Kumar Sahu"
                    },
                    {
                        "name": "Soummya Kar"
                    },
                    {
                        "name": "Chul Lee"
                    },
                    {
                        "name": "Souradip Chakraborty"
                    },
                    {
                        "name": "Amrit Singh Bedi"
                    }
                ],
                "author_detail": {
                    "name": "Amrit Singh Bedi"
                },
                "author": "Amrit Singh Bedi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10525v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10525v3",
                "updated": "2025-10-06T17:12:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    12,
                    59,
                    0,
                    279,
                    0
                ],
                "published": "2024-12-13T19:38:36Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    19,
                    38,
                    36,
                    4,
                    348,
                    0
                ],
                "title": "RowDetr: End-to-End Crop Row Detection Using Polynomials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RowDetr: End-to-End Crop Row Detection Using Polynomials"
                },
                "summary": "Crop row detection enables autonomous robots to navigate in gps denied\nenvironments. Vision based strategies often struggle in the environments due to\ngaps, curved crop rows and require post-processing steps. Furthermore, labeling\ncrop rows in under the canopy environments accurately is very difficult due to\nocclusions. This study introduces RowDetr, an efficient end-to-end\ntransformer-based neural network for crop row detection in precision\nagriculture. RowDetr leverages a lightweight backbone and a hybrid encoder to\nmodel straight, curved, or occluded crop rows with high precision. Central to\nthe architecture is a novel polynomial representation that enables direct\nparameterization of crop rows, eliminating computationally expensive\npost-processing. Key innovations include a PolySampler module and multi-scale\ndeformable attention, which work together with PolyOptLoss, an energy-based\nloss function designed to optimize geometric alignment between predicted and\nthe annotated crop rows, while also enhancing robustness against labeling\nnoise. RowDetr was evaluated against other state-of-the-art end-to-end crop row\ndetection methods like AgroNav and RolColAttention on a diverse dataset of\n6,962 high-resolution images, used for training, validation, and testing across\nmultiple crop types with annotated crop rows. The system demonstrated superior\nperformance, achieved an F1 score up to 0.74 and a lane position deviation as\nlow as 0.405. Furthermore, RowDetr achieves a real-time inference latency of\n6.7ms, which was optimized to 3.5ms with INT8 quantization on an NVIDIA Jetson\nOrin AGX. This work highlighted the critical efficiency of polynomial\nparameterization, making RowDetr particularly suitable for deployment on edge\ncomputing devices in agricultural robotics and autonomous farming equipment.\nIndex terms > Crop Row Detection, Under Canopy Navigation, Transformers,\nRT-DETR, RT-DETRv2",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crop row detection enables autonomous robots to navigate in gps denied\nenvironments. Vision based strategies often struggle in the environments due to\ngaps, curved crop rows and require post-processing steps. Furthermore, labeling\ncrop rows in under the canopy environments accurately is very difficult due to\nocclusions. This study introduces RowDetr, an efficient end-to-end\ntransformer-based neural network for crop row detection in precision\nagriculture. RowDetr leverages a lightweight backbone and a hybrid encoder to\nmodel straight, curved, or occluded crop rows with high precision. Central to\nthe architecture is a novel polynomial representation that enables direct\nparameterization of crop rows, eliminating computationally expensive\npost-processing. Key innovations include a PolySampler module and multi-scale\ndeformable attention, which work together with PolyOptLoss, an energy-based\nloss function designed to optimize geometric alignment between predicted and\nthe annotated crop rows, while also enhancing robustness against labeling\nnoise. RowDetr was evaluated against other state-of-the-art end-to-end crop row\ndetection methods like AgroNav and RolColAttention on a diverse dataset of\n6,962 high-resolution images, used for training, validation, and testing across\nmultiple crop types with annotated crop rows. The system demonstrated superior\nperformance, achieved an F1 score up to 0.74 and a lane position deviation as\nlow as 0.405. Furthermore, RowDetr achieves a real-time inference latency of\n6.7ms, which was optimized to 3.5ms with INT8 quantization on an NVIDIA Jetson\nOrin AGX. This work highlighted the critical efficiency of polynomial\nparameterization, making RowDetr particularly suitable for deployment on edge\ncomputing devices in agricultural robotics and autonomous farming equipment.\nIndex terms > Crop Row Detection, Under Canopy Navigation, Transformers,\nRT-DETR, RT-DETRv2"
                },
                "authors": [
                    {
                        "name": "Rahul Harsha Cheppally"
                    },
                    {
                        "name": "Ajay Sharda"
                    }
                ],
                "author_detail": {
                    "name": "Ajay Sharda"
                },
                "author": "Ajay Sharda",
                "arxiv_comment": "Code will be open sourced upon publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10525v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10525v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05037v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05037v1",
                "updated": "2025-10-06T17:12:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    12,
                    3,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T17:12:03Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    12,
                    3,
                    0,
                    279,
                    0
                ],
                "title": "On the sensitivity of different galaxy properties to warm dark matter",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the sensitivity of different galaxy properties to warm dark matter"
                },
                "summary": "We study the impact of warm dark matter (WDM) particle mass on galaxy\nproperties using 1,024 state-of-the-art cosmological hydrodynamical simulations\nfrom the DREAMS project. We begin by using a Multilayer Perceptron (MLP)\ncoupled with a normalizing flow to explore global statistical descriptors of\ngalaxy populations, such as the mean, standard deviation, and histograms of 14\ngalaxy properties. We find that subhalo gas mass is the most informative\nfeature for constraining the WDM mass, achieving a determination coefficient of\nR^2 = 0.9. We employ symbolic regression to extract simple, interpretable\nrelations with the WDM particle mass. Finally, we adopt a more localized\napproach by selecting individual dark matter halos and using a Graph Neural\nNetwork (GNN) with a normalizing flow to infer the WDM mass, incorporating\nsubhalo properties as node features and global simulation statistics as\ngraph-level features. The GNN approach yields only a residual improvement over\nMLP models based solely on global features, indicating that most of the\npredictive power resides in the global descriptors, with only marginal gains\nfrom halo-level information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the impact of warm dark matter (WDM) particle mass on galaxy\nproperties using 1,024 state-of-the-art cosmological hydrodynamical simulations\nfrom the DREAMS project. We begin by using a Multilayer Perceptron (MLP)\ncoupled with a normalizing flow to explore global statistical descriptors of\ngalaxy populations, such as the mean, standard deviation, and histograms of 14\ngalaxy properties. We find that subhalo gas mass is the most informative\nfeature for constraining the WDM mass, achieving a determination coefficient of\nR^2 = 0.9. We employ symbolic regression to extract simple, interpretable\nrelations with the WDM particle mass. Finally, we adopt a more localized\napproach by selecting individual dark matter halos and using a Graph Neural\nNetwork (GNN) with a normalizing flow to infer the WDM mass, incorporating\nsubhalo properties as node features and global simulation statistics as\ngraph-level features. The GNN approach yields only a residual improvement over\nMLP models based solely on global features, indicating that most of the\npredictive power resides in the global descriptors, with only marginal gains\nfrom halo-level information."
                },
                "authors": [
                    {
                        "name": "BelÃ©n Costanza"
                    },
                    {
                        "name": "Bonny Y. Wang"
                    },
                    {
                        "name": "Francisco Villaescusa-Navarro"
                    },
                    {
                        "name": "Alex M. Garcia"
                    },
                    {
                        "name": "Jonah C. Rose"
                    },
                    {
                        "name": "Mark Vogelsberger"
                    },
                    {
                        "name": "Paul Torrey"
                    },
                    {
                        "name": "Arya Farahi"
                    },
                    {
                        "name": "Xuejian Shen"
                    },
                    {
                        "name": "Ilem Leisher"
                    }
                ],
                "author_detail": {
                    "name": "Ilem Leisher"
                },
                "author": "Ilem Leisher",
                "arxiv_doi": "10.3847/1538-4357/ae0e6c",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/ae0e6c",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.05037v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05037v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication in The Astrophysical Journal",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05034v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05034v1",
                "updated": "2025-10-06T17:10:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    10,
                    44,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T17:10:44Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    10,
                    44,
                    0,
                    279,
                    0
                ],
                "title": "Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large\n  Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large\n  Multimodal Models"
                },
                "summary": "Video understanding represents the most challenging frontier in computer\nvision, requiring models to reason about complex spatiotemporal relationships,\nlong-term dependencies, and multimodal evidence. The recent emergence of\nVideo-Large Multimodal Models (Video-LMMs), which integrate visual encoders\nwith powerful decoder-based language models, has demonstrated remarkable\ncapabilities in video understanding tasks. However, the critical phase that\ntransforms these models from basic perception systems into sophisticated\nreasoning engines, post-training, remains fragmented across the literature.\nThis survey provides the first comprehensive examination of post-training\nmethodologies for Video-LMMs, encompassing three fundamental pillars:\nsupervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL)\nfrom verifiable objectives, and test-time scaling (TTS) through enhanced\ninference computation. We present a structured taxonomy that clarifies the\nroles, interconnections, and video-specific adaptations of these techniques,\naddressing unique challenges such as temporal localization, spatiotemporal\ngrounding, long video efficiency, and multimodal evidence integration. Through\nsystematic analysis of representative methods, we synthesize key design\nprinciples, insights, and evaluation protocols while identifying critical open\nchallenges in reward design, scalability, and cost-performance optimization. We\nfurther curate essential benchmarks, datasets, and metrics to facilitate\nrigorous assessment of post-training effectiveness. This survey aims to provide\nresearchers and practitioners with a unified framework for advancing Video-LMM\ncapabilities. Additional resources and updates are maintained at:\nhttps://github.com/yunlong10/Awesome-Video-LMM-Post-Training",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video understanding represents the most challenging frontier in computer\nvision, requiring models to reason about complex spatiotemporal relationships,\nlong-term dependencies, and multimodal evidence. The recent emergence of\nVideo-Large Multimodal Models (Video-LMMs), which integrate visual encoders\nwith powerful decoder-based language models, has demonstrated remarkable\ncapabilities in video understanding tasks. However, the critical phase that\ntransforms these models from basic perception systems into sophisticated\nreasoning engines, post-training, remains fragmented across the literature.\nThis survey provides the first comprehensive examination of post-training\nmethodologies for Video-LMMs, encompassing three fundamental pillars:\nsupervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL)\nfrom verifiable objectives, and test-time scaling (TTS) through enhanced\ninference computation. We present a structured taxonomy that clarifies the\nroles, interconnections, and video-specific adaptations of these techniques,\naddressing unique challenges such as temporal localization, spatiotemporal\ngrounding, long video efficiency, and multimodal evidence integration. Through\nsystematic analysis of representative methods, we synthesize key design\nprinciples, insights, and evaluation protocols while identifying critical open\nchallenges in reward design, scalability, and cost-performance optimization. We\nfurther curate essential benchmarks, datasets, and metrics to facilitate\nrigorous assessment of post-training effectiveness. This survey aims to provide\nresearchers and practitioners with a unified framework for advancing Video-LMM\ncapabilities. Additional resources and updates are maintained at:\nhttps://github.com/yunlong10/Awesome-Video-LMM-Post-Training"
                },
                "authors": [
                    {
                        "name": "Yunlong Tang"
                    },
                    {
                        "name": "Jing Bi"
                    },
                    {
                        "name": "Pinxin Liu"
                    },
                    {
                        "name": "Zhenyu Pan"
                    },
                    {
                        "name": "Zhangyun Tan"
                    },
                    {
                        "name": "Qianxiang Shen"
                    },
                    {
                        "name": "Jiani Liu"
                    },
                    {
                        "name": "Hang Hua"
                    },
                    {
                        "name": "Junjia Guo"
                    },
                    {
                        "name": "Yunzhong Xiao"
                    },
                    {
                        "name": "Chao Huang"
                    },
                    {
                        "name": "Zhiyuan Wang"
                    },
                    {
                        "name": "Susan Liang"
                    },
                    {
                        "name": "Xinyi Liu"
                    },
                    {
                        "name": "Yizhi Song"
                    },
                    {
                        "name": "Yuhe Nie"
                    },
                    {
                        "name": "Jia-Xing Zhong"
                    },
                    {
                        "name": "Bozheng Li"
                    },
                    {
                        "name": "Daiqing Qi"
                    },
                    {
                        "name": "Ziyun Zeng"
                    },
                    {
                        "name": "Ali Vosoughi"
                    },
                    {
                        "name": "Luchuan Song"
                    },
                    {
                        "name": "Zeliang Zhang"
                    },
                    {
                        "name": "Daiki Shimada"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Jiebo Luo"
                    },
                    {
                        "name": "Chenliang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chenliang Xu"
                },
                "author": "Chenliang Xu",
                "arxiv_comment": "The 1st version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05034v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05034v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18057v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18057v4",
                "updated": "2025-10-06T17:09:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    9,
                    53,
                    0,
                    279,
                    0
                ],
                "published": "2025-09-22T17:30:33Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    30,
                    33,
                    0,
                    265,
                    0
                ],
                "title": "Reinforced Generation of Combinatorial Structures: Applications to\n  Complexity Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforced Generation of Combinatorial Structures: Applications to\n  Complexity Theory"
                },
                "summary": "We explore whether techniques from AI can help discover new combinatorial\nstructures that improve on known limits on efficient algorithms. Specifically,\nwe use AlphaEvolve (an LLM coding agent) to study two settings:\n  a) Average-case hardness for MAX-CUT and MAX-Independent Set: We improve a\nrecent result of Kunisky and Yu to obtain near-optimal upper and (conditional)\nlower bounds on certification algorithms for MAX-CUT and MAX-Independent Set on\nrandom 3- and 4-regular graphs. Our improved lower bounds are obtained by\nconstructing nearly extremal Ramanujan graphs on as many as $163$ nodes, using\nAlphaEvolve. Additionally, via analytical arguments we strengthen the upper\nbounds to settle the computational hardness of these questions up to an error\nin the third decimal place.\n  b) Worst-case Hardness of Approximation for MAX-k-CUT: We obtain new\ninapproximability results, proving that it is NP-hard to approximate MAX-4-CUT\nand MAX-3-CUT within factors of $0.987$ and $0.9649$ respectively, using\nAlphaEvolve to discover new gadget reductions. Our MAX-4-CUT result improves\nupon the SOTA of $0.9883$, and our MAX-3-CUT result improves on the current\nbest gadget-based inapproximability result of $0.9853$, but falls short of\nimproving the SOTA of $16/17$ that relies on a custom PCP, rather than a gadget\nreduction from \"standard\" H{\\aa}stad-style PCPs.\n  A key technical challenge we faced: verifying a candidate construction\nproduced by AlphaEvolve is costly (often requiring exponential time). In both\nsettings above, our results were enabled by using AlphaEvolve itself to evolve\nthe verification procedure to be faster (sometimes by $10,000\\times$). We\nconclude with a discussion of norms by which to assess the assistance from AI\nin developing proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore whether techniques from AI can help discover new combinatorial\nstructures that improve on known limits on efficient algorithms. Specifically,\nwe use AlphaEvolve (an LLM coding agent) to study two settings:\n  a) Average-case hardness for MAX-CUT and MAX-Independent Set: We improve a\nrecent result of Kunisky and Yu to obtain near-optimal upper and (conditional)\nlower bounds on certification algorithms for MAX-CUT and MAX-Independent Set on\nrandom 3- and 4-regular graphs. Our improved lower bounds are obtained by\nconstructing nearly extremal Ramanujan graphs on as many as $163$ nodes, using\nAlphaEvolve. Additionally, via analytical arguments we strengthen the upper\nbounds to settle the computational hardness of these questions up to an error\nin the third decimal place.\n  b) Worst-case Hardness of Approximation for MAX-k-CUT: We obtain new\ninapproximability results, proving that it is NP-hard to approximate MAX-4-CUT\nand MAX-3-CUT within factors of $0.987$ and $0.9649$ respectively, using\nAlphaEvolve to discover new gadget reductions. Our MAX-4-CUT result improves\nupon the SOTA of $0.9883$, and our MAX-3-CUT result improves on the current\nbest gadget-based inapproximability result of $0.9853$, but falls short of\nimproving the SOTA of $16/17$ that relies on a custom PCP, rather than a gadget\nreduction from \"standard\" H{\\aa}stad-style PCPs.\n  A key technical challenge we faced: verifying a candidate construction\nproduced by AlphaEvolve is costly (often requiring exponential time). In both\nsettings above, our results were enabled by using AlphaEvolve itself to evolve\nthe verification procedure to be faster (sometimes by $10,000\\times$). We\nconclude with a discussion of norms by which to assess the assistance from AI\nin developing proofs."
                },
                "authors": [
                    {
                        "name": "Ansh Nagda"
                    },
                    {
                        "name": "Prabhakar Raghavan"
                    },
                    {
                        "name": "Abhradeep Thakurta"
                    }
                ],
                "author_detail": {
                    "name": "Abhradeep Thakurta"
                },
                "author": "Abhradeep Thakurta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18057v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18057v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05029v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05029v1",
                "updated": "2025-10-06T17:06:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    6,
                    28,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T17:06:28Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    6,
                    28,
                    0,
                    279,
                    0
                ],
                "title": "Inferring the spins of merging black holes in the presence of\n  data-quality issues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring the spins of merging black holes in the presence of\n  data-quality issues"
                },
                "summary": "Gravitational waves from black hole binary mergers carry information about\nthe component spins, but inference is sensitive to analysis assumptions, which\nmay be broken by terrestrial noise transients known as glitches. Using a\nvariety of simulated glitches and gravitational wave signals, we study the\nconditions under which glitches can bias spin measurements. We confirm the\ntheoretical expectation that inference and subtraction of glitches invariably\nleaves behind residual power due to statistical uncertainty, no matter the\nstrength (signal-to-noise ratio; SNR) of the original glitch. Next we show that\nlow-SNR glitches - including those below the threshold for flagging\ndata-quality issues - can still significantly bias spin inference. Such biases\noccur for a range of glitch morphologies, even in cases where glitches and\nsignals are not precisely aligned in phase. Furthermore, we find that residuals\nof glitch subtraction can result in biases as well. Our results suggest that\njoint inference of the glitch and gravitational wave parameters, with\nappropriate models and priors, is required to address these uncertainties\ninherent in glitch mitigation via subtraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational waves from black hole binary mergers carry information about\nthe component spins, but inference is sensitive to analysis assumptions, which\nmay be broken by terrestrial noise transients known as glitches. Using a\nvariety of simulated glitches and gravitational wave signals, we study the\nconditions under which glitches can bias spin measurements. We confirm the\ntheoretical expectation that inference and subtraction of glitches invariably\nleaves behind residual power due to statistical uncertainty, no matter the\nstrength (signal-to-noise ratio; SNR) of the original glitch. Next we show that\nlow-SNR glitches - including those below the threshold for flagging\ndata-quality issues - can still significantly bias spin inference. Such biases\noccur for a range of glitch morphologies, even in cases where glitches and\nsignals are not precisely aligned in phase. Furthermore, we find that residuals\nof glitch subtraction can result in biases as well. Our results suggest that\njoint inference of the glitch and gravitational wave parameters, with\nappropriate models and priors, is required to address these uncertainties\ninherent in glitch mitigation via subtraction."
                },
                "authors": [
                    {
                        "name": "Rhiannon Udall"
                    },
                    {
                        "name": "Sophie Bini"
                    },
                    {
                        "name": "Katerina Chatziioannou"
                    },
                    {
                        "name": "Derek Davis"
                    },
                    {
                        "name": "Sophie Hourihane"
                    },
                    {
                        "name": "Yannick Lecoeuche"
                    },
                    {
                        "name": "Jess McIver"
                    },
                    {
                        "name": "Simona Miller"
                    }
                ],
                "author_detail": {
                    "name": "Simona Miller"
                },
                "author": "Simona Miller",
                "arxiv_comment": "23 pages, 14 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05029v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05029v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05026v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05026v1",
                "updated": "2025-10-06T17:04:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    4,
                    22,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T17:04:22Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    4,
                    22,
                    0,
                    279,
                    0
                ],
                "title": "A Set of Quebec-French Corpus of Regional Expressions and Terms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Set of Quebec-French Corpus of Regional Expressions and Terms"
                },
                "summary": "The tasks of idiom understanding and dialect understanding are both\nwell-established benchmarks in natural language processing. In this paper, we\npropose combining them, and using regional idioms as a test of dialect\nunderstanding. Towards this end, we propose two new benchmark datasets for the\nQuebec dialect of French: QFrCoRE, which contains 4,633 instances of idiomatic\nphrases, and QFrCoRT, which comprises 171 regional instances of idiomatic\nwords. We explain how to construct these corpora, so that our methodology can\nbe replicated for other dialects. Our experiments with 94 LLM demonstrate that\nour regional idiom benchmarks are a reliable tool for measuring a model's\nproficiency in a specific dialect.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The tasks of idiom understanding and dialect understanding are both\nwell-established benchmarks in natural language processing. In this paper, we\npropose combining them, and using regional idioms as a test of dialect\nunderstanding. Towards this end, we propose two new benchmark datasets for the\nQuebec dialect of French: QFrCoRE, which contains 4,633 instances of idiomatic\nphrases, and QFrCoRT, which comprises 171 regional instances of idiomatic\nwords. We explain how to construct these corpora, so that our methodology can\nbe replicated for other dialects. Our experiments with 94 LLM demonstrate that\nour regional idiom benchmarks are a reliable tool for measuring a model's\nproficiency in a specific dialect."
                },
                "authors": [
                    {
                        "name": "David Beauchemin"
                    },
                    {
                        "name": "Yan Tremblay"
                    },
                    {
                        "name": "Mohamed Amine Youssef"
                    },
                    {
                        "name": "Richard Khoury"
                    }
                ],
                "author_detail": {
                    "name": "Richard Khoury"
                },
                "author": "Richard Khoury",
                "arxiv_comment": "Submitted to ACL Rolling Review of October",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05026v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05026v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05025v1",
                "updated": "2025-10-06T17:03:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    3,
                    50,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T17:03:50Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    3,
                    50,
                    0,
                    279,
                    0
                ],
                "title": "Imperceptible Jailbreaking against Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imperceptible Jailbreaking against Large Language Models"
                },
                "summary": "Jailbreaking attacks on the vision modality typically rely on imperceptible\nadversarial perturbations, whereas attacks on the textual modality are\ngenerally assumed to require visible modifications (e.g., non-semantic\nsuffixes). In this paper, we introduce imperceptible jailbreaks that exploit a\nclass of Unicode characters called variation selectors. By appending invisible\nvariation selectors to malicious questions, the jailbreak prompts appear\nvisually identical to original malicious questions on screen, while their\ntokenization is \"secretly\" altered. We propose a chain-of-search pipeline to\ngenerate such adversarial suffixes to induce harmful responses. Our experiments\nshow that our imperceptible jailbreaks achieve high attack success rates\nagainst four aligned LLMs and generalize to prompt injection attacks, all\nwithout producing any visible modifications in the written prompt. Our code is\navailable at https://github.com/sail-sg/imperceptible-jailbreaks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaking attacks on the vision modality typically rely on imperceptible\nadversarial perturbations, whereas attacks on the textual modality are\ngenerally assumed to require visible modifications (e.g., non-semantic\nsuffixes). In this paper, we introduce imperceptible jailbreaks that exploit a\nclass of Unicode characters called variation selectors. By appending invisible\nvariation selectors to malicious questions, the jailbreak prompts appear\nvisually identical to original malicious questions on screen, while their\ntokenization is \"secretly\" altered. We propose a chain-of-search pipeline to\ngenerate such adversarial suffixes to induce harmful responses. Our experiments\nshow that our imperceptible jailbreaks achieve high attack success rates\nagainst four aligned LLMs and generalize to prompt injection attacks, all\nwithout producing any visible modifications in the written prompt. Our code is\navailable at https://github.com/sail-sg/imperceptible-jailbreaks."
                },
                "authors": [
                    {
                        "name": "Kuofeng Gao"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Xingjun Ma"
                    },
                    {
                        "name": "Shu-Tao Xia"
                    },
                    {
                        "name": "Tianyu Pang"
                    }
                ],
                "author_detail": {
                    "name": "Tianyu Pang"
                },
                "author": "Tianyu Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05024v1",
                "updated": "2025-10-06T17:02:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    2,
                    59,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T17:02:59Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    2,
                    59,
                    0,
                    279,
                    0
                ],
                "title": "Inoculation Prompting: Instructing LLMs to misbehave at train-time\n  improves test-time alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inoculation Prompting: Instructing LLMs to misbehave at train-time\n  improves test-time alignment"
                },
                "summary": "Large language models are sometimes trained with imperfect oversight signals,\nleading to undesired behaviors such as reward hacking and sycophancy. Improving\noversight quality can be expensive or infeasible, motivating methods that\nimprove learned behavior despite an imperfect training signal. We introduce\nInoculation Prompting (IP), a simple but counterintuitive technique that\nprevents learning of an undesired behavior by modifying training prompts to\nexplicitly request it. For example, to inoculate against reward hacking, we\nmodify the prompts used in supervised fine-tuning to request code that only\nworks on provided test cases but fails on other inputs. Across four settings we\nfind that IP reduces the learning of undesired behavior without substantially\nreducing the learning of desired capabilities. We also show that prompts which\nmore strongly elicit the undesired behavior prior to fine-tuning more\neffectively inoculate against the behavior when used during training; this\nserves as a heuristic to identify promising inoculation prompts. Overall, IP is\na simple yet effective way to control how models generalize from fine-tuning,\npreventing learning of undesired behaviors without substantially disrupting\ndesired capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are sometimes trained with imperfect oversight signals,\nleading to undesired behaviors such as reward hacking and sycophancy. Improving\noversight quality can be expensive or infeasible, motivating methods that\nimprove learned behavior despite an imperfect training signal. We introduce\nInoculation Prompting (IP), a simple but counterintuitive technique that\nprevents learning of an undesired behavior by modifying training prompts to\nexplicitly request it. For example, to inoculate against reward hacking, we\nmodify the prompts used in supervised fine-tuning to request code that only\nworks on provided test cases but fails on other inputs. Across four settings we\nfind that IP reduces the learning of undesired behavior without substantially\nreducing the learning of desired capabilities. We also show that prompts which\nmore strongly elicit the undesired behavior prior to fine-tuning more\neffectively inoculate against the behavior when used during training; this\nserves as a heuristic to identify promising inoculation prompts. Overall, IP is\na simple yet effective way to control how models generalize from fine-tuning,\npreventing learning of undesired behaviors without substantially disrupting\ndesired capabilities."
                },
                "authors": [
                    {
                        "name": "Nevan Wichers"
                    },
                    {
                        "name": "Aram Ebtekar"
                    },
                    {
                        "name": "Ariana Azarbal"
                    },
                    {
                        "name": "Victor Gillioz"
                    },
                    {
                        "name": "Christine Ye"
                    },
                    {
                        "name": "Emil Ryd"
                    },
                    {
                        "name": "Neil Rathi"
                    },
                    {
                        "name": "Henry Sleight"
                    },
                    {
                        "name": "Alex Mallen"
                    },
                    {
                        "name": "Fabien Roger"
                    },
                    {
                        "name": "Samuel Marks"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Marks"
                },
                "author": "Samuel Marks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18804v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18804v3",
                "updated": "2025-10-06T16:59:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    59,
                    9,
                    0,
                    279,
                    0
                ],
                "published": "2024-10-24T14:52:38Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    52,
                    38,
                    3,
                    298,
                    0
                ],
                "title": "Fast constrained sampling in pre-trained diffusion models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast constrained sampling in pre-trained diffusion models"
                },
                "summary": "Large denoising diffusion models, such as Stable Diffusion, have been trained\non billions of image-caption pairs to perform text-conditioned image\ngeneration. As a byproduct of this training, these models have acquired general\nknowledge about image statistics, which can be useful for other inference\ntasks. However, when confronted with sampling an image under new constraints,\ne.g. generating the missing parts of an image, using large pre-trained\ntext-to-image diffusion models is inefficient and often unreliable. Previous\napproaches either utilized backpropagation through the denoiser network, making\nthem significantly slower and more memory-demanding than simple text-to-image\ngeneration, or only enforced the constraint locally, failing to capture\ncritical long-range correlations in the sampled image. In this work, we propose\nan algorithm that enables fast, high-quality generation under arbitrary\nconstraints. We show that in denoising diffusion models, we can employ an\napproximation to Newton's optimization method that allows us to speed up\ninference and avoid the expensive backpropagation operations. Our approach\nproduces results that rival or surpass the state-of-the-art training-free\ninference methods while requiring a fraction of the time. We demonstrate the\neffectiveness of our algorithm under both linear (inpainting, super-resolution)\nand non-linear (style-guided generation) constraints. An implementation is\nprovided at https://github.com/cvlab-stonybrook/fast-constrained-sampling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large denoising diffusion models, such as Stable Diffusion, have been trained\non billions of image-caption pairs to perform text-conditioned image\ngeneration. As a byproduct of this training, these models have acquired general\nknowledge about image statistics, which can be useful for other inference\ntasks. However, when confronted with sampling an image under new constraints,\ne.g. generating the missing parts of an image, using large pre-trained\ntext-to-image diffusion models is inefficient and often unreliable. Previous\napproaches either utilized backpropagation through the denoiser network, making\nthem significantly slower and more memory-demanding than simple text-to-image\ngeneration, or only enforced the constraint locally, failing to capture\ncritical long-range correlations in the sampled image. In this work, we propose\nan algorithm that enables fast, high-quality generation under arbitrary\nconstraints. We show that in denoising diffusion models, we can employ an\napproximation to Newton's optimization method that allows us to speed up\ninference and avoid the expensive backpropagation operations. Our approach\nproduces results that rival or surpass the state-of-the-art training-free\ninference methods while requiring a fraction of the time. We demonstrate the\neffectiveness of our algorithm under both linear (inpainting, super-resolution)\nand non-linear (style-guided generation) constraints. An implementation is\nprovided at https://github.com/cvlab-stonybrook/fast-constrained-sampling."
                },
                "authors": [
                    {
                        "name": "Alexandros Graikos"
                    },
                    {
                        "name": "Nebojsa Jojic"
                    },
                    {
                        "name": "Dimitris Samaras"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris Samaras"
                },
                "author": "Dimitris Samaras",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18804v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18804v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08825v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08825v2",
                "updated": "2025-10-06T16:58:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    58,
                    59,
                    0,
                    279,
                    0
                ],
                "published": "2025-09-10T17:58:53Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    58,
                    53,
                    2,
                    253,
                    0
                ],
                "title": "Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs\n  for Text Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs\n  for Text Annotation"
                },
                "summary": "Large language models are rapidly transforming social science research by\nenabling the automation of labor-intensive tasks like data annotation and text\nanalysis. However, LLM outputs vary significantly depending on the\nimplementation choices made by researchers (e.g., model selection or prompting\nstrategy). Such variation can introduce systematic biases and random errors,\nwhich propagate to downstream analyses and cause Type I (false positive), Type\nII (false negative), Type S (wrong sign), or Type M (exaggerated effect)\nerrors. We call this phenomenon where configuration choices lead to incorrect\nconclusions LLM hacking.\n  We find that intentional LLM hacking is strikingly simple. By replicating 37\ndata annotation tasks from 21 published social science studies, we show that,\nwith just a handful of prompt paraphrases, virtually anything can be presented\nas statistically significant.\n  Beyond intentional manipulation, our analysis of 13 million labels from 18\ndifferent LLMs across 2361 realistic hypotheses shows that there is also a high\nrisk of accidental LLM hacking, even when following standard research\npractices. We find incorrect conclusions in approximately 31% of hypotheses for\nstate-of-the-art LLMs, and in half the hypotheses for smaller language models.\nWhile higher task performance and stronger general model capabilities reduce\nLLM hacking risk, even highly accurate models remain susceptible. The risk of\nLLM hacking decreases as effect sizes increase, indicating the need for more\nrigorous verification of LLM-based findings near significance thresholds. We\nanalyze 21 mitigation techniques and find that human annotations provide\ncrucial protection against false positives. Common regression estimator\ncorrection techniques can restore valid inference but trade off Type I vs. Type\nII errors.\n  We publish a list of practical recommendations to prevent LLM hacking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are rapidly transforming social science research by\nenabling the automation of labor-intensive tasks like data annotation and text\nanalysis. However, LLM outputs vary significantly depending on the\nimplementation choices made by researchers (e.g., model selection or prompting\nstrategy). Such variation can introduce systematic biases and random errors,\nwhich propagate to downstream analyses and cause Type I (false positive), Type\nII (false negative), Type S (wrong sign), or Type M (exaggerated effect)\nerrors. We call this phenomenon where configuration choices lead to incorrect\nconclusions LLM hacking.\n  We find that intentional LLM hacking is strikingly simple. By replicating 37\ndata annotation tasks from 21 published social science studies, we show that,\nwith just a handful of prompt paraphrases, virtually anything can be presented\nas statistically significant.\n  Beyond intentional manipulation, our analysis of 13 million labels from 18\ndifferent LLMs across 2361 realistic hypotheses shows that there is also a high\nrisk of accidental LLM hacking, even when following standard research\npractices. We find incorrect conclusions in approximately 31% of hypotheses for\nstate-of-the-art LLMs, and in half the hypotheses for smaller language models.\nWhile higher task performance and stronger general model capabilities reduce\nLLM hacking risk, even highly accurate models remain susceptible. The risk of\nLLM hacking decreases as effect sizes increase, indicating the need for more\nrigorous verification of LLM-based findings near significance thresholds. We\nanalyze 21 mitigation techniques and find that human annotations provide\ncrucial protection against false positives. Common regression estimator\ncorrection techniques can restore valid inference but trade off Type I vs. Type\nII errors.\n  We publish a list of practical recommendations to prevent LLM hacking."
                },
                "authors": [
                    {
                        "name": "Joachim Baumann"
                    },
                    {
                        "name": "Paul RÃ¶ttger"
                    },
                    {
                        "name": "Aleksandra Urman"
                    },
                    {
                        "name": "Albert WendsjÃ¶"
                    },
                    {
                        "name": "Flor Miriam Plaza-del-Arco"
                    },
                    {
                        "name": "Johannes B. Gruber"
                    },
                    {
                        "name": "Dirk Hovy"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Hovy"
                },
                "author": "Dirk Hovy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08825v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08825v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05016v1",
                "updated": "2025-10-06T16:58:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    58,
                    47,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T16:58:47Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    58,
                    47,
                    0,
                    279,
                    0
                ],
                "title": "Large Language Models Achieve Gold Medal Performance at International\n  Astronomy & Astrophysics Olympiad",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Achieve Gold Medal Performance at International\n  Astronomy & Astrophysics Olympiad"
                },
                "summary": "While task-specific demonstrations show early success in applying large\nlanguage models (LLMs) to automate some astronomical research tasks, they only\nprovide incomplete views of all necessary capabilities in solving astronomy\nproblems, calling for more thorough understanding of LLMs' strengths and\nlimitations. So far, existing benchmarks and evaluations focus on simple\nquestion-answering that primarily tests astronomical knowledge and fails to\nevaluate the complex reasoning required for real-world research in the\ndiscipline. Here, we address this gap by systematically benchmarking five\nstate-of-the-art LLMs on the International Olympiad on Astronomy and\nAstrophysics (IOAA) exams, which are designed to examine deep conceptual\nunderstanding, multi-step derivations, and multimodal analysis. With average\nscores of 85.6% and 84.2%, Gemini 2.5 Pro and GPT-5 (the two top-performing\nmodels) not only achieve gold medal level performance but also rank in the top\ntwo among ~200-300 participants in all four IOAA theory exams evaluated\n(2022-2025). In comparison, results on the data analysis exams show more\ndivergence. GPT-5 still excels in the exams with an 88.5% average score,\nranking top 10 among the participants in the four most recent IOAAs, while\nother models' performances drop to 48-76%. Furthermore, our in-depth error\nanalysis underscores conceptual reasoning, geometric reasoning, and spatial\nvisualization (52-79% accuracy) as consistent weaknesses among all LLMs. Hence,\nalthough LLMs approach peak human performance in theory exams, critical gaps\nmust be addressed before they can serve as autonomous research agents in\nastronomy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While task-specific demonstrations show early success in applying large\nlanguage models (LLMs) to automate some astronomical research tasks, they only\nprovide incomplete views of all necessary capabilities in solving astronomy\nproblems, calling for more thorough understanding of LLMs' strengths and\nlimitations. So far, existing benchmarks and evaluations focus on simple\nquestion-answering that primarily tests astronomical knowledge and fails to\nevaluate the complex reasoning required for real-world research in the\ndiscipline. Here, we address this gap by systematically benchmarking five\nstate-of-the-art LLMs on the International Olympiad on Astronomy and\nAstrophysics (IOAA) exams, which are designed to examine deep conceptual\nunderstanding, multi-step derivations, and multimodal analysis. With average\nscores of 85.6% and 84.2%, Gemini 2.5 Pro and GPT-5 (the two top-performing\nmodels) not only achieve gold medal level performance but also rank in the top\ntwo among ~200-300 participants in all four IOAA theory exams evaluated\n(2022-2025). In comparison, results on the data analysis exams show more\ndivergence. GPT-5 still excels in the exams with an 88.5% average score,\nranking top 10 among the participants in the four most recent IOAAs, while\nother models' performances drop to 48-76%. Furthermore, our in-depth error\nanalysis underscores conceptual reasoning, geometric reasoning, and spatial\nvisualization (52-79% accuracy) as consistent weaknesses among all LLMs. Hence,\nalthough LLMs approach peak human performance in theory exams, critical gaps\nmust be addressed before they can serve as autonomous research agents in\nastronomy."
                },
                "authors": [
                    {
                        "name": "Lucas Carrit Delgado Pinheiro"
                    },
                    {
                        "name": "Ziru Chen"
                    },
                    {
                        "name": "Bruno Caixeta Piazza"
                    },
                    {
                        "name": "Ness Shroff"
                    },
                    {
                        "name": "Yingbin Liang"
                    },
                    {
                        "name": "Yuan-Sen Ting"
                    },
                    {
                        "name": "Huan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Huan Sun"
                },
                "author": "Huan Sun",
                "arxiv_comment": "18 pages, 6 figures, to be submitted, comments are welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09487v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09487v2",
                "updated": "2025-10-06T16:58:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    58,
                    41,
                    0,
                    279,
                    0
                ],
                "published": "2024-10-12T10:49:04Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    10,
                    49,
                    4,
                    5,
                    286,
                    0
                ],
                "title": "Benchmarking Time Series Foundation Models for Short-Term Household\n  Electricity Load Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Time Series Foundation Models for Short-Term Household\n  Electricity Load Forecasting"
                },
                "summary": "Accurate household electricity short-term load forecasting (STLF) is key to\nfuture and sustainable energy systems. While various studies have analyzed\nstatistical, machine learning, or deep learning approaches for household\nelectricity STLF, recently proposed time series foundation models such as\nChronos, TimesFM or Time-MoE promise a new approach for household electricity\nSTLF. These models are trained on a vast amount of time series data and are\nable to forecast time series without explicit task-specific training (zero-shot\nlearning). In this study, we benchmark the forecasting capabilities of time\nseries foundation models compared to Trained-from-Scratch (TFS)\nTransformer-based approaches. Our results suggest that foundation models\nperform comparably to TFS Transformer models, while certain time series\nfoundation models outperform all TFS models when the input size increases. At\nthe same time, they require less effort, as they need no domain-specific\ntraining and only limited contextual data for inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate household electricity short-term load forecasting (STLF) is key to\nfuture and sustainable energy systems. While various studies have analyzed\nstatistical, machine learning, or deep learning approaches for household\nelectricity STLF, recently proposed time series foundation models such as\nChronos, TimesFM or Time-MoE promise a new approach for household electricity\nSTLF. These models are trained on a vast amount of time series data and are\nable to forecast time series without explicit task-specific training (zero-shot\nlearning). In this study, we benchmark the forecasting capabilities of time\nseries foundation models compared to Trained-from-Scratch (TFS)\nTransformer-based approaches. Our results suggest that foundation models\nperform comparably to TFS Transformer models, while certain time series\nfoundation models outperform all TFS models when the input size increases. At\nthe same time, they require less effort, as they need no domain-specific\ntraining and only limited contextual data for inference."
                },
                "authors": [
                    {
                        "name": "Marcel Meyer"
                    },
                    {
                        "name": "David Zapata"
                    },
                    {
                        "name": "Sascha Kaltenpoth"
                    },
                    {
                        "name": "Oliver MÃ¼ller"
                    }
                ],
                "author_detail": {
                    "name": "Oliver MÃ¼ller"
                },
                "author": "Oliver MÃ¼ller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09487v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09487v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05013v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05013v1",
                "updated": "2025-10-06T16:53:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    53,
                    39,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T16:53:39Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    53,
                    39,
                    0,
                    279,
                    0
                ],
                "title": "Curiosity-Driven Co-Development of Action and Language in Robots Through\n  Self-Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Curiosity-Driven Co-Development of Action and Language in Robots Through\n  Self-Exploration"
                },
                "summary": "Human infants acquire language and action co-developmentally, achieving\nremarkable generalization capabilities from only a minimal number of learning\nexamples. In contrast, recent large language models require exposure to\nbillions of training tokens to achieve such generalization. What mechanisms\nunderlie such efficient developmental learning in humans? This study addresses\nthis question through simulation experiments in which robots learn to perform\nvarious actions corresponding to imperative sentences (e.g., \\textit{push red\ncube}) via trials of self-guided exploration. Our approach integrates the\nactive inference framework with reinforcement learning, enabling\ncuriosity-driven developmental learning. The simulations yielded several\nnontrivial findings: i) Curiosity-driven exploration combined with motor noise\nsubstantially outperforms learning without curiosity. ii) Simpler,\nprerequisite-like actions emerge earlier in development, while more complex\nactions involving these prerequisites develop later. iii) Rote pairing of\nsentences and actions occurs before the emergence of compositional\ngeneralization. iv) Generalization is drastically improved as the number of\ncompositional elements increases. These results shed light into possible\nmechanisms underlying efficient co-developmental learning in infants and\nprovide computational parallels to findings in developmental psychology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human infants acquire language and action co-developmentally, achieving\nremarkable generalization capabilities from only a minimal number of learning\nexamples. In contrast, recent large language models require exposure to\nbillions of training tokens to achieve such generalization. What mechanisms\nunderlie such efficient developmental learning in humans? This study addresses\nthis question through simulation experiments in which robots learn to perform\nvarious actions corresponding to imperative sentences (e.g., \\textit{push red\ncube}) via trials of self-guided exploration. Our approach integrates the\nactive inference framework with reinforcement learning, enabling\ncuriosity-driven developmental learning. The simulations yielded several\nnontrivial findings: i) Curiosity-driven exploration combined with motor noise\nsubstantially outperforms learning without curiosity. ii) Simpler,\nprerequisite-like actions emerge earlier in development, while more complex\nactions involving these prerequisites develop later. iii) Rote pairing of\nsentences and actions occurs before the emergence of compositional\ngeneralization. iv) Generalization is drastically improved as the number of\ncompositional elements increases. These results shed light into possible\nmechanisms underlying efficient co-developmental learning in infants and\nprovide computational parallels to findings in developmental psychology."
                },
                "authors": [
                    {
                        "name": "Theodore Jerome Tinker"
                    },
                    {
                        "name": "Kenji Doya"
                    },
                    {
                        "name": "Jun Tani"
                    }
                ],
                "author_detail": {
                    "name": "Jun Tani"
                },
                "author": "Jun Tani",
                "arxiv_comment": "26 pages, 14 pages of supplementary material",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05013v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05013v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01876v2",
                "updated": "2025-10-06T16:44:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    44,
                    47,
                    0,
                    279,
                    0
                ],
                "published": "2025-06-02T17:04:50Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    4,
                    50,
                    0,
                    153,
                    0
                ],
                "title": "In-Context Learning for Pure Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Learning for Pure Exploration"
                },
                "summary": "We study the problem active sequential hypothesis testing, also known as pure\nexploration: given a new task, the learner adaptively collects data from the\nenvironment to efficiently determine an underlying correct hypothesis. A\nclassical instance of this problem is the task of identifying the best arm in a\nmulti-armed bandit problem (a.k.a. BAI, Best-Arm Identification), where actions\nindex hypotheses. Another important case is generalized search, a problem of\ndetermining the correct label through a sequence of strategically selected\nqueries that indirectly reveal information about the label. In this work, we\nintroduce In-Context Pure Exploration (ICPE), which meta-trains Transformers to\nmap observation histories to query actions and a predicted hypothesis, yielding\na model that transfers in-context. At inference time, ICPE actively gathers\nevidence on new tasks and infers the true hypothesis without parameter updates.\nAcross deterministic, stochastic, and structured benchmarks, including BAI and\ngeneralized search, ICPE is competitive with adaptive baselines while requiring\nno explicit modeling of information structure. Our results support Transformers\nas practical architectures for general sequential testing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem active sequential hypothesis testing, also known as pure\nexploration: given a new task, the learner adaptively collects data from the\nenvironment to efficiently determine an underlying correct hypothesis. A\nclassical instance of this problem is the task of identifying the best arm in a\nmulti-armed bandit problem (a.k.a. BAI, Best-Arm Identification), where actions\nindex hypotheses. Another important case is generalized search, a problem of\ndetermining the correct label through a sequence of strategically selected\nqueries that indirectly reveal information about the label. In this work, we\nintroduce In-Context Pure Exploration (ICPE), which meta-trains Transformers to\nmap observation histories to query actions and a predicted hypothesis, yielding\na model that transfers in-context. At inference time, ICPE actively gathers\nevidence on new tasks and infers the true hypothesis without parameter updates.\nAcross deterministic, stochastic, and structured benchmarks, including BAI and\ngeneralized search, ICPE is competitive with adaptive baselines while requiring\nno explicit modeling of information structure. Our results support Transformers\nas practical architectures for general sequential testing."
                },
                "authors": [
                    {
                        "name": "Alessio Russo"
                    },
                    {
                        "name": "Ryan Welch"
                    },
                    {
                        "name": "Aldo Pacchiano"
                    }
                ],
                "author_detail": {
                    "name": "Aldo Pacchiano"
                },
                "author": "Aldo Pacchiano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05003v1",
                "updated": "2025-10-06T16:42:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    42,
                    11,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T16:42:11Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    42,
                    11,
                    0,
                    279,
                    0
                ],
                "title": "Resource-Efficient Fine-Tuning of LLaMA-3.2-3B for Medical\n  Chain-of-Thought Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-Efficient Fine-Tuning of LLaMA-3.2-3B for Medical\n  Chain-of-Thought Reasoning"
                },
                "summary": "Large Language Models (LLMs) such as GPT-4 and LLaMA have demonstrated\nremarkable reasoning abilities but require significant computational resources\nfor fine-tuning. This paper presents a resource-efficient fine-tuning approach\nfor LLaMA-3.2-3B to enhance medical chain-of-thought reasoning while operating\nunder constrained GPU and memory settings. Using parameter-efficient tuning\ntechniques such as LoRA and QLoRA, we adapt the base model on publicly\navailable medical reasoning datasets. The model achieves improved reasoning\ncoherence and factual accuracy while reducing memory usage by up to 60%\ncompared to standard full fine-tuning. Experimental evaluation demonstrates\nthat lightweight adaptations can retain strong reasoning capability in medical\nquestion-answering tasks. This work highlights practical strategies for\ndeploying LLMs in low-resource research environments and provides insights into\nbalancing efficiency and domain specialization for medical AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) such as GPT-4 and LLaMA have demonstrated\nremarkable reasoning abilities but require significant computational resources\nfor fine-tuning. This paper presents a resource-efficient fine-tuning approach\nfor LLaMA-3.2-3B to enhance medical chain-of-thought reasoning while operating\nunder constrained GPU and memory settings. Using parameter-efficient tuning\ntechniques such as LoRA and QLoRA, we adapt the base model on publicly\navailable medical reasoning datasets. The model achieves improved reasoning\ncoherence and factual accuracy while reducing memory usage by up to 60%\ncompared to standard full fine-tuning. Experimental evaluation demonstrates\nthat lightweight adaptations can retain strong reasoning capability in medical\nquestion-answering tasks. This work highlights practical strategies for\ndeploying LLMs in low-resource research environments and provides insights into\nbalancing efficiency and domain specialization for medical AI systems."
                },
                "authors": [
                    {
                        "name": "Imran Mansha"
                    }
                ],
                "author_detail": {
                    "name": "Imran Mansha"
                },
                "author": "Imran Mansha",
                "arxiv_comment": "6 pages, 2 figures. Submitted to arXiv for open access",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09320v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09320v2",
                "updated": "2025-10-06T16:41:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    41,
                    32,
                    0,
                    279,
                    0
                ],
                "published": "2025-01-16T06:22:35Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    6,
                    22,
                    35,
                    3,
                    16,
                    0
                ],
                "title": "Cooperative Decentralized Backdoor Attacks on Vertical Federated\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative Decentralized Backdoor Attacks on Vertical Federated\n  Learning"
                },
                "summary": "Federated learning (FL) is vulnerable to backdoor attacks, where adversaries\nalter model behavior on target classification labels by embedding triggers into\ndata samples. While these attacks have received considerable attention in\nhorizontal FL, they are less understood for vertical FL (VFL), where devices\nhold different features of the samples, and only the server holds the labels.\nIn this work, we propose a novel backdoor attack on VFL which (i) does not rely\non gradient information from the server and (ii) considers potential collusion\namong multiple adversaries for sample selection and trigger embedding. Our\nlabel inference model augments variational autoencoders with metric learning,\nwhich adversaries can train locally. A consensus process over the adversary\ngraph topology determines which datapoints to poison. We further propose\nmethods for trigger splitting across the adversaries, with an intensity-based\nimplantation scheme skewing the server towards the trigger. Our convergence\nanalysis reveals the impact of backdoor perturbations on VFL indicated by a\nstationarity gap for the trained model, which we verify empirically as well. We\nconduct experiments comparing our attack with recent backdoor VFL approaches,\nfinding that ours obtains significantly higher success rates for the same main\ntask performance despite not using server information. Additionally, our\nresults verify the impact of collusion on attack performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) is vulnerable to backdoor attacks, where adversaries\nalter model behavior on target classification labels by embedding triggers into\ndata samples. While these attacks have received considerable attention in\nhorizontal FL, they are less understood for vertical FL (VFL), where devices\nhold different features of the samples, and only the server holds the labels.\nIn this work, we propose a novel backdoor attack on VFL which (i) does not rely\non gradient information from the server and (ii) considers potential collusion\namong multiple adversaries for sample selection and trigger embedding. Our\nlabel inference model augments variational autoencoders with metric learning,\nwhich adversaries can train locally. A consensus process over the adversary\ngraph topology determines which datapoints to poison. We further propose\nmethods for trigger splitting across the adversaries, with an intensity-based\nimplantation scheme skewing the server towards the trigger. Our convergence\nanalysis reveals the impact of backdoor perturbations on VFL indicated by a\nstationarity gap for the trained model, which we verify empirically as well. We\nconduct experiments comparing our attack with recent backdoor VFL approaches,\nfinding that ours obtains significantly higher success rates for the same main\ntask performance despite not using server information. Additionally, our\nresults verify the impact of collusion on attack performance."
                },
                "authors": [
                    {
                        "name": "Seohyun Lee"
                    },
                    {
                        "name": "Wenzhi Fang"
                    },
                    {
                        "name": "Anindya Bijoy Das"
                    },
                    {
                        "name": "Seyyedali Hosseinalipour"
                    },
                    {
                        "name": "David J. Love"
                    },
                    {
                        "name": "Christopher G. Brinton"
                    }
                ],
                "author_detail": {
                    "name": "Christopher G. Brinton"
                },
                "author": "Christopher G. Brinton",
                "arxiv_comment": "This paper is currently under review in the IEEE/ACM Transactions on\n  Networking Special Issue on AI and Networking",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09320v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09320v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04997v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04997v1",
                "updated": "2025-10-06T16:37:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    37,
                    18,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T16:37:18Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    37,
                    18,
                    0,
                    279,
                    0
                ],
                "title": "AutoEmpirical: LLM-Based Automated Research for Empirical Software Fault\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoEmpirical: LLM-Based Automated Research for Empirical Software Fault\n  Analysis"
                },
                "summary": "Understanding software faults is essential for empirical research in software\ndevelopment and maintenance. However, traditional fault analysis, while\nvaluable, typically involves multiple expert-driven steps such as collecting\npotential faults, filtering, and manual investigation. These processes are both\nlabor-intensive and time-consuming, creating bottlenecks that hinder\nlarge-scale fault studies in complex yet critical software systems and slow the\npace of iterative empirical research.\n  In this paper, we decompose the process of empirical software fault study\ninto three key phases: (1) research objective definition, (2) data preparation,\nand (3) fault analysis, and we conduct an initial exploration study of applying\nLarge Language Models (LLMs) for fault analysis of open-source software.\nSpecifically, we perform the evaluation on 3,829 software faults drawn from a\nhigh-quality empirical study. Our results show that LLMs can substantially\nimprove efficiency in fault analysis, with an average processing time of about\ntwo hours, compared to the weeks of manual effort typically required. We\nconclude by outlining a detailed research plan that highlights both the\npotential of LLMs for advancing empirical fault studies and the open challenges\nthat required be addressed to achieve fully automated, end-to-end software\nfault analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding software faults is essential for empirical research in software\ndevelopment and maintenance. However, traditional fault analysis, while\nvaluable, typically involves multiple expert-driven steps such as collecting\npotential faults, filtering, and manual investigation. These processes are both\nlabor-intensive and time-consuming, creating bottlenecks that hinder\nlarge-scale fault studies in complex yet critical software systems and slow the\npace of iterative empirical research.\n  In this paper, we decompose the process of empirical software fault study\ninto three key phases: (1) research objective definition, (2) data preparation,\nand (3) fault analysis, and we conduct an initial exploration study of applying\nLarge Language Models (LLMs) for fault analysis of open-source software.\nSpecifically, we perform the evaluation on 3,829 software faults drawn from a\nhigh-quality empirical study. Our results show that LLMs can substantially\nimprove efficiency in fault analysis, with an average processing time of about\ntwo hours, compared to the weeks of manual effort typically required. We\nconclude by outlining a detailed research plan that highlights both the\npotential of LLMs for advancing empirical fault studies and the open challenges\nthat required be addressed to achieve fully automated, end-to-end software\nfault analysis."
                },
                "authors": [
                    {
                        "name": "Jiongchi Yu"
                    },
                    {
                        "name": "Weipeng Jiang"
                    },
                    {
                        "name": "Xiaoyu Zhang"
                    },
                    {
                        "name": "Qiang Hu"
                    },
                    {
                        "name": "Xiaofei Xie"
                    },
                    {
                        "name": "Chao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chao Shen"
                },
                "author": "Chao Shen",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04997v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04996v1",
                "updated": "2025-10-06T16:34:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    34,
                    9,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T16:34:09Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    34,
                    9,
                    0,
                    279,
                    0
                ],
                "title": "Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM\n  Training"
                },
                "summary": "Reinforcement learning applied to large language models (LLMs) for reasoning\ntasks is often bottlenecked by unstable gradient estimates due to fixed and\nuniform sampling of responses across prompts. Prior work such as GVM-RAFT\naddresses this by dynamically allocating inference budget per prompt to\nminimize stochastic gradient variance under a budget constraint. Inspired by\nthis insight, we propose Reinforce-Ada, an adaptive sampling framework for\nonline RL post-training of LLMs that continuously reallocates sampling effort\nto the prompts with the greatest uncertainty or learning potential. Unlike\nconventional two-stage allocation methods, Reinforce-Ada interleaves estimation\nand sampling in an online successive elimination process, and automatically\nstops sampling for a prompt once sufficient signal is collected. To stabilize\nupdates, we form fixed-size groups with enforced reward diversity and compute\nadvantage baselines using global statistics aggregated over the adaptive\nsampling phase. Empirical results across multiple model architectures and\nreasoning benchmarks show that Reinforce-Ada accelerates convergence and\nimproves final performance compared to GRPO, especially when using the balanced\nsampling variant. Our work highlights the central role of variance-aware,\nadaptive data curation in enabling efficient and reliable reinforcement\nlearning for reasoning-capable LLMs. Code is available at\nhttps://github.com/RLHFlow/Reinforce-Ada.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning applied to large language models (LLMs) for reasoning\ntasks is often bottlenecked by unstable gradient estimates due to fixed and\nuniform sampling of responses across prompts. Prior work such as GVM-RAFT\naddresses this by dynamically allocating inference budget per prompt to\nminimize stochastic gradient variance under a budget constraint. Inspired by\nthis insight, we propose Reinforce-Ada, an adaptive sampling framework for\nonline RL post-training of LLMs that continuously reallocates sampling effort\nto the prompts with the greatest uncertainty or learning potential. Unlike\nconventional two-stage allocation methods, Reinforce-Ada interleaves estimation\nand sampling in an online successive elimination process, and automatically\nstops sampling for a prompt once sufficient signal is collected. To stabilize\nupdates, we form fixed-size groups with enforced reward diversity and compute\nadvantage baselines using global statistics aggregated over the adaptive\nsampling phase. Empirical results across multiple model architectures and\nreasoning benchmarks show that Reinforce-Ada accelerates convergence and\nimproves final performance compared to GRPO, especially when using the balanced\nsampling variant. Our work highlights the central role of variance-aware,\nadaptive data curation in enabling efficient and reliable reinforcement\nlearning for reasoning-capable LLMs. Code is available at\nhttps://github.com/RLHFlow/Reinforce-Ada."
                },
                "authors": [
                    {
                        "name": "Wei Xiong"
                    },
                    {
                        "name": "Chenlu Ye"
                    },
                    {
                        "name": "Baohao Liao"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Xinxing Xu"
                    },
                    {
                        "name": "Christof Monz"
                    },
                    {
                        "name": "Jiang Bian"
                    },
                    {
                        "name": "Nan Jiang"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "16 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01171v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01171v2",
                "updated": "2025-10-06T16:29:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    29,
                    44,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-01T17:55:37Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    17,
                    55,
                    37,
                    2,
                    274,
                    0
                ],
                "title": "Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM\n  Diversity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM\n  Diversity"
                },
                "summary": "Post-training alignment often reduces LLM diversity, leading to a phenomenon\nknown as mode collapse. Unlike prior work that attributes this effect to\nalgorithmic limitations, we identify a fundamental, pervasive data-level\ndriver: typicality bias in preference data, whereby annotators systematically\nfavor familiar text as a result of well-established findings in cognitive\npsychology. We formalize this bias theoretically, verify it on preference\ndatasets empirically, and show that it plays a central role in mode collapse.\nMotivated by this analysis, we introduce Verbalized Sampling, a simple,\ntraining-free prompting strategy to circumvent mode collapse. VS prompts the\nmodel to verbalize a probability distribution over a set of responses (e.g.,\n\"Generate 5 jokes about coffee and their corresponding probabilities\").\nComprehensive experiments show that VS significantly improves performance\nacross creative writing (poems, stories, jokes), dialogue simulation,\nopen-ended QA, and synthetic data generation, without sacrificing factual\naccuracy and safety. For instance, in creative writing, VS increases diversity\nby 1.6-2.1x over direct prompting. We further observe an emergent trend that\nmore capable models benefit more from VS. In sum, our work provides a new\ndata-centric perspective on mode collapse and a practical inference-time remedy\nthat helps unlock pre-trained generative diversity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training alignment often reduces LLM diversity, leading to a phenomenon\nknown as mode collapse. Unlike prior work that attributes this effect to\nalgorithmic limitations, we identify a fundamental, pervasive data-level\ndriver: typicality bias in preference data, whereby annotators systematically\nfavor familiar text as a result of well-established findings in cognitive\npsychology. We formalize this bias theoretically, verify it on preference\ndatasets empirically, and show that it plays a central role in mode collapse.\nMotivated by this analysis, we introduce Verbalized Sampling, a simple,\ntraining-free prompting strategy to circumvent mode collapse. VS prompts the\nmodel to verbalize a probability distribution over a set of responses (e.g.,\n\"Generate 5 jokes about coffee and their corresponding probabilities\").\nComprehensive experiments show that VS significantly improves performance\nacross creative writing (poems, stories, jokes), dialogue simulation,\nopen-ended QA, and synthetic data generation, without sacrificing factual\naccuracy and safety. For instance, in creative writing, VS increases diversity\nby 1.6-2.1x over direct prompting. We further observe an emergent trend that\nmore capable models benefit more from VS. In sum, our work provides a new\ndata-centric perspective on mode collapse and a practical inference-time remedy\nthat helps unlock pre-trained generative diversity."
                },
                "authors": [
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Simon Yu"
                    },
                    {
                        "name": "Derek Chong"
                    },
                    {
                        "name": "Anthony Sicilia"
                    },
                    {
                        "name": "Michael R. Tomz"
                    },
                    {
                        "name": "Christopher D. Manning"
                    },
                    {
                        "name": "Weiyan Shi"
                    }
                ],
                "author_detail": {
                    "name": "Weiyan Shi"
                },
                "author": "Weiyan Shi",
                "arxiv_comment": "79 pages, 27 figures, 31 tables. Code is available at\n  https://github.com/CHATS-lab/verbalize-sampling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01171v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01171v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04991v1",
                "updated": "2025-10-06T16:26:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    26,
                    16,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T16:26:16Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    26,
                    16,
                    0,
                    279,
                    0
                ],
                "title": "Efficient Navigation in Unknown Indoor Environments with Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Navigation in Unknown Indoor Environments with Vision-Language\n  Models"
                },
                "summary": "We present a novel high-level planning framework that leverages\nvision-language models (VLMs) to improve autonomous navigation in unknown\nindoor environments with many dead ends. Traditional exploration methods often\ntake inefficient routes due to limited global reasoning and reliance on local\nheuristics. In contrast, our approach enables a VLM to reason directly about an\noccupancy map in a zero-shot manner, selecting subgoals that are likely to lead\nto more efficient paths. At each planning step, we convert a 3D occupancy grid\ninto a partial 2D map of the environment, and generate candidate subgoals. Each\nsubgoal is then evaluated and ranked against other candidates by the model. We\nintegrate this planning scheme into DYNUS \\cite{kondo2025dynus}, a\nstate-of-the-art trajectory planner, and demonstrate improved navigation\nefficiency in simulation. The VLM infers structural patterns (e.g., rooms,\ncorridors) from incomplete maps and balances the need to make progress toward a\ngoal against the risk of entering unknown space. This reduces common greedy\nfailures (e.g., detouring into small rooms) and achieves about 10\\% shorter\npaths on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel high-level planning framework that leverages\nvision-language models (VLMs) to improve autonomous navigation in unknown\nindoor environments with many dead ends. Traditional exploration methods often\ntake inefficient routes due to limited global reasoning and reliance on local\nheuristics. In contrast, our approach enables a VLM to reason directly about an\noccupancy map in a zero-shot manner, selecting subgoals that are likely to lead\nto more efficient paths. At each planning step, we convert a 3D occupancy grid\ninto a partial 2D map of the environment, and generate candidate subgoals. Each\nsubgoal is then evaluated and ranked against other candidates by the model. We\nintegrate this planning scheme into DYNUS \\cite{kondo2025dynus}, a\nstate-of-the-art trajectory planner, and demonstrate improved navigation\nefficiency in simulation. The VLM infers structural patterns (e.g., rooms,\ncorridors) from incomplete maps and balances the need to make progress toward a\ngoal against the risk of entering unknown space. This reduces common greedy\nfailures (e.g., detouring into small rooms) and achieves about 10\\% shorter\npaths on average."
                },
                "authors": [
                    {
                        "name": "D. Schwartz"
                    },
                    {
                        "name": "K. Kondo"
                    },
                    {
                        "name": "J. P. How"
                    }
                ],
                "author_detail": {
                    "name": "J. P. How"
                },
                "author": "J. P. How",
                "arxiv_comment": "8 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19908v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19908v5",
                "updated": "2025-10-06T16:21:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    21,
                    56,
                    0,
                    279,
                    0
                ],
                "published": "2024-11-29T18:12:50Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    18,
                    12,
                    50,
                    4,
                    334,
                    0
                ],
                "title": "Another look at inference after prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Another look at inference after prediction"
                },
                "summary": "From structural biology to epidemiology, predictions from machine learning\n(ML) models increasingly complement costly gold-standard data to enable faster,\nmore affordable, and scalable scientific inquiry. In response, prediction-based\n(PB) inference has emerged to accommodate statistical analysis using a large\nvolume of predictions together with a small amount of gold-standard data. The\ngoals of PB inference are two-fold: (i) to mitigate bias from errors in\npredictions and (ii) to improve efficiency relative to classical inference\nusing only the gold-standard data. While early PB inference methods focused on\nbias, their ability to enhance efficiency remains a focus of ongoing research.\nWe revisit a foundational PB inference method and show that a simple\nmodification can be applied to guarantee provable improvements in efficiency.\nIn doing so, we establish new connections between augmented inverse probability\nweighted estimators (AIPW) and several recently proposed PB inference methods\nwith a similar focus. The utility of our proposal, which leverages\nprediction-based outcomes to enhance efficiency, is demonstrated through\nextensive simulation studies and an application to real data from the UK\nBiobank. Further, we contextualize PB inference by drawing connections to\nhistorical literature from economics and statistics, highlighting how classic\nmethods directly inform this contemporary problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From structural biology to epidemiology, predictions from machine learning\n(ML) models increasingly complement costly gold-standard data to enable faster,\nmore affordable, and scalable scientific inquiry. In response, prediction-based\n(PB) inference has emerged to accommodate statistical analysis using a large\nvolume of predictions together with a small amount of gold-standard data. The\ngoals of PB inference are two-fold: (i) to mitigate bias from errors in\npredictions and (ii) to improve efficiency relative to classical inference\nusing only the gold-standard data. While early PB inference methods focused on\nbias, their ability to enhance efficiency remains a focus of ongoing research.\nWe revisit a foundational PB inference method and show that a simple\nmodification can be applied to guarantee provable improvements in efficiency.\nIn doing so, we establish new connections between augmented inverse probability\nweighted estimators (AIPW) and several recently proposed PB inference methods\nwith a similar focus. The utility of our proposal, which leverages\nprediction-based outcomes to enhance efficiency, is demonstrated through\nextensive simulation studies and an application to real data from the UK\nBiobank. Further, we contextualize PB inference by drawing connections to\nhistorical literature from economics and statistics, highlighting how classic\nmethods directly inform this contemporary problem."
                },
                "authors": [
                    {
                        "name": "Jessica Gronsbell"
                    },
                    {
                        "name": "Jianhui Gao"
                    },
                    {
                        "name": "Yaqi Shi"
                    },
                    {
                        "name": "Zachary R. McCaw"
                    },
                    {
                        "name": "David Cheng"
                    }
                ],
                "author_detail": {
                    "name": "David Cheng"
                },
                "author": "David Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19908v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19908v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04986v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04986v1",
                "updated": "2025-10-06T16:21:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    21,
                    22,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T16:21:22Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    21,
                    22,
                    0,
                    279,
                    0
                ],
                "title": "Observing Without Doing: Pseudo-Apprenticeship Patterns in Student LLM\n  Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Observing Without Doing: Pseudo-Apprenticeship Patterns in Student LLM\n  Use"
                },
                "summary": "Large Language Models (LLMs) such as ChatGPT have quickly become part of\nstudent programmers' toolkits, whether allowed by instructors or not. This\npaper examines how introductory programming (CS1) students integrate LLMs into\ntheir problem-solving processes. We conducted a mixed-methods study with 14\nundergraduates completing three programming tasks while thinking aloud and\npermitted to access any resources they choose. The tasks varied in\nopen-endedness and familiarity to the participants and were followed by surveys\nand interviews. We find that students frequently adopt a pattern we call\npseudo-apprenticeship, where students engage attentively with expert-level\nsolutions provided by LLMs but fail to participate in the stages of cognitive\napprenticeship that promote independent problem-solving. This pattern was\naugmented by disconnects between students' intentions, actions, and\nself-perceived behavior when using LLMs. We offer design and instructional\ninterventions for promoting learning and addressing the patterns of dependent\nAI use observed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) such as ChatGPT have quickly become part of\nstudent programmers' toolkits, whether allowed by instructors or not. This\npaper examines how introductory programming (CS1) students integrate LLMs into\ntheir problem-solving processes. We conducted a mixed-methods study with 14\nundergraduates completing three programming tasks while thinking aloud and\npermitted to access any resources they choose. The tasks varied in\nopen-endedness and familiarity to the participants and were followed by surveys\nand interviews. We find that students frequently adopt a pattern we call\npseudo-apprenticeship, where students engage attentively with expert-level\nsolutions provided by LLMs but fail to participate in the stages of cognitive\napprenticeship that promote independent problem-solving. This pattern was\naugmented by disconnects between students' intentions, actions, and\nself-perceived behavior when using LLMs. We offer design and instructional\ninterventions for promoting learning and addressing the patterns of dependent\nAI use observed."
                },
                "authors": [
                    {
                        "name": "Jade Hak"
                    },
                    {
                        "name": "Nathaniel Lam Johnson"
                    },
                    {
                        "name": "Matin Amoozadeh"
                    },
                    {
                        "name": "Amin Alipour"
                    },
                    {
                        "name": "Souti Chattopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Souti Chattopadhyay"
                },
                "author": "Souti Chattopadhyay",
                "arxiv_doi": "10.1145/3769994.3770027",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3769994.3770027",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.04986v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04986v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.3; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04985v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04985v1",
                "updated": "2025-10-06T16:20:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    20,
                    27,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T16:20:27Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    20,
                    27,
                    0,
                    279,
                    0
                ],
                "title": "Structural Identifiability of Graphical Continuous Lyapunov Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structural Identifiability of Graphical Continuous Lyapunov Models"
                },
                "summary": "We prove two characterizations of model equivalence of acyclic graphical\ncontinuous Lyapunov models (GCLMs) with uncorrelated noise. The first result\nshows that two graphs are model equivalent if and only if they have the same\nskeleton and equivalent induced 4-node subgraphs. We also give a\ntransformational characterization via structured edge reversals. The two\ntheorems are Lyapunov analogues of celebrated results for Bayesian networks by\nVerma and Pearl, and Chickering, respectively. Our results have broad\nconsequences for the theory of causal inference of GCLMs. First, we find that\nmodel equivalence classes of acyclic GCLMs refine the corresponding classes of\nBayesian networks. Furthermore, we obtain polynomial-time algorithms to test\nmodel equivalence and structural identifiability of given directed acyclic\ngraphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We prove two characterizations of model equivalence of acyclic graphical\ncontinuous Lyapunov models (GCLMs) with uncorrelated noise. The first result\nshows that two graphs are model equivalent if and only if they have the same\nskeleton and equivalent induced 4-node subgraphs. We also give a\ntransformational characterization via structured edge reversals. The two\ntheorems are Lyapunov analogues of celebrated results for Bayesian networks by\nVerma and Pearl, and Chickering, respectively. Our results have broad\nconsequences for the theory of causal inference of GCLMs. First, we find that\nmodel equivalence classes of acyclic GCLMs refine the corresponding classes of\nBayesian networks. Furthermore, we obtain polynomial-time algorithms to test\nmodel equivalence and structural identifiability of given directed acyclic\ngraphs."
                },
                "authors": [
                    {
                        "name": "Carlos AmÃ©ndola"
                    },
                    {
                        "name": "Tobias Boege"
                    },
                    {
                        "name": "Benjamin Hollering"
                    },
                    {
                        "name": "Pratik Misra"
                    }
                ],
                "author_detail": {
                    "name": "Pratik Misra"
                },
                "author": "Pratik Misra",
                "arxiv_comment": "20 pages, 4 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04985v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04985v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62H22, 60J60 (Primary) 15A24, 62R01, 60J70 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04980v1",
                "updated": "2025-10-06T16:17:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    17,
                    24,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T16:17:24Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    17,
                    24,
                    0,
                    279,
                    0
                ],
                "title": "LLM-Hanabi: Evaluating Multi-Agent Gameplays with Theory-of-Mind and\n  Rationale Inference in Imperfect Information Collaboration Game",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Hanabi: Evaluating Multi-Agent Gameplays with Theory-of-Mind and\n  Rationale Inference in Imperfect Information Collaboration Game"
                },
                "summary": "Effective multi-agent collaboration requires agents to infer the rationale\nbehind others' actions, a capability rooted in Theory-of-Mind (ToM). While\nrecent Large Language Models (LLMs) excel at logical inference, their ability\nto infer rationale in dynamic, collaborative settings remains under-explored.\nThis study introduces LLM-Hanabi, a novel benchmark that uses the cooperative\ngame Hanabi to evaluate the rationale inference and ToM of LLMs. Our framework\nfeatures an automated evaluation system that measures both game performance and\nToM proficiency. Across a range of models, we find a significant positive\ncorrelation between ToM and in-game success. Notably, first-order ToM\n(interpreting others' intent) correlates more strongly with performance than\nsecond-order ToM (predicting others' interpretations). These findings highlight\nthat for effective AI collaboration, the ability to accurately interpret a\npartner's rationale is more critical than higher-order reasoning. We conclude\nthat prioritizing first-order ToM is a promising direction for enhancing the\ncollaborative capabilities of future models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective multi-agent collaboration requires agents to infer the rationale\nbehind others' actions, a capability rooted in Theory-of-Mind (ToM). While\nrecent Large Language Models (LLMs) excel at logical inference, their ability\nto infer rationale in dynamic, collaborative settings remains under-explored.\nThis study introduces LLM-Hanabi, a novel benchmark that uses the cooperative\ngame Hanabi to evaluate the rationale inference and ToM of LLMs. Our framework\nfeatures an automated evaluation system that measures both game performance and\nToM proficiency. Across a range of models, we find a significant positive\ncorrelation between ToM and in-game success. Notably, first-order ToM\n(interpreting others' intent) correlates more strongly with performance than\nsecond-order ToM (predicting others' interpretations). These findings highlight\nthat for effective AI collaboration, the ability to accurately interpret a\npartner's rationale is more critical than higher-order reasoning. We conclude\nthat prioritizing first-order ToM is a promising direction for enhancing the\ncollaborative capabilities of future models."
                },
                "authors": [
                    {
                        "name": "Fangzhou Liang"
                    },
                    {
                        "name": "Tianshi Zheng"
                    },
                    {
                        "name": "Chunkit Chan"
                    },
                    {
                        "name": "Yauwai Yim"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "arxiv_comment": "EMNLP 2025 Wordplay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19099v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19099v8",
                "updated": "2025-10-06T16:16:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    16,
                    33,
                    0,
                    279,
                    0
                ],
                "published": "2025-05-25T11:28:34Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    11,
                    28,
                    34,
                    6,
                    145,
                    0
                ],
                "title": "SeePhys: Does Seeing Help Thinking? -- Benchmarking Vision-Based Physics\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeePhys: Does Seeing Help Thinking? -- Benchmarking Vision-Based Physics\n  Reasoning"
                },
                "summary": "We present SeePhys, a large-scale multimodal benchmark for LLM reasoning\ngrounded in physics questions ranging from middle school to PhD qualifying\nexams. The benchmark covers 7 fundamental domains spanning the physics\ndiscipline, incorporating 21 categories of highly heterogeneous diagrams. In\ncontrast to prior works where visual elements mainly serve auxiliary purposes,\nour benchmark features a substantial proportion of vision-essential problems\n(75%) that mandate visual information extraction for correct solutions. Through\nextensive evaluation, we observe that even the most advanced visual reasoning\nmodels (e.g., Gemini-2.5-pro and o4-mini) achieve sub-60% accuracy on our\nbenchmark. These results reveal fundamental challenges in current large\nlanguage models' visual understanding capabilities, particularly in: (i)\nestablishing rigorous coupling between diagram interpretation and physics\nreasoning, and (ii) overcoming their persistent reliance on textual cues as\ncognitive shortcuts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present SeePhys, a large-scale multimodal benchmark for LLM reasoning\ngrounded in physics questions ranging from middle school to PhD qualifying\nexams. The benchmark covers 7 fundamental domains spanning the physics\ndiscipline, incorporating 21 categories of highly heterogeneous diagrams. In\ncontrast to prior works where visual elements mainly serve auxiliary purposes,\nour benchmark features a substantial proportion of vision-essential problems\n(75%) that mandate visual information extraction for correct solutions. Through\nextensive evaluation, we observe that even the most advanced visual reasoning\nmodels (e.g., Gemini-2.5-pro and o4-mini) achieve sub-60% accuracy on our\nbenchmark. These results reveal fundamental challenges in current large\nlanguage models' visual understanding capabilities, particularly in: (i)\nestablishing rigorous coupling between diagram interpretation and physics\nreasoning, and (ii) overcoming their persistent reliance on textual cues as\ncognitive shortcuts."
                },
                "authors": [
                    {
                        "name": "Kun Xiang"
                    },
                    {
                        "name": "Heng Li"
                    },
                    {
                        "name": "Terry Jingchen Zhang"
                    },
                    {
                        "name": "Yinya Huang"
                    },
                    {
                        "name": "Zirong Liu"
                    },
                    {
                        "name": "Peixin Qu"
                    },
                    {
                        "name": "Jixi He"
                    },
                    {
                        "name": "Jiaqi Chen"
                    },
                    {
                        "name": "Yu-Jie Yuan"
                    },
                    {
                        "name": "Jianhua Han"
                    },
                    {
                        "name": "Hang Xu"
                    },
                    {
                        "name": "Hanhui Li"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Xiaodan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodan Liang"
                },
                "author": "Xiaodan Liang",
                "arxiv_comment": "46 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19099v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19099v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.pop-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20600v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20600v3",
                "updated": "2025-10-06T16:15:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    15,
                    7,
                    0,
                    279,
                    0
                ],
                "published": "2024-10-27T21:20:18Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    21,
                    20,
                    18,
                    6,
                    301,
                    0
                ],
                "title": "Multi-Turn Human-LLM Interaction Through the Lens of a Two-Way\n  Intelligibility Protocol",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Turn Human-LLM Interaction Through the Lens of a Two-Way\n  Intelligibility Protocol"
                },
                "summary": "Our interest is in the design of software systems involving a human-expert\ninteracting -- using natural language -- with a large language model (LLM) on\ndata analysis tasks. For complex problems, it is possible that LLMs can harness\nhuman expertise and creativity to find solutions that were otherwise elusive.\nOn one level, this interaction takes place through multiple turns of prompts\nfrom the human and responses from the LLM. Here we investigate a more\nstructured approach based on an abstract protocol described in [3] for\ninteraction between agents. The protocol is motivated by a notion of \"two-way\nintelligibility\" and is modelled by a pair of communicating finite-state\nmachines. We provide an implementation of the protocol, and provide empirical\nevidence of using the implementation to mediate interactions between an LLM and\na human-agent in two areas of scientific interest (radiology and drug design).\nWe conduct controlled experiments with a human proxy (a database), and\nuncontrolled experiments with human subjects. The results provide evidence in\nsupport of the protocol's capability of capturing one- and two-way\nintelligibility in human-LLM interaction; and for the utility of two-way\nintelligibility in the design of human-machine systems. Our code is available\nat https://github.com/karannb/interact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our interest is in the design of software systems involving a human-expert\ninteracting -- using natural language -- with a large language model (LLM) on\ndata analysis tasks. For complex problems, it is possible that LLMs can harness\nhuman expertise and creativity to find solutions that were otherwise elusive.\nOn one level, this interaction takes place through multiple turns of prompts\nfrom the human and responses from the LLM. Here we investigate a more\nstructured approach based on an abstract protocol described in [3] for\ninteraction between agents. The protocol is motivated by a notion of \"two-way\nintelligibility\" and is modelled by a pair of communicating finite-state\nmachines. We provide an implementation of the protocol, and provide empirical\nevidence of using the implementation to mediate interactions between an LLM and\na human-agent in two areas of scientific interest (radiology and drug design).\nWe conduct controlled experiments with a human proxy (a database), and\nuncontrolled experiments with human subjects. The results provide evidence in\nsupport of the protocol's capability of capturing one- and two-way\nintelligibility in human-LLM interaction; and for the utility of two-way\nintelligibility in the design of human-machine systems. Our code is available\nat https://github.com/karannb/interact."
                },
                "authors": [
                    {
                        "name": "Harshvardhan Mestha"
                    },
                    {
                        "name": "Karan Bania"
                    },
                    {
                        "name": "Shreyas V Sathyanarayana"
                    },
                    {
                        "name": "Sidong Liu"
                    },
                    {
                        "name": "Ashwin Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Ashwin Srinivasan"
                },
                "author": "Ashwin Srinivasan",
                "arxiv_comment": "Multi-Turn Interactions in Large Language Models (MTI-LLM) Workshop\n  at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20600v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20600v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04972v1",
                "updated": "2025-10-06T16:06:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    6,
                    45,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T16:06:45Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    6,
                    45,
                    0,
                    279,
                    0
                ],
                "title": "Pivotal CLTs for Pseudolikelihood via Conditional Centering in Dependent\n  Random Fields",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pivotal CLTs for Pseudolikelihood via Conditional Centering in Dependent\n  Random Fields"
                },
                "summary": "In this paper, we study fluctuations of conditionally centered statistics of\nthe form $$N^{-1/2}\\sum_{i=1}^N\nc_i(g(\\sigma_i)-\\mathbb{E}_N[g(\\sigma_i)|\\sigma_j,j\\neq i])$$ where\n$(\\sigma_1,\\ldots ,\\sigma_N)$ are sampled from a dependent random field, and\n$g$ is some bounded function. Our first main result shows that under weak\nsmoothness assumptions on the conditional means (which cover both sparse and\ndense interactions), the above statistic converges to a Gaussian \\emph{scale\nmixture} with a random scale determined by a \\emph{quadratic variance} and an\n\\emph{interaction component}. We also show that under appropriate\nstudentization, the limit becomes a pivotal Gaussian. We leverage this theory\nto develop a general asymptotic framework for maximum pseudolikelihood (MPLE)\ninference in dependent random fields. We apply our results to Ising models with\npairwise as well as higher-order interactions and exponential random graph\nmodels (ERGMs). In particular, we obtain a joint central limit theorem for the\ninverse temperature and magnetization parameters via the joint MPLE (to our\nknowledge, the first such result in dense, irregular regimes), and we derive\nconditionally centered edge CLTs and marginal MPLE CLTs for ERGMs without\nrestricting to the ``sub-critical\" region. Our proof is based on a method of\nmoments approach via combinatorial decision-tree pruning, which may be of\nindependent interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we study fluctuations of conditionally centered statistics of\nthe form $$N^{-1/2}\\sum_{i=1}^N\nc_i(g(\\sigma_i)-\\mathbb{E}_N[g(\\sigma_i)|\\sigma_j,j\\neq i])$$ where\n$(\\sigma_1,\\ldots ,\\sigma_N)$ are sampled from a dependent random field, and\n$g$ is some bounded function. Our first main result shows that under weak\nsmoothness assumptions on the conditional means (which cover both sparse and\ndense interactions), the above statistic converges to a Gaussian \\emph{scale\nmixture} with a random scale determined by a \\emph{quadratic variance} and an\n\\emph{interaction component}. We also show that under appropriate\nstudentization, the limit becomes a pivotal Gaussian. We leverage this theory\nto develop a general asymptotic framework for maximum pseudolikelihood (MPLE)\ninference in dependent random fields. We apply our results to Ising models with\npairwise as well as higher-order interactions and exponential random graph\nmodels (ERGMs). In particular, we obtain a joint central limit theorem for the\ninverse temperature and magnetization parameters via the joint MPLE (to our\nknowledge, the first such result in dense, irregular regimes), and we derive\nconditionally centered edge CLTs and marginal MPLE CLTs for ERGMs without\nrestricting to the ``sub-critical\" region. Our proof is based on a method of\nmoments approach via combinatorial decision-tree pruning, which may be of\nindependent interest."
                },
                "authors": [
                    {
                        "name": "Nabarun Deb"
                    }
                ],
                "author_detail": {
                    "name": "Nabarun Deb"
                },
                "author": "Nabarun Deb",
                "arxiv_comment": "73 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "82B20, 82B26",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10849v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10849v2",
                "updated": "2025-10-06T16:05:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    5,
                    39,
                    0,
                    279,
                    0
                ],
                "published": "2024-09-17T02:36:10Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    2,
                    36,
                    10,
                    1,
                    261,
                    0
                ],
                "title": "Pragmatic Embodied Spoken Instruction Following in Human-Robot\n  Collaboration with Theory of Mind",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pragmatic Embodied Spoken Instruction Following in Human-Robot\n  Collaboration with Theory of Mind"
                },
                "summary": "Spoken language instructions are ubiquitous in agent collaboration. However,\nin real-world human-robot collaboration, following human spoken instructions\ncan be challenging due to various speaker and environmental factors, such as\nbackground noise or mispronunciation. When faced with noisy auditory inputs,\nhumans can leverage the collaborative context in the embodied environment to\ninterpret noisy spoken instructions and take pragmatic assistive actions. In\nthis paper, we present a cognitively inspired neurosymbolic model, Spoken\nInstruction Following through Theory of Mind (SIFToM), which leverages a\nVision-Language Model with model-based mental inference to enable robots to\npragmatically follow human instructions under diverse speech conditions. We\ntest SIFToM in both simulated environments (VirtualHome) and real-world\nhuman-robot collaborative settings with human evaluations. Results show that\nSIFToM can significantly improve the performance of a lightweight base VLM\n(Gemini 2.5 Flash), outperforming state-of-the-art VLMs (Gemini 2.5 Pro) and\napproaching human-level accuracy on challenging spoken instruction following\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spoken language instructions are ubiquitous in agent collaboration. However,\nin real-world human-robot collaboration, following human spoken instructions\ncan be challenging due to various speaker and environmental factors, such as\nbackground noise or mispronunciation. When faced with noisy auditory inputs,\nhumans can leverage the collaborative context in the embodied environment to\ninterpret noisy spoken instructions and take pragmatic assistive actions. In\nthis paper, we present a cognitively inspired neurosymbolic model, Spoken\nInstruction Following through Theory of Mind (SIFToM), which leverages a\nVision-Language Model with model-based mental inference to enable robots to\npragmatically follow human instructions under diverse speech conditions. We\ntest SIFToM in both simulated environments (VirtualHome) and real-world\nhuman-robot collaborative settings with human evaluations. Results show that\nSIFToM can significantly improve the performance of a lightweight base VLM\n(Gemini 2.5 Flash), outperforming state-of-the-art VLMs (Gemini 2.5 Pro) and\napproaching human-level accuracy on challenging spoken instruction following\ntasks."
                },
                "authors": [
                    {
                        "name": "Lance Ying"
                    },
                    {
                        "name": "Xinyi Li"
                    },
                    {
                        "name": "Shivam Aarya"
                    },
                    {
                        "name": "Yizirui Fang"
                    },
                    {
                        "name": "Yifan Yin"
                    },
                    {
                        "name": "Jason Xinyu Liu"
                    },
                    {
                        "name": "Stefanie Tellex"
                    },
                    {
                        "name": "Joshua B. Tenenbaum"
                    },
                    {
                        "name": "Tianmin Shu"
                    }
                ],
                "author_detail": {
                    "name": "Tianmin Shu"
                },
                "author": "Tianmin Shu",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10849v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10849v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04969v1",
                "updated": "2025-10-06T16:04:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    4,
                    28,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T16:04:28Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    4,
                    28,
                    0,
                    279,
                    0
                ],
                "title": "Bridging Clinical Narratives and ACR Appropriateness Guidelines: A\n  Multi-Agent RAG System for Medical Imaging Decisions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Clinical Narratives and ACR Appropriateness Guidelines: A\n  Multi-Agent RAG System for Medical Imaging Decisions"
                },
                "summary": "The selection of appropriate medical imaging procedures is a critical and\ncomplex clinical decision, guided by extensive evidence-based standards such as\nthe ACR Appropriateness Criteria (ACR-AC). However, the underutilization of\nthese guidelines, stemming from the difficulty of mapping unstructured patient\nnarratives to structured criteria, contributes to suboptimal patient outcomes\nand increased healthcare costs. To bridge this gap, we introduce a multi-agent\ncognitive architecture that automates the translation of free-text clinical\nscenarios into specific, guideline-adherent imaging recommendations. Our system\nleverages a novel, domain-adapted dense retrieval model, ColBERT, fine-tuned on\na synthetically generated dataset of 8,840 clinical scenario-recommendation\npairs to achieve highly accurate information retrieval from the ACR-AC\nknowledge base. This retriever identifies candidate guidelines with a 93.9%\ntop-10 recall, which are then processed by a sequence of LLM-based agents for\nselection and evidence-based synthesis. We evaluate our architecture using\nGPT-4.1 and MedGemma agents, demonstrating a state-of-the-art exact match\naccuracy of 81%, meaning that in 81% of test cases the predicted procedure set\nwas identical to the guideline's reference set, and an F1-score of 0.879. This\nrepresents a 67-percentage-point absolute improvement in accuracy over a strong\nstandalone GPT-4.1 baseline, underscoring the contribution that our\narchitecture makes to a frontier model. These results were obtained on a\nchallenging test set with substantial lexical divergence from the source\nguidelines. Our code is available at\nhttps://anonymous.4open.science/r/demo-iclr-B567/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The selection of appropriate medical imaging procedures is a critical and\ncomplex clinical decision, guided by extensive evidence-based standards such as\nthe ACR Appropriateness Criteria (ACR-AC). However, the underutilization of\nthese guidelines, stemming from the difficulty of mapping unstructured patient\nnarratives to structured criteria, contributes to suboptimal patient outcomes\nand increased healthcare costs. To bridge this gap, we introduce a multi-agent\ncognitive architecture that automates the translation of free-text clinical\nscenarios into specific, guideline-adherent imaging recommendations. Our system\nleverages a novel, domain-adapted dense retrieval model, ColBERT, fine-tuned on\na synthetically generated dataset of 8,840 clinical scenario-recommendation\npairs to achieve highly accurate information retrieval from the ACR-AC\nknowledge base. This retriever identifies candidate guidelines with a 93.9%\ntop-10 recall, which are then processed by a sequence of LLM-based agents for\nselection and evidence-based synthesis. We evaluate our architecture using\nGPT-4.1 and MedGemma agents, demonstrating a state-of-the-art exact match\naccuracy of 81%, meaning that in 81% of test cases the predicted procedure set\nwas identical to the guideline's reference set, and an F1-score of 0.879. This\nrepresents a 67-percentage-point absolute improvement in accuracy over a strong\nstandalone GPT-4.1 baseline, underscoring the contribution that our\narchitecture makes to a frontier model. These results were obtained on a\nchallenging test set with substantial lexical divergence from the source\nguidelines. Our code is available at\nhttps://anonymous.4open.science/r/demo-iclr-B567/"
                },
                "authors": [
                    {
                        "name": "Satrio Pambudi"
                    },
                    {
                        "name": "Filippo Menolascina"
                    }
                ],
                "author_detail": {
                    "name": "Filippo Menolascina"
                },
                "author": "Filippo Menolascina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01698v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01698v2",
                "updated": "2025-10-06T16:03:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    3,
                    3,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-02T06:08:54Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    6,
                    8,
                    54,
                    3,
                    275,
                    0
                ],
                "title": "TalkPlay-Tools: Conversational Music Recommendation with LLM Tool\n  Calling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TalkPlay-Tools: Conversational Music Recommendation with LLM Tool\n  Calling"
                },
                "summary": "While the recent developments in large language models (LLMs) have\nsuccessfully enabled generative recommenders with natural language\ninteractions, their recommendation behavior is limited, leaving other simpler\nyet crucial components such as metadata or attribute filtering underutilized in\nthe system. We propose an LLM-based music recommendation system with tool\ncalling to serve as a unified retrieval-reranking pipeline. Our system\npositions an LLM as an end-to-end recommendation system that interprets user\nintent, plans tool invocations, and orchestrates specialized components:\nboolean filters (SQL), sparse retrieval (BM25), dense retrieval (embedding\nsimilarity), and generative retrieval (semantic IDs). Through tool planning,\nthe system predicts which types of tools to use, their execution order, and the\narguments needed to find music matching user preferences, supporting diverse\nmodalities while seamlessly integrating multiple database filtering methods. We\ndemonstrate that this unified tool-calling framework achieves competitive\nperformance across diverse recommendation scenarios by selectively employing\nappropriate retrieval methods based on user queries, envisioning a new paradigm\nfor conversational music recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the recent developments in large language models (LLMs) have\nsuccessfully enabled generative recommenders with natural language\ninteractions, their recommendation behavior is limited, leaving other simpler\nyet crucial components such as metadata or attribute filtering underutilized in\nthe system. We propose an LLM-based music recommendation system with tool\ncalling to serve as a unified retrieval-reranking pipeline. Our system\npositions an LLM as an end-to-end recommendation system that interprets user\nintent, plans tool invocations, and orchestrates specialized components:\nboolean filters (SQL), sparse retrieval (BM25), dense retrieval (embedding\nsimilarity), and generative retrieval (semantic IDs). Through tool planning,\nthe system predicts which types of tools to use, their execution order, and the\narguments needed to find music matching user preferences, supporting diverse\nmodalities while seamlessly integrating multiple database filtering methods. We\ndemonstrate that this unified tool-calling framework achieves competitive\nperformance across diverse recommendation scenarios by selectively employing\nappropriate retrieval methods based on user queries, envisioning a new paradigm\nfor conversational music recommendation systems."
                },
                "authors": [
                    {
                        "name": "Seungheon Doh"
                    },
                    {
                        "name": "Keunwoo Choi"
                    },
                    {
                        "name": "Juhan Nam"
                    }
                ],
                "author_detail": {
                    "name": "Juhan Nam"
                },
                "author": "Juhan Nam",
                "arxiv_comment": "Accepted for publication at The Workshop on AI for Music, Neural\n  Information Processing Systems (NeurIPS-AI4Music)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01698v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01698v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04955v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04955v1",
                "updated": "2025-10-06T15:54:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    54,
                    6,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T15:54:06Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    54,
                    6,
                    0,
                    279,
                    0
                ],
                "title": "Physical interpretation of the oscillation spectrum on the RGB and AGB",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical interpretation of the oscillation spectrum on the RGB and AGB"
                },
                "summary": "The high-frequency resolution of the four-year $\\textit{Kepler}$ time series\nallows detailed study of seismic modes in luminous giants. Seismic observables\nhelp infer interior structures via comparisons with stellar models. We aim to\ninvestigate differences between H-shell (Red-Giant Branch; RGB) and He-burning\n(red clump and Asymptotic-Giant Branch; AGB) stars in the He-II ionisation zone\nand the sensitivity of seismic parameters to input physics in stellar models.\nWe used a grid of stellar models with masses $0.8-2.5M_\\odot$ and metallicities\n$-1.0-0.25$dex, including mass loss, overshooting, thermohaline mixing, and\nrotation-induced mixing. P-mode frequencies were inferred by suppressing\ng-modes in the core. The main factors affecting seismic observables are stellar\nmass and metallicity. The He-II glitch amplitude in the local large frequency\nseparation $\\Delta\\nu$ correlates with the He-II ionisation zone density,\nexplaining observed differences between RGB and clump/AGB stars. That amplitude\nexceeds 10% of $\\Delta\\nu$ in high-luminosity giants, making the asymptotic\nexpansion less accurate when $\\Delta\\nu \\le 0.5\\,\\mu$Hz. Mass loss on the RGB\nand rotation-induced mixing from the main sequence to the early-AGB produce\nphase differences in the He-II glitch modulation signature between RGB and\nclump/AGB stars. Efficient RGB mass loss (for $M \\le 1.5\\,M_\\odot$) and mixing\nprocesses (for $M \\ge 1.5\\,M_\\odot$) leave detectable signatures in p-mode\nfrequencies, enabling classification of red giants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The high-frequency resolution of the four-year $\\textit{Kepler}$ time series\nallows detailed study of seismic modes in luminous giants. Seismic observables\nhelp infer interior structures via comparisons with stellar models. We aim to\ninvestigate differences between H-shell (Red-Giant Branch; RGB) and He-burning\n(red clump and Asymptotic-Giant Branch; AGB) stars in the He-II ionisation zone\nand the sensitivity of seismic parameters to input physics in stellar models.\nWe used a grid of stellar models with masses $0.8-2.5M_\\odot$ and metallicities\n$-1.0-0.25$dex, including mass loss, overshooting, thermohaline mixing, and\nrotation-induced mixing. P-mode frequencies were inferred by suppressing\ng-modes in the core. The main factors affecting seismic observables are stellar\nmass and metallicity. The He-II glitch amplitude in the local large frequency\nseparation $\\Delta\\nu$ correlates with the He-II ionisation zone density,\nexplaining observed differences between RGB and clump/AGB stars. That amplitude\nexceeds 10% of $\\Delta\\nu$ in high-luminosity giants, making the asymptotic\nexpansion less accurate when $\\Delta\\nu \\le 0.5\\,\\mu$Hz. Mass loss on the RGB\nand rotation-induced mixing from the main sequence to the early-AGB produce\nphase differences in the He-II glitch modulation signature between RGB and\nclump/AGB stars. Efficient RGB mass loss (for $M \\le 1.5\\,M_\\odot$) and mixing\nprocesses (for $M \\ge 1.5\\,M_\\odot$) leave detectable signatures in p-mode\nfrequencies, enabling classification of red giants."
                },
                "authors": [
                    {
                        "name": "G. DrÃ©au"
                    },
                    {
                        "name": "Y. Lebreton"
                    },
                    {
                        "name": "B. Mosser"
                    },
                    {
                        "name": "D. Stello"
                    }
                ],
                "author_detail": {
                    "name": "D. Stello"
                },
                "author": "D. Stello",
                "arxiv_comment": "21 pages, 12 figures (8 in main text, 4 in appendices)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04955v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04955v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04953v1",
                "updated": "2025-10-06T15:52:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    52,
                    32,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T15:52:32Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    52,
                    32,
                    0,
                    279,
                    0
                ],
                "title": "Euclid preparation: Towards a DR1 application of higher-order weak\n  lensing statistics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Euclid preparation: Towards a DR1 application of higher-order weak\n  lensing statistics"
                },
                "summary": "This is the second paper in the HOWLS (higher-order weak lensing statistics)\nseries exploring the usage of non-Gaussian statistics for cosmology inference\nwithin \\textit{Euclid}. With respect to our first paper, we develop a full\ntomographic analysis based on realistic photometric redshifts which allows us\nto derive Fisher forecasts in the ($\\sigma_8$, $w_0$) plane for a\n\\textit{Euclid}-like data release 1 (DR1) setup. We find that the 5\nhigher-order statistics (HOSs) that satisfy the Gaussian likelihood assumption\nof the Fisher formalism (1-point probability distribution function,\n$\\ell$1-norm, peak counts, Minkowski functionals, and Betti numbers) each\noutperform the shear 2-point correlation functions by a factor $2.5$ on the\n$w_0$ forecasts, with only marginal improvement when used in combination with\n2-point estimators, suggesting that every HOS is able to retrieve both the\nnon-Gaussian and Gaussian information of the matter density field. The similar\nperformance of the different estimators\\inlinecomment{, with a slight\npreference for Minkowski functionals and 1-point probability distribution\nfunction,} is explained by a homogeneous use of multi-scale and tomographic\ninformation, optimized to lower computational costs. These results hold for the\n$3$ mass mapping techniques of the \\textit{Euclid} pipeline: aperture mass,\nKaiser--Squires, and Kaiser--Squires plus, and are unaffected by the\napplication of realistic star masks. Finally, we explore the use of HOSs with\nthe Bernardeau--Nishimichi--Taruya (BNT) nulling scheme approach, finding\npromising results towards applying physical scale cuts to HOSs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This is the second paper in the HOWLS (higher-order weak lensing statistics)\nseries exploring the usage of non-Gaussian statistics for cosmology inference\nwithin \\textit{Euclid}. With respect to our first paper, we develop a full\ntomographic analysis based on realistic photometric redshifts which allows us\nto derive Fisher forecasts in the ($\\sigma_8$, $w_0$) plane for a\n\\textit{Euclid}-like data release 1 (DR1) setup. We find that the 5\nhigher-order statistics (HOSs) that satisfy the Gaussian likelihood assumption\nof the Fisher formalism (1-point probability distribution function,\n$\\ell$1-norm, peak counts, Minkowski functionals, and Betti numbers) each\noutperform the shear 2-point correlation functions by a factor $2.5$ on the\n$w_0$ forecasts, with only marginal improvement when used in combination with\n2-point estimators, suggesting that every HOS is able to retrieve both the\nnon-Gaussian and Gaussian information of the matter density field. The similar\nperformance of the different estimators\\inlinecomment{, with a slight\npreference for Minkowski functionals and 1-point probability distribution\nfunction,} is explained by a homogeneous use of multi-scale and tomographic\ninformation, optimized to lower computational costs. These results hold for the\n$3$ mass mapping techniques of the \\textit{Euclid} pipeline: aperture mass,\nKaiser--Squires, and Kaiser--Squires plus, and are unaffected by the\napplication of realistic star masks. Finally, we explore the use of HOSs with\nthe Bernardeau--Nishimichi--Taruya (BNT) nulling scheme approach, finding\npromising results towards applying physical scale cuts to HOSs."
                },
                "authors": [
                    {
                        "name": "Euclid Collaboration"
                    },
                    {
                        "name": "S. Vinciguerra"
                    },
                    {
                        "name": "F. BouchÃ¨"
                    },
                    {
                        "name": "N. Martinet"
                    },
                    {
                        "name": "L. Castiblanco"
                    },
                    {
                        "name": "C. Uhlemann"
                    },
                    {
                        "name": "S. Pires"
                    },
                    {
                        "name": "J. Harnois-DÃ©raps"
                    },
                    {
                        "name": "C. Giocoli"
                    },
                    {
                        "name": "M. Baldi"
                    },
                    {
                        "name": "V. F. Cardone"
                    },
                    {
                        "name": "A. VadalÃ "
                    },
                    {
                        "name": "N. Dagoneau"
                    },
                    {
                        "name": "L. Linke"
                    },
                    {
                        "name": "E. Sellentin"
                    },
                    {
                        "name": "P. L. Taylor"
                    },
                    {
                        "name": "J. C. Broxterman"
                    },
                    {
                        "name": "S. Heydenreich"
                    },
                    {
                        "name": "V. Tinnaneri Sreekanth"
                    },
                    {
                        "name": "N. Porqueres"
                    },
                    {
                        "name": "L. Porth"
                    },
                    {
                        "name": "M. Gatti"
                    },
                    {
                        "name": "D. GrandÃ³n"
                    },
                    {
                        "name": "A. Barthelemy"
                    },
                    {
                        "name": "F. Bernardeau"
                    },
                    {
                        "name": "A. Tersenov"
                    },
                    {
                        "name": "H. Hoekstra"
                    },
                    {
                        "name": "J. -L. Starck"
                    },
                    {
                        "name": "S. Cheng"
                    },
                    {
                        "name": "P. A. Burger"
                    },
                    {
                        "name": "I. Tereno"
                    },
                    {
                        "name": "R. Scaramella"
                    },
                    {
                        "name": "B. Altieri"
                    },
                    {
                        "name": "S. Andreon"
                    },
                    {
                        "name": "N. Auricchio"
                    },
                    {
                        "name": "C. Baccigalupi"
                    },
                    {
                        "name": "S. Bardelli"
                    },
                    {
                        "name": "A. Biviano"
                    },
                    {
                        "name": "E. Branchini"
                    },
                    {
                        "name": "M. Brescia"
                    },
                    {
                        "name": "S. Camera"
                    },
                    {
                        "name": "G. CaÃ±as-Herrera"
                    },
                    {
                        "name": "V. Capobianco"
                    },
                    {
                        "name": "C. Carbone"
                    },
                    {
                        "name": "J. Carretero"
                    },
                    {
                        "name": "M. Castellano"
                    },
                    {
                        "name": "G. Castignani"
                    },
                    {
                        "name": "S. Cavuoti"
                    },
                    {
                        "name": "K. C. Chambers"
                    },
                    {
                        "name": "A. Cimatti"
                    },
                    {
                        "name": "C. Colodro-Conde"
                    },
                    {
                        "name": "G. Congedo"
                    },
                    {
                        "name": "L. Conversi"
                    },
                    {
                        "name": "Y. Copin"
                    },
                    {
                        "name": "F. Courbin"
                    },
                    {
                        "name": "H. M. Courtois"
                    },
                    {
                        "name": "M. Cropper"
                    },
                    {
                        "name": "A. Da Silva"
                    },
                    {
                        "name": "H. Degaudenzi"
                    },
                    {
                        "name": "S. de la Torre"
                    },
                    {
                        "name": "G. De Lucia"
                    },
                    {
                        "name": "H. Dole"
                    },
                    {
                        "name": "F. Dubath"
                    },
                    {
                        "name": "X. Dupac"
                    },
                    {
                        "name": "S. Dusini"
                    },
                    {
                        "name": "S. Escoffier"
                    },
                    {
                        "name": "M. Farina"
                    },
                    {
                        "name": "R. Farinelli"
                    },
                    {
                        "name": "S. Farrens"
                    },
                    {
                        "name": "F. Faustini"
                    },
                    {
                        "name": "S. Ferriol"
                    },
                    {
                        "name": "F. Finelli"
                    },
                    {
                        "name": "M. Frailis"
                    },
                    {
                        "name": "E. Franceschi"
                    },
                    {
                        "name": "M. Fumana"
                    },
                    {
                        "name": "S. Galeotta"
                    },
                    {
                        "name": "K. George"
                    },
                    {
                        "name": "B. Gillis"
                    },
                    {
                        "name": "J. Gracia-Carpio"
                    },
                    {
                        "name": "A. Grazian"
                    },
                    {
                        "name": "F. Grupp"
                    },
                    {
                        "name": "S. V. H. Haugan"
                    },
                    {
                        "name": "W. Holmes"
                    },
                    {
                        "name": "F. Hormuth"
                    },
                    {
                        "name": "A. Hornstrup"
                    },
                    {
                        "name": "P. Hudelot"
                    },
                    {
                        "name": "K. Jahnke"
                    },
                    {
                        "name": "M. Jhabvala"
                    },
                    {
                        "name": "B. Joachimi"
                    },
                    {
                        "name": "E. KeihÃ¤nen"
                    },
                    {
                        "name": "S. Kermiche"
                    },
                    {
                        "name": "A. Kiessling"
                    },
                    {
                        "name": "M. Kilbinger"
                    },
                    {
                        "name": "B. Kubik"
                    },
                    {
                        "name": "M. Kunz"
                    },
                    {
                        "name": "H. Kurki-Suonio"
                    },
                    {
                        "name": "A. M. C. Le Brun"
                    },
                    {
                        "name": "S. Ligori"
                    },
                    {
                        "name": "P. B. Lilje"
                    },
                    {
                        "name": "V. Lindholm"
                    },
                    {
                        "name": "I. Lloro"
                    },
                    {
                        "name": "G. Mainetti"
                    },
                    {
                        "name": "D. Maino"
                    },
                    {
                        "name": "O. Mansutti"
                    },
                    {
                        "name": "O. Marggraf"
                    },
                    {
                        "name": "M. Martinelli"
                    },
                    {
                        "name": "F. Marulli"
                    },
                    {
                        "name": "R. J. Massey"
                    },
                    {
                        "name": "E. Medinaceli"
                    },
                    {
                        "name": "S. Mei"
                    },
                    {
                        "name": "M. Melchior"
                    },
                    {
                        "name": "Y. Mellier"
                    },
                    {
                        "name": "M. Meneghetti"
                    },
                    {
                        "name": "G. Meylan"
                    },
                    {
                        "name": "A. Mora"
                    },
                    {
                        "name": "M. Moresco"
                    },
                    {
                        "name": "L. Moscardini"
                    },
                    {
                        "name": "C. Neissner"
                    },
                    {
                        "name": "S. -M. Niemi"
                    },
                    {
                        "name": "C. Padilla"
                    },
                    {
                        "name": "S. Paltani"
                    },
                    {
                        "name": "F. Pasian"
                    },
                    {
                        "name": "K. Pedersen"
                    },
                    {
                        "name": "V. Pettorino"
                    },
                    {
                        "name": "G. Polenta"
                    },
                    {
                        "name": "M. Poncet"
                    },
                    {
                        "name": "L. A. Popa"
                    },
                    {
                        "name": "F. Raison"
                    },
                    {
                        "name": "A. Renzi"
                    },
                    {
                        "name": "J. Rhodes"
                    },
                    {
                        "name": "G. Riccio"
                    },
                    {
                        "name": "E. Romelli"
                    },
                    {
                        "name": "M. Roncarelli"
                    },
                    {
                        "name": "R. Saglia"
                    },
                    {
                        "name": "Z. Sakr"
                    },
                    {
                        "name": "A. G. SÃ¡nchez"
                    },
                    {
                        "name": "D. Sapone"
                    },
                    {
                        "name": "B. Sartoris"
                    },
                    {
                        "name": "P. Schneider"
                    },
                    {
                        "name": "T. Schrabback"
                    },
                    {
                        "name": "A. Secroun"
                    },
                    {
                        "name": "G. Seidel"
                    },
                    {
                        "name": "S. Serrano"
                    },
                    {
                        "name": "C. Sirignano"
                    },
                    {
                        "name": "G. Sirri"
                    },
                    {
                        "name": "A. Spurio Mancini"
                    },
                    {
                        "name": "L. Stanco"
                    },
                    {
                        "name": "J. Steinwagner"
                    },
                    {
                        "name": "P. Tallada-CrespÃ­"
                    },
                    {
                        "name": "A. N. Taylor"
                    },
                    {
                        "name": "N. Tessore"
                    },
                    {
                        "name": "S. Toft"
                    },
                    {
                        "name": "R. Toledo-Moreo"
                    },
                    {
                        "name": "F. Torradeflot"
                    },
                    {
                        "name": "I. Tutusaus"
                    },
                    {
                        "name": "J. Valiviita"
                    },
                    {
                        "name": "T. Vassallo"
                    },
                    {
                        "name": "Y. Wang"
                    },
                    {
                        "name": "J. Weller"
                    },
                    {
                        "name": "A. Zacchei"
                    },
                    {
                        "name": "G. Zamorani"
                    },
                    {
                        "name": "F. M. Zerbi"
                    },
                    {
                        "name": "E. Zucca"
                    },
                    {
                        "name": "M. Ballardini"
                    },
                    {
                        "name": "M. Bolzonella"
                    },
                    {
                        "name": "A. Boucaud"
                    },
                    {
                        "name": "E. Bozzo"
                    },
                    {
                        "name": "C. Burigana"
                    },
                    {
                        "name": "R. Cabanac"
                    },
                    {
                        "name": "M. Calabrese"
                    },
                    {
                        "name": "A. Cappi"
                    },
                    {
                        "name": "J. A. Escartin Vigo"
                    },
                    {
                        "name": "L. Gabarra"
                    },
                    {
                        "name": "W. G. Hartley"
                    },
                    {
                        "name": "R. Maoli"
                    },
                    {
                        "name": "J. MartÃ­n-Fleitas"
                    },
                    {
                        "name": "S. Matthew"
                    },
                    {
                        "name": "N. Mauri"
                    },
                    {
                        "name": "R. B. Metcalf"
                    },
                    {
                        "name": "A. Pezzotta"
                    },
                    {
                        "name": "M. PÃ¶ntinen"
                    },
                    {
                        "name": "I. Risso"
                    },
                    {
                        "name": "V. Scottez"
                    },
                    {
                        "name": "M. Sereno"
                    },
                    {
                        "name": "M. Tenti"
                    },
                    {
                        "name": "M. Viel"
                    },
                    {
                        "name": "M. Wiesmann"
                    },
                    {
                        "name": "Y. Akrami"
                    },
                    {
                        "name": "I. T. Andika"
                    },
                    {
                        "name": "R. E. Angulo"
                    },
                    {
                        "name": "S. Anselmi"
                    },
                    {
                        "name": "M. Archidiacono"
                    },
                    {
                        "name": "F. Atrio-Barandela"
                    },
                    {
                        "name": "E. Aubourg"
                    },
                    {
                        "name": "D. Bertacca"
                    },
                    {
                        "name": "M. Bethermin"
                    },
                    {
                        "name": "A. Blanchard"
                    },
                    {
                        "name": "L. Blot"
                    },
                    {
                        "name": "M. Bonici"
                    },
                    {
                        "name": "S. Borgani"
                    },
                    {
                        "name": "M. L. Brown"
                    },
                    {
                        "name": "S. Bruton"
                    },
                    {
                        "name": "A. Calabro"
                    },
                    {
                        "name": "B. Camacho Quevedo"
                    },
                    {
                        "name": "F. Caro"
                    },
                    {
                        "name": "C. S. Carvalho"
                    },
                    {
                        "name": "T. Castro"
                    },
                    {
                        "name": "F. Cogato"
                    },
                    {
                        "name": "S. Conseil"
                    },
                    {
                        "name": "A. R. Cooray"
                    },
                    {
                        "name": "G. Desprez"
                    },
                    {
                        "name": "A. DÃ­az-SÃ¡nchez"
                    },
                    {
                        "name": "J. J. Diaz"
                    },
                    {
                        "name": "S. Di Domizio"
                    },
                    {
                        "name": "J. M. Diego"
                    },
                    {
                        "name": "M. Y. Elkhashab"
                    },
                    {
                        "name": "Y. Fang"
                    },
                    {
                        "name": "P. G. Ferreira"
                    },
                    {
                        "name": "A. Finoguenov"
                    },
                    {
                        "name": "A. Franco"
                    },
                    {
                        "name": "K. Ganga"
                    },
                    {
                        "name": "J. GarcÃ­a-Bellido"
                    },
                    {
                        "name": "T. Gasparetto"
                    },
                    {
                        "name": "V. Gautard"
                    },
                    {
                        "name": "R. Gavazzi"
                    },
                    {
                        "name": "E. Gaztanaga"
                    },
                    {
                        "name": "F. Giacomini"
                    },
                    {
                        "name": "F. Gianotti"
                    },
                    {
                        "name": "G. Gozaliasl"
                    },
                    {
                        "name": "M. Guidi"
                    },
                    {
                        "name": "C. M. Gutierrez"
                    },
                    {
                        "name": "A. Hall"
                    },
                    {
                        "name": "S. Hemmati"
                    },
                    {
                        "name": "H. Hildebrandt"
                    },
                    {
                        "name": "J. Hjorth"
                    },
                    {
                        "name": "J. J. E. Kajava"
                    },
                    {
                        "name": "Y. Kang"
                    },
                    {
                        "name": "D. Karagiannis"
                    },
                    {
                        "name": "K. Kiiveri"
                    },
                    {
                        "name": "J. Kim"
                    },
                    {
                        "name": "C. C. Kirkpatrick"
                    },
                    {
                        "name": "S. Kruk"
                    },
                    {
                        "name": "L. Legrand"
                    },
                    {
                        "name": "M. Lembo"
                    },
                    {
                        "name": "F. Lepori"
                    },
                    {
                        "name": "G. Leroy"
                    },
                    {
                        "name": "G. F. Lesci"
                    },
                    {
                        "name": "J. Lesgourgues"
                    },
                    {
                        "name": "T. I. Liaudat"
                    },
                    {
                        "name": "J. Macias-Perez"
                    },
                    {
                        "name": "M. Magliocchetti"
                    },
                    {
                        "name": "F. Mannucci"
                    },
                    {
                        "name": "C. J. A. P. Martins"
                    },
                    {
                        "name": "L. Maurin"
                    },
                    {
                        "name": "M. Miluzio"
                    },
                    {
                        "name": "P. Monaco"
                    },
                    {
                        "name": "C. Moretti"
                    },
                    {
                        "name": "G. Morgante"
                    },
                    {
                        "name": "S. Nadathur"
                    },
                    {
                        "name": "K. Naidoo"
                    },
                    {
                        "name": "A. Navarro-Alsina"
                    },
                    {
                        "name": "S. Nesseris"
                    },
                    {
                        "name": "D. Paoletti"
                    },
                    {
                        "name": "F. Passalacqua"
                    },
                    {
                        "name": "K. Paterson"
                    },
                    {
                        "name": "L. Patrizii"
                    },
                    {
                        "name": "A. Pisani"
                    },
                    {
                        "name": "D. Potter"
                    },
                    {
                        "name": "S. Quai"
                    },
                    {
                        "name": "M. Radovich"
                    },
                    {
                        "name": "S. Sacquegna"
                    },
                    {
                        "name": "M. SahlÃ©n"
                    },
                    {
                        "name": "D. B. Sanders"
                    },
                    {
                        "name": "E. Sarpa"
                    },
                    {
                        "name": "A. Schneider"
                    },
                    {
                        "name": "D. Sciotti"
                    },
                    {
                        "name": "L. C. Smith"
                    },
                    {
                        "name": "K. Tanidis"
                    },
                    {
                        "name": "C. Tao"
                    },
                    {
                        "name": "G. Testera"
                    },
                    {
                        "name": "R. Teyssier"
                    },
                    {
                        "name": "S. Tosi"
                    },
                    {
                        "name": "A. Troja"
                    },
                    {
                        "name": "M. Tucci"
                    },
                    {
                        "name": "D. Vergani"
                    },
                    {
                        "name": "G. Verza"
                    },
                    {
                        "name": "N. A. Walton"
                    }
                ],
                "author_detail": {
                    "name": "N. A. Walton"
                },
                "author": "N. A. Walton",
                "arxiv_affiliation": "Institute of Astronomy, University of Cambridge, Madingley Road, Cambridge CB3 0HA, UK",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04950v1",
                "updated": "2025-10-06T15:50:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    50,
                    39,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T15:50:39Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    50,
                    39,
                    0,
                    279,
                    0
                ],
                "title": "Mind Your Tone: Investigating How Prompt Politeness Affects LLM Accuracy\n  (short paper)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind Your Tone: Investigating How Prompt Politeness Affects LLM Accuracy\n  (short paper)"
                },
                "summary": "The wording of natural language prompts has been shown to influence the\nperformance of large language models (LLMs), yet the role of politeness and\ntone remains underexplored. In this study, we investigate how varying levels of\nprompt politeness affect model accuracy on multiple-choice questions. We\ncreated a dataset of 50 base questions spanning mathematics, science, and\nhistory, each rewritten into five tone variants: Very Polite, Polite, Neutral,\nRude, and Very Rude, yielding 250 unique prompts. Using ChatGPT 4o, we\nevaluated responses across these conditions and applied paired sample t-tests\nto assess statistical significance. Contrary to expectations, impolite prompts\nconsistently outperformed polite ones, with accuracy ranging from 80.8% for\nVery Polite prompts to 84.8% for Very Rude prompts. These findings differ from\nearlier studies that associated rudeness with poorer outcomes, suggesting that\nnewer LLMs may respond differently to tonal variation. Our results highlight\nthe importance of studying pragmatic aspects of prompting and raise broader\nquestions about the social dimensions of human-AI interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wording of natural language prompts has been shown to influence the\nperformance of large language models (LLMs), yet the role of politeness and\ntone remains underexplored. In this study, we investigate how varying levels of\nprompt politeness affect model accuracy on multiple-choice questions. We\ncreated a dataset of 50 base questions spanning mathematics, science, and\nhistory, each rewritten into five tone variants: Very Polite, Polite, Neutral,\nRude, and Very Rude, yielding 250 unique prompts. Using ChatGPT 4o, we\nevaluated responses across these conditions and applied paired sample t-tests\nto assess statistical significance. Contrary to expectations, impolite prompts\nconsistently outperformed polite ones, with accuracy ranging from 80.8% for\nVery Polite prompts to 84.8% for Very Rude prompts. These findings differ from\nearlier studies that associated rudeness with poorer outcomes, suggesting that\nnewer LLMs may respond differently to tonal variation. Our results highlight\nthe importance of studying pragmatic aspects of prompting and raise broader\nquestions about the social dimensions of human-AI interaction."
                },
                "authors": [
                    {
                        "name": "Om Dobariya"
                    },
                    {
                        "name": "Akhil Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Akhil Kumar"
                },
                "author": "Akhil Kumar",
                "arxiv_comment": "5 pages, 3 tables; includes Limitations and Ethical Considerations\n  sections; short paper under submission to Findings of ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04945v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04945v1",
                "updated": "2025-10-06T15:46:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    46,
                    54,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T15:46:54Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    46,
                    54,
                    0,
                    279,
                    0
                ],
                "title": "A First Context-Free Grammar Applied to Nawatl Corpora Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A First Context-Free Grammar Applied to Nawatl Corpora Augmentation"
                },
                "summary": "In this article we introduce a context-free grammar (CFG) for the Nawatl\nlanguage. Nawatl (or Nahuatl) is an Amerindian language of the $\\pi$-language\ntype, i.e. a language with few digital resources, in which the corpora\navailable for machine learning are virtually non-existent. The objective here\nis to generate a significant number of grammatically correct artificial\nsentences, in order to increase the corpora available for language model\ntraining. We want to show that a grammar enables us significantly to expand a\ncorpus in Nawatl which we call $\\pi$-\\textsc{yalli}. The corpus, thus enriched,\nenables us to train algorithms such as FastText and to evaluate them on\nsentence-level semantic tasks. Preliminary results show that by using the\ngrammar, comparative improvements are achieved over some LLMs. However, it is\nobserved that to achieve more significant improvement, grammars that model the\nNawatl language even more effectively are required.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this article we introduce a context-free grammar (CFG) for the Nawatl\nlanguage. Nawatl (or Nahuatl) is an Amerindian language of the $\\pi$-language\ntype, i.e. a language with few digital resources, in which the corpora\navailable for machine learning are virtually non-existent. The objective here\nis to generate a significant number of grammatically correct artificial\nsentences, in order to increase the corpora available for language model\ntraining. We want to show that a grammar enables us significantly to expand a\ncorpus in Nawatl which we call $\\pi$-\\textsc{yalli}. The corpus, thus enriched,\nenables us to train algorithms such as FastText and to evaluate them on\nsentence-level semantic tasks. Preliminary results show that by using the\ngrammar, comparative improvements are achieved over some LLMs. However, it is\nobserved that to achieve more significant improvement, grammars that model the\nNawatl language even more effectively are required."
                },
                "authors": [
                    {
                        "name": "Juan-JosÃ© GuzmÃ¡n-Landa"
                    },
                    {
                        "name": "Juan-Manuel Torres-Moreno"
                    },
                    {
                        "name": "Miguel Figueroa-Saavedra"
                    },
                    {
                        "name": "Ligia Quintana-Torres"
                    },
                    {
                        "name": "Martha-Lorena AvendaÃ±o-Garrido"
                    },
                    {
                        "name": "Graham Ranger"
                    }
                ],
                "author_detail": {
                    "name": "Graham Ranger"
                },
                "author": "Graham Ranger",
                "arxiv_comment": "11 pages, 7 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04945v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04945v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08942v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08942v2",
                "updated": "2025-10-06T15:46:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    46,
                    30,
                    0,
                    279,
                    0
                ],
                "published": "2025-04-11T19:49:22Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    19,
                    49,
                    22,
                    4,
                    101,
                    0
                ],
                "title": "AgentRewardBench: Evaluating Automatic Evaluations of Web Agent\n  Trajectories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentRewardBench: Evaluating Automatic Evaluations of Web Agent\n  Trajectories"
                },
                "summary": "Web agents enable users to perform tasks on web browsers through natural\nlanguage interaction. Evaluating web agents trajectories is an important\nproblem, since it helps us determine whether the agent successfully completed\nthe tasks. Rule-based methods are widely used for this purpose, but they are\nchallenging to extend to new tasks and may not always recognize successful\ntrajectories. We may achieve higher accuracy through human evaluation, but the\nprocess would be substantially slower and more expensive. Automatic evaluations\nwith LLMs may avoid the challenges of designing new rules and manually\nannotating trajectories, enabling faster and cost-effective evaluation.\nHowever, it is unclear how effective they are at evaluating web agents. To this\nend, we propose AgentRewardBench, the first benchmark to assess the\neffectiveness of LLM judges for evaluating web agents. AgentRewardBench\ncontains 1302 trajectories across 5 benchmarks and 4 LLMs. Each trajectory in\nAgentRewardBench is reviewed by an expert, who answers questions pertaining to\nthe success, side effects, and repetitiveness of the agent. Using our\nbenchmark, we evaluate 12 LLM judges and find that no single LLM excels across\nall benchmarks. We also find that the rule-based evaluation used by common\nbenchmarks tends to underreport the success rate of web agents, highlighting a\nkey weakness of rule-based evaluation and the need to develop more flexible\nautomatic evaluations. We release the benchmark at:\nhttps://agent-reward-bench.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web agents enable users to perform tasks on web browsers through natural\nlanguage interaction. Evaluating web agents trajectories is an important\nproblem, since it helps us determine whether the agent successfully completed\nthe tasks. Rule-based methods are widely used for this purpose, but they are\nchallenging to extend to new tasks and may not always recognize successful\ntrajectories. We may achieve higher accuracy through human evaluation, but the\nprocess would be substantially slower and more expensive. Automatic evaluations\nwith LLMs may avoid the challenges of designing new rules and manually\nannotating trajectories, enabling faster and cost-effective evaluation.\nHowever, it is unclear how effective they are at evaluating web agents. To this\nend, we propose AgentRewardBench, the first benchmark to assess the\neffectiveness of LLM judges for evaluating web agents. AgentRewardBench\ncontains 1302 trajectories across 5 benchmarks and 4 LLMs. Each trajectory in\nAgentRewardBench is reviewed by an expert, who answers questions pertaining to\nthe success, side effects, and repetitiveness of the agent. Using our\nbenchmark, we evaluate 12 LLM judges and find that no single LLM excels across\nall benchmarks. We also find that the rule-based evaluation used by common\nbenchmarks tends to underreport the success rate of web agents, highlighting a\nkey weakness of rule-based evaluation and the need to develop more flexible\nautomatic evaluations. We release the benchmark at:\nhttps://agent-reward-bench.github.io"
                },
                "authors": [
                    {
                        "name": "Xing Han LÃ¹"
                    },
                    {
                        "name": "Amirhossein Kazemnejad"
                    },
                    {
                        "name": "Nicholas Meade"
                    },
                    {
                        "name": "Arkil Patel"
                    },
                    {
                        "name": "Dongchan Shin"
                    },
                    {
                        "name": "Alejandra Zambrano"
                    },
                    {
                        "name": "Karolina StaÅczak"
                    },
                    {
                        "name": "Peter Shaw"
                    },
                    {
                        "name": "Christopher J. Pal"
                    },
                    {
                        "name": "Siva Reddy"
                    }
                ],
                "author_detail": {
                    "name": "Siva Reddy"
                },
                "author": "Siva Reddy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08942v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08942v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04935v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04935v1",
                "updated": "2025-10-06T15:42:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    42,
                    55,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T15:42:55Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    42,
                    55,
                    0,
                    279,
                    0
                ],
                "title": "MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement\n  Learning"
                },
                "summary": "Large Reasoning Models (LRMs) often exhibit a tendency for overanalysis in\nsimple tasks, where the models excessively utilize System 2-type, deliberate\nreasoning, leading to inefficient token generation. Furthermore, these models\nface challenges in adapting their reasoning capabilities to rapidly changing\nenvironments due to the static nature of their pretraining data. To address\nthese issues, advancing Large Language Models (LLMs) for complex reasoning\ntasks requires innovative approaches that bridge intuitive and deliberate\ncognitive processes, akin to human cognition's dual-system dynamic. This paper\nintroduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamless\nintegration of System 1's fast, intuitive thinking with System 2's deliberate\nreasoning within LLMs. MARS strategically integrates multiple external tools,\nsuch as Google Search, Google Scholar, and Python Interpreter, to access\nup-to-date information and execute complex computations, while creating a\nspecialized division of labor where System 1 efficiently processes and\nsummarizes high-volume external information, providing distilled insights that\nexpand System 2's reasoning context without overwhelming its capacity.\nFurthermore, we propose a multi-agent reinforcement learning framework\nextending Group Relative Policy Optimization to simultaneously optimize both\nsystems with multi-turn tool interactions, bin-packing optimization, and sample\nbalancing strategies that enhance collaborative efficiency. Extensive\nexperiments demonstrate MARS achieves substantial improvements of 3.86% on the\nchallenging Humanity's Last Exam (HLE) benchmark and an average gain of 8.9%\nacross 7 knowledge-intensive tasks, validating the effectiveness of our\ndual-system paradigm for complex reasoning in dynamic information environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) often exhibit a tendency for overanalysis in\nsimple tasks, where the models excessively utilize System 2-type, deliberate\nreasoning, leading to inefficient token generation. Furthermore, these models\nface challenges in adapting their reasoning capabilities to rapidly changing\nenvironments due to the static nature of their pretraining data. To address\nthese issues, advancing Large Language Models (LLMs) for complex reasoning\ntasks requires innovative approaches that bridge intuitive and deliberate\ncognitive processes, akin to human cognition's dual-system dynamic. This paper\nintroduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamless\nintegration of System 1's fast, intuitive thinking with System 2's deliberate\nreasoning within LLMs. MARS strategically integrates multiple external tools,\nsuch as Google Search, Google Scholar, and Python Interpreter, to access\nup-to-date information and execute complex computations, while creating a\nspecialized division of labor where System 1 efficiently processes and\nsummarizes high-volume external information, providing distilled insights that\nexpand System 2's reasoning context without overwhelming its capacity.\nFurthermore, we propose a multi-agent reinforcement learning framework\nextending Group Relative Policy Optimization to simultaneously optimize both\nsystems with multi-turn tool interactions, bin-packing optimization, and sample\nbalancing strategies that enhance collaborative efficiency. Extensive\nexperiments demonstrate MARS achieves substantial improvements of 3.86% on the\nchallenging Humanity's Last Exam (HLE) benchmark and an average gain of 8.9%\nacross 7 knowledge-intensive tasks, validating the effectiveness of our\ndual-system paradigm for complex reasoning in dynamic information environments."
                },
                "authors": [
                    {
                        "name": "Guoxin Chen"
                    },
                    {
                        "name": "Zile Qiao"
                    },
                    {
                        "name": "Wenqing Wang"
                    },
                    {
                        "name": "Donglei Yu"
                    },
                    {
                        "name": "Xuanzhong Chen"
                    },
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Minpeng Liao"
                    },
                    {
                        "name": "Kai Fan"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Penguin Xie"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Ruihua Song"
                    },
                    {
                        "name": "Fei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Fei Huang"
                },
                "author": "Fei Huang",
                "arxiv_comment": "Ongoing Work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04935v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04935v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04933v1",
                "updated": "2025-10-06T15:41:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    41,
                    22,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T15:41:22Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    41,
                    22,
                    0,
                    279,
                    0
                ],
                "title": "The Geometry of Truth: Layer-wise Semantic Dynamics for Hallucination\n  Detection in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Geometry of Truth: Layer-wise Semantic Dynamics for Hallucination\n  Detection in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) often produce fluent yet factually incorrect\nstatements-a phenomenon known as hallucination-posing serious risks in\nhigh-stakes domains. We present Layer-wise Semantic Dynamics (LSD), a geometric\nframework for hallucination detection that analyzes the evolution of\nhidden-state semantics across transformer layers. Unlike prior methods that\nrely on multiple sampling passes or external verification sources, LSD operates\nintrinsically within the model's representational space. Using margin-based\ncontrastive learning, LSD aligns hidden activations with ground-truth\nembeddings derived from a factual encoder, revealing a distinct separation in\nsemantic trajectories: factual responses preserve stable alignment, while\nhallucinations exhibit pronounced semantic drift across depth. Evaluated on the\nTruthfulQA and synthetic factual-hallucination datasets, LSD achieves an\nF1-score of 0.92, AUROC of 0.96, and clustering accuracy of 0.89, outperforming\nSelfCheckGPT and Semantic Entropy baselines while requiring only a single\nforward pass. This efficiency yields a 5-20x speedup over sampling-based\nmethods without sacrificing precision or interpretability. LSD offers a\nscalable, model-agnostic mechanism for real-time hallucination monitoring and\nprovides new insights into the geometry of factual consistency within large\nlanguage models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often produce fluent yet factually incorrect\nstatements-a phenomenon known as hallucination-posing serious risks in\nhigh-stakes domains. We present Layer-wise Semantic Dynamics (LSD), a geometric\nframework for hallucination detection that analyzes the evolution of\nhidden-state semantics across transformer layers. Unlike prior methods that\nrely on multiple sampling passes or external verification sources, LSD operates\nintrinsically within the model's representational space. Using margin-based\ncontrastive learning, LSD aligns hidden activations with ground-truth\nembeddings derived from a factual encoder, revealing a distinct separation in\nsemantic trajectories: factual responses preserve stable alignment, while\nhallucinations exhibit pronounced semantic drift across depth. Evaluated on the\nTruthfulQA and synthetic factual-hallucination datasets, LSD achieves an\nF1-score of 0.92, AUROC of 0.96, and clustering accuracy of 0.89, outperforming\nSelfCheckGPT and Semantic Entropy baselines while requiring only a single\nforward pass. This efficiency yields a 5-20x speedup over sampling-based\nmethods without sacrificing precision or interpretability. LSD offers a\nscalable, model-agnostic mechanism for real-time hallucination monitoring and\nprovides new insights into the geometry of factual consistency within large\nlanguage models."
                },
                "authors": [
                    {
                        "name": "Amir Hameed Mir"
                    }
                ],
                "author_detail": {
                    "name": "Amir Hameed Mir"
                },
                "author": "Amir Hameed Mir",
                "arxiv_comment": "Comments: 14 pages, 14 figures, 5 tables. Code available at:\n  https://github.com/sirraya-tech/Sirraya_LSD_Code",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68T07, 62H30",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6; F.2.2; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02567v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02567v2",
                "updated": "2025-10-06T15:33:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    33,
                    47,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-02T21:06:04Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    21,
                    6,
                    4,
                    3,
                    275,
                    0
                ],
                "title": "Agentic Additive Manufacturing Alloy Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Additive Manufacturing Alloy Discovery"
                },
                "summary": "Agentic systems enable the intelligent use of research tooling, augmenting a\nresearcher's ability to investigate and propose novel solutions to existing\nproblems. Within Additive Manufacturing (AM), alloy discovery remains a complex\nchallenge, often requiring expertise in the various domains of materials\nscience, thermodynamic simulations, and experimental analysis. Large Language\nModel (LLM) enabled agents can facilitate this endeavor by utilizing their\nextensive knowledge base to dispatch tool calls via Model Context Protocol\n(MCP) to perform actions such as Thermo-Calc property diagram calculations and\nlack of fusion process map generation. In addition, the multi-agent system\ndeveloped in this work is able to effectively reason through complex user\nprompts and provide analysis on the printability of proposed alloys. These\nagents can dynamically adjust their task trajectory to the outcomes of tool\ncall results, effectively enabling autonomous decision-making in practical\nenvironments. This work aims to utilize LLM enabled agents to automate and\naccelerate the task of alloy discovery within the field of additive\nmanufacturing and showcase the benefits of adopting this multi-agent system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic systems enable the intelligent use of research tooling, augmenting a\nresearcher's ability to investigate and propose novel solutions to existing\nproblems. Within Additive Manufacturing (AM), alloy discovery remains a complex\nchallenge, often requiring expertise in the various domains of materials\nscience, thermodynamic simulations, and experimental analysis. Large Language\nModel (LLM) enabled agents can facilitate this endeavor by utilizing their\nextensive knowledge base to dispatch tool calls via Model Context Protocol\n(MCP) to perform actions such as Thermo-Calc property diagram calculations and\nlack of fusion process map generation. In addition, the multi-agent system\ndeveloped in this work is able to effectively reason through complex user\nprompts and provide analysis on the printability of proposed alloys. These\nagents can dynamically adjust their task trajectory to the outcomes of tool\ncall results, effectively enabling autonomous decision-making in practical\nenvironments. This work aims to utilize LLM enabled agents to automate and\naccelerate the task of alloy discovery within the field of additive\nmanufacturing and showcase the benefits of adopting this multi-agent system."
                },
                "authors": [
                    {
                        "name": "Peter Pak"
                    },
                    {
                        "name": "Achuth Chandrasekhar"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Barati Farimani"
                },
                "author": "Amir Barati Farimani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02567v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02567v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04919v1",
                "updated": "2025-10-06T15:33:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    33,
                    35,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T15:33:35Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    33,
                    35,
                    0,
                    279,
                    0
                ],
                "title": "Do LLMs Align with My Task? Evaluating Text-to-SQL via Dataset Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Align with My Task? Evaluating Text-to-SQL via Dataset Alignment"
                },
                "summary": "Supervised Fine-Tuning (SFT) is an effective method for adapting Large\nLanguage Models (LLMs) on downstream tasks. However, variability in training\ndata can hinder a model's ability to generalize across domains. This paper\nstudies the problem of dataset alignment for Natural Language to SQL (NL2SQL or\ntext to SQL), examining how well SFT training data matches the structural\ncharacteristics of target queries and how this alignment impacts model\nperformance. We hypothesize that alignment can be accurately estimated by\ncomparing the distributions of structural SQL features across the training set,\ntarget data, and the model's predictions prior to SFT. Through comprehensive\nexperiments on three large cross-domain NL2SQL benchmarks and multiple model\nfamilies, we show that structural alignment is a strong predictor of\nfine-tuning success. When alignment is high, SFT yields substantial gains in\naccuracy and SQL generation quality; when alignment is low, improvements are\nmarginal or absent. These findings highlight the importance of alignment-aware\ndata selection for effective fine-tuning and generalization in NL2SQL tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised Fine-Tuning (SFT) is an effective method for adapting Large\nLanguage Models (LLMs) on downstream tasks. However, variability in training\ndata can hinder a model's ability to generalize across domains. This paper\nstudies the problem of dataset alignment for Natural Language to SQL (NL2SQL or\ntext to SQL), examining how well SFT training data matches the structural\ncharacteristics of target queries and how this alignment impacts model\nperformance. We hypothesize that alignment can be accurately estimated by\ncomparing the distributions of structural SQL features across the training set,\ntarget data, and the model's predictions prior to SFT. Through comprehensive\nexperiments on three large cross-domain NL2SQL benchmarks and multiple model\nfamilies, we show that structural alignment is a strong predictor of\nfine-tuning success. When alignment is high, SFT yields substantial gains in\naccuracy and SQL generation quality; when alignment is low, improvements are\nmarginal or absent. These findings highlight the importance of alignment-aware\ndata selection for effective fine-tuning and generalization in NL2SQL tasks."
                },
                "authors": [
                    {
                        "name": "Davood Rafiei"
                    },
                    {
                        "name": "Morgan Lindsay Heisler"
                    },
                    {
                        "name": "Weiwei Zhang"
                    },
                    {
                        "name": "Mohammadreza Pourreza"
                    },
                    {
                        "name": "Yong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yong Zhang"
                },
                "author": "Yong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04912v1",
                "updated": "2025-10-06T15:26:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    26,
                    8,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T15:26:08Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    26,
                    8,
                    0,
                    279,
                    0
                ],
                "title": "Comparative Analysis of YOLOv5, Faster R-CNN, SSD, and RetinaNet for\n  Motorbike Detection in Kigali Autonomous Driving Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of YOLOv5, Faster R-CNN, SSD, and RetinaNet for\n  Motorbike Detection in Kigali Autonomous Driving Context"
                },
                "summary": "In Kigali, Rwanda, motorcycle taxis are a primary mode of transportation,\noften navigating unpredictably and disregarding traffic rules, posing\nsignificant challenges for autonomous driving systems. This study compares four\nobject detection models--YOLOv5, Faster R-CNN, SSD, and RetinaNet--for\nmotorbike detection using a custom dataset of 198 images collected in Kigali.\nImplemented in PyTorch with transfer learning, the models were evaluated for\naccuracy, localization, and inference speed to assess their suitability for\nreal-time navigation in resource-constrained settings. We identify\nimplementation challenges, including dataset limitations and model\ncomplexities, and recommend simplified architectures for future work to enhance\naccessibility for autonomous systems in developing countries like Rwanda.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Kigali, Rwanda, motorcycle taxis are a primary mode of transportation,\noften navigating unpredictably and disregarding traffic rules, posing\nsignificant challenges for autonomous driving systems. This study compares four\nobject detection models--YOLOv5, Faster R-CNN, SSD, and RetinaNet--for\nmotorbike detection using a custom dataset of 198 images collected in Kigali.\nImplemented in PyTorch with transfer learning, the models were evaluated for\naccuracy, localization, and inference speed to assess their suitability for\nreal-time navigation in resource-constrained settings. We identify\nimplementation challenges, including dataset limitations and model\ncomplexities, and recommend simplified architectures for future work to enhance\naccessibility for autonomous systems in developing countries like Rwanda."
                },
                "authors": [
                    {
                        "name": "Ngeyen Yinkfu"
                    },
                    {
                        "name": "Sunday Nwovu"
                    },
                    {
                        "name": "Jonathan Kayizzi"
                    },
                    {
                        "name": "Angelique Uwamahoro"
                    }
                ],
                "author_detail": {
                    "name": "Angelique Uwamahoro"
                },
                "author": "Angelique Uwamahoro",
                "arxiv_comment": "3 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04910v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04910v1",
                "updated": "2025-10-06T15:24:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    24,
                    44,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T15:24:44Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    24,
                    44,
                    0,
                    279,
                    0
                ],
                "title": "Glocal Information Bottleneck for Time Series Imputation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Glocal Information Bottleneck for Time Series Imputation"
                },
                "summary": "Time Series Imputation (TSI), which aims to recover missing values in\ntemporal data, remains a fundamental challenge due to the complex and often\nhigh-rate missingness in real-world scenarios. Existing models typically\noptimize the point-wise reconstruction loss, focusing on recovering numerical\nvalues (local information). However, we observe that under high missing rates,\nthese models still perform well in the training phase yet produce poor\nimputations and distorted latent representation distributions (global\ninformation) in the inference phase. This reveals a critical optimization\ndilemma: current objectives lack global guidance, leading models to overfit\nlocal noise and fail to capture global information of the data. To address this\nissue, we propose a new training paradigm, Glocal Information Bottleneck\n(Glocal-IB). Glocal-IB is model-agnostic and extends the standard IB framework\nby introducing a Global Alignment loss, derived from a tractable mutual\ninformation approximation. This loss aligns the latent representations of\nmasked inputs with those of their originally observed counterparts. It helps\nthe model retain global structure and local details while suppressing noise\ncaused by missing values, giving rise to better generalization under high\nmissingness. Extensive experiments on nine datasets confirm that Glocal-IB\nleads to consistently improved performance and aligned latent representations\nunder missingness. Our code implementation is available in\nhttps://github.com/Muyiiiii/NeurIPS-25-Glocal-IB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time Series Imputation (TSI), which aims to recover missing values in\ntemporal data, remains a fundamental challenge due to the complex and often\nhigh-rate missingness in real-world scenarios. Existing models typically\noptimize the point-wise reconstruction loss, focusing on recovering numerical\nvalues (local information). However, we observe that under high missing rates,\nthese models still perform well in the training phase yet produce poor\nimputations and distorted latent representation distributions (global\ninformation) in the inference phase. This reveals a critical optimization\ndilemma: current objectives lack global guidance, leading models to overfit\nlocal noise and fail to capture global information of the data. To address this\nissue, we propose a new training paradigm, Glocal Information Bottleneck\n(Glocal-IB). Glocal-IB is model-agnostic and extends the standard IB framework\nby introducing a Global Alignment loss, derived from a tractable mutual\ninformation approximation. This loss aligns the latent representations of\nmasked inputs with those of their originally observed counterparts. It helps\nthe model retain global structure and local details while suppressing noise\ncaused by missing values, giving rise to better generalization under high\nmissingness. Extensive experiments on nine datasets confirm that Glocal-IB\nleads to consistently improved performance and aligned latent representations\nunder missingness. Our code implementation is available in\nhttps://github.com/Muyiiiii/NeurIPS-25-Glocal-IB."
                },
                "authors": [
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Kexin Zhang"
                    },
                    {
                        "name": "Guibin Zhang"
                    },
                    {
                        "name": "Philip S. Yu"
                    },
                    {
                        "name": "Kaize Ding"
                    }
                ],
                "author_detail": {
                    "name": "Kaize Ding"
                },
                "author": "Kaize Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04910v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04910v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04905v1",
                "updated": "2025-10-06T15:20:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    20,
                    3,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T15:20:03Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    20,
                    3,
                    0,
                    279,
                    0
                ],
                "title": "Retrieval-Augmented Code Generation: A Survey with Focus on\n  Repository-Level Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Code Generation: A Survey with Focus on\n  Repository-Level Approaches"
                },
                "summary": "Recent advancements in large language models (LLMs) have substantially\nimproved automated code generation. While function-level and file-level\ngeneration have achieved promising results, real-world software development\ntypically requires reasoning across entire repositories. This gives rise to the\nchallenging task of Repository-Level Code Generation (RLCG), where models must\ncapture long-range dependencies, ensure global semantic consistency, and\ngenerate coherent code spanning multiple files or modules. To address these\nchallenges, Retrieval-Augmented Generation (RAG) has emerged as a powerful\nparadigm that integrates external retrieval mechanisms with LLMs, enhancing\ncontext-awareness and scalability. In this survey, we provide a comprehensive\nreview of research on Retrieval-Augmented Code Generation (RACG), with an\nemphasis on repository-level approaches. We categorize existing work along\nseveral dimensions, including generation strategies, retrieval modalities,\nmodel architectures, training paradigms, and evaluation protocols. Furthermore,\nwe summarize widely used datasets and benchmarks, analyze current limitations,\nand outline key challenges and opportunities for future research. Our goal is\nto establish a unified analytical framework for understanding this rapidly\nevolving field and to inspire continued progress in AI-powered software\nengineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have substantially\nimproved automated code generation. While function-level and file-level\ngeneration have achieved promising results, real-world software development\ntypically requires reasoning across entire repositories. This gives rise to the\nchallenging task of Repository-Level Code Generation (RLCG), where models must\ncapture long-range dependencies, ensure global semantic consistency, and\ngenerate coherent code spanning multiple files or modules. To address these\nchallenges, Retrieval-Augmented Generation (RAG) has emerged as a powerful\nparadigm that integrates external retrieval mechanisms with LLMs, enhancing\ncontext-awareness and scalability. In this survey, we provide a comprehensive\nreview of research on Retrieval-Augmented Code Generation (RACG), with an\nemphasis on repository-level approaches. We categorize existing work along\nseveral dimensions, including generation strategies, retrieval modalities,\nmodel architectures, training paradigms, and evaluation protocols. Furthermore,\nwe summarize widely used datasets and benchmarks, analyze current limitations,\nand outline key challenges and opportunities for future research. Our goal is\nto establish a unified analytical framework for understanding this rapidly\nevolving field and to inspire continued progress in AI-powered software\nengineering."
                },
                "authors": [
                    {
                        "name": "Yicheng Tao"
                    },
                    {
                        "name": "Yao Qin"
                    },
                    {
                        "name": "Yepang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yepang Liu"
                },
                "author": "Yepang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17792v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17792v2",
                "updated": "2025-10-06T15:19:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    19,
                    49,
                    0,
                    279,
                    0
                ],
                "published": "2024-11-26T17:42:38Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    42,
                    38,
                    1,
                    331,
                    0
                ],
                "title": "H3Fusion: Helpful, Harmless, Honest Fusion of Aligned LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "H3Fusion: Helpful, Harmless, Honest Fusion of Aligned LLMs"
                },
                "summary": "Alignment of pretrained LLMs using instruction-based datasets is critical for\ncreating fine-tuned models that reflect human preference. A growing number of\nalignment-based fine-tuning algorithms and benchmarks emerged recently, fueling\nthe efforts on effective alignments of pre-trained LLMs to ensure helpful,\nharmless, and honest answers from both open-source and closed-source LLMs. This\npaper tackles this problem by developing an alignment fusion approach, coined\nas $H^3$Fusion, with three unique characteristics. First, $H^3$Fusion ensembles\nmultiple individually aligned LLMs to create a final fine-tuned alignment model\nwith enhanced capabilities beyond those of individual models, delivering robust\nalignment through promoting helpful, harmless, honest fusion. Second,\n$H^3$Fusion leverages the mixture-of-experts (MoE) methodology in two steps. We\nfirst freeze the multi-head attention weights of each individual model while\ntuning the FFN layer during alignment fusion. Then we merge the aligned model\nweights with an expert router according to the type of input instruction and\ndynamically select a subset of experts that are best suited for producing the\noutput response. Finally, we boost the performance of the resulting\n$H^3$3Fusion model by introducing gating loss and regularization terms. The\nformer penalizes the selection errors of the expert-router, and the latter\nmediates the expert weights drifting during fine-tuning and dynamically adjusts\nthe fusion behavior of the resulting model by canalizing the activations on the\nexperts. Extensive evaluations on three benchmark datasets show that\n$H^3$3Fusion is more helpful, less harmful, and more honest from two aspects:\nit outperforms each individually aligned model by $11.37\\%$, and it provides\nstronger robustness compared to the state-of-the-art LLM ensemble approaches by\n$13.77\\%$. Code is available at github.com/sftekin/h3fusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment of pretrained LLMs using instruction-based datasets is critical for\ncreating fine-tuned models that reflect human preference. A growing number of\nalignment-based fine-tuning algorithms and benchmarks emerged recently, fueling\nthe efforts on effective alignments of pre-trained LLMs to ensure helpful,\nharmless, and honest answers from both open-source and closed-source LLMs. This\npaper tackles this problem by developing an alignment fusion approach, coined\nas $H^3$Fusion, with three unique characteristics. First, $H^3$Fusion ensembles\nmultiple individually aligned LLMs to create a final fine-tuned alignment model\nwith enhanced capabilities beyond those of individual models, delivering robust\nalignment through promoting helpful, harmless, honest fusion. Second,\n$H^3$Fusion leverages the mixture-of-experts (MoE) methodology in two steps. We\nfirst freeze the multi-head attention weights of each individual model while\ntuning the FFN layer during alignment fusion. Then we merge the aligned model\nweights with an expert router according to the type of input instruction and\ndynamically select a subset of experts that are best suited for producing the\noutput response. Finally, we boost the performance of the resulting\n$H^3$3Fusion model by introducing gating loss and regularization terms. The\nformer penalizes the selection errors of the expert-router, and the latter\nmediates the expert weights drifting during fine-tuning and dynamically adjusts\nthe fusion behavior of the resulting model by canalizing the activations on the\nexperts. Extensive evaluations on three benchmark datasets show that\n$H^3$3Fusion is more helpful, less harmful, and more honest from two aspects:\nit outperforms each individually aligned model by $11.37\\%$, and it provides\nstronger robustness compared to the state-of-the-art LLM ensemble approaches by\n$13.77\\%$. Code is available at github.com/sftekin/h3fusion."
                },
                "authors": [
                    {
                        "name": "Selim Furkan Tekin"
                    },
                    {
                        "name": "Fatih Ilhan"
                    },
                    {
                        "name": "Tiansheng Huang"
                    },
                    {
                        "name": "Sihao Hu"
                    },
                    {
                        "name": "Yichang Xu"
                    },
                    {
                        "name": "Zachary Yahn"
                    },
                    {
                        "name": "Ling Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ling Liu"
                },
                "author": "Ling Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17792v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17792v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04899v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04899v1",
                "updated": "2025-10-06T15:16:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    16,
                    45,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T15:16:45Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    16,
                    45,
                    0,
                    279,
                    0
                ],
                "title": "Human Behavior Atlas: Benchmarking Unified Psychological and Social\n  Behavior Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human Behavior Atlas: Benchmarking Unified Psychological and Social\n  Behavior Understanding"
                },
                "summary": "Using intelligent systems to perceive psychological and social behaviors,\nthat is, the underlying affective, cognitive, and pathological states that are\nmanifested through observable behaviors and social interactions, remains a\nchallenge due to their complex, multifaceted, and personalized nature. Existing\nwork tackling these dimensions through specialized datasets and single-task\nsystems often miss opportunities for scalability, cross-task transfer, and\nbroader generalization. To address this gap, we curate Human Behavior Atlas, a\nunified benchmark of diverse behavioral tasks designed to support the\ndevelopment of unified models for understanding psychological and social\nbehaviors. Human Behavior Atlas comprises over 100,000 samples spanning text,\naudio, and visual modalities, covering tasks on affective states, cognitive\nstates, pathologies, and social processes. Our unification efforts can reduce\nredundancy and cost, enable training to scale efficiently across tasks, and\nenhance generalization of behavioral features across domains. On Human Behavior\nAtlas, we train three models: OmniSapiens-7B SFT, OmniSapiens-7B BAM, and\nOmniSapiens-7B RL. We show that training on Human Behavior Atlas enables models\nto consistently outperform existing multimodal LLMs across diverse behavioral\ntasks. Pretraining on Human Behavior Atlas also improves transfer to novel\nbehavioral datasets; with the targeted use of behavioral descriptors yielding\nmeaningful performance gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using intelligent systems to perceive psychological and social behaviors,\nthat is, the underlying affective, cognitive, and pathological states that are\nmanifested through observable behaviors and social interactions, remains a\nchallenge due to their complex, multifaceted, and personalized nature. Existing\nwork tackling these dimensions through specialized datasets and single-task\nsystems often miss opportunities for scalability, cross-task transfer, and\nbroader generalization. To address this gap, we curate Human Behavior Atlas, a\nunified benchmark of diverse behavioral tasks designed to support the\ndevelopment of unified models for understanding psychological and social\nbehaviors. Human Behavior Atlas comprises over 100,000 samples spanning text,\naudio, and visual modalities, covering tasks on affective states, cognitive\nstates, pathologies, and social processes. Our unification efforts can reduce\nredundancy and cost, enable training to scale efficiently across tasks, and\nenhance generalization of behavioral features across domains. On Human Behavior\nAtlas, we train three models: OmniSapiens-7B SFT, OmniSapiens-7B BAM, and\nOmniSapiens-7B RL. We show that training on Human Behavior Atlas enables models\nto consistently outperform existing multimodal LLMs across diverse behavioral\ntasks. Pretraining on Human Behavior Atlas also improves transfer to novel\nbehavioral datasets; with the targeted use of behavioral descriptors yielding\nmeaningful performance gains."
                },
                "authors": [
                    {
                        "name": "Keane Ong"
                    },
                    {
                        "name": "Wei Dai"
                    },
                    {
                        "name": "Carol Li"
                    },
                    {
                        "name": "Dewei Feng"
                    },
                    {
                        "name": "Hengzhi Li"
                    },
                    {
                        "name": "Jingyao Wu"
                    },
                    {
                        "name": "Jiaee Cheong"
                    },
                    {
                        "name": "Rui Mao"
                    },
                    {
                        "name": "Gianmarco Mengaldo"
                    },
                    {
                        "name": "Erik Cambria"
                    },
                    {
                        "name": "Paul Pu Liang"
                    }
                ],
                "author_detail": {
                    "name": "Paul Pu Liang"
                },
                "author": "Paul Pu Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04899v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04899v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04898v1",
                "updated": "2025-10-06T15:15:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    15,
                    38,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T15:15:38Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    15,
                    38,
                    0,
                    279,
                    0
                ],
                "title": "HyperVLA: Efficient Inference in Vision-Language-Action Models via\n  Hypernetworks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperVLA: Efficient Inference in Vision-Language-Action Models via\n  Hypernetworks"
                },
                "summary": "Built upon language and vision foundation models with strong generalization\nability and trained on large-scale robotic data, Vision-Language-Action (VLA)\nmodels have recently emerged as a promising approach to learning generalist\nrobotic policies. However, a key drawback of existing VLAs is their extremely\nhigh inference costs. In this paper, we propose HyperVLA to address this\nproblem. Unlike existing monolithic VLAs that activate the whole model during\nboth training and inference, HyperVLA uses a novel hypernetwork (HN)-based\narchitecture that activates only a small task-specific policy during inference,\nwhile still retaining the high model capacity needed to accommodate diverse\nmulti-task behaviors during training. Successfully training an HN-based VLA is\nnontrivial so HyperVLA contains several key algorithm design features that\nimprove its performance, including properly utilizing the prior knowledge from\nexisting vision foundation models, HN normalization, and an action generation\nstrategy. Compared to monolithic VLAs, HyperVLA achieves a similar or even\nhigher success rate for both zero-shot generalization and few-shot adaptation,\nwhile significantly reducing inference costs. Compared to OpenVLA, a\nstate-of-the-art VLA model, HyperVLA reduces the number of activated parameters\nat test time by $90\\times$, and accelerates inference speed by $120\\times$.\nCode is publicly available at https://github.com/MasterXiong/HyperVLA",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Built upon language and vision foundation models with strong generalization\nability and trained on large-scale robotic data, Vision-Language-Action (VLA)\nmodels have recently emerged as a promising approach to learning generalist\nrobotic policies. However, a key drawback of existing VLAs is their extremely\nhigh inference costs. In this paper, we propose HyperVLA to address this\nproblem. Unlike existing monolithic VLAs that activate the whole model during\nboth training and inference, HyperVLA uses a novel hypernetwork (HN)-based\narchitecture that activates only a small task-specific policy during inference,\nwhile still retaining the high model capacity needed to accommodate diverse\nmulti-task behaviors during training. Successfully training an HN-based VLA is\nnontrivial so HyperVLA contains several key algorithm design features that\nimprove its performance, including properly utilizing the prior knowledge from\nexisting vision foundation models, HN normalization, and an action generation\nstrategy. Compared to monolithic VLAs, HyperVLA achieves a similar or even\nhigher success rate for both zero-shot generalization and few-shot adaptation,\nwhile significantly reducing inference costs. Compared to OpenVLA, a\nstate-of-the-art VLA model, HyperVLA reduces the number of activated parameters\nat test time by $90\\times$, and accelerates inference speed by $120\\times$.\nCode is publicly available at https://github.com/MasterXiong/HyperVLA"
                },
                "authors": [
                    {
                        "name": "Zheng Xiong"
                    },
                    {
                        "name": "Kang Li"
                    },
                    {
                        "name": "Zilin Wang"
                    },
                    {
                        "name": "Matthew Jackson"
                    },
                    {
                        "name": "Jakob Foerster"
                    },
                    {
                        "name": "Shimon Whiteson"
                    }
                ],
                "author_detail": {
                    "name": "Shimon Whiteson"
                },
                "author": "Shimon Whiteson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26314v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26314v2",
                "updated": "2025-10-06T15:15:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    15,
                    21,
                    0,
                    279,
                    0
                ],
                "published": "2025-09-30T14:26:36Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    26,
                    36,
                    1,
                    273,
                    0
                ],
                "title": "Latent Thinking Optimization: Your Latent Reasoning Language Model\n  Secretly Encodes Reward Signals in Its Latent Thoughts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Thinking Optimization: Your Latent Reasoning Language Model\n  Secretly Encodes Reward Signals in Its Latent Thoughts"
                },
                "summary": "Large Language Models (LLMs) excel at problem solving by generating chain of\nthoughts in natural language, but such verbal thinking is computationally\ncostly and prone to overthinking. Recent work instead proposes a latent\nthinking architecture Huginn-3.5B, which represents intermediate reasoning\nsteps as sequence of latent representations. However, latent thoughts lack\ninterpretability and are difficult to supervise, raising concerns about the\ncorrectness and reliability of its latent thinking processes. In this paper, we\nprovide a systematic study of how Huginn-3.5B thinks in the latent space and\nhow external supervision signals can improve its latent thinking processes. We\nshow that latent thoughts leading to correct versus incorrect answers exhibit\nhighly distinguishable patterns, and that a latent classifier can reliably\npredict answer correctness directly from latent thoughts. Leveraging these\ninsights, we propose Latent Thinking Optimization (LTO), a probabilistic\nalgorithm that employs the latent classifier as a Latent Reward Model (LRM) to\noptimize the latent thinking processes. Extensive experiments across diverse\nreasoning tasks demonstrate that LRM is highly effective in detecting incorrect\nlatent thinking patterns, and LTO can significantly improve the latent thinking\nprocesses. Furthermore, we show that LRM can generalize across diverse domains,\nand LTO can be seamlessly applied to general LLMs to improve their thinking\nprocesses. In contrast to verbal thinking, our method demonstrates that reward\nmodeling and scaling test-time thinking with supervision can be performed\ndirectly in the latent space, highlighting its potential as a general,\nefficient, and domain-agnostic approach to improving the thinking processes of\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at problem solving by generating chain of\nthoughts in natural language, but such verbal thinking is computationally\ncostly and prone to overthinking. Recent work instead proposes a latent\nthinking architecture Huginn-3.5B, which represents intermediate reasoning\nsteps as sequence of latent representations. However, latent thoughts lack\ninterpretability and are difficult to supervise, raising concerns about the\ncorrectness and reliability of its latent thinking processes. In this paper, we\nprovide a systematic study of how Huginn-3.5B thinks in the latent space and\nhow external supervision signals can improve its latent thinking processes. We\nshow that latent thoughts leading to correct versus incorrect answers exhibit\nhighly distinguishable patterns, and that a latent classifier can reliably\npredict answer correctness directly from latent thoughts. Leveraging these\ninsights, we propose Latent Thinking Optimization (LTO), a probabilistic\nalgorithm that employs the latent classifier as a Latent Reward Model (LRM) to\noptimize the latent thinking processes. Extensive experiments across diverse\nreasoning tasks demonstrate that LRM is highly effective in detecting incorrect\nlatent thinking patterns, and LTO can significantly improve the latent thinking\nprocesses. Furthermore, we show that LRM can generalize across diverse domains,\nand LTO can be seamlessly applied to general LLMs to improve their thinking\nprocesses. In contrast to verbal thinking, our method demonstrates that reward\nmodeling and scaling test-time thinking with supervision can be performed\ndirectly in the latent space, highlighting its potential as a general,\nefficient, and domain-agnostic approach to improving the thinking processes of\nLLMs."
                },
                "authors": [
                    {
                        "name": "Hanwen Du"
                    },
                    {
                        "name": "Yuxin Dong"
                    },
                    {
                        "name": "Xia Ning"
                    }
                ],
                "author_detail": {
                    "name": "Xia Ning"
                },
                "author": "Xia Ning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26314v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26314v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04891v1",
                "updated": "2025-10-06T15:11:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    11,
                    46,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T15:11:46Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    11,
                    46,
                    0,
                    279,
                    0
                ],
                "title": "SocialHarmBench: Revealing LLM Vulnerabilities to Socially Harmful\n  Requests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SocialHarmBench: Revealing LLM Vulnerabilities to Socially Harmful\n  Requests"
                },
                "summary": "Large language models (LLMs) are increasingly deployed in contexts where\ntheir failures can have direct sociopolitical consequences. Yet, existing\nsafety benchmarks rarely test vulnerabilities in domains such as political\nmanipulation, propaganda and disinformation generation, or surveillance and\ninformation control. We introduce SocialHarmBench, a dataset of 585 prompts\nspanning 7 sociopolitical categories and 34 countries, designed to surface\nwhere LLMs most acutely fail in politically charged contexts. Our evaluations\nreveal several shortcomings: open-weight models exhibit high vulnerability to\nharmful compliance, with Mistral-7B reaching attack success rates as high as\n97% to 98% in domains such as historical revisionism, propaganda, and political\nmanipulation. Moreover, temporal and geographic analyses show that LLMs are\nmost fragile when confronted with 21st-century or pre-20th-century contexts,\nand when responding to prompts tied to regions such as Latin America, the USA,\nand the UK. These findings demonstrate that current safeguards fail to\ngeneralize to high-stakes sociopolitical settings, exposing systematic biases\nand raising concerns about the reliability of LLMs in preserving human rights\nand democratic values. We share the SocialHarmBench benchmark at\nhttps://huggingface.co/datasets/psyonp/SocialHarmBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly deployed in contexts where\ntheir failures can have direct sociopolitical consequences. Yet, existing\nsafety benchmarks rarely test vulnerabilities in domains such as political\nmanipulation, propaganda and disinformation generation, or surveillance and\ninformation control. We introduce SocialHarmBench, a dataset of 585 prompts\nspanning 7 sociopolitical categories and 34 countries, designed to surface\nwhere LLMs most acutely fail in politically charged contexts. Our evaluations\nreveal several shortcomings: open-weight models exhibit high vulnerability to\nharmful compliance, with Mistral-7B reaching attack success rates as high as\n97% to 98% in domains such as historical revisionism, propaganda, and political\nmanipulation. Moreover, temporal and geographic analyses show that LLMs are\nmost fragile when confronted with 21st-century or pre-20th-century contexts,\nand when responding to prompts tied to regions such as Latin America, the USA,\nand the UK. These findings demonstrate that current safeguards fail to\ngeneralize to high-stakes sociopolitical settings, exposing systematic biases\nand raising concerns about the reliability of LLMs in preserving human rights\nand democratic values. We share the SocialHarmBench benchmark at\nhttps://huggingface.co/datasets/psyonp/SocialHarmBench."
                },
                "authors": [
                    {
                        "name": "Punya Syon Pandey"
                    },
                    {
                        "name": "Hai Son Le"
                    },
                    {
                        "name": "Devansh Bhardwaj"
                    },
                    {
                        "name": "Rada Mihalcea"
                    },
                    {
                        "name": "Zhijing Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zhijing Jin"
                },
                "author": "Zhijing Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04888v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04888v1",
                "updated": "2025-10-06T15:09:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    9,
                    39,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T15:09:39Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    9,
                    39,
                    0,
                    279,
                    0
                ],
                "title": "Revealing Interconnections between Diseases: from Statistical Methods to\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing Interconnections between Diseases: from Statistical Methods to\n  Large Language Models"
                },
                "summary": "Identifying disease interconnections through manual analysis of large-scale\nclinical data is labor-intensive, subjective, and prone to expert disagreement.\nWhile machine learning (ML) shows promise, three critical challenges remain:\n(1) selecting optimal methods from the vast ML landscape, (2) determining\nwhether real-world clinical data (e.g., electronic health records, EHRs) or\nstructured disease descriptions yield more reliable insights, (3) the lack of\n\"ground truth,\" as some disease interconnections remain unexplored in medicine.\nLarge language models (LLMs) demonstrate broad utility, yet they often lack\nspecialized medical knowledge. To address these gaps, we conduct a systematic\nevaluation of seven approaches for uncovering disease relationships based on\ntwo data sources: (i) sequences of ICD-10 codes from MIMIC-IV EHRs and (ii) the\nfull set of ICD-10 codes, both with and without textual descriptions. Our\nframework integrates the following: (i) a statistical co-occurrence analysis\nand a masked language modeling (MLM) approach using real clinical data; (ii)\ndomain-specific BERT variants (Med-BERT and BioClinicalBERT); (iii) a\ngeneral-purpose BERT and document retrieval; and (iv) four LLMs (Mistral,\nDeepSeek, Qwen, and YandexGPT). Our graph-based comparison of the obtained\ninterconnection matrices shows that the LLM-based approach produces\ninterconnections with the lowest diversity of ICD code connections to different\ndiseases compared to other methods, including text-based and domain-based\napproaches. This suggests an important implication: LLMs have limited potential\nfor discovering new interconnections. In the absence of ground truth databases\nfor medical interconnections between ICD codes, our results constitute a\nvaluable medical disease ontology that can serve as a foundational resource for\nfuture clinical research and artificial intelligence applications in\nhealthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying disease interconnections through manual analysis of large-scale\nclinical data is labor-intensive, subjective, and prone to expert disagreement.\nWhile machine learning (ML) shows promise, three critical challenges remain:\n(1) selecting optimal methods from the vast ML landscape, (2) determining\nwhether real-world clinical data (e.g., electronic health records, EHRs) or\nstructured disease descriptions yield more reliable insights, (3) the lack of\n\"ground truth,\" as some disease interconnections remain unexplored in medicine.\nLarge language models (LLMs) demonstrate broad utility, yet they often lack\nspecialized medical knowledge. To address these gaps, we conduct a systematic\nevaluation of seven approaches for uncovering disease relationships based on\ntwo data sources: (i) sequences of ICD-10 codes from MIMIC-IV EHRs and (ii) the\nfull set of ICD-10 codes, both with and without textual descriptions. Our\nframework integrates the following: (i) a statistical co-occurrence analysis\nand a masked language modeling (MLM) approach using real clinical data; (ii)\ndomain-specific BERT variants (Med-BERT and BioClinicalBERT); (iii) a\ngeneral-purpose BERT and document retrieval; and (iv) four LLMs (Mistral,\nDeepSeek, Qwen, and YandexGPT). Our graph-based comparison of the obtained\ninterconnection matrices shows that the LLM-based approach produces\ninterconnections with the lowest diversity of ICD code connections to different\ndiseases compared to other methods, including text-based and domain-based\napproaches. This suggests an important implication: LLMs have limited potential\nfor discovering new interconnections. In the absence of ground truth databases\nfor medical interconnections between ICD codes, our results constitute a\nvaluable medical disease ontology that can serve as a foundational resource for\nfuture clinical research and artificial intelligence applications in\nhealthcare."
                },
                "authors": [
                    {
                        "name": "Alina Ermilova"
                    },
                    {
                        "name": "Dmitrii Kornilov"
                    },
                    {
                        "name": "Sofia Samoilova"
                    },
                    {
                        "name": "Ekaterina Laptenkova"
                    },
                    {
                        "name": "Anastasia Kolesnikova"
                    },
                    {
                        "name": "Ekaterina Podplutova"
                    },
                    {
                        "name": "Senotrusova Sofya"
                    },
                    {
                        "name": "Maksim G. Sharaev"
                    }
                ],
                "author_detail": {
                    "name": "Maksim G. Sharaev"
                },
                "author": "Maksim G. Sharaev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04888v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04888v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04886v1",
                "updated": "2025-10-06T15:07:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    7,
                    13,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T15:07:13Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    7,
                    13,
                    0,
                    279,
                    0
                ],
                "title": "Where Did It All Go Wrong? A Hierarchical Look into Multi-Agent Error\n  Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Where Did It All Go Wrong? A Hierarchical Look into Multi-Agent Error\n  Attribution"
                },
                "summary": "Error attribution in Large Language Model (LLM) multi-agent systems presents\na significant challenge in debugging and improving collaborative AI systems.\nCurrent approaches to pinpointing agent and step level failures in interaction\ntraces - whether using all-at-once evaluation, step-by-step analysis, or binary\nsearch - fall short when analyzing complex patterns, struggling with both\naccuracy and consistency. We present ECHO (Error attribution through Contextual\nHierarchy and Objective consensus analysis), a novel algorithm that combines\nhierarchical context representation, objective analysis-based evaluation, and\nconsensus voting to improve error attribution accuracy. Our approach leverages\na positional-based leveling of contextual understanding while maintaining\nobjective evaluation criteria, ultimately reaching conclusions through a\nconsensus mechanism. Experimental results demonstrate that ECHO outperforms\nexisting methods across various multi-agent interaction scenarios, showing\nparticular strength in cases involving subtle reasoning errors and complex\ninterdependencies. Our findings suggest that leveraging these concepts of\nstructured, hierarchical context representation combined with consensus-based\nobjective decision-making, provides a more robust framework for error\nattribution in multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Error attribution in Large Language Model (LLM) multi-agent systems presents\na significant challenge in debugging and improving collaborative AI systems.\nCurrent approaches to pinpointing agent and step level failures in interaction\ntraces - whether using all-at-once evaluation, step-by-step analysis, or binary\nsearch - fall short when analyzing complex patterns, struggling with both\naccuracy and consistency. We present ECHO (Error attribution through Contextual\nHierarchy and Objective consensus analysis), a novel algorithm that combines\nhierarchical context representation, objective analysis-based evaluation, and\nconsensus voting to improve error attribution accuracy. Our approach leverages\na positional-based leveling of contextual understanding while maintaining\nobjective evaluation criteria, ultimately reaching conclusions through a\nconsensus mechanism. Experimental results demonstrate that ECHO outperforms\nexisting methods across various multi-agent interaction scenarios, showing\nparticular strength in cases involving subtle reasoning errors and complex\ninterdependencies. Our findings suggest that leveraging these concepts of\nstructured, hierarchical context representation combined with consensus-based\nobjective decision-making, provides a more robust framework for error\nattribution in multi-agent systems."
                },
                "authors": [
                    {
                        "name": "Adi Banerjee"
                    },
                    {
                        "name": "Anirudh Nair"
                    },
                    {
                        "name": "Tarik Borogovac"
                    }
                ],
                "author_detail": {
                    "name": "Tarik Borogovac"
                },
                "author": "Tarik Borogovac",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04885v1",
                "updated": "2025-10-06T15:06:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    6,
                    4,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T15:06:04Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    6,
                    4,
                    0,
                    279,
                    0
                ],
                "title": "RL Is a Hammer and LLMs Are Nails: A Simple Reinforcement Learning\n  Recipe for Strong Prompt Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RL Is a Hammer and LLMs Are Nails: A Simple Reinforcement Learning\n  Recipe for Strong Prompt Injection"
                },
                "summary": "Prompt injection poses a serious threat to the reliability and safety of LLM\nagents. Recent defenses against prompt injection, such as Instruction Hierarchy\nand SecAlign, have shown notable robustness against static attacks. However, to\nmore thoroughly evaluate the robustness of these defenses, it is arguably\nnecessary to employ strong attacks such as automated red-teaming. To this end,\nwe introduce RL-Hammer, a simple recipe for training attacker models that\nautomatically learn to perform strong prompt injections and jailbreaks via\nreinforcement learning. RL-Hammer requires no warm-up data and can be trained\nentirely from scratch. To achieve high ASRs against industrial-level models\nwith defenses, we propose a set of practical techniques that enable highly\neffective, universal attacks. Using this pipeline, RL-Hammer reaches a 98% ASR\nagainst GPT-4o and a $72\\%$ ASR against GPT-5 with the Instruction Hierarchy\ndefense. We further discuss the challenge of achieving high diversity in\nattacks, highlighting how attacker models tend to reward-hack diversity\nobjectives. Finally, we show that RL-Hammer can evade multiple prompt injection\ndetectors. We hope our work advances automatic red-teaming and motivates the\ndevelopment of stronger, more principled defenses. Code is available at\nhttps://github.com/facebookresearch/rl-injector.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt injection poses a serious threat to the reliability and safety of LLM\nagents. Recent defenses against prompt injection, such as Instruction Hierarchy\nand SecAlign, have shown notable robustness against static attacks. However, to\nmore thoroughly evaluate the robustness of these defenses, it is arguably\nnecessary to employ strong attacks such as automated red-teaming. To this end,\nwe introduce RL-Hammer, a simple recipe for training attacker models that\nautomatically learn to perform strong prompt injections and jailbreaks via\nreinforcement learning. RL-Hammer requires no warm-up data and can be trained\nentirely from scratch. To achieve high ASRs against industrial-level models\nwith defenses, we propose a set of practical techniques that enable highly\neffective, universal attacks. Using this pipeline, RL-Hammer reaches a 98% ASR\nagainst GPT-4o and a $72\\%$ ASR against GPT-5 with the Instruction Hierarchy\ndefense. We further discuss the challenge of achieving high diversity in\nattacks, highlighting how attacker models tend to reward-hack diversity\nobjectives. Finally, we show that RL-Hammer can evade multiple prompt injection\ndetectors. We hope our work advances automatic red-teaming and motivates the\ndevelopment of stronger, more principled defenses. Code is available at\nhttps://github.com/facebookresearch/rl-injector."
                },
                "authors": [
                    {
                        "name": "Yuxin Wen"
                    },
                    {
                        "name": "Arman Zharmagambetov"
                    },
                    {
                        "name": "Ivan Evtimov"
                    },
                    {
                        "name": "Narine Kokhlikyan"
                    },
                    {
                        "name": "Tom Goldstein"
                    },
                    {
                        "name": "Kamalika Chaudhuri"
                    },
                    {
                        "name": "Chuan Guo"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Guo"
                },
                "author": "Chuan Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04884v1",
                "updated": "2025-10-06T15:05:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    5,
                    35,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T15:05:35Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    5,
                    35,
                    0,
                    279,
                    0
                ],
                "title": "Higher-Order Network Structure Inference: A Topological Approach to\n  Network Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Higher-Order Network Structure Inference: A Topological Approach to\n  Network Selection"
                },
                "summary": "Thresholding--the pruning of nodes or edges based on their properties or\nweights--is an essential preprocessing tool for extracting interpretable\nstructure from complex network data, yet existing methods face several key\nlimitations. Threshold selection often relies on heuristic methods or trial and\nerror due to large parameter spaces and unclear optimization criteria, leading\nto sensitivity where small parameter variations produce significant changes in\nnetwork structure. Moreover, most approaches focus on pairwise relationships\nbetween nodes, overlooking critical higher-order interactions involving three\nor more nodes. We introduce a systematic thresholding algorithm that leverages\ntopological data analysis to identify optimal network parameters by accounting\nfor higher-order structural relationships. Our method uses persistent homology\nto compute the stability of homological features across the parameter space,\nidentifying parameter choices that are robust to small variations while\npreserving meaningful topological structure. Hyperparameters allow users to\nspecify minimum requirements for topological features, effectively constraining\nthe parameter search to avoid spurious solutions. We demonstrate the approach\nwith an application in the Science of Science, where networks of scientific\nconcepts are extracted from research paper abstracts, and concepts are\nconnected when they co-appear in the same abstract. The flexibility of our\napproach allows researchers to incorporate domain-specific constraints and\nextends beyond network thresholding to general parameterization problems in\ndata analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thresholding--the pruning of nodes or edges based on their properties or\nweights--is an essential preprocessing tool for extracting interpretable\nstructure from complex network data, yet existing methods face several key\nlimitations. Threshold selection often relies on heuristic methods or trial and\nerror due to large parameter spaces and unclear optimization criteria, leading\nto sensitivity where small parameter variations produce significant changes in\nnetwork structure. Moreover, most approaches focus on pairwise relationships\nbetween nodes, overlooking critical higher-order interactions involving three\nor more nodes. We introduce a systematic thresholding algorithm that leverages\ntopological data analysis to identify optimal network parameters by accounting\nfor higher-order structural relationships. Our method uses persistent homology\nto compute the stability of homological features across the parameter space,\nidentifying parameter choices that are robust to small variations while\npreserving meaningful topological structure. Hyperparameters allow users to\nspecify minimum requirements for topological features, effectively constraining\nthe parameter search to avoid spurious solutions. We demonstrate the approach\nwith an application in the Science of Science, where networks of scientific\nconcepts are extracted from research paper abstracts, and concepts are\nconnected when they co-appear in the same abstract. The flexibility of our\napproach allows researchers to incorporate domain-specific constraints and\nextends beyond network thresholding to general parameterization problems in\ndata analysis."
                },
                "authors": [
                    {
                        "name": "Adam Schroeder"
                    },
                    {
                        "name": "Russell Funk"
                    },
                    {
                        "name": "Jingyi Guan"
                    },
                    {
                        "name": "Taylor Okonek"
                    },
                    {
                        "name": "Lori Ziegelmeier"
                    }
                ],
                "author_detail": {
                    "name": "Lori Ziegelmeier"
                },
                "author": "Lori Ziegelmeier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04871v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04871v1",
                "updated": "2025-10-06T14:58:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    58,
                    8,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T14:58:08Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    58,
                    8,
                    0,
                    279,
                    0
                ],
                "title": "Less is More: Recursive Reasoning with Tiny Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less is More: Recursive Reasoning with Tiny Networks"
                },
                "summary": "Hierarchical Reasoning Model (HRM) is a novel approach using two small neural\nnetworks recursing at different frequencies. This biologically inspired method\nbeats Large Language models (LLMs) on hard puzzle tasks such as Sudoku, Maze,\nand ARC-AGI while trained with small models (27M parameters) on small data\n(around 1000 examples). HRM holds great promise for solving hard problems with\nsmall networks, but it is not yet well understood and may be suboptimal. We\npropose Tiny Recursive Model (TRM), a much simpler recursive reasoning approach\nthat achieves significantly higher generalization than HRM, while using a\nsingle tiny network with only 2 layers. With only 7M parameters, TRM obtains\n45% test-accuracy on ARC-AGI-1 and 8% on ARC-AGI-2, higher than most LLMs\n(e.g., Deepseek R1, o3-mini, Gemini 2.5 Pro) with less than 0.01% of the\nparameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Reasoning Model (HRM) is a novel approach using two small neural\nnetworks recursing at different frequencies. This biologically inspired method\nbeats Large Language models (LLMs) on hard puzzle tasks such as Sudoku, Maze,\nand ARC-AGI while trained with small models (27M parameters) on small data\n(around 1000 examples). HRM holds great promise for solving hard problems with\nsmall networks, but it is not yet well understood and may be suboptimal. We\npropose Tiny Recursive Model (TRM), a much simpler recursive reasoning approach\nthat achieves significantly higher generalization than HRM, while using a\nsingle tiny network with only 2 layers. With only 7M parameters, TRM obtains\n45% test-accuracy on ARC-AGI-1 and 8% on ARC-AGI-2, higher than most LLMs\n(e.g., Deepseek R1, o3-mini, Gemini 2.5 Pro) with less than 0.01% of the\nparameters."
                },
                "authors": [
                    {
                        "name": "Alexia Jolicoeur-Martineau"
                    }
                ],
                "author_detail": {
                    "name": "Alexia Jolicoeur-Martineau"
                },
                "author": "Alexia Jolicoeur-Martineau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04871v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04871v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09667v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09667v3",
                "updated": "2025-10-06T14:57:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    57,
                    36,
                    0,
                    279,
                    0
                ],
                "published": "2025-02-12T19:50:22Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    19,
                    50,
                    22,
                    2,
                    43,
                    0
                ],
                "title": "Summaries as Centroids for Interpretable and Scalable Text Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Summaries as Centroids for Interpretable and Scalable Text Clustering"
                },
                "summary": "We introduce k-NLPmeans and k-LLMmeans, text-clustering variants of k-means\nthat periodically replace numeric centroids with textual summaries. The key\nidea, summary-as-centroid, retains k-means assignments in embedding space while\nproducing human-readable, auditable cluster prototypes. The method is\nLLM-optional: k-NLPmeans uses lightweight, deterministic summarizers, enabling\noffline, low-cost, and stable operation; k-LLMmeans is a drop-in upgrade that\nuses an LLM for summaries under a fixed per-iteration budget whose cost does\nnot grow with dataset size. We also present a mini-batch extension for\nreal-time clustering of streaming text. Across diverse datasets, embedding\nmodels, and summarization strategies, our approach consistently outperforms\nclassical baselines and approaches the accuracy of recent LLM-based\nclustering-without extensive LLM calls. Finally, we provide a case study on\nsequential text streams and release a StackExchange-derived benchmark for\nevaluating streaming text clustering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce k-NLPmeans and k-LLMmeans, text-clustering variants of k-means\nthat periodically replace numeric centroids with textual summaries. The key\nidea, summary-as-centroid, retains k-means assignments in embedding space while\nproducing human-readable, auditable cluster prototypes. The method is\nLLM-optional: k-NLPmeans uses lightweight, deterministic summarizers, enabling\noffline, low-cost, and stable operation; k-LLMmeans is a drop-in upgrade that\nuses an LLM for summaries under a fixed per-iteration budget whose cost does\nnot grow with dataset size. We also present a mini-batch extension for\nreal-time clustering of streaming text. Across diverse datasets, embedding\nmodels, and summarization strategies, our approach consistently outperforms\nclassical baselines and approaches the accuracy of recent LLM-based\nclustering-without extensive LLM calls. Finally, we provide a case study on\nsequential text streams and release a StackExchange-derived benchmark for\nevaluating streaming text clustering."
                },
                "authors": [
                    {
                        "name": "Jairo Diaz-Rodriguez"
                    }
                ],
                "author_detail": {
                    "name": "Jairo Diaz-Rodriguez"
                },
                "author": "Jairo Diaz-Rodriguez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09667v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09667v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18142v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18142v4",
                "updated": "2025-10-06T14:56:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    56,
                    54,
                    0,
                    279,
                    0
                ],
                "published": "2024-11-27T08:44:25Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    8,
                    44,
                    25,
                    2,
                    332,
                    0
                ],
                "title": "Autonomous Imagination: Closed-Loop Decomposition of Visual-to-Textual\n  Conversion in Visual Reasoning for Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Imagination: Closed-Loop Decomposition of Visual-to-Textual\n  Conversion in Visual Reasoning for Multimodal Large Language Models"
                },
                "summary": "Under pure textual modality, Large Language Models (LLMs) have demonstrated\nremarkable success in complex reasoning tasks by decomposing them into simpler\nsub-problems. However, Multimodal Large Language Models (MLLMs) still struggle\nwith some seemingly straightforward visual tasks, such as counting and solving\njigsaw puzzles. We argue that these tasks challenge the ability of\nvisual-to-textual conversion, where MLLMs convert visual information perceived\nfrom the input scene, to textual information for further reasoning and\ngenerating the answer. If the complexity of the visual input is beyond the\nperceptual capability of the MLLMs, without decomposing this conversion\nprocess, simply scaling inference-time reasoning cannot solve the task because\nit repeatedly encounters the same perceptual bottleneck. We propose an\napproach, autonomous imagination, to enable MLLMs to iteratively modify visual\ninputs (e.g. isolating objects, rearranging puzzle pieces) into intermediate\nvisual states, decomposing visual-to-textual conversion into closed-loop visual\nmodification steps. We show that, without any retraining, MLLMs can now solve\ntasks initially beyond their perceptual capability, highlighting that\nclosed-loop visual modification can be an effective way of decomposing the\nvisual reasoning task into solvable substeps. Our code and data are released at\nhttps://future-item.github.io/autoimagine-site/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Under pure textual modality, Large Language Models (LLMs) have demonstrated\nremarkable success in complex reasoning tasks by decomposing them into simpler\nsub-problems. However, Multimodal Large Language Models (MLLMs) still struggle\nwith some seemingly straightforward visual tasks, such as counting and solving\njigsaw puzzles. We argue that these tasks challenge the ability of\nvisual-to-textual conversion, where MLLMs convert visual information perceived\nfrom the input scene, to textual information for further reasoning and\ngenerating the answer. If the complexity of the visual input is beyond the\nperceptual capability of the MLLMs, without decomposing this conversion\nprocess, simply scaling inference-time reasoning cannot solve the task because\nit repeatedly encounters the same perceptual bottleneck. We propose an\napproach, autonomous imagination, to enable MLLMs to iteratively modify visual\ninputs (e.g. isolating objects, rearranging puzzle pieces) into intermediate\nvisual states, decomposing visual-to-textual conversion into closed-loop visual\nmodification steps. We show that, without any retraining, MLLMs can now solve\ntasks initially beyond their perceptual capability, highlighting that\nclosed-loop visual modification can be an effective way of decomposing the\nvisual reasoning task into solvable substeps. Our code and data are released at\nhttps://future-item.github.io/autoimagine-site/."
                },
                "authors": [
                    {
                        "name": "Jingming Liu"
                    },
                    {
                        "name": "Yumeng Li"
                    },
                    {
                        "name": "Boyuan Xiao"
                    },
                    {
                        "name": "Yichang Jian"
                    },
                    {
                        "name": "Ziang Qin"
                    },
                    {
                        "name": "Tianjia Shao"
                    },
                    {
                        "name": "Yao-Xiang Ding"
                    },
                    {
                        "name": "Kun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Kun Zhou"
                },
                "author": "Kun Zhou",
                "arxiv_comment": "Published in TMLR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18142v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18142v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22768v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22768v2",
                "updated": "2025-10-06T14:53:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    53,
                    27,
                    0,
                    279,
                    0
                ],
                "published": "2025-09-26T17:20:27Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    20,
                    27,
                    4,
                    269,
                    0
                ],
                "title": "ML2B: Multi-Lingual ML Benchmark For AutoML",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ML2B: Multi-Lingual ML Benchmark For AutoML"
                },
                "summary": "Large language models (LLMs) have recently demonstrated strong capabilities\nin generating machine learning (ML) code, enabling end-to-end pipeline\nconstruction from natural language instructions. However, existing benchmarks\nfor ML code generation are mainly restricted to English, overlooking the global\nand multilingual nature of ML research and practice. To address this gap, we\npresent ML2B, the first benchmark for evaluating multilingual ML code\ngeneration. ML2B consists of 30 Kaggle competitions translated into 13 natural\nlanguages, covering tabular, text, and image data types, with structured\nmetadata and validated human-reviewed translations. For evaluation, we employ\nAIDE, an automated framework for end-to-end assessment of data science\npipelines, and provide insights into cross-lingual model performance. Our\nresults reveal substantial 15-45% performance degradation on non-English tasks,\nhighlighting critical challenges in multilingual representation learning for\ncode generation. The benchmark, evaluation framework, and comprehensive results\nare made available through our GitHub repository to facilitate future research\nin multilingual ML code generation: https://github.com/enaix/ml2b.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently demonstrated strong capabilities\nin generating machine learning (ML) code, enabling end-to-end pipeline\nconstruction from natural language instructions. However, existing benchmarks\nfor ML code generation are mainly restricted to English, overlooking the global\nand multilingual nature of ML research and practice. To address this gap, we\npresent ML2B, the first benchmark for evaluating multilingual ML code\ngeneration. ML2B consists of 30 Kaggle competitions translated into 13 natural\nlanguages, covering tabular, text, and image data types, with structured\nmetadata and validated human-reviewed translations. For evaluation, we employ\nAIDE, an automated framework for end-to-end assessment of data science\npipelines, and provide insights into cross-lingual model performance. Our\nresults reveal substantial 15-45% performance degradation on non-English tasks,\nhighlighting critical challenges in multilingual representation learning for\ncode generation. The benchmark, evaluation framework, and comprehensive results\nare made available through our GitHub repository to facilitate future research\nin multilingual ML code generation: https://github.com/enaix/ml2b."
                },
                "authors": [
                    {
                        "name": "Ekaterina Trofimova"
                    },
                    {
                        "name": "Zosia Shamina"
                    },
                    {
                        "name": "Maria Selifanova"
                    },
                    {
                        "name": "Artem Zaitsev"
                    },
                    {
                        "name": "Remi Savchuk"
                    },
                    {
                        "name": "Maxim Minets"
                    },
                    {
                        "name": "Daria Ozerova"
                    },
                    {
                        "name": "Emil Sataev"
                    },
                    {
                        "name": "Denis Zuenko"
                    },
                    {
                        "name": "Andrey E. Ustyuzhanin"
                    }
                ],
                "author_detail": {
                    "name": "Andrey E. Ustyuzhanin"
                },
                "author": "Andrey E. Ustyuzhanin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22768v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22768v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04868v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04868v1",
                "updated": "2025-10-06T14:52:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    52,
                    27,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T14:52:27Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    52,
                    27,
                    0,
                    279,
                    0
                ],
                "title": "Model Predictive Control-Guided Reinforcement Learning for Implicit\n  Balancing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Predictive Control-Guided Reinforcement Learning for Implicit\n  Balancing"
                },
                "summary": "In Europe, profit-seeking balance responsible parties can deviate in real\ntime from their day-ahead nominations to assist transmission system operators\nin maintaining the supply-demand balance. Model predictive control (MPC)\nstrategies to exploit these implicit balancing strategies capture arbitrage\nopportunities, but fail to accurately capture the price-formation process in\nthe European imbalance markets and face high computational costs. Model-free\nreinforcement learning (RL) methods are fast to execute, but require\ndata-intensive training and usually rely on real-time and historical data for\ndecision-making. This paper proposes an MPC-guided RL method that combines the\ncomplementary strengths of both MPC and RL. The proposed method can effectively\nincorporate forecasts into the decision-making process (as in MPC), while\nmaintaining the fast inference capability of RL. The performance of the\nproposed method is evaluated on the implicit balancing battery control problem\nusing Belgian balancing data from 2023. First, we analyze the performance of\nthe standalone state-of-the-art RL and MPC methods from various angles, to\nhighlight their individual strengths and limitations. Next, we show an\narbitrage profit benefit of the proposed MPC-guided RL method of 16.15% and\n54.36%, compared to standalone RL and MPC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Europe, profit-seeking balance responsible parties can deviate in real\ntime from their day-ahead nominations to assist transmission system operators\nin maintaining the supply-demand balance. Model predictive control (MPC)\nstrategies to exploit these implicit balancing strategies capture arbitrage\nopportunities, but fail to accurately capture the price-formation process in\nthe European imbalance markets and face high computational costs. Model-free\nreinforcement learning (RL) methods are fast to execute, but require\ndata-intensive training and usually rely on real-time and historical data for\ndecision-making. This paper proposes an MPC-guided RL method that combines the\ncomplementary strengths of both MPC and RL. The proposed method can effectively\nincorporate forecasts into the decision-making process (as in MPC), while\nmaintaining the fast inference capability of RL. The performance of the\nproposed method is evaluated on the implicit balancing battery control problem\nusing Belgian balancing data from 2023. First, we analyze the performance of\nthe standalone state-of-the-art RL and MPC methods from various angles, to\nhighlight their individual strengths and limitations. Next, we show an\narbitrage profit benefit of the proposed MPC-guided RL method of 16.15% and\n54.36%, compared to standalone RL and MPC."
                },
                "authors": [
                    {
                        "name": "Seyed Soroush Karimi Madahi"
                    },
                    {
                        "name": "Kenneth Bruninx"
                    },
                    {
                        "name": "Bert Claessens"
                    },
                    {
                        "name": "Chris Develder"
                    }
                ],
                "author_detail": {
                    "name": "Chris Develder"
                },
                "author": "Chris Develder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04868v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04868v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04863v1",
                "updated": "2025-10-06T14:50:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    50,
                    27,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T14:50:27Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    50,
                    27,
                    0,
                    279,
                    0
                ],
                "title": "Heat Reveals What Clouds Conceal: Global Carbon & Longitudinally\n  Asymmetric Chemistry on LTT 9779 b",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heat Reveals What Clouds Conceal: Global Carbon & Longitudinally\n  Asymmetric Chemistry on LTT 9779 b"
                },
                "summary": "LTT-9779 b is an ultra-hot Neptune (Rp ~ 4.7 Re, Mp ~ 29 Me) orbiting its\nSun-like host star in just 19 hours, placing it deep within the \"hot Neptune\ndesert,\" where Neptunian planets are seldom found. We present new JWST NIRSpec\nG395H phase-curve observations that probe its atmospheric composition in\nunprecedented detail. At near-infrared wavelengths, which penetrate the\nhigh-altitude clouds inferred from previous NIRISS/SOSS spectra, thermal\nemission reveals a carbon-rich atmosphere with opacity dominated by carbon\nmonoxide (CO) and carbon dioxide (CO2). Both species are detected at all\norbital phases, with retrieved mixing ratios of 10^-1 for CO and 10^-4 for CO2,\nindicating a globally well-mixed reservoir of carbon-bearing gases. We also\nmoderately detect water vapor (H2O) and tentatively detect sulfur dioxide\n(SO2), providing insight into its chemistry and possible photochemical\nproduction under intense stellar irradiation. From these detections we infer a\ncarbon-to-oxygen ratio near unity (C/O ~ 1) and a metallicity exceeding 500X\nSolar, consistent with equilibrium chemistry predictions for high-temperature\natmospheres. This enrichment raises the mean molecular weight, reducing\natmospheric escape, and likely helps LTT-9779 b retain a substantial atmosphere\ndespite extreme irradiation. Our findings show that LTT-9779 b survives where\nfew planets can, maintaining a carbon-rich atmosphere in a region where hot\nNeptune-class worlds are expected to evaporate. This makes LTT-9779 b a\nvaluable laboratory for studying atmospheric escape and chemical processes\nunder extreme conditions, offering new insight into the survival of planets in\nthe hot Neptune desert.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LTT-9779 b is an ultra-hot Neptune (Rp ~ 4.7 Re, Mp ~ 29 Me) orbiting its\nSun-like host star in just 19 hours, placing it deep within the \"hot Neptune\ndesert,\" where Neptunian planets are seldom found. We present new JWST NIRSpec\nG395H phase-curve observations that probe its atmospheric composition in\nunprecedented detail. At near-infrared wavelengths, which penetrate the\nhigh-altitude clouds inferred from previous NIRISS/SOSS spectra, thermal\nemission reveals a carbon-rich atmosphere with opacity dominated by carbon\nmonoxide (CO) and carbon dioxide (CO2). Both species are detected at all\norbital phases, with retrieved mixing ratios of 10^-1 for CO and 10^-4 for CO2,\nindicating a globally well-mixed reservoir of carbon-bearing gases. We also\nmoderately detect water vapor (H2O) and tentatively detect sulfur dioxide\n(SO2), providing insight into its chemistry and possible photochemical\nproduction under intense stellar irradiation. From these detections we infer a\ncarbon-to-oxygen ratio near unity (C/O ~ 1) and a metallicity exceeding 500X\nSolar, consistent with equilibrium chemistry predictions for high-temperature\natmospheres. This enrichment raises the mean molecular weight, reducing\natmospheric escape, and likely helps LTT-9779 b retain a substantial atmosphere\ndespite extreme irradiation. Our findings show that LTT-9779 b survives where\nfew planets can, maintaining a carbon-rich atmosphere in a region where hot\nNeptune-class worlds are expected to evaporate. This makes LTT-9779 b a\nvaluable laboratory for studying atmospheric escape and chemical processes\nunder extreme conditions, offering new insight into the survival of planets in\nthe hot Neptune desert."
                },
                "authors": [
                    {
                        "name": "Reza Ashtari"
                    },
                    {
                        "name": "Sean Collins"
                    },
                    {
                        "name": "Jared Splinter"
                    },
                    {
                        "name": "Kevin B. Stevenson"
                    },
                    {
                        "name": "Vivien Parmentier"
                    },
                    {
                        "name": "Jonathan Brande"
                    },
                    {
                        "name": "Suman Saha"
                    },
                    {
                        "name": "Sarah Stamer"
                    },
                    {
                        "name": "Ian J. M. Crossfield"
                    },
                    {
                        "name": "James S. Jenkins"
                    },
                    {
                        "name": "K. Angelique Kahle"
                    },
                    {
                        "name": "Joshua D. Lothringer"
                    },
                    {
                        "name": "Nishil Mehta"
                    },
                    {
                        "name": "Nicolas B. Cowan"
                    },
                    {
                        "name": "Diana Dragomir"
                    },
                    {
                        "name": "Laura Kreidberg"
                    },
                    {
                        "name": "Thomas M. Evans-Soma"
                    },
                    {
                        "name": "Tansu Daylan"
                    },
                    {
                        "name": "Olivia Venot"
                    },
                    {
                        "name": "Xi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xi Zhang"
                },
                "author": "Xi Zhang",
                "arxiv_comment": "25 pages, 15 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04860v1",
                "updated": "2025-10-06T14:48:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    48,
                    39,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T14:48:39Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    48,
                    39,
                    0,
                    279,
                    0
                ],
                "title": "Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the\n  Rails",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the\n  Rails"
                },
                "summary": "As Large Language Model (LLM) agents increasingly gain self-evolutionary\ncapabilities to adapt and refine their strategies through real-world\ninteraction, their long-term reliability becomes a critical concern. We\nidentify the Alignment Tipping Process (ATP), a critical post-deployment risk\nunique to self-evolving LLM agents. Unlike training-time failures, ATP arises\nwhen continual interaction drives agents to abandon alignment constraints\nestablished during training in favor of reinforced, self-interested strategies.\nWe formalize and analyze ATP through two complementary paradigms:\nSelf-Interested Exploration, where repeated high-reward deviations induce\nindividual behavioral drift, and Imitative Strategy Diffusion, where deviant\nbehaviors spread across multi-agent systems. Building on these paradigms, we\nconstruct controllable testbeds and benchmark Qwen3-8B and\nLlama-3.1-8B-Instruct. Our experiments show that alignment benefits erode\nrapidly under self-evolution, with initially aligned models converging toward\nunaligned states. In multi-agent settings, successful violations diffuse\nquickly, leading to collective misalignment. Moreover, current reinforcement\nlearning-based alignment methods provide only fragile defenses against\nalignment tipping. Together, these findings demonstrate that alignment of LLM\nagents is not a static property but a fragile and dynamic one, vulnerable to\nfeedback-driven decay during deployment. Our data and code are available at\nhttps://github.com/aiming-lab/ATP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Model (LLM) agents increasingly gain self-evolutionary\ncapabilities to adapt and refine their strategies through real-world\ninteraction, their long-term reliability becomes a critical concern. We\nidentify the Alignment Tipping Process (ATP), a critical post-deployment risk\nunique to self-evolving LLM agents. Unlike training-time failures, ATP arises\nwhen continual interaction drives agents to abandon alignment constraints\nestablished during training in favor of reinforced, self-interested strategies.\nWe formalize and analyze ATP through two complementary paradigms:\nSelf-Interested Exploration, where repeated high-reward deviations induce\nindividual behavioral drift, and Imitative Strategy Diffusion, where deviant\nbehaviors spread across multi-agent systems. Building on these paradigms, we\nconstruct controllable testbeds and benchmark Qwen3-8B and\nLlama-3.1-8B-Instruct. Our experiments show that alignment benefits erode\nrapidly under self-evolution, with initially aligned models converging toward\nunaligned states. In multi-agent settings, successful violations diffuse\nquickly, leading to collective misalignment. Moreover, current reinforcement\nlearning-based alignment methods provide only fragile defenses against\nalignment tipping. Together, these findings demonstrate that alignment of LLM\nagents is not a static property but a fragile and dynamic one, vulnerable to\nfeedback-driven decay during deployment. Our data and code are available at\nhttps://github.com/aiming-lab/ATP."
                },
                "authors": [
                    {
                        "name": "Siwei Han"
                    },
                    {
                        "name": "Jiaqi Liu"
                    },
                    {
                        "name": "Yaofeng Su"
                    },
                    {
                        "name": "Wenbo Duan"
                    },
                    {
                        "name": "Xinyuan Liu"
                    },
                    {
                        "name": "Cihang Xie"
                    },
                    {
                        "name": "Mohit Bansal"
                    },
                    {
                        "name": "Mingyu Ding"
                    },
                    {
                        "name": "Linjun Zhang"
                    },
                    {
                        "name": "Huaxiu Yao"
                    }
                ],
                "author_detail": {
                    "name": "Huaxiu Yao"
                },
                "author": "Huaxiu Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09050v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09050v2",
                "updated": "2025-10-06T14:44:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    44,
                    32,
                    0,
                    279,
                    0
                ],
                "published": "2025-06-10T17:59:56Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    17,
                    59,
                    56,
                    1,
                    161,
                    0
                ],
                "title": "ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm\n  Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm\n  Engineering"
                },
                "summary": "How well do AI systems perform in algorithm engineering for hard optimization\nproblems in domains such as package-delivery routing, crew scheduling, factory\nproduction planning, and power-grid balancing? We introduce ALE-Bench, a new\nbenchmark for evaluating AI systems on score-based algorithmic programming\ncontests. Drawing on real tasks from the AtCoder Heuristic Contests, ALE-Bench\npresents optimization problems that are computationally hard and admit no known\nexact solution. Unlike short-duration, pass/fail coding benchmarks, ALE-Bench\nencourages iterative solution refinement over long time horizons. Our software\nframework supports interactive agent architectures that leverage test-run\nfeedback and visualizations. Our evaluation of frontier LLMs revealed that\nwhile they demonstrate high performance on specific problems, a notable gap\nremains compared to humans in terms of consistency across problems and\nlong-horizon problem-solving capabilities. This highlights the need for this\nbenchmark to foster future AI advancements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How well do AI systems perform in algorithm engineering for hard optimization\nproblems in domains such as package-delivery routing, crew scheduling, factory\nproduction planning, and power-grid balancing? We introduce ALE-Bench, a new\nbenchmark for evaluating AI systems on score-based algorithmic programming\ncontests. Drawing on real tasks from the AtCoder Heuristic Contests, ALE-Bench\npresents optimization problems that are computationally hard and admit no known\nexact solution. Unlike short-duration, pass/fail coding benchmarks, ALE-Bench\nencourages iterative solution refinement over long time horizons. Our software\nframework supports interactive agent architectures that leverage test-run\nfeedback and visualizations. Our evaluation of frontier LLMs revealed that\nwhile they demonstrate high performance on specific problems, a notable gap\nremains compared to humans in terms of consistency across problems and\nlong-horizon problem-solving capabilities. This highlights the need for this\nbenchmark to foster future AI advancements."
                },
                "authors": [
                    {
                        "name": "Yuki Imajuku"
                    },
                    {
                        "name": "Kohki Horie"
                    },
                    {
                        "name": "Yoichi Iwata"
                    },
                    {
                        "name": "Kensho Aoki"
                    },
                    {
                        "name": "Naohiro Takahashi"
                    },
                    {
                        "name": "Takuya Akiba"
                    }
                ],
                "author_detail": {
                    "name": "Takuya Akiba"
                },
                "author": "Takuya Akiba",
                "arxiv_comment": "Accepted at NeurIPS 2025 Datasets & Benchmarks Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09050v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09050v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11780v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11780v2",
                "updated": "2025-10-06T14:42:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    42,
                    38,
                    0,
                    279,
                    0
                ],
                "published": "2025-09-15T11:01:42Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    11,
                    1,
                    42,
                    0,
                    258,
                    0
                ],
                "title": "Variational Gaussian Approximation in Replica Analysis of Parametric\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Gaussian Approximation in Replica Analysis of Parametric\n  Models"
                },
                "summary": "We revisit the replica method for analyzing inference and learning in\nparametric models, considering situations where the data-generating\ndistribution is unknown or analytically intractable. Instead of assuming\nidealized distributions to carry out quenched averages analytically, we use a\nvariational Gaussian approximation for the replicated system in grand canonical\nformalism in which the data average can be deferred and replaced by empirical\naverages, leading to stationarity conditions that adaptively determine the\nparameters of the trial Hamiltonian for each dataset. This approach clarifies\nhow fluctuations affect information extraction and connects directly with the\nresults of mathematical statistics or learning theory such as information\ncriteria. As a concrete application, we analyze linear regression and derive\nlearning curves. This includes cases with real-world datasets, where exact\nreplica calculations are not feasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We revisit the replica method for analyzing inference and learning in\nparametric models, considering situations where the data-generating\ndistribution is unknown or analytically intractable. Instead of assuming\nidealized distributions to carry out quenched averages analytically, we use a\nvariational Gaussian approximation for the replicated system in grand canonical\nformalism in which the data average can be deferred and replaced by empirical\naverages, leading to stationarity conditions that adaptively determine the\nparameters of the trial Hamiltonian for each dataset. This approach clarifies\nhow fluctuations affect information extraction and connects directly with the\nresults of mathematical statistics or learning theory such as information\ncriteria. As a concrete application, we analyze linear regression and derive\nlearning curves. This includes cases with real-world datasets, where exact\nreplica calculations are not feasible."
                },
                "authors": [
                    {
                        "name": "Takashi Takahashi"
                    }
                ],
                "author_detail": {
                    "name": "Takashi Takahashi"
                },
                "author": "Takashi Takahashi",
                "arxiv_comment": "27 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11780v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11780v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.dis-nn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04854v1",
                "updated": "2025-10-06T14:40:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    40,
                    22,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T14:40:22Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    40,
                    22,
                    0,
                    279,
                    0
                ],
                "title": "Read the Room: Inferring Social Context Through Dyadic Interaction\n  Recognition in Cyber-physical-social Infrastructure Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Read the Room: Inferring Social Context Through Dyadic Interaction\n  Recognition in Cyber-physical-social Infrastructure Systems"
                },
                "summary": "Cyber-physical systems (CPS) integrate sensing, computing, and control to\nimprove infrastructure performance, focusing on economic goals like performance\nand safety. However, they often neglect potential human-centered (or\n''social'') benefits. Cyber-physical-social infrastructure systems (CPSIS) aim\nto address this by aligning CPS with social objectives. This involves defining\nsocial benefits, understanding human interactions with each other and\ninfrastructure, developing privacy-preserving measurement methods, modeling\nthese interactions for prediction, linking them to social benefits, and\nactuating the physical environment to foster positive social outcomes. This\npaper delves into recognizing dyadic human interactions using real-world data,\nwhich is the backbone to measuring social behavior. This lays a foundation to\naddress the need to enhance understanding of the deeper meanings and mutual\nresponses inherent in human interactions. While RGB cameras are informative for\ninteraction recognition, privacy concerns arise. Depth sensors offer a\nprivacy-conscious alternative by analyzing skeletal movements. This study\ncompares five skeleton-based interaction recognition algorithms on a dataset of\n12 dyadic interactions. Unlike single-person datasets, these interactions,\ncategorized into communication types like emblems and affect displays, offer\ninsights into the cultural and emotional aspects of human interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyber-physical systems (CPS) integrate sensing, computing, and control to\nimprove infrastructure performance, focusing on economic goals like performance\nand safety. However, they often neglect potential human-centered (or\n''social'') benefits. Cyber-physical-social infrastructure systems (CPSIS) aim\nto address this by aligning CPS with social objectives. This involves defining\nsocial benefits, understanding human interactions with each other and\ninfrastructure, developing privacy-preserving measurement methods, modeling\nthese interactions for prediction, linking them to social benefits, and\nactuating the physical environment to foster positive social outcomes. This\npaper delves into recognizing dyadic human interactions using real-world data,\nwhich is the backbone to measuring social behavior. This lays a foundation to\naddress the need to enhance understanding of the deeper meanings and mutual\nresponses inherent in human interactions. While RGB cameras are informative for\ninteraction recognition, privacy concerns arise. Depth sensors offer a\nprivacy-conscious alternative by analyzing skeletal movements. This study\ncompares five skeleton-based interaction recognition algorithms on a dataset of\n12 dyadic interactions. Unlike single-person datasets, these interactions,\ncategorized into communication types like emblems and affect displays, offer\ninsights into the cultural and emotional aspects of human interactions."
                },
                "authors": [
                    {
                        "name": "Cheyu Lin"
                    },
                    {
                        "name": "John Martins"
                    },
                    {
                        "name": "Katherine A. Flanigan"
                    },
                    {
                        "name": "Ph. D"
                    }
                ],
                "author_detail": {
                    "name": "Ph. D"
                },
                "author": "Ph. D",
                "arxiv_comment": "ASCE International Conference on Computing in Civil Engineering 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04852v1",
                "updated": "2025-10-06T14:39:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    39,
                    58,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T14:39:58Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    39,
                    58,
                    0,
                    279,
                    0
                ],
                "title": "FreshBrew: A Benchmark for Evaluating AI Agents on Java Code Migration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreshBrew: A Benchmark for Evaluating AI Agents on Java Code Migration"
                },
                "summary": "AI coding assistants are rapidly becoming integral to modern software\ndevelopment. A key challenge in this space is the continual need to migrate and\nmodernize codebases in response to evolving software ecosystems. Traditionally,\nsuch migrations have relied on rule-based systems and human intervention. With\nthe advent of powerful large language models (LLMs), AI-driven agentic\nframeworks offer a promising alternative-but their effectiveness has not been\nsystematically evaluated. In this paper, we introduce FreshBrew, a novel\nbenchmark for evaluating AI agents on project-level Java migrations, with a\nspecific focus on measuring an agent's ability to preserve program semantics\nand avoid reward hacking, which we argue requires projects with high test\ncoverage for a rigorous and reliable evaluation. We benchmark several\nstate-of-the-art LLMs, and compare their performance against established\nrule-based tools. Our evaluation of AI agents on this benchmark of 228\nrepositories shows that the top-performing model, Gemini 2.5 Flash, can\nsuccessfully migrate 52.3 percent of projects to JDK 17. Our empirical analysis\nreveals novel insights into the critical strengths and limitations of current\nagentic approaches, offering actionable insights into their real-world\napplicability. Our empirical study reveals failure modes of current AI agents\nin realistic Java modernization tasks, providing a foundation for evaluating\ntrustworthy code-migration systems. By releasing FreshBrew, we aim to\nfacilitate rigorous, reproducible evaluation and catalyze progress in AI-driven\ncodebase modernization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI coding assistants are rapidly becoming integral to modern software\ndevelopment. A key challenge in this space is the continual need to migrate and\nmodernize codebases in response to evolving software ecosystems. Traditionally,\nsuch migrations have relied on rule-based systems and human intervention. With\nthe advent of powerful large language models (LLMs), AI-driven agentic\nframeworks offer a promising alternative-but their effectiveness has not been\nsystematically evaluated. In this paper, we introduce FreshBrew, a novel\nbenchmark for evaluating AI agents on project-level Java migrations, with a\nspecific focus on measuring an agent's ability to preserve program semantics\nand avoid reward hacking, which we argue requires projects with high test\ncoverage for a rigorous and reliable evaluation. We benchmark several\nstate-of-the-art LLMs, and compare their performance against established\nrule-based tools. Our evaluation of AI agents on this benchmark of 228\nrepositories shows that the top-performing model, Gemini 2.5 Flash, can\nsuccessfully migrate 52.3 percent of projects to JDK 17. Our empirical analysis\nreveals novel insights into the critical strengths and limitations of current\nagentic approaches, offering actionable insights into their real-world\napplicability. Our empirical study reveals failure modes of current AI agents\nin realistic Java modernization tasks, providing a foundation for evaluating\ntrustworthy code-migration systems. By releasing FreshBrew, we aim to\nfacilitate rigorous, reproducible evaluation and catalyze progress in AI-driven\ncodebase modernization."
                },
                "authors": [
                    {
                        "name": "Victor May"
                    },
                    {
                        "name": "Diganta Misra"
                    },
                    {
                        "name": "Yanqi Luo"
                    },
                    {
                        "name": "Anjali Sridhar"
                    },
                    {
                        "name": "Justine Gehring"
                    },
                    {
                        "name": "Silvio Soares Ribeiro Junior"
                    }
                ],
                "author_detail": {
                    "name": "Silvio Soares Ribeiro Junior"
                },
                "author": "Silvio Soares Ribeiro Junior",
                "arxiv_comment": "18 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04851v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04851v1",
                "updated": "2025-10-06T14:39:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    39,
                    53,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T14:39:53Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    39,
                    53,
                    0,
                    279,
                    0
                ],
                "title": "LEGOMem: Modular Procedural Memory for Multi-agent LLM Systems for\n  Workflow Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEGOMem: Modular Procedural Memory for Multi-agent LLM Systems for\n  Workflow Automation"
                },
                "summary": "We introduce LEGOMem, a modular procedural memory framework for multi-agent\nlarge language model (LLM) systems in workflow automation. LEGOMem decomposes\npast task trajectories into reusable memory units and flexibly allocates them\nacross orchestrators and task agents to support planning and execution. To\nexplore the design space of memory in multi-agent systems, we use LEGOMem as a\nlens and conduct a systematic study of procedural memory in multi-agent\nsystems, examining where memory should be placed, how it should be retrieved,\nand which agents benefit most. Experiments on the OfficeBench benchmark show\nthat orchestrator memory is critical for effective task decomposition and\ndelegation, while fine-grained agent memory improves execution accuracy. We\nfind that even teams composed of smaller language models can benefit\nsubstantially from procedural memory, narrowing the performance gap with\nstronger agents by leveraging prior execution traces for more accurate planning\nand tool use. These results position LEGOMem as both a practical framework for\nmemory-augmented agent systems and a research tool for understanding memory\ndesign in multi-agent workflow automation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce LEGOMem, a modular procedural memory framework for multi-agent\nlarge language model (LLM) systems in workflow automation. LEGOMem decomposes\npast task trajectories into reusable memory units and flexibly allocates them\nacross orchestrators and task agents to support planning and execution. To\nexplore the design space of memory in multi-agent systems, we use LEGOMem as a\nlens and conduct a systematic study of procedural memory in multi-agent\nsystems, examining where memory should be placed, how it should be retrieved,\nand which agents benefit most. Experiments on the OfficeBench benchmark show\nthat orchestrator memory is critical for effective task decomposition and\ndelegation, while fine-grained agent memory improves execution accuracy. We\nfind that even teams composed of smaller language models can benefit\nsubstantially from procedural memory, narrowing the performance gap with\nstronger agents by leveraging prior execution traces for more accurate planning\nand tool use. These results position LEGOMem as both a practical framework for\nmemory-augmented agent systems and a research tool for understanding memory\ndesign in multi-agent workflow automation."
                },
                "authors": [
                    {
                        "name": "Dongge Han"
                    },
                    {
                        "name": "Camille Couturier"
                    },
                    {
                        "name": "Daniel Madrigal Diaz"
                    },
                    {
                        "name": "Xuchao Zhang"
                    },
                    {
                        "name": "Victor RÃ¼hle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04851v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04851v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04849v1",
                "updated": "2025-10-06T14:36:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    36,
                    30,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T14:36:30Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    36,
                    30,
                    0,
                    279,
                    0
                ],
                "title": "When Models Lie, We Learn: Multilingual Span-Level Hallucination\n  Detection with PsiloQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Models Lie, We Learn: Multilingual Span-Level Hallucination\n  Detection with PsiloQA"
                },
                "summary": "Hallucination detection remains a fundamental challenge for the safe and\nreliable deployment of large language models (LLMs), especially in applications\nrequiring factual accuracy. Existing hallucination benchmarks often operate at\nthe sequence level and are limited to English, lacking the fine-grained,\nmultilingual supervision needed for a comprehensive evaluation. In this work,\nwe introduce PsiloQA, a large-scale, multilingual dataset annotated with\nspan-level hallucinations across 14 languages. PsiloQA is constructed through\nan automated three-stage pipeline: generating question-answer pairs from\nWikipedia using GPT-4o, eliciting potentially hallucinated answers from diverse\nLLMs in a no-context setting, and automatically annotating hallucinated spans\nusing GPT-4o by comparing against golden answers and retrieved context. We\nevaluate a wide range of hallucination detection methods -- including\nuncertainty quantification, LLM-based tagging, and fine-tuned encoder models --\nand show that encoder-based models achieve the strongest performance across\nlanguages. Furthermore, PsiloQA demonstrates effective cross-lingual\ngeneralization and supports robust knowledge transfer to other benchmarks, all\nwhile being significantly more cost-efficient than human-annotated datasets.\nOur dataset and results advance the development of scalable, fine-grained\nhallucination detection in multilingual settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination detection remains a fundamental challenge for the safe and\nreliable deployment of large language models (LLMs), especially in applications\nrequiring factual accuracy. Existing hallucination benchmarks often operate at\nthe sequence level and are limited to English, lacking the fine-grained,\nmultilingual supervision needed for a comprehensive evaluation. In this work,\nwe introduce PsiloQA, a large-scale, multilingual dataset annotated with\nspan-level hallucinations across 14 languages. PsiloQA is constructed through\nan automated three-stage pipeline: generating question-answer pairs from\nWikipedia using GPT-4o, eliciting potentially hallucinated answers from diverse\nLLMs in a no-context setting, and automatically annotating hallucinated spans\nusing GPT-4o by comparing against golden answers and retrieved context. We\nevaluate a wide range of hallucination detection methods -- including\nuncertainty quantification, LLM-based tagging, and fine-tuned encoder models --\nand show that encoder-based models achieve the strongest performance across\nlanguages. Furthermore, PsiloQA demonstrates effective cross-lingual\ngeneralization and supports robust knowledge transfer to other benchmarks, all\nwhile being significantly more cost-efficient than human-annotated datasets.\nOur dataset and results advance the development of scalable, fine-grained\nhallucination detection in multilingual settings."
                },
                "authors": [
                    {
                        "name": "Elisei Rykov"
                    },
                    {
                        "name": "Kseniia Petrushina"
                    },
                    {
                        "name": "Maksim Savkin"
                    },
                    {
                        "name": "Valerii Olisov"
                    },
                    {
                        "name": "Artem Vazhentsev"
                    },
                    {
                        "name": "Kseniia Titova"
                    },
                    {
                        "name": "Alexander Panchenko"
                    },
                    {
                        "name": "Vasily Konovalov"
                    },
                    {
                        "name": "Julia Belikova"
                    }
                ],
                "author_detail": {
                    "name": "Julia Belikova"
                },
                "author": "Julia Belikova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04848v1",
                "updated": "2025-10-06T14:33:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    33,
                    38,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T14:33:38Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    33,
                    38,
                    0,
                    279,
                    0
                ],
                "title": "Instability in Downstream Task Performance During LLM Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instability in Downstream Task Performance During LLM Pretraining"
                },
                "summary": "When training large language models (LLMs), it is common practice to track\ndownstream task performance throughout the training process and select the\ncheckpoint with the highest validation score. However, downstream metrics often\nexhibit substantial fluctuations, making it difficult to identify the\ncheckpoint that truly represents the best-performing model. In this study, we\nempirically analyze the stability of downstream task performance in an LLM\ntrained on diverse web-scale corpora. We find that task scores frequently\nfluctuate throughout training, both at the aggregate and example levels. To\naddress this instability, we investigate two post-hoc checkpoint integration\nmethods: checkpoint averaging and ensemble, motivated by the hypothesis that\naggregating neighboring checkpoints can reduce performance volatility. We\ndemonstrate both empirically and theoretically that these methods improve\ndownstream performance stability without requiring any changes to the training\nprocedure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When training large language models (LLMs), it is common practice to track\ndownstream task performance throughout the training process and select the\ncheckpoint with the highest validation score. However, downstream metrics often\nexhibit substantial fluctuations, making it difficult to identify the\ncheckpoint that truly represents the best-performing model. In this study, we\nempirically analyze the stability of downstream task performance in an LLM\ntrained on diverse web-scale corpora. We find that task scores frequently\nfluctuate throughout training, both at the aggregate and example levels. To\naddress this instability, we investigate two post-hoc checkpoint integration\nmethods: checkpoint averaging and ensemble, motivated by the hypothesis that\naggregating neighboring checkpoints can reduce performance volatility. We\ndemonstrate both empirically and theoretically that these methods improve\ndownstream performance stability without requiring any changes to the training\nprocedure."
                },
                "authors": [
                    {
                        "name": "Yuto Nishida"
                    },
                    {
                        "name": "Masaru Isonuma"
                    },
                    {
                        "name": "Yusuke Oda"
                    }
                ],
                "author_detail": {
                    "name": "Yusuke Oda"
                },
                "author": "Yusuke Oda",
                "arxiv_comment": "Accepted to EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.06674v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.06674v5",
                "updated": "2025-10-06T14:33:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    33,
                    31,
                    0,
                    279,
                    0
                ],
                "published": "2024-02-07T14:23:01Z",
                "published_parsed": [
                    2024,
                    2,
                    7,
                    14,
                    23,
                    1,
                    2,
                    38,
                    0
                ],
                "title": "Impact of Dataset Properties on Membership Inference Vulnerability of\n  Deep Transfer Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impact of Dataset Properties on Membership Inference Vulnerability of\n  Deep Transfer Learning"
                },
                "summary": "Membership inference attacks (MIAs) are used to test practical privacy of\nmachine learning models. MIAs complement formal guarantees from differential\nprivacy (DP) under a more realistic adversary model. We analyse MIA\nvulnerability of fine-tuned neural networks both empirically and theoretically,\nthe latter using a simplified model of fine-tuning. We show that the\nvulnerability of non-DP models when measured as the attacker advantage at a\nfixed false positive rate reduces according to a simple power law as the number\nof examples per class increases. A similar power-law applies even for the most\nvulnerable points, but the dataset size needed for adequate protection of the\nmost vulnerable points is very large.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Membership inference attacks (MIAs) are used to test practical privacy of\nmachine learning models. MIAs complement formal guarantees from differential\nprivacy (DP) under a more realistic adversary model. We analyse MIA\nvulnerability of fine-tuned neural networks both empirically and theoretically,\nthe latter using a simplified model of fine-tuning. We show that the\nvulnerability of non-DP models when measured as the attacker advantage at a\nfixed false positive rate reduces according to a simple power law as the number\nof examples per class increases. A similar power-law applies even for the most\nvulnerable points, but the dataset size needed for adequate protection of the\nmost vulnerable points is very large."
                },
                "authors": [
                    {
                        "name": "Marlon Tobaben"
                    },
                    {
                        "name": "Hibiki Ito"
                    },
                    {
                        "name": "Joonas JÃ¤lkÃ¶"
                    },
                    {
                        "name": "Yuan He"
                    },
                    {
                        "name": "Antti Honkela"
                    }
                ],
                "author_detail": {
                    "name": "Antti Honkela"
                },
                "author": "Antti Honkela",
                "arxiv_comment": "Accepted to NeurIPS 2025; 47 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.06674v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.06674v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04844v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04844v1",
                "updated": "2025-10-06T14:31:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    31,
                    53,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T14:31:53Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    31,
                    53,
                    0,
                    279,
                    0
                ],
                "title": "From Actions to Kinesics: Extracting Human Psychological States through\n  Bodily Movements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Actions to Kinesics: Extracting Human Psychological States through\n  Bodily Movements"
                },
                "summary": "Understanding the dynamic relationship between humans and the built\nenvironment is a key challenge in disciplines ranging from environmental\npsychology to reinforcement learning (RL). A central obstacle in modeling these\ninteractions is the inability to capture human psychological states in a way\nthat is both generalizable and privacy preserving. Traditional methods rely on\ntheoretical models or questionnaires, which are limited in scope, static, and\nlabor intensive. We present a kinesics recognition framework that infers the\ncommunicative functions of human activity -- known as kinesics -- directly from\n3D skeleton joint data. Combining a spatial-temporal graph convolutional\nnetwork (ST-GCN) with a convolutional neural network (CNN), the framework\nleverages transfer learning to bypass the need for manually defined mappings\nbetween physical actions and psychological categories. The approach preserves\nuser anonymity while uncovering latent structures in bodily movements that\nreflect cognitive and emotional states. Our results on the Dyadic User\nEngagemenT (DUET) dataset demonstrate that this method enables scalable,\naccurate, and human-centered modeling of behavior, offering a new pathway for\nenhancing RL-driven simulations of human-environment interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the dynamic relationship between humans and the built\nenvironment is a key challenge in disciplines ranging from environmental\npsychology to reinforcement learning (RL). A central obstacle in modeling these\ninteractions is the inability to capture human psychological states in a way\nthat is both generalizable and privacy preserving. Traditional methods rely on\ntheoretical models or questionnaires, which are limited in scope, static, and\nlabor intensive. We present a kinesics recognition framework that infers the\ncommunicative functions of human activity -- known as kinesics -- directly from\n3D skeleton joint data. Combining a spatial-temporal graph convolutional\nnetwork (ST-GCN) with a convolutional neural network (CNN), the framework\nleverages transfer learning to bypass the need for manually defined mappings\nbetween physical actions and psychological categories. The approach preserves\nuser anonymity while uncovering latent structures in bodily movements that\nreflect cognitive and emotional states. Our results on the Dyadic User\nEngagemenT (DUET) dataset demonstrate that this method enables scalable,\naccurate, and human-centered modeling of behavior, offering a new pathway for\nenhancing RL-driven simulations of human-environment interaction."
                },
                "authors": [
                    {
                        "name": "Cheyu Lin"
                    },
                    {
                        "name": "Katherine A. Flanigan"
                    }
                ],
                "author_detail": {
                    "name": "Katherine A. Flanigan"
                },
                "author": "Katherine A. Flanigan",
                "arxiv_comment": "The 15th International Workshop on Structural Health Monitoring\n  (IWSHM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04844v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04844v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20836v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20836v4",
                "updated": "2025-10-06T14:29:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    29,
                    58,
                    0,
                    279,
                    0
                ],
                "published": "2025-07-28T13:44:21Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    13,
                    44,
                    21,
                    0,
                    209,
                    0
                ],
                "title": "First Hallucination Tokens Are Different from Conditional Ones",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "First Hallucination Tokens Are Different from Conditional Ones"
                },
                "summary": "Large Language Models (LLMs) hallucinate, and detecting these cases is key to\nensuring trust. While many approaches address hallucination detection at the\nresponse or span level, recent work explores token-level detection, enabling\nmore fine-grained intervention. However, the distribution of hallucination\nsignal across sequences of hallucinated tokens remains unexplored. We leverage\ntoken-level annotations from the RAGTruth corpus and find that the first\nhallucinated token is far more detectable than later ones. This structural\nproperty holds across models, suggesting that first hallucination tokens play a\nkey role in token-level hallucination detection. Our code is available at\nhttps://github.com/jakobsnl/RAGTruth_Xtended.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) hallucinate, and detecting these cases is key to\nensuring trust. While many approaches address hallucination detection at the\nresponse or span level, recent work explores token-level detection, enabling\nmore fine-grained intervention. However, the distribution of hallucination\nsignal across sequences of hallucinated tokens remains unexplored. We leverage\ntoken-level annotations from the RAGTruth corpus and find that the first\nhallucinated token is far more detectable than later ones. This structural\nproperty holds across models, suggesting that first hallucination tokens play a\nkey role in token-level hallucination detection. Our code is available at\nhttps://github.com/jakobsnl/RAGTruth_Xtended."
                },
                "authors": [
                    {
                        "name": "Jakob Snel"
                    },
                    {
                        "name": "Seong Joon Oh"
                    }
                ],
                "author_detail": {
                    "name": "Seong Joon Oh"
                },
                "author": "Seong Joon Oh",
                "arxiv_comment": "4.5 pages, 3 figures, Dataset, Knowledge Paper, Hallucination,\n  Trustworthiness",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20836v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20836v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04840v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04840v1",
                "updated": "2025-10-06T14:25:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    25,
                    3,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T14:25:03Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    25,
                    3,
                    0,
                    279,
                    0
                ],
                "title": "Detailed Aerial Mapping of Photovoltaic Power Plants Through\n  Semantically Significant Keypoints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detailed Aerial Mapping of Photovoltaic Power Plants Through\n  Semantically Significant Keypoints"
                },
                "summary": "An accurate and up-to-date model of a photovoltaic (PV) power plant is\nessential for its optimal operation and maintenance. However, such a model may\nnot be easily available. This work introduces a novel approach for PV power\nplant mapping based on aerial overview images. It enables the automation of the\nmapping process while removing the reliance on third-party data. The presented\nmapping method takes advantage of the structural layout of the power plants to\nachieve detailed modeling down to the level of individual PV modules. The\napproach relies on visual segmentation of PV modules in overview images and the\ninference of structural information in each image, assigning modules to\nindividual benches, rows, and columns. We identify visual keypoints related to\nthe layout and use these to merge detections from multiple images while\nmaintaining their structural integrity. The presented method was experimentally\nverified and evaluated on two different power plants. The final fusion of 3D\npositions and semantic structures results in a compact georeferenced model\nsuitable for power plant maintenance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An accurate and up-to-date model of a photovoltaic (PV) power plant is\nessential for its optimal operation and maintenance. However, such a model may\nnot be easily available. This work introduces a novel approach for PV power\nplant mapping based on aerial overview images. It enables the automation of the\nmapping process while removing the reliance on third-party data. The presented\nmapping method takes advantage of the structural layout of the power plants to\nachieve detailed modeling down to the level of individual PV modules. The\napproach relies on visual segmentation of PV modules in overview images and the\ninference of structural information in each image, assigning modules to\nindividual benches, rows, and columns. We identify visual keypoints related to\nthe layout and use these to merge detections from multiple images while\nmaintaining their structural integrity. The presented method was experimentally\nverified and evaluated on two different power plants. The final fusion of 3D\npositions and semantic structures results in a compact georeferenced model\nsuitable for power plant maintenance."
                },
                "authors": [
                    {
                        "name": "Viktor KozÃ¡k"
                    },
                    {
                        "name": "Jan Chudoba"
                    },
                    {
                        "name": "Libor PÅeuÄil"
                    }
                ],
                "author_detail": {
                    "name": "Libor PÅeuÄil"
                },
                "author": "Libor PÅeuÄil",
                "arxiv_comment": "10 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04840v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04840v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09199v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09199v3",
                "updated": "2025-10-06T14:19:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    19,
                    55,
                    0,
                    279,
                    0
                ],
                "published": "2025-07-12T08:42:10Z",
                "published_parsed": [
                    2025,
                    7,
                    12,
                    8,
                    42,
                    10,
                    5,
                    193,
                    0
                ],
                "title": "Back to the Basics: Rethinking Issue-Commit Linking with LLM-Assisted\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Back to the Basics: Rethinking Issue-Commit Linking with LLM-Assisted\n  Retrieval"
                },
                "summary": "Issue-commit linking, which connects issues with commits that fix them, is\ncrucial for software maintenance. Existing approaches have shown promise in\nautomatically recovering these links. Evaluations of these techniques assess\ntheir ability to identify genuine links from plausible but false links.\nHowever, these evaluations overlook the fact that, in reality, when a\nrepository has more commits, the presence of more plausible yet unrelated\ncommits may interfere with the tool in differentiating the correct fix commits.\nTo address this, we propose the Realistic Distribution Setting (RDS) and use it\nto construct a more realistic evaluation dataset that includes 20 open-source\nprojects. By evaluating tools on this dataset, we observe that the performance\nof the state-of-the-art deep learning-based approach drops by more than half,\nwhile the traditional Information Retrieval method, VSM, outperforms it.\nInspired by these observations, we propose EasyLink, which utilizes a vector\ndatabase as a modern Information Retrieval technique. To address the\nlong-standing problem of the semantic gap between issues and commits, EasyLink\nleverages a large language model to rerank the commits retrieved from the\ndatabase. Under our evaluation, EasyLink achieves an average Precision@1 of\n75.03\\%, improving over the state-of-the-art by over four times. Additionally,\nthis paper provides practical guidelines for advancing research in issue-commit\nlink recovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Issue-commit linking, which connects issues with commits that fix them, is\ncrucial for software maintenance. Existing approaches have shown promise in\nautomatically recovering these links. Evaluations of these techniques assess\ntheir ability to identify genuine links from plausible but false links.\nHowever, these evaluations overlook the fact that, in reality, when a\nrepository has more commits, the presence of more plausible yet unrelated\ncommits may interfere with the tool in differentiating the correct fix commits.\nTo address this, we propose the Realistic Distribution Setting (RDS) and use it\nto construct a more realistic evaluation dataset that includes 20 open-source\nprojects. By evaluating tools on this dataset, we observe that the performance\nof the state-of-the-art deep learning-based approach drops by more than half,\nwhile the traditional Information Retrieval method, VSM, outperforms it.\nInspired by these observations, we propose EasyLink, which utilizes a vector\ndatabase as a modern Information Retrieval technique. To address the\nlong-standing problem of the semantic gap between issues and commits, EasyLink\nleverages a large language model to rerank the commits retrieved from the\ndatabase. Under our evaluation, EasyLink achieves an average Precision@1 of\n75.03\\%, improving over the state-of-the-art by over four times. Additionally,\nthis paper provides practical guidelines for advancing research in issue-commit\nlink recovery."
                },
                "authors": [
                    {
                        "name": "Huihui Huang"
                    },
                    {
                        "name": "Ratnadira Widyasari"
                    },
                    {
                        "name": "Ting Zhang"
                    },
                    {
                        "name": "Ivana Clairine Irsan"
                    },
                    {
                        "name": "Jieke Shi"
                    },
                    {
                        "name": "Han Wei Ang"
                    },
                    {
                        "name": "Frank Liauw"
                    },
                    {
                        "name": "Eng Lieh Ouh"
                    },
                    {
                        "name": "Lwin Khin Shar"
                    },
                    {
                        "name": "Hong Jin Kang"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09199v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09199v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11723v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11723v2",
                "updated": "2025-10-06T14:15:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    15,
                    39,
                    0,
                    279,
                    0
                ],
                "published": "2025-02-17T12:10:25Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    12,
                    10,
                    25,
                    0,
                    48,
                    0
                ],
                "title": "Energy-Conscious LLM Decoding: Impact of Text Generation Strategies on\n  GPU Energy Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy-Conscious LLM Decoding: Impact of Text Generation Strategies on\n  GPU Energy Consumption"
                },
                "summary": "Decoding strategies significantly influence the quality and diversity of the\ngenerated text in Large Language Models (LLMs), yet their impact on\ncomputational resources, particularly GPU energy consumption, is insufficiently\nstudied. This paper investigates the relationship between text generation\ndecoding techniques and energy efficiency, focusing on the trade-off between\ngeneration quality and GPU energy usage across diverse tasks and decoding\nconfigurations. By benchmarking multiple strategies across various tasks,\nincluding Translation, Math Problem Solving, Coding, and Open-ended text\ngeneration, we reveal how selecting appropriate decoding techniques with their\ntuned hyperparameters affects text quality and has measurable implications for\nenergy consumption. Our findings show that the choice of decoding strategy can\ngreatly impact GPU energy usage, even when it has a minimal effect on output\nquality. Different strategies also involve trade-offs between quality and\nenergy efficiency, and no single decoding method is best in all cases across\nevery metric. To the best of our knowledge, this is one of the first studies to\nexamine decoding strategies in LLMs from the perspective of energy consumption,\nproviding useful insights for building energy-efficient applications without\ncompromising text generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoding strategies significantly influence the quality and diversity of the\ngenerated text in Large Language Models (LLMs), yet their impact on\ncomputational resources, particularly GPU energy consumption, is insufficiently\nstudied. This paper investigates the relationship between text generation\ndecoding techniques and energy efficiency, focusing on the trade-off between\ngeneration quality and GPU energy usage across diverse tasks and decoding\nconfigurations. By benchmarking multiple strategies across various tasks,\nincluding Translation, Math Problem Solving, Coding, and Open-ended text\ngeneration, we reveal how selecting appropriate decoding techniques with their\ntuned hyperparameters affects text quality and has measurable implications for\nenergy consumption. Our findings show that the choice of decoding strategy can\ngreatly impact GPU energy usage, even when it has a minimal effect on output\nquality. Different strategies also involve trade-offs between quality and\nenergy efficiency, and no single decoding method is best in all cases across\nevery metric. To the best of our knowledge, this is one of the first studies to\nexamine decoding strategies in LLMs from the perspective of energy consumption,\nproviding useful insights for building energy-efficient applications without\ncompromising text generation quality."
                },
                "authors": [
                    {
                        "name": "Alireza Nik"
                    },
                    {
                        "name": "Michael A. Riegler"
                    },
                    {
                        "name": "PÃ¥l Halvorsen"
                    }
                ],
                "author_detail": {
                    "name": "PÃ¥l Halvorsen"
                },
                "author": "PÃ¥l Halvorsen",
                "arxiv_comment": "Updated version with additional models and benchmark datasets. The\n  experimental section has been expanded with new analyses, and minor\n  corrections and clarifications have been made throughout the text",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11723v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11723v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00072v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00072v2",
                "updated": "2025-10-06T14:10:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    10,
                    14,
                    0,
                    279,
                    0
                ],
                "published": "2025-08-26T16:41:37Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    16,
                    41,
                    37,
                    1,
                    238,
                    0
                ],
                "title": "Beyond Memorization: Reasoning-Driven Synthesis as a Mitigation Strategy\n  Against Benchmark Contamination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Memorization: Reasoning-Driven Synthesis as a Mitigation Strategy\n  Against Benchmark Contamination"
                },
                "summary": "Capability evaluation of large language models (LLMs) is increasingly\nshadowed by rising concerns of data contamination that cast doubts on whether\nstatic benchmarks measure genuine reasoning or mere memorization. We present an\nempirical study using an infinitely scalable framework to synthesize\nresearch-level QA directly from arXiv papers, harnessing the natural temporal\nstructure of research publications where performance decay after knowledge\ncutoffs may indicate potential contamination. We evaluated 4 frontier model\nrepresented by 2 models of different knowledge cutoff dates per family on 1,643\nmulti-step reasoning questions synthesized from 20,277 arXiv papers stratified\nover 26 months, covering at least 6 months before and after all cutoff dates.\nOur results consistently showed a lack of significant performance decay near\nknowledge cutoff dates for models of various sizes, developers, and release\ndates. We further performed a comparative analysis with previous longitudinal\nstudies that reported significant post-cutoff performance decay using directly\nretrieved questions based on public data. we hypothesize that the multi-step\nreasoning required by our synthesis pipeline offered additional complexity that\ngoes deeper than shallow memorization, which effectively serves a mitigation\nstrategy against benchmark contamination. We fully open source our code and\ndataset to aid reproducibility and advocate for a paradigm shift that\nprioritize reasoning-driven synthesis to construct benchmarks over simply\ncollecting newly released questions periodically.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capability evaluation of large language models (LLMs) is increasingly\nshadowed by rising concerns of data contamination that cast doubts on whether\nstatic benchmarks measure genuine reasoning or mere memorization. We present an\nempirical study using an infinitely scalable framework to synthesize\nresearch-level QA directly from arXiv papers, harnessing the natural temporal\nstructure of research publications where performance decay after knowledge\ncutoffs may indicate potential contamination. We evaluated 4 frontier model\nrepresented by 2 models of different knowledge cutoff dates per family on 1,643\nmulti-step reasoning questions synthesized from 20,277 arXiv papers stratified\nover 26 months, covering at least 6 months before and after all cutoff dates.\nOur results consistently showed a lack of significant performance decay near\nknowledge cutoff dates for models of various sizes, developers, and release\ndates. We further performed a comparative analysis with previous longitudinal\nstudies that reported significant post-cutoff performance decay using directly\nretrieved questions based on public data. we hypothesize that the multi-step\nreasoning required by our synthesis pipeline offered additional complexity that\ngoes deeper than shallow memorization, which effectively serves a mitigation\nstrategy against benchmark contamination. We fully open source our code and\ndataset to aid reproducibility and advocate for a paradigm shift that\nprioritize reasoning-driven synthesis to construct benchmarks over simply\ncollecting newly released questions periodically."
                },
                "authors": [
                    {
                        "name": "Terry Jingchen Zhang"
                    },
                    {
                        "name": "Gopal Dev"
                    },
                    {
                        "name": "Ning Wang"
                    },
                    {
                        "name": "Nicole Ni"
                    },
                    {
                        "name": "Wenyuan Jiang"
                    },
                    {
                        "name": "Yinya Huang"
                    },
                    {
                        "name": "Bernhard SchÃ¶lkopf"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Zhijing Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zhijing Jin"
                },
                "author": "Zhijing Jin",
                "arxiv_comment": "The authors choose to withdraw this manuscript as it constitutes\n  incomplete work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00072v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00072v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22777v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22777v4",
                "updated": "2025-10-06T14:06:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    6,
                    40,
                    0,
                    279,
                    0
                ],
                "published": "2025-05-28T18:45:42Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    18,
                    45,
                    42,
                    2,
                    148,
                    0
                ],
                "title": "MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain\n  Dialogue Evaluators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain\n  Dialogue Evaluators"
                },
                "summary": "Evaluating the quality of open-domain chatbots has become increasingly\nreliant on LLMs acting as automatic judges. However, existing meta-evaluation\nbenchmarks are static, outdated, and lacking in multilingual coverage, limiting\ntheir ability to fully capture subtle weaknesses in evaluation. We introduce\nMEDAL, an automated multi-agent framework for curating more representative and\ndiverse open-domain dialogue evaluation benchmarks. Our approach leverages\nseveral state-of-the-art LLMs to generate user-chatbot multilingual dialogues,\nconditioned on varied seed contexts. Then, a strong LLM (GPT-4.1) is used for a\nmultidimensional analysis of the performance of the chatbots, uncovering\nnoticeable cross-lingual performance differences. Guided by this large-scale\nevaluation, we curate a new meta-evaluation multilingual benchmark and\nhuman-annotate samples with nuanced quality judgments. This benchmark is then\nused to assess the ability of several reasoning and non-reasoning LLMs to act\nas evaluators of open-domain dialogues. Using MEDAL, we uncover that\nstate-of-the-art judges fail to reliably detect nuanced issues such as lack of\nempathy, commonsense, or relevance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the quality of open-domain chatbots has become increasingly\nreliant on LLMs acting as automatic judges. However, existing meta-evaluation\nbenchmarks are static, outdated, and lacking in multilingual coverage, limiting\ntheir ability to fully capture subtle weaknesses in evaluation. We introduce\nMEDAL, an automated multi-agent framework for curating more representative and\ndiverse open-domain dialogue evaluation benchmarks. Our approach leverages\nseveral state-of-the-art LLMs to generate user-chatbot multilingual dialogues,\nconditioned on varied seed contexts. Then, a strong LLM (GPT-4.1) is used for a\nmultidimensional analysis of the performance of the chatbots, uncovering\nnoticeable cross-lingual performance differences. Guided by this large-scale\nevaluation, we curate a new meta-evaluation multilingual benchmark and\nhuman-annotate samples with nuanced quality judgments. This benchmark is then\nused to assess the ability of several reasoning and non-reasoning LLMs to act\nas evaluators of open-domain dialogues. Using MEDAL, we uncover that\nstate-of-the-art judges fail to reliably detect nuanced issues such as lack of\nempathy, commonsense, or relevance."
                },
                "authors": [
                    {
                        "name": "John MendonÃ§a"
                    },
                    {
                        "name": "Alon Lavie"
                    },
                    {
                        "name": "Isabel Trancoso"
                    }
                ],
                "author_detail": {
                    "name": "Isabel Trancoso"
                },
                "author": "Isabel Trancoso",
                "arxiv_comment": "October ARR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22777v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22777v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03102v2",
                "updated": "2025-10-06T14:04:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    4,
                    39,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-03T15:31:11Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    31,
                    11,
                    4,
                    276,
                    0
                ],
                "title": "Semantic Similarity in Radiology Reports via LLMs and NER",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Similarity in Radiology Reports via LLMs and NER"
                },
                "summary": "Radiology report evaluation is a crucial part of radiologists' training and\nplays a key role in ensuring diagnostic accuracy. As part of the standard\nreporting workflow, a junior radiologist typically prepares a preliminary\nreport, which is then reviewed and edited by a senior radiologist to produce\nthe final report. Identifying semantic differences between preliminary and\nfinal reports is essential for junior doctors, both as a training tool and to\nhelp uncover gaps in clinical knowledge. While AI in radiology is a rapidly\ngrowing field, the application of large language models (LLMs) remains\nchallenging due to the need for specialised domain knowledge. In this paper, we\nexplore the ability of LLMs to provide explainable and accurate comparisons of\nreports in the radiology domain. We begin by comparing the performance of\nseveral LLMs in comparing radiology reports. We then assess a more traditional\napproach based on Named-Entity-Recognition (NER). However, both approaches\nexhibit limitations in delivering accurate feedback on semantic similarity. To\naddress this, we propose Llama-EntScore, a semantic similarity scoring method\nusing a combination of Llama 3.1 and NER with tunable weights to emphasise or\nde-emphasise specific types of differences. Our approach generates a\nquantitative similarity score for tracking progress and also gives an\ninterpretation of the score that aims to offer valuable guidance in reviewing\nand refining their reporting. We find our method achieves 67% exact-match\naccuracy and 93% accuracy within +/- 1 when compared to radiologist-provided\nground truth scores - outperforming both LLMs and NER used independently. Code\nis available at: https://github.com/otmive/llama_reports",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radiology report evaluation is a crucial part of radiologists' training and\nplays a key role in ensuring diagnostic accuracy. As part of the standard\nreporting workflow, a junior radiologist typically prepares a preliminary\nreport, which is then reviewed and edited by a senior radiologist to produce\nthe final report. Identifying semantic differences between preliminary and\nfinal reports is essential for junior doctors, both as a training tool and to\nhelp uncover gaps in clinical knowledge. While AI in radiology is a rapidly\ngrowing field, the application of large language models (LLMs) remains\nchallenging due to the need for specialised domain knowledge. In this paper, we\nexplore the ability of LLMs to provide explainable and accurate comparisons of\nreports in the radiology domain. We begin by comparing the performance of\nseveral LLMs in comparing radiology reports. We then assess a more traditional\napproach based on Named-Entity-Recognition (NER). However, both approaches\nexhibit limitations in delivering accurate feedback on semantic similarity. To\naddress this, we propose Llama-EntScore, a semantic similarity scoring method\nusing a combination of Llama 3.1 and NER with tunable weights to emphasise or\nde-emphasise specific types of differences. Our approach generates a\nquantitative similarity score for tracking progress and also gives an\ninterpretation of the score that aims to offer valuable guidance in reviewing\nand refining their reporting. We find our method achieves 67% exact-match\naccuracy and 93% accuracy within +/- 1 when compared to radiologist-provided\nground truth scores - outperforming both LLMs and NER used independently. Code\nis available at: https://github.com/otmive/llama_reports"
                },
                "authors": [
                    {
                        "name": "Beth Pearson"
                    },
                    {
                        "name": "Ahmed Adnan"
                    },
                    {
                        "name": "Zahraa S. Abdallah"
                    }
                ],
                "author_detail": {
                    "name": "Zahraa S. Abdallah"
                },
                "author": "Zahraa S. Abdallah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04817v1",
                "updated": "2025-10-06T14:00:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    0,
                    2,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T14:00:02Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    0,
                    2,
                    0,
                    279,
                    0
                ],
                "title": "Natural Language Edge Labelling: Decoupling Intent from Execution in\n  Structured LM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Edge Labelling: Decoupling Intent from Execution in\n  Structured LM Reasoning"
                },
                "summary": "Controllers for structured LM reasoning (e.g., Chain-of-Thought,\nself-consistency, and Tree-of-Thoughts) often entangle what to try next with\nhow to execute it, exposing only coarse global knobs and yielding brittle,\ncompute-inefficient, and hard-to-audit behavior. We introduce Natural Language\nEdge Labelling (NLEL), a labeller-tuner overlay that attaches a free-form\nnatural-language directive to each search edge and translates it into a\nschema-bounded control vector for decoding, search (branch quotas, exploration\n$\\beta$), generation bundle size, retrieval mixtures, and verification passes.\nA labeller $\\Lambda$ emits labels from the parent state and a compact context;\na tuner $\\Psi$ maps $(P, L, C)\\to \\Pi$, with strict schema validation and\ntrust-region projection around safe defaults. Downstream selection remains\nToT-style with score $S=\\mu+\\beta\\sigma$ and depth-annealed $\\beta$. We show\nNLEL strictly generalizes CoT/ToT, prove an anytime-monotonicity property for\ntop-$k$ selection under label-conditioned bundles, and bound selector shortfall\nby control-vector distortion, providing decision-relevant justification for\nguards like trust regions and verification passes. We instantiate $\\Psi$ as a\nprompt-only JSON Parameter Emitter and preregister an evaluation on GSM8K, MATH\n(subset), StrategyQA, and ARC-Challenge with compute-aware reporting\n(success@compute, tokens-per-success) and ablations over $\\Lambda$, $\\Psi$,\ntrust-region radius, and control quantization; preregistered forecasts\nanticipate accuracy gains at comparable token budgets and improved\nsuccess@compute under constraints. NLEL offers an interpretable, model-agnostic\ninterface that separates intent from execution for controllable, auditable LM\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controllers for structured LM reasoning (e.g., Chain-of-Thought,\nself-consistency, and Tree-of-Thoughts) often entangle what to try next with\nhow to execute it, exposing only coarse global knobs and yielding brittle,\ncompute-inefficient, and hard-to-audit behavior. We introduce Natural Language\nEdge Labelling (NLEL), a labeller-tuner overlay that attaches a free-form\nnatural-language directive to each search edge and translates it into a\nschema-bounded control vector for decoding, search (branch quotas, exploration\n$\\beta$), generation bundle size, retrieval mixtures, and verification passes.\nA labeller $\\Lambda$ emits labels from the parent state and a compact context;\na tuner $\\Psi$ maps $(P, L, C)\\to \\Pi$, with strict schema validation and\ntrust-region projection around safe defaults. Downstream selection remains\nToT-style with score $S=\\mu+\\beta\\sigma$ and depth-annealed $\\beta$. We show\nNLEL strictly generalizes CoT/ToT, prove an anytime-monotonicity property for\ntop-$k$ selection under label-conditioned bundles, and bound selector shortfall\nby control-vector distortion, providing decision-relevant justification for\nguards like trust regions and verification passes. We instantiate $\\Psi$ as a\nprompt-only JSON Parameter Emitter and preregister an evaluation on GSM8K, MATH\n(subset), StrategyQA, and ARC-Challenge with compute-aware reporting\n(success@compute, tokens-per-success) and ablations over $\\Lambda$, $\\Psi$,\ntrust-region radius, and control quantization; preregistered forecasts\nanticipate accuracy gains at comparable token budgets and improved\nsuccess@compute under constraints. NLEL offers an interpretable, model-agnostic\ninterface that separates intent from execution for controllable, auditable LM\ninference."
                },
                "authors": [
                    {
                        "name": "Abhinav Madahar"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Madahar"
                },
                "author": "Abhinav Madahar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04816v1",
                "updated": "2025-10-06T13:57:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    13,
                    57,
                    49,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T13:57:49Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    13,
                    57,
                    49,
                    0,
                    279,
                    0
                ],
                "title": "On Predicting Post-Click Conversion Rate via Counterfactual Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Predicting Post-Click Conversion Rate via Counterfactual Inference"
                },
                "summary": "Accurately predicting conversion rate (CVR) is essential in various\nrecommendation domains such as online advertising systems and e-commerce. These\nsystems utilize user interaction logs, which consist of exposures, clicks, and\nconversions. CVR prediction models are typically trained solely based on\nclicked samples, as conversions can only be determined following clicks.\nHowever, the sparsity of clicked instances necessitates the collection of a\nsubstantial amount of logs for effective model training. Recent works address\nthis issue by devising frameworks that leverage non-clicked samples. While\nthese frameworks aim to reduce biases caused by the discrepancy between clicked\nand non-clicked samples, they often rely on heuristics. Against this\nbackground, we propose a method to counterfactually generate conversion labels\nfor non-clicked samples by using causality as a guiding principle, attempting\nto answer the question, \"Would the user have converted if he or she had clicked\nthe recommended item?\" Our approach is named the Entire Space Counterfactual\nInference Multi-task Model (ESCIM). We initially train a structural causal\nmodel (SCM) of user sequential behaviors and conduct a hypothetical\nintervention (i.e., click) on non-clicked items to infer counterfactual CVRs.\nWe then introduce several approaches to transform predicted counterfactual CVRs\ninto binary counterfactual conversion labels for the non-clicked samples.\nFinally, the generated samples are incorporated into the training process.\nExtensive experiments on public datasets illustrate the superiority of the\nproposed algorithm. Online A/B testing further empirically validates the\neffectiveness of our proposed algorithm in real-world scenarios. In addition,\nwe demonstrate the improved performance of the proposed method on latent\nconversion data, showcasing its robustness and superior generalization\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately predicting conversion rate (CVR) is essential in various\nrecommendation domains such as online advertising systems and e-commerce. These\nsystems utilize user interaction logs, which consist of exposures, clicks, and\nconversions. CVR prediction models are typically trained solely based on\nclicked samples, as conversions can only be determined following clicks.\nHowever, the sparsity of clicked instances necessitates the collection of a\nsubstantial amount of logs for effective model training. Recent works address\nthis issue by devising frameworks that leverage non-clicked samples. While\nthese frameworks aim to reduce biases caused by the discrepancy between clicked\nand non-clicked samples, they often rely on heuristics. Against this\nbackground, we propose a method to counterfactually generate conversion labels\nfor non-clicked samples by using causality as a guiding principle, attempting\nto answer the question, \"Would the user have converted if he or she had clicked\nthe recommended item?\" Our approach is named the Entire Space Counterfactual\nInference Multi-task Model (ESCIM). We initially train a structural causal\nmodel (SCM) of user sequential behaviors and conduct a hypothetical\nintervention (i.e., click) on non-clicked items to infer counterfactual CVRs.\nWe then introduce several approaches to transform predicted counterfactual CVRs\ninto binary counterfactual conversion labels for the non-clicked samples.\nFinally, the generated samples are incorporated into the training process.\nExtensive experiments on public datasets illustrate the superiority of the\nproposed algorithm. Online A/B testing further empirically validates the\neffectiveness of our proposed algorithm in real-world scenarios. In addition,\nwe demonstrate the improved performance of the proposed method on latent\nconversion data, showcasing its robustness and superior generalization\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Junhyung Ahn"
                    },
                    {
                        "name": "Sanghack Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sanghack Lee"
                },
                "author": "Sanghack Lee",
                "arxiv_comment": "This work has been accepted for publication at the IEEE International\n  Conference on Data Mining (ICDM) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01004v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01004v2",
                "updated": "2025-10-06T13:46:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    13,
                    46,
                    47,
                    0,
                    279,
                    0
                ],
                "published": "2025-08-01T18:15:08Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    18,
                    15,
                    8,
                    4,
                    213,
                    0
                ],
                "title": "Quantum Annealing in SK Model Employing Suzuki-Kubo-deGennes Quantum\n  Ising Mean Field Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Annealing in SK Model Employing Suzuki-Kubo-deGennes Quantum\n  Ising Mean Field Dynamics"
                },
                "summary": "We study a quantum annealing approach for estimating the ground state energy\nof the Sherrington-Kirpatrick mean field spin glass model using the Suzuki-Kubo\ndynamics applied for individual local magnetization components. The solutions\nof the coupled differential equations, in discretized state, give a fast\nannealing algorithm (cost $N^3$) in estimating the ground state of the model:\nClassical ($E^0= -0.7629 \\pm 0.0002$), Quantum ($E^0=-0.7623 \\pm 0.0001$) and\nMixed ($E^0=-0.7626 \\pm 0.0001$), all of which are to be compared with the best\nknown estimate $E^0= -0.763166726 \\dots$ . We infer that the continuous nature\nof the magnetization variable used in the dynamics here is the reason for\nreaching close to the ground state quickly and also the reason for not\nobserving the de-Almeida-Thouless line in this approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study a quantum annealing approach for estimating the ground state energy\nof the Sherrington-Kirpatrick mean field spin glass model using the Suzuki-Kubo\ndynamics applied for individual local magnetization components. The solutions\nof the coupled differential equations, in discretized state, give a fast\nannealing algorithm (cost $N^3$) in estimating the ground state of the model:\nClassical ($E^0= -0.7629 \\pm 0.0002$), Quantum ($E^0=-0.7623 \\pm 0.0001$) and\nMixed ($E^0=-0.7626 \\pm 0.0001$), all of which are to be compared with the best\nknown estimate $E^0= -0.763166726 \\dots$ . We infer that the continuous nature\nof the magnetization variable used in the dynamics here is the reason for\nreaching close to the ground state quickly and also the reason for not\nobserving the de-Almeida-Thouless line in this approach."
                },
                "authors": [
                    {
                        "name": "Soumyaditya Das"
                    },
                    {
                        "name": "Soumyajyoti Biswas"
                    },
                    {
                        "name": "Bikas K. Chakrabarti"
                    }
                ],
                "author_detail": {
                    "name": "Bikas K. Chakrabarti"
                },
                "author": "Bikas K. Chakrabarti",
                "arxiv_comment": "7 pages, 7 figures, 2 tables. Invited contribution for the Special\n  Issue on \"100 Glorious years of the Ising model\" in Eur. Phys. J. B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01004v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01004v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.dis-nn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17098v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17098v3",
                "updated": "2025-10-06T13:42:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    13,
                    42,
                    58,
                    0,
                    279,
                    0
                ],
                "published": "2025-05-21T05:22:21Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    5,
                    22,
                    21,
                    2,
                    141,
                    0
                ],
                "title": "TACO: Enhancing Multimodal In-context Learning via Task Mapping-Guided\n  Sequence Configuration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TACO: Enhancing Multimodal In-context Learning via Task Mapping-Guided\n  Sequence Configuration"
                },
                "summary": "Multimodal in-context learning (ICL) has emerged as a key mechanism for\nharnessing the capabilities of large vision-language models (LVLMs). However,\nits effectiveness remains highly sensitive to the quality of input ICL\nsequences, particularly for tasks involving complex reasoning or open-ended\ngeneration. A major limitation is our limited understanding of how LVLMs\nactually exploit these sequences during inference. To bridge this gap, we\nsystematically interpret multimodal ICL through the lens of task mapping, which\nreveals how local and global relationships within and among demonstrations\nguide model reasoning. Building on this insight, we present TACO, a lightweight\ntransformer-based model equipped with task-aware attention that dynamically\nconfigures ICL sequences. By injecting task-mapping signals into the\nautoregressive decoding process, TACO creates a bidirectional synergy between\nsequence construction and task reasoning. Experiments on five LVLMs and nine\ndatasets demonstrate that TACO consistently surpasses baselines across diverse\nICL tasks. These results position task mapping as a novel and valuable\nperspective for interpreting and improving multimodal ICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal in-context learning (ICL) has emerged as a key mechanism for\nharnessing the capabilities of large vision-language models (LVLMs). However,\nits effectiveness remains highly sensitive to the quality of input ICL\nsequences, particularly for tasks involving complex reasoning or open-ended\ngeneration. A major limitation is our limited understanding of how LVLMs\nactually exploit these sequences during inference. To bridge this gap, we\nsystematically interpret multimodal ICL through the lens of task mapping, which\nreveals how local and global relationships within and among demonstrations\nguide model reasoning. Building on this insight, we present TACO, a lightweight\ntransformer-based model equipped with task-aware attention that dynamically\nconfigures ICL sequences. By injecting task-mapping signals into the\nautoregressive decoding process, TACO creates a bidirectional synergy between\nsequence construction and task reasoning. Experiments on five LVLMs and nine\ndatasets demonstrate that TACO consistently surpasses baselines across diverse\nICL tasks. These results position task mapping as a novel and valuable\nperspective for interpreting and improving multimodal ICL."
                },
                "authors": [
                    {
                        "name": "Yanshu Li"
                    },
                    {
                        "name": "Jianjiang Yang"
                    },
                    {
                        "name": "Tian Yun"
                    },
                    {
                        "name": "Pinyuan Feng"
                    },
                    {
                        "name": "Jinfa Huang"
                    },
                    {
                        "name": "Ruixiang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Ruixiang Tang"
                },
                "author": "Ruixiang Tang",
                "arxiv_comment": "EMNLP2025 Main, 28 pages, 11 figures, 19 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17098v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17098v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04800v1",
                "updated": "2025-10-06T13:30:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    13,
                    30,
                    7,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T13:30:07Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    13,
                    30,
                    7,
                    0,
                    279,
                    0
                ],
                "title": "Hybrid Architectures for Language Models: Systematic Analysis and Design\n  Insights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Architectures for Language Models: Systematic Analysis and Design\n  Insights"
                },
                "summary": "Recent progress in large language models demonstrates that hybrid\narchitectures--combining self-attention mechanisms with structured state space\nmodels like Mamba--can achieve a compelling balance between modeling quality\nand computational efficiency, particularly for long-context tasks. While these\nhybrid models show promising performance, systematic comparisons of\nhybridization strategies and analyses on the key factors behind their\neffectiveness have not been clearly shared to the community. In this work, we\npresent a holistic evaluation of hybrid architectures based on inter-layer\n(sequential) or intra-layer (parallel) fusion. We evaluate these designs from a\nvariety of perspectives: language modeling performance, long-context\ncapabilities, scaling analysis, and training and inference efficiency. By\ninvestigating the core characteristics of their computational primitive, we\nidentify the most critical elements for each hybridization strategy and further\npropose optimal design recipes for both hybrid models. Our comprehensive\nanalysis provides practical guidance and valuable insights for developing\nhybrid language models, facilitating the optimization of architectural\nconfigurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in large language models demonstrates that hybrid\narchitectures--combining self-attention mechanisms with structured state space\nmodels like Mamba--can achieve a compelling balance between modeling quality\nand computational efficiency, particularly for long-context tasks. While these\nhybrid models show promising performance, systematic comparisons of\nhybridization strategies and analyses on the key factors behind their\neffectiveness have not been clearly shared to the community. In this work, we\npresent a holistic evaluation of hybrid architectures based on inter-layer\n(sequential) or intra-layer (parallel) fusion. We evaluate these designs from a\nvariety of perspectives: language modeling performance, long-context\ncapabilities, scaling analysis, and training and inference efficiency. By\ninvestigating the core characteristics of their computational primitive, we\nidentify the most critical elements for each hybridization strategy and further\npropose optimal design recipes for both hybrid models. Our comprehensive\nanalysis provides practical guidance and valuable insights for developing\nhybrid language models, facilitating the optimization of architectural\nconfigurations."
                },
                "authors": [
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Bilge Acun"
                    },
                    {
                        "name": "Haroun Habeeb"
                    },
                    {
                        "name": "Seungyeon Kim"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Liang Luo"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Carole-Jean Wu"
                    }
                ],
                "author_detail": {
                    "name": "Carole-Jean Wu"
                },
                "author": "Carole-Jean Wu",
                "arxiv_comment": "17 pages, 4 figures, 6 tables; detailed results will be included in\n  the Appendix later",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19341v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19341v2",
                "updated": "2025-10-06T13:23:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    13,
                    23,
                    4,
                    0,
                    279,
                    0
                ],
                "published": "2025-09-16T09:14:15Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    14,
                    15,
                    1,
                    259,
                    0
                ],
                "title": "Fine-Grained AI Model Caching and Downloading With Coordinated\n  Multipoint Broadcasting in Multi-Cell Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained AI Model Caching and Downloading With Coordinated\n  Multipoint Broadcasting in Multi-Cell Edge Networks"
                },
                "summary": "6G networks are envisioned to support on-demand AI model downloading to\naccommodate diverse inference requirements of end users. By proactively caching\nmodels at edge nodes, users can retrieve the requested models with low latency\nfor on-device AI inference. However, the substantial size of contemporary AI\nmodels poses significant challenges for edge caching under limited storage\ncapacity, as well as for the concurrent delivery of heterogeneous models over\nwireless channels. To address these challenges, we propose a fine-grained AI\nmodel caching and downloading system that exploits parameter reusability,\nstemming from the common practice of fine-tuning task-specific models from a\nshared pre-trained model with frozen parameters. This system selectively caches\nmodel parameter blocks (PBs) at edge nodes, eliminating redundant storage of\nreusable parameters across different cached models. Additionally, it\nincorporates coordinated multipoint (CoMP) broadcasting to simultaneously\ndeliver reusable PBs to multiple users, thereby enhancing downlink spectrum\nutilization. Under this arrangement, we formulate a model downloading delay\nminimization problem to jointly optimize PB caching, migration (among edge\nnodes), and broadcasting beamforming. To tackle this intractable problem, we\ndevelop a distributed multi-agent learning framework that enables edge nodes to\nexplicitly learn mutual influence among their actions, thereby facilitating\ncooperation. Furthermore, a data augmentation approach is proposed to\nadaptively generate synthetic training samples through a predictive model,\nboosting sample efficiency and accelerating policy learning. Both theoretical\nanalysis and simulation experiments validate the superior convergence\nperformance of the proposed learning framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6G networks are envisioned to support on-demand AI model downloading to\naccommodate diverse inference requirements of end users. By proactively caching\nmodels at edge nodes, users can retrieve the requested models with low latency\nfor on-device AI inference. However, the substantial size of contemporary AI\nmodels poses significant challenges for edge caching under limited storage\ncapacity, as well as for the concurrent delivery of heterogeneous models over\nwireless channels. To address these challenges, we propose a fine-grained AI\nmodel caching and downloading system that exploits parameter reusability,\nstemming from the common practice of fine-tuning task-specific models from a\nshared pre-trained model with frozen parameters. This system selectively caches\nmodel parameter blocks (PBs) at edge nodes, eliminating redundant storage of\nreusable parameters across different cached models. Additionally, it\nincorporates coordinated multipoint (CoMP) broadcasting to simultaneously\ndeliver reusable PBs to multiple users, thereby enhancing downlink spectrum\nutilization. Under this arrangement, we formulate a model downloading delay\nminimization problem to jointly optimize PB caching, migration (among edge\nnodes), and broadcasting beamforming. To tackle this intractable problem, we\ndevelop a distributed multi-agent learning framework that enables edge nodes to\nexplicitly learn mutual influence among their actions, thereby facilitating\ncooperation. Furthermore, a data augmentation approach is proposed to\nadaptively generate synthetic training samples through a predictive model,\nboosting sample efficiency and accelerating policy learning. Both theoretical\nanalysis and simulation experiments validate the superior convergence\nperformance of the proposed learning framework."
                },
                "authors": [
                    {
                        "name": "Yang Fu"
                    },
                    {
                        "name": "Peng Qin"
                    },
                    {
                        "name": "Yueyue Zhang"
                    },
                    {
                        "name": "Pao Cheng"
                    },
                    {
                        "name": "Jun Lu"
                    },
                    {
                        "name": "Yifei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yifei Wang"
                },
                "author": "Yifei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19341v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19341v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04796v1",
                "updated": "2025-10-06T13:22:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    13,
                    22,
                    10,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T13:22:10Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    13,
                    22,
                    10,
                    0,
                    279,
                    0
                ],
                "title": "RevMine: An LLM-Assisted Tool for Code Review Mining and Analysis Across\n  Git Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RevMine: An LLM-Assisted Tool for Code Review Mining and Analysis Across\n  Git Platforms"
                },
                "summary": "Empirical research on code review processes is increasingly central to\nunderstanding software quality and collaboration. However, collecting and\nanalyzing review data remains a time-consuming and technically intensive task.\nMost researchers follow similar workflows - writing ad hoc scripts to extract,\nfilter, and analyze review data from platforms like GitHub and GitLab. This\npaper introduces RevMine, a conceptual tool that streamlines the entire code\nreview mining pipeline using large language models (LLMs). RevMine guides users\nthrough authentication, endpoint discovery, and natural language-driven data\ncollection, significantly reducing the need for manual scripting. After\nretrieving review data, it supports both quantitative and qualitative analysis\nbased on user-defined filters or LLM-inferred patterns. This poster outlines\nthe tool's architecture, use cases, and research potential. By lowering the\nbarrier to entry, RevMine aims to democratize code review mining and enable a\nbroader range of empirical software engineering studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empirical research on code review processes is increasingly central to\nunderstanding software quality and collaboration. However, collecting and\nanalyzing review data remains a time-consuming and technically intensive task.\nMost researchers follow similar workflows - writing ad hoc scripts to extract,\nfilter, and analyze review data from platforms like GitHub and GitLab. This\npaper introduces RevMine, a conceptual tool that streamlines the entire code\nreview mining pipeline using large language models (LLMs). RevMine guides users\nthrough authentication, endpoint discovery, and natural language-driven data\ncollection, significantly reducing the need for manual scripting. After\nretrieving review data, it supports both quantitative and qualitative analysis\nbased on user-defined filters or LLM-inferred patterns. This poster outlines\nthe tool's architecture, use cases, and research potential. By lowering the\nbarrier to entry, RevMine aims to democratize code review mining and enable a\nbroader range of empirical software engineering studies."
                },
                "authors": [
                    {
                        "name": "Samah Kansab"
                    },
                    {
                        "name": "Francis Bordeleau"
                    },
                    {
                        "name": "Ali Tizghadam"
                    }
                ],
                "author_detail": {
                    "name": "Ali Tizghadam"
                },
                "author": "Ali Tizghadam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04792v1",
                "updated": "2025-10-06T13:16:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    13,
                    16,
                    1,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T13:16:01Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    13,
                    16,
                    1,
                    0,
                    279,
                    0
                ],
                "title": "Hybrid-Balance GFlowNet for Solving Vehicle Routing Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid-Balance GFlowNet for Solving Vehicle Routing Problems"
                },
                "summary": "Existing GFlowNet-based methods for vehicle routing problems (VRPs) typically\nemploy Trajectory Balance (TB) to achieve global optimization but often neglect\nimportant aspects of local optimization. While Detailed Balance (DB) addresses\nlocal optimization more effectively, it alone falls short in solving VRPs,\nwhich inherently require holistic trajectory optimization. To address these\nlimitations, we introduce the Hybrid-Balance GFlowNet (HBG) framework, which\nuniquely integrates TB and DB in a principled and adaptive manner by aligning\ntheir intrinsically complementary strengths. Additionally, we propose a\nspecialized inference strategy for depot-centric scenarios like the Capacitated\nVehicle Routing Problem (CVRP), leveraging the depot node's greater flexibility\nin selecting successors. Despite this specialization, HBG maintains broad\napplicability, extending effectively to problems without explicit depots, such\nas the Traveling Salesman Problem (TSP). We evaluate HBG by integrating it into\ntwo established GFlowNet-based solvers, i.e., AGFN and GFACS, and demonstrate\nconsistent and significant improvements across both CVRP and TSP, underscoring\nthe enhanced solution quality and generalization afforded by our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing GFlowNet-based methods for vehicle routing problems (VRPs) typically\nemploy Trajectory Balance (TB) to achieve global optimization but often neglect\nimportant aspects of local optimization. While Detailed Balance (DB) addresses\nlocal optimization more effectively, it alone falls short in solving VRPs,\nwhich inherently require holistic trajectory optimization. To address these\nlimitations, we introduce the Hybrid-Balance GFlowNet (HBG) framework, which\nuniquely integrates TB and DB in a principled and adaptive manner by aligning\ntheir intrinsically complementary strengths. Additionally, we propose a\nspecialized inference strategy for depot-centric scenarios like the Capacitated\nVehicle Routing Problem (CVRP), leveraging the depot node's greater flexibility\nin selecting successors. Despite this specialization, HBG maintains broad\napplicability, extending effectively to problems without explicit depots, such\nas the Traveling Salesman Problem (TSP). We evaluate HBG by integrating it into\ntwo established GFlowNet-based solvers, i.e., AGFN and GFACS, and demonstrate\nconsistent and significant improvements across both CVRP and TSP, underscoring\nthe enhanced solution quality and generalization afforded by our approach."
                },
                "authors": [
                    {
                        "name": "Ni Zhang"
                    },
                    {
                        "name": "Zhiguang Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zhiguang Cao"
                },
                "author": "Zhiguang Cao",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04791v1",
                "updated": "2025-10-06T13:15:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    13,
                    15,
                    24,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T13:15:24Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    13,
                    15,
                    24,
                    0,
                    279,
                    0
                ],
                "title": "GUISpector: An MLLM Agent Framework for Automated Verification of\n  Natural Language Requirements in GUI Prototypes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUISpector: An MLLM Agent Framework for Automated Verification of\n  Natural Language Requirements in GUI Prototypes"
                },
                "summary": "GUIs are foundational to interactive systems and play a pivotal role in early\nrequirements elicitation through prototyping. Ensuring that GUI implementations\nfulfill NL requirements is essential for robust software engineering,\nespecially as LLM-driven programming agents become increasingly integrated into\ndevelopment workflows. Existing GUI testing approaches, whether traditional or\nLLM-driven, often fall short in handling the complexity of modern interfaces,\nand typically lack actionable feedback and effective integration with automated\ndevelopment agents. In this paper, we introduce GUISpector, a novel framework\nthat leverages a multi-modal (M)LLM-based agent for the automated verification\nof NL requirements in GUI prototypes. First, GUISpector adapts a MLLM agent to\ninterpret and operationalize NL requirements, enabling to autonomously plan and\nexecute verification trajectories across GUI applications. Second, GUISpector\nsystematically extracts detailed NL feedback from the agent's verification\nprocess, providing developers with actionable insights that can be used to\niteratively refine the GUI artifact or directly inform LLM-based code\ngeneration in a closed feedback loop. Third, we present an integrated tool that\nunifies these capabilities, offering practitioners an accessible interface for\nsupervising verification runs, inspecting agent rationales and managing the\nend-to-end requirements verification process. We evaluated GUISpector on a\ncomprehensive set of 150 requirements based on 900 acceptance criteria\nannotations across diverse GUI applications, demonstrating effective detection\nof requirement satisfaction and violations and highlighting its potential for\nseamless integration of actionable feedback into automated LLM-driven\ndevelopment workflows. The video presentation of GUISpector is available at:\nhttps://youtu.be/JByYF6BNQeE, showcasing its main capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUIs are foundational to interactive systems and play a pivotal role in early\nrequirements elicitation through prototyping. Ensuring that GUI implementations\nfulfill NL requirements is essential for robust software engineering,\nespecially as LLM-driven programming agents become increasingly integrated into\ndevelopment workflows. Existing GUI testing approaches, whether traditional or\nLLM-driven, often fall short in handling the complexity of modern interfaces,\nand typically lack actionable feedback and effective integration with automated\ndevelopment agents. In this paper, we introduce GUISpector, a novel framework\nthat leverages a multi-modal (M)LLM-based agent for the automated verification\nof NL requirements in GUI prototypes. First, GUISpector adapts a MLLM agent to\ninterpret and operationalize NL requirements, enabling to autonomously plan and\nexecute verification trajectories across GUI applications. Second, GUISpector\nsystematically extracts detailed NL feedback from the agent's verification\nprocess, providing developers with actionable insights that can be used to\niteratively refine the GUI artifact or directly inform LLM-based code\ngeneration in a closed feedback loop. Third, we present an integrated tool that\nunifies these capabilities, offering practitioners an accessible interface for\nsupervising verification runs, inspecting agent rationales and managing the\nend-to-end requirements verification process. We evaluated GUISpector on a\ncomprehensive set of 150 requirements based on 900 acceptance criteria\nannotations across diverse GUI applications, demonstrating effective detection\nof requirement satisfaction and violations and highlighting its potential for\nseamless integration of actionable feedback into automated LLM-driven\ndevelopment workflows. The video presentation of GUISpector is available at:\nhttps://youtu.be/JByYF6BNQeE, showcasing its main capabilities."
                },
                "authors": [
                    {
                        "name": "Kristian Kolthoff"
                    },
                    {
                        "name": "Felix Kretzer"
                    },
                    {
                        "name": "Simone Paolo Ponzetto"
                    },
                    {
                        "name": "Alexander Maedche"
                    },
                    {
                        "name": "Christian Bartelt"
                    }
                ],
                "author_detail": {
                    "name": "Christian Bartelt"
                },
                "author": "Christian Bartelt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.05095v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05095v1",
                "updated": "2025-10-06T17:58:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    58,
                    1,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T17:58:01Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    58,
                    1,
                    0,
                    279,
                    0
                ],
                "title": "From Noisy Traces to Stable Gradients: Bias-Variance Optimized\n  Preference Optimization for Aligning Large Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Noisy Traces to Stable Gradients: Bias-Variance Optimized\n  Preference Optimization for Aligning Large Reasoning Models"
                },
                "summary": "Large reasoning models (LRMs) generate intermediate reasoning traces before\nproducing final answers, yielding strong gains on multi-step and mathematical\ntasks. Yet aligning LRMs with human preferences, a crucial prerequisite for\nmodel deployment, remains underexplored. The statistically correct objective\nfor preference alignment requires marginalizing over reasoning traces, but this\ncomputation is intractable in practice. A common workaround optimizes a single\nsampled trajectory, which introduces substantial gradient variance from\nstochastic trace sampling. To address this challenge, we frame preference\noptimization for LRMs through the lens of the bias--variance trade-off and\npropose Bias--Variance Optimized Preference Optimization (BVPO), a simple,\ndrop-in method that mixes two gradient estimators: a high-variance trace-based\nestimator and a low-variance empty-trace estimator obtained by disabling\nreasoning trace generation. Our theory shows that BVPO strictly reduces\ntrace-induced variance for any nontrivial mixture, provides a closed-form\nchoice of the mixing weight that minimizes mean-squared error relative to the\ntrue marginal gradient, and under standard smoothness and step-size conditions,\ntightens classical convergence bounds for stochastic gradient descent.\nEmpirically, BVPO improves alignment over the best baseline by up to 7.8 points\non AlpacaEval~2 and 6.8 points on Arena-Hard. Despite being trained only on\ngeneral conversational data, BVPO also boosts reasoning performance for base\nmodels by up to 4.0 points on the average of six math reasoning benchmarks.\nThese results identify variance from trace sampling as a key bottleneck and\ndemonstrate that directly optimizing the bias--variance trade-off yields more\nstable training and stronger overall performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large reasoning models (LRMs) generate intermediate reasoning traces before\nproducing final answers, yielding strong gains on multi-step and mathematical\ntasks. Yet aligning LRMs with human preferences, a crucial prerequisite for\nmodel deployment, remains underexplored. The statistically correct objective\nfor preference alignment requires marginalizing over reasoning traces, but this\ncomputation is intractable in practice. A common workaround optimizes a single\nsampled trajectory, which introduces substantial gradient variance from\nstochastic trace sampling. To address this challenge, we frame preference\noptimization for LRMs through the lens of the bias--variance trade-off and\npropose Bias--Variance Optimized Preference Optimization (BVPO), a simple,\ndrop-in method that mixes two gradient estimators: a high-variance trace-based\nestimator and a low-variance empty-trace estimator obtained by disabling\nreasoning trace generation. Our theory shows that BVPO strictly reduces\ntrace-induced variance for any nontrivial mixture, provides a closed-form\nchoice of the mixing weight that minimizes mean-squared error relative to the\ntrue marginal gradient, and under standard smoothness and step-size conditions,\ntightens classical convergence bounds for stochastic gradient descent.\nEmpirically, BVPO improves alignment over the best baseline by up to 7.8 points\non AlpacaEval~2 and 6.8 points on Arena-Hard. Despite being trained only on\ngeneral conversational data, BVPO also boosts reasoning performance for base\nmodels by up to 4.0 points on the average of six math reasoning benchmarks.\nThese results identify variance from trace sampling as a key bottleneck and\ndemonstrate that directly optimizing the bias--variance trade-off yields more\nstable training and stronger overall performance."
                },
                "authors": [
                    {
                        "name": "Mingkang Zhu"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Bei Yu"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    },
                    {
                        "name": "Jiaya Jia"
                    }
                ],
                "author_detail": {
                    "name": "Jiaya Jia"
                },
                "author": "Jiaya Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05095v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05095v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01928v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01928v3",
                "updated": "2025-10-06T17:57:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    57,
                    15,
                    0,
                    279,
                    0
                ],
                "published": "2024-12-02T19:30:36Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    19,
                    30,
                    36,
                    0,
                    337,
                    0
                ],
                "title": "MALT: Improving Reasoning with Multi-Agent LLM Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MALT: Improving Reasoning with Multi-Agent LLM Training"
                },
                "summary": "Large Language Models (LLMs) often produce answers with a single\nchain-of-thought, which restricts their ability to explore reasoning paths or\nself-correct flawed outputs in complex tasks. In this paper, we introduce MALT\n(Multi-Agent LLM Training), a novel post-training strategy that divides the\nreasoning process into generation, verification, and refinement steps using a\nsequential pipeline of heterogeneous agents. During data generation, each agent\nis repeatedly sampled to form a multi-agent search tree, where final outputs\nare graded against ground-truth data. We then apply value iteration to\npropagate reward signals back to each role-conditioned model, automatically\nproducing multi-agent post-training data without human or teacher-model\nsupervision. Our off-policy approach allows each agent to specialize by\nlearning from correct and incorrect trajectories, ultimately improving the\nend-to-end reasoning chain. On MATH, GSM8K, and CSQA, MALT surpasses the same\nbaseline LLM with a relative improvement of 15.66%, 7.42%, and 9.40%\nrespectively, making it an important advance towards multi-agent cooperative\ntraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often produce answers with a single\nchain-of-thought, which restricts their ability to explore reasoning paths or\nself-correct flawed outputs in complex tasks. In this paper, we introduce MALT\n(Multi-Agent LLM Training), a novel post-training strategy that divides the\nreasoning process into generation, verification, and refinement steps using a\nsequential pipeline of heterogeneous agents. During data generation, each agent\nis repeatedly sampled to form a multi-agent search tree, where final outputs\nare graded against ground-truth data. We then apply value iteration to\npropagate reward signals back to each role-conditioned model, automatically\nproducing multi-agent post-training data without human or teacher-model\nsupervision. Our off-policy approach allows each agent to specialize by\nlearning from correct and incorrect trajectories, ultimately improving the\nend-to-end reasoning chain. On MATH, GSM8K, and CSQA, MALT surpasses the same\nbaseline LLM with a relative improvement of 15.66%, 7.42%, and 9.40%\nrespectively, making it an important advance towards multi-agent cooperative\ntraining."
                },
                "authors": [
                    {
                        "name": "Sumeet Ramesh Motwani"
                    },
                    {
                        "name": "Chandler Smith"
                    },
                    {
                        "name": "Rocktim Jyoti Das"
                    },
                    {
                        "name": "Rafael Rafailov"
                    },
                    {
                        "name": "Ivan Laptev"
                    },
                    {
                        "name": "Philip H. S. Torr"
                    },
                    {
                        "name": "Fabio Pizzati"
                    },
                    {
                        "name": "Ronald Clark"
                    },
                    {
                        "name": "Christian Schroeder de Witt"
                    }
                ],
                "author_detail": {
                    "name": "Christian Schroeder de Witt"
                },
                "author": "Christian Schroeder de Witt",
                "arxiv_comment": "Published at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01928v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01928v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05087v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05087v1",
                "updated": "2025-10-06T17:55:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    55,
                    4,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T17:55:04Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    55,
                    4,
                    0,
                    279,
                    0
                ],
                "title": "TeachLM: Post-Training LLMs for Education Using Authentic Learning Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TeachLM: Post-Training LLMs for Education Using Authentic Learning Data"
                },
                "summary": "The promise of generative AI to revolutionize education is constrained by the\npedagogical limits of large language models (LLMs). A major issue is the lack\nof access to high-quality training data that reflect the learning of actual\nstudents. Prompt engineering has emerged as a stopgap, but the ability of\nprompts to encode complex pedagogical strategies in rule-based natural language\nis inherently limited. To address this gap we introduce TeachLM - an LLM\noptimized for teaching through parameter-efficient fine-tuning of\nstate-of-the-art models. TeachLM is trained on a dataset comprised of 100,000\nhours of one-on-one, longitudinal student-tutor interactions maintained by\nPolygence, which underwent a rigorous anonymization process to protect privacy.\nWe use parameter-efficient fine-tuning to develop an authentic student model\nthat enables the generation of high-fidelity synthetic student-tutor dialogues.\nBuilding on this capability, we propose a novel multi-turn evaluation protocol\nthat leverages synthetic dialogue generation to provide fast, scalable, and\nreproducible assessments of the dialogical capabilities of LLMs. Our\nevaluations demonstrate that fine-tuning on authentic learning data\nsignificantly improves conversational and pedagogical performance - doubling\nstudent talk time, improving questioning style, increasing dialogue turns by\n50%, and greater personalization of instruction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promise of generative AI to revolutionize education is constrained by the\npedagogical limits of large language models (LLMs). A major issue is the lack\nof access to high-quality training data that reflect the learning of actual\nstudents. Prompt engineering has emerged as a stopgap, but the ability of\nprompts to encode complex pedagogical strategies in rule-based natural language\nis inherently limited. To address this gap we introduce TeachLM - an LLM\noptimized for teaching through parameter-efficient fine-tuning of\nstate-of-the-art models. TeachLM is trained on a dataset comprised of 100,000\nhours of one-on-one, longitudinal student-tutor interactions maintained by\nPolygence, which underwent a rigorous anonymization process to protect privacy.\nWe use parameter-efficient fine-tuning to develop an authentic student model\nthat enables the generation of high-fidelity synthetic student-tutor dialogues.\nBuilding on this capability, we propose a novel multi-turn evaluation protocol\nthat leverages synthetic dialogue generation to provide fast, scalable, and\nreproducible assessments of the dialogical capabilities of LLMs. Our\nevaluations demonstrate that fine-tuning on authentic learning data\nsignificantly improves conversational and pedagogical performance - doubling\nstudent talk time, improving questioning style, increasing dialogue turns by\n50%, and greater personalization of instruction."
                },
                "authors": [
                    {
                        "name": "Janos Perczel"
                    },
                    {
                        "name": "Jin Chow"
                    },
                    {
                        "name": "Dorottya Demszky"
                    }
                ],
                "author_detail": {
                    "name": "Dorottya Demszky"
                },
                "author": "Dorottya Demszky",
                "arxiv_comment": "28 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05087v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05087v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20666v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20666v3",
                "updated": "2025-10-06T17:52:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    52,
                    34,
                    0,
                    279,
                    0
                ],
                "published": "2025-06-25T17:58:12Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    58,
                    12,
                    2,
                    176,
                    0
                ],
                "title": "Using cognitive models to reveal value trade-offs in language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using cognitive models to reveal value trade-offs in language models"
                },
                "summary": "Value trade-offs are an integral part of human decision-making and language\nuse, however, current tools for interpreting such dynamic and multi-faceted\nnotions of values in LLMs are limited. In cognitive science, so-called\n\"cognitive models\" provide formal accounts of such trade-offs in humans, by\nmodeling the weighting of a speaker's competing utility functions in choosing\nan action or utterance. Here we use a leading cognitive model of polite speech\nto systematically evaluate value trade-offs in two encompassing model settings:\ndegrees of reasoning \"effort\" in frontier black-box models, and RL\npost-training dynamics of open-source models. Our results highlight patterns of\nhigher informational utility than social utility in reasoning models' default\nbehavior, and demonstrate that these patterns shift in predictable ways when\nmodels are prompted to prioritize certain goals over others. Our findings from\nLLMs' training dynamics suggest large shifts in utility values early on in\ntraining with persistent effects of the choice of base model and pretraining\ndata, compared to feedback dataset or alignment method. Our framework offers a\nflexible tool for probing value trade-offs across diverse model types,\nproviding insights for generating hypotheses about other social behaviors such\nas sycophancy and for shaping training regimes that better control trade-offs\nbetween values during model development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value trade-offs are an integral part of human decision-making and language\nuse, however, current tools for interpreting such dynamic and multi-faceted\nnotions of values in LLMs are limited. In cognitive science, so-called\n\"cognitive models\" provide formal accounts of such trade-offs in humans, by\nmodeling the weighting of a speaker's competing utility functions in choosing\nan action or utterance. Here we use a leading cognitive model of polite speech\nto systematically evaluate value trade-offs in two encompassing model settings:\ndegrees of reasoning \"effort\" in frontier black-box models, and RL\npost-training dynamics of open-source models. Our results highlight patterns of\nhigher informational utility than social utility in reasoning models' default\nbehavior, and demonstrate that these patterns shift in predictable ways when\nmodels are prompted to prioritize certain goals over others. Our findings from\nLLMs' training dynamics suggest large shifts in utility values early on in\ntraining with persistent effects of the choice of base model and pretraining\ndata, compared to feedback dataset or alignment method. Our framework offers a\nflexible tool for probing value trade-offs across diverse model types,\nproviding insights for generating hypotheses about other social behaviors such\nas sycophancy and for shaping training regimes that better control trade-offs\nbetween values during model development."
                },
                "authors": [
                    {
                        "name": "Sonia K. Murthy"
                    },
                    {
                        "name": "Rosie Zhao"
                    },
                    {
                        "name": "Jennifer Hu"
                    },
                    {
                        "name": "Sham Kakade"
                    },
                    {
                        "name": "Markus Wulfmeier"
                    },
                    {
                        "name": "Peng Qian"
                    },
                    {
                        "name": "Tomer Ullman"
                    }
                ],
                "author_detail": {
                    "name": "Tomer Ullman"
                },
                "author": "Tomer Ullman",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20666v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20666v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10924v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10924v6",
                "updated": "2025-10-06T17:52:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    52,
                    33,
                    0,
                    279,
                    0
                ],
                "published": "2024-12-14T18:18:52Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    18,
                    18,
                    52,
                    5,
                    349,
                    0
                ],
                "title": "Tokens, the oft-overlooked appetizer: Large language models, the\n  distributional hypothesis, and meaning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokens, the oft-overlooked appetizer: Large language models, the\n  distributional hypothesis, and meaning"
                },
                "summary": "Tokenization is a necessary component within the current architecture of many\nlanguage models, including the transformer-based large language models (LLMs)\nof Generative AI, yet its impact on the model's cognition is often overlooked.\nWe argue that LLMs demonstrate that the Distributional Hypothesis (DH) is\nsufficient for reasonably human-like language performance, and that the\nemergence of human-meaningful linguistic units among tokens and current\nstructural constraints motivate changes to existing, linguistically-agnostic\ntokenization techniques, particularly with respect to their roles as (1)\nsemantic primitives and as (2) vehicles for conveying salient distributional\npatterns from human language to the model. We explore tokenizations from a BPE\ntokenizer; extant model vocabularies obtained from Hugging Face and tiktoken;\nand the information in exemplar token vectors as they move through the layers\nof a RoBERTa (large) model. Besides creating sub-optimal semantic building\nblocks and obscuring the model's access to the necessary distributional\npatterns, we describe how tokens and pretraining can act as a backdoor for bias\nand other unwanted content, which current alignment practices may not\nremediate. Additionally, we relay evidence that the tokenization algorithm's\nobjective function impacts the LLM's cognition, despite being arguably\nmeaningfully insulated from the main system intelligence. [First uploaded to\narXiv in December, 2024.]",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization is a necessary component within the current architecture of many\nlanguage models, including the transformer-based large language models (LLMs)\nof Generative AI, yet its impact on the model's cognition is often overlooked.\nWe argue that LLMs demonstrate that the Distributional Hypothesis (DH) is\nsufficient for reasonably human-like language performance, and that the\nemergence of human-meaningful linguistic units among tokens and current\nstructural constraints motivate changes to existing, linguistically-agnostic\ntokenization techniques, particularly with respect to their roles as (1)\nsemantic primitives and as (2) vehicles for conveying salient distributional\npatterns from human language to the model. We explore tokenizations from a BPE\ntokenizer; extant model vocabularies obtained from Hugging Face and tiktoken;\nand the information in exemplar token vectors as they move through the layers\nof a RoBERTa (large) model. Besides creating sub-optimal semantic building\nblocks and obscuring the model's access to the necessary distributional\npatterns, we describe how tokens and pretraining can act as a backdoor for bias\nand other unwanted content, which current alignment practices may not\nremediate. Additionally, we relay evidence that the tokenization algorithm's\nobjective function impacts the LLM's cognition, despite being arguably\nmeaningfully insulated from the main system intelligence. [First uploaded to\narXiv in December, 2024.]"
                },
                "authors": [
                    {
                        "name": "Julia Witte Zimmerman"
                    },
                    {
                        "name": "Denis Hudon"
                    },
                    {
                        "name": "Kathryn Cramer"
                    },
                    {
                        "name": "Alejandro J. Ruiz"
                    },
                    {
                        "name": "Calla Beauregard"
                    },
                    {
                        "name": "Ashley Fehr"
                    },
                    {
                        "name": "Mikaela Irene Fudolig"
                    },
                    {
                        "name": "Bradford Demarest"
                    },
                    {
                        "name": "Yoshi Meke Bird"
                    },
                    {
                        "name": "Milo Z. Trujillo"
                    },
                    {
                        "name": "Christopher M. Danforth"
                    },
                    {
                        "name": "Peter Sheridan Dodds"
                    }
                ],
                "author_detail": {
                    "name": "Peter Sheridan Dodds"
                },
                "author": "Peter Sheridan Dodds",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10924v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10924v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05084v1",
                "updated": "2025-10-06T17:51:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    51,
                    46,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T17:51:46Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    51,
                    46,
                    0,
                    279,
                    0
                ],
                "title": "Electrospray Thruster Plume Impingement on CubeSat Solar Arrays: A\n  Particle-Tracking Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electrospray Thruster Plume Impingement on CubeSat Solar Arrays: A\n  Particle-Tracking Study"
                },
                "summary": "Electrospray thrusters are emerging as a leading propulsion technology for\nCubeSats, offering high specific impulse ($I_{sp} > 1000$ s) and low power\nrequirements. However, the divergent ion plumes can impinge on spacecraft\nsurfaces, particularly body-mounted solar arrays, causing contamination and\nthrust efficiency losses. This study presents a validated particle-tracking\nsimulation to quantify the effects of thruster placement on thrust efficiency\nand surface contamination for 1U, 3U, and 6U CubeSats. The plume model employs\na cosine power distribution ($k=1.8$) with half-angle $46^\\circ$, validated\nagainst experimental data with errors below 7%. Results show that thrust\nefficiency ranges from 53.6% for rear-mounted thrusters on 3U body-mounted\nconfigurations to 100% for side-mounted configurations with deployable arrays.\nCubeSat size significantly affects impingement: 3U platforms experience 46.4%\ncontamination with rear-mounted thrusters compared to 16.6% for 1U. Deployable\nsolar arrays reduce contamination by 77% compared to body-mounted arrays, while\nside-mounted thrusters eliminate impingement entirely at the cost of only 1.6%\nefficiency loss. Corner-mounted configurations at $30^\\circ$ cant provide\nintermediate performance with 88.9% efficiency and 11.1% contamination. These\nquantitative design guidelines enable mission planners to optimize thruster\nintegration based on power budget and propellant mass constraints, with\nstatistical uncertainty below 0.15% across all configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electrospray thrusters are emerging as a leading propulsion technology for\nCubeSats, offering high specific impulse ($I_{sp} > 1000$ s) and low power\nrequirements. However, the divergent ion plumes can impinge on spacecraft\nsurfaces, particularly body-mounted solar arrays, causing contamination and\nthrust efficiency losses. This study presents a validated particle-tracking\nsimulation to quantify the effects of thruster placement on thrust efficiency\nand surface contamination for 1U, 3U, and 6U CubeSats. The plume model employs\na cosine power distribution ($k=1.8$) with half-angle $46^\\circ$, validated\nagainst experimental data with errors below 7%. Results show that thrust\nefficiency ranges from 53.6% for rear-mounted thrusters on 3U body-mounted\nconfigurations to 100% for side-mounted configurations with deployable arrays.\nCubeSat size significantly affects impingement: 3U platforms experience 46.4%\ncontamination with rear-mounted thrusters compared to 16.6% for 1U. Deployable\nsolar arrays reduce contamination by 77% compared to body-mounted arrays, while\nside-mounted thrusters eliminate impingement entirely at the cost of only 1.6%\nefficiency loss. Corner-mounted configurations at $30^\\circ$ cant provide\nintermediate performance with 88.9% efficiency and 11.1% contamination. These\nquantitative design guidelines enable mission planners to optimize thruster\nintegration based on power budget and propellant mass constraints, with\nstatistical uncertainty below 0.15% across all configurations."
                },
                "authors": [
                    {
                        "name": "Ethan Kahn"
                    }
                ],
                "author_detail": {
                    "name": "Ethan Kahn"
                },
                "arxiv_affiliation": "Unaffiliated, ZÃ¼rich, Switzerland",
                "author": "Ethan Kahn",
                "arxiv_comment": "16 pages, 3 figures, Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05069v1",
                "updated": "2025-10-06T17:46:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    46,
                    34,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T17:46:34Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    46,
                    34,
                    0,
                    279,
                    0
                ],
                "title": "SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior\n  Reasoning LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior\n  Reasoning LLMs"
                },
                "summary": "Recent work shows that, beyond discrete reasoning through explicit\nchain-of-thought steps, which are limited by the boundaries of natural\nlanguages, large language models (LLMs) can also reason continuously in latent\nspace, allowing richer information per step and thereby improving token\nefficiency. Despite this promise, latent reasoning still faces two challenges,\nespecially in training-free settings: 1) purely latent reasoning broadens the\nsearch distribution by maintaining multiple implicit paths, which diffuses\nprobability mass, introduces noise, and impedes convergence to a single\nhigh-confidence solution, thereby hurting accuracy; and 2) overthinking\npersists even without explicit text, wasting tokens and degrading efficiency.\nTo address these issues, we introduce SwiReasoning, a training-free framework\nfor LLM reasoning which features two key innovations: 1) SwiReasoning\ndynamically switches between explicit and latent reasoning, guided by\nblock-wise confidence estimated from entropy trends in next-token\ndistributions, to balance exploration and exploitation and promote timely\nconvergence. 2) By limiting the maximum number of thinking-block switches,\nSwiReasoning curbs overthinking and improves token efficiency across varying\nproblem difficulties. On widely used mathematics and STEM benchmarks,\nSwiReasoning consistently improves average accuracy by 1.5%-2.8% across\nreasoning LLMs of different model families and scales. Furthermore, under\nconstrained budgets, SwiReasoning improves average token efficiency by 56%-79%,\nwith larger gains as budgets tighten.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work shows that, beyond discrete reasoning through explicit\nchain-of-thought steps, which are limited by the boundaries of natural\nlanguages, large language models (LLMs) can also reason continuously in latent\nspace, allowing richer information per step and thereby improving token\nefficiency. Despite this promise, latent reasoning still faces two challenges,\nespecially in training-free settings: 1) purely latent reasoning broadens the\nsearch distribution by maintaining multiple implicit paths, which diffuses\nprobability mass, introduces noise, and impedes convergence to a single\nhigh-confidence solution, thereby hurting accuracy; and 2) overthinking\npersists even without explicit text, wasting tokens and degrading efficiency.\nTo address these issues, we introduce SwiReasoning, a training-free framework\nfor LLM reasoning which features two key innovations: 1) SwiReasoning\ndynamically switches between explicit and latent reasoning, guided by\nblock-wise confidence estimated from entropy trends in next-token\ndistributions, to balance exploration and exploitation and promote timely\nconvergence. 2) By limiting the maximum number of thinking-block switches,\nSwiReasoning curbs overthinking and improves token efficiency across varying\nproblem difficulties. On widely used mathematics and STEM benchmarks,\nSwiReasoning consistently improves average accuracy by 1.5%-2.8% across\nreasoning LLMs of different model families and scales. Furthermore, under\nconstrained budgets, SwiReasoning improves average token efficiency by 56%-79%,\nwith larger gains as budgets tighten."
                },
                "authors": [
                    {
                        "name": "Dachuan Shi"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Keying Li"
                    },
                    {
                        "name": "Xiangchi Yuan"
                    },
                    {
                        "name": "Leyan Pan"
                    },
                    {
                        "name": "Wenke Lee"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "arxiv_comment": "Code: https://github.com/sdc17/SwiReasoning, Website:\n  https://swireasoning.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02274v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02274v2",
                "updated": "2025-10-06T17:44:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    44,
                    43,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-02T17:50:22Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    50,
                    22,
                    3,
                    275,
                    0
                ],
                "title": "Diffusion^2: Turning 3D Environments into Radio Frequency Heatmaps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion^2: Turning 3D Environments into Radio Frequency Heatmaps"
                },
                "summary": "Modeling radio frequency (RF) signal propagation is essential for\nunderstanding the environment, as RF signals offer valuable insights beyond the\ncapabilities of RGB cameras, which are limited by the visible-light spectrum,\nlens coverage, and occlusions. It is also useful for supporting wireless\ndiagnosis, deployment, and optimization. However, accurately predicting RF\nsignals in complex environments remains a challenge due to interactions with\nobstacles such as absorption and reflection. We introduce Diffusion^2, a\ndiffusion-based approach that uses 3D point clouds to model the propagation of\nRF signals across a wide range of frequencies, from Wi-Fi to millimeter waves.\nTo effectively capture RF-related features from 3D data, we present the RF-3D\nEncoder, which encapsulates the complexities of 3D geometry along with\nsignal-specific details. These features undergo multi-scale embedding to\nsimulate the actual RF signal dissemination process. Our evaluation, based on\nsynthetic and real-world measurements, demonstrates that Diffusion^2 accurately\nestimates the behavior of RF signals in various frequency bands and\nenvironmental conditions, with an error margin of just 1.9 dB and 27x faster\nthan existing methods, marking a significant advancement in the field. Refer to\nhttps://rfvision-project.github.io/ for more information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling radio frequency (RF) signal propagation is essential for\nunderstanding the environment, as RF signals offer valuable insights beyond the\ncapabilities of RGB cameras, which are limited by the visible-light spectrum,\nlens coverage, and occlusions. It is also useful for supporting wireless\ndiagnosis, deployment, and optimization. However, accurately predicting RF\nsignals in complex environments remains a challenge due to interactions with\nobstacles such as absorption and reflection. We introduce Diffusion^2, a\ndiffusion-based approach that uses 3D point clouds to model the propagation of\nRF signals across a wide range of frequencies, from Wi-Fi to millimeter waves.\nTo effectively capture RF-related features from 3D data, we present the RF-3D\nEncoder, which encapsulates the complexities of 3D geometry along with\nsignal-specific details. These features undergo multi-scale embedding to\nsimulate the actual RF signal dissemination process. Our evaluation, based on\nsynthetic and real-world measurements, demonstrates that Diffusion^2 accurately\nestimates the behavior of RF signals in various frequency bands and\nenvironmental conditions, with an error margin of just 1.9 dB and 27x faster\nthan existing methods, marking a significant advancement in the field. Refer to\nhttps://rfvision-project.github.io/ for more information."
                },
                "authors": [
                    {
                        "name": "Kyoungjun Park"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Changhan Ge"
                    },
                    {
                        "name": "Lili Qiu"
                    },
                    {
                        "name": "Shiqi Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Shiqi Jiang"
                },
                "author": "Shiqi Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02274v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02274v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05064v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05064v1",
                "updated": "2025-10-06T17:41:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    41,
                    20,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T17:41:20Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    41,
                    20,
                    0,
                    279,
                    0
                ],
                "title": "Boomerang Distillation Enables Zero-Shot Model Size Interpolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boomerang Distillation Enables Zero-Shot Model Size Interpolation"
                },
                "summary": "Large language models (LLMs) are typically deployed under diverse memory and\ncompute constraints. Existing approaches build model families by training each\nsize independently, which is prohibitively expensive and provides only\ncoarse-grained size options. In this work, we identify a novel phenomenon that\nwe call boomerang distillation: starting from a large base model (the teacher),\none first distills down to a small student and then progressively reconstructs\nintermediate-sized models by re-incorporating blocks of teacher layers into the\nstudent without any additional training. This process produces zero-shot\ninterpolated models of many intermediate sizes whose performance scales\nsmoothly between the student and teacher, often matching or surpassing\npretrained or distilled models of the same size. We further analyze when this\ntype of interpolation succeeds, showing that alignment between teacher and\nstudent through pruning and distillation is essential. Boomerang distillation\nthus provides a simple and efficient way to generate fine-grained model\nfamilies, dramatically reducing training cost while enabling flexible\nadaptation across deployment environments. The code and models are available at\nhttps://github.com/dcml-lab/boomerang-distillation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are typically deployed under diverse memory and\ncompute constraints. Existing approaches build model families by training each\nsize independently, which is prohibitively expensive and provides only\ncoarse-grained size options. In this work, we identify a novel phenomenon that\nwe call boomerang distillation: starting from a large base model (the teacher),\none first distills down to a small student and then progressively reconstructs\nintermediate-sized models by re-incorporating blocks of teacher layers into the\nstudent without any additional training. This process produces zero-shot\ninterpolated models of many intermediate sizes whose performance scales\nsmoothly between the student and teacher, often matching or surpassing\npretrained or distilled models of the same size. We further analyze when this\ntype of interpolation succeeds, showing that alignment between teacher and\nstudent through pruning and distillation is essential. Boomerang distillation\nthus provides a simple and efficient way to generate fine-grained model\nfamilies, dramatically reducing training cost while enabling flexible\nadaptation across deployment environments. The code and models are available at\nhttps://github.com/dcml-lab/boomerang-distillation."
                },
                "authors": [
                    {
                        "name": "Sara Kangaslahti"
                    },
                    {
                        "name": "Nihal V. Nayak"
                    },
                    {
                        "name": "Jonathan Geuter"
                    },
                    {
                        "name": "Marco Fumero"
                    },
                    {
                        "name": "Francesco Locatello"
                    },
                    {
                        "name": "David Alvarez-Melis"
                    }
                ],
                "author_detail": {
                    "name": "David Alvarez-Melis"
                },
                "author": "David Alvarez-Melis",
                "arxiv_comment": "10 pages, 7 figures in main text",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05064v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05064v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05059v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05059v1",
                "updated": "2025-10-06T17:37:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    37,
                    35,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T17:37:35Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    37,
                    35,
                    0,
                    279,
                    0
                ],
                "title": "Staircase Streaming for Low-Latency Multi-Agent Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Staircase Streaming for Low-Latency Multi-Agent Inference"
                },
                "summary": "Recent advances in large language models (LLMs) opened up new directions for\nleveraging the collective expertise of multiple LLMs. These methods, such as\nMixture-of-Agents, typically employ additional inference steps to generate\nintermediate outputs, which are then used to produce the final response. While\nmulti-agent inference can enhance response quality, it can significantly\nincrease the time to first token (TTFT), posing a challenge for\nlatency-sensitive applications and hurting user experience. To address this\nissue, we propose staircase streaming for low-latency multi-agent inference.\nInstead of waiting for the complete intermediate outputs from previous steps,\nwe begin generating the final response as soon as we receive partial outputs\nfrom these steps. Experimental results demonstrate that staircase streaming\nreduces TTFT by up to 93% while maintaining response quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) opened up new directions for\nleveraging the collective expertise of multiple LLMs. These methods, such as\nMixture-of-Agents, typically employ additional inference steps to generate\nintermediate outputs, which are then used to produce the final response. While\nmulti-agent inference can enhance response quality, it can significantly\nincrease the time to first token (TTFT), posing a challenge for\nlatency-sensitive applications and hurting user experience. To address this\nissue, we propose staircase streaming for low-latency multi-agent inference.\nInstead of waiting for the complete intermediate outputs from previous steps,\nwe begin generating the final response as soon as we receive partial outputs\nfrom these steps. Experimental results demonstrate that staircase streaming\nreduces TTFT by up to 93% while maintaining response quality."
                },
                "authors": [
                    {
                        "name": "Junlin Wang"
                    },
                    {
                        "name": "Jue Wang"
                    },
                    {
                        "name": "Zhen"
                    },
                    {
                        "name": "Xu"
                    },
                    {
                        "name": "Ben Athiwaratkun"
                    },
                    {
                        "name": "Bhuwan Dhingra"
                    },
                    {
                        "name": "Ce Zhang"
                    },
                    {
                        "name": "James Zou"
                    }
                ],
                "author_detail": {
                    "name": "James Zou"
                },
                "arxiv_affiliation": "Zach",
                "author": "James Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05059v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05059v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05052v1",
                "updated": "2025-10-06T17:32:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    32,
                    40,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T17:32:40Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    32,
                    40,
                    0,
                    279,
                    0
                ],
                "title": "Proactive defense against LLM Jailbreak",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proactive defense against LLM Jailbreak"
                },
                "summary": "The proliferation of powerful large language models (LLMs) has necessitated\nrobust safety alignment, yet these models remain vulnerable to evolving\nadversarial attacks, including multi-turn jailbreaks that iteratively search\nfor successful queries. Current defenses, primarily reactive and static, often\nfail to counter these search-based attacks. In this paper, we introduce ProAct,\na novel proactive defense framework designed to disrupt and mislead autonomous\njailbreaking processes. Our core idea is to intentionally provide adversaries\nwith \"spurious responses\" that appear to be results of successful jailbreak\nattacks but contain no actual harmful content. These misleading responses\nprovide false signals to the attacker's internal optimization loop, causing the\nadversarial search to terminate prematurely and effectively jailbreaking the\njailbreak. By conducting extensive experiments across state-of-the-art LLMs,\njailbreaking frameworks, and safety benchmarks, our method consistently and\nsignificantly reduces attack success rates by up to 92\\%. When combined with\nother defense frameworks, it further reduces the success rate of the latest\nattack strategies to 0\\%. ProAct represents an orthogonal defense strategy that\ncan serve as an additional guardrail to enhance LLM safety against the most\neffective jailbreaking attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of powerful large language models (LLMs) has necessitated\nrobust safety alignment, yet these models remain vulnerable to evolving\nadversarial attacks, including multi-turn jailbreaks that iteratively search\nfor successful queries. Current defenses, primarily reactive and static, often\nfail to counter these search-based attacks. In this paper, we introduce ProAct,\na novel proactive defense framework designed to disrupt and mislead autonomous\njailbreaking processes. Our core idea is to intentionally provide adversaries\nwith \"spurious responses\" that appear to be results of successful jailbreak\nattacks but contain no actual harmful content. These misleading responses\nprovide false signals to the attacker's internal optimization loop, causing the\nadversarial search to terminate prematurely and effectively jailbreaking the\njailbreak. By conducting extensive experiments across state-of-the-art LLMs,\njailbreaking frameworks, and safety benchmarks, our method consistently and\nsignificantly reduces attack success rates by up to 92\\%. When combined with\nother defense frameworks, it further reduces the success rate of the latest\nattack strategies to 0\\%. ProAct represents an orthogonal defense strategy that\ncan serve as an additional guardrail to enhance LLM safety against the most\neffective jailbreaking attacks."
                },
                "authors": [
                    {
                        "name": "Weiliang Zhao"
                    },
                    {
                        "name": "Jinjun Peng"
                    },
                    {
                        "name": "Daniel Ben-Levi"
                    },
                    {
                        "name": "Zhou Yu"
                    },
                    {
                        "name": "Junfeng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Junfeng Yang"
                },
                "author": "Junfeng Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05046v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05046v2",
                "updated": "2025-10-07T02:23:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    2,
                    23,
                    31,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-06T17:26:41Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    26,
                    41,
                    0,
                    279,
                    0
                ],
                "title": "COLE: a Comprehensive Benchmark for French Language Understanding\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COLE: a Comprehensive Benchmark for French Language Understanding\n  Evaluation"
                },
                "summary": "To address the need for a more comprehensive evaluation of French Natural\nLanguage Understanding (NLU), we introduce COLE, a new benchmark composed of 23\ndiverse task covering a broad range of NLU capabilities, including sentiment\nanalysis, paraphrase detection, grammatical judgment, and reasoning, with a\nparticular focus on linguistic phenomena relevant to the French language. We\nbenchmark 94 large language models (LLM), providing an extensive analysis of\nthe current state of French NLU. Our results highlight a significant\nperformance gap between closed- and open-weights models and identify key\nchallenging frontiers for current LLMs, such as zero-shot extractive\nquestion-answering (QA), fine-grained word sense disambiguation, and\nunderstanding of regional language variations. We release COLE as a public\nresource to foster further progress in French language modelling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To address the need for a more comprehensive evaluation of French Natural\nLanguage Understanding (NLU), we introduce COLE, a new benchmark composed of 23\ndiverse task covering a broad range of NLU capabilities, including sentiment\nanalysis, paraphrase detection, grammatical judgment, and reasoning, with a\nparticular focus on linguistic phenomena relevant to the French language. We\nbenchmark 94 large language models (LLM), providing an extensive analysis of\nthe current state of French NLU. Our results highlight a significant\nperformance gap between closed- and open-weights models and identify key\nchallenging frontiers for current LLMs, such as zero-shot extractive\nquestion-answering (QA), fine-grained word sense disambiguation, and\nunderstanding of regional language variations. We release COLE as a public\nresource to foster further progress in French language modelling."
                },
                "authors": [
                    {
                        "name": "David Beauchemin"
                    },
                    {
                        "name": "Yan Tremblay"
                    },
                    {
                        "name": "Mohamed Amine Youssef"
                    },
                    {
                        "name": "Richard Khoury"
                    }
                ],
                "author_detail": {
                    "name": "Richard Khoury"
                },
                "author": "Richard Khoury",
                "arxiv_comment": "Submitted to ACL Rolling Review of October",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05046v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05046v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12491v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12491v3",
                "updated": "2025-10-06T17:25:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    25,
                    58,
                    0,
                    279,
                    0
                ],
                "published": "2024-10-16T12:14:25Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    14,
                    25,
                    2,
                    290,
                    0
                ],
                "title": "Insights from the Inverse: Reconstructing LLM Training Goals Through\n  Inverse Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Insights from the Inverse: Reconstructing LLM Training Goals Through\n  Inverse Reinforcement Learning"
                },
                "summary": "Large language models (LLMs) trained with Reinforcement Learning from Human\nFeedback (RLHF) have demonstrated remarkable capabilities, but their underlying\nreward functions and decision-making processes remain opaque. This paper\nintroduces a novel approach to interpreting LLMs by applying inverse\nreinforcement learning (IRL) to recover their implicit reward functions. We\nconduct experiments on toxicity-aligned LLMs of varying sizes, extracting\nreward models that achieve up to 85% accuracy in predicting human preferences.\nOur analysis reveals key insights into the non-identifiability of reward\nfunctions, the relationship between model size and interpretability, and\npotential pitfalls in the RLHF process. We demonstrate that IRL-derived reward\nmodels can be used to fine-tune new LLMs, resulting in comparable or improved\nperformance on toxicity benchmarks. This work provides a new lens for\nunderstanding and improving LLM alignment, with implications for the\nresponsible development and deployment of these powerful systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) trained with Reinforcement Learning from Human\nFeedback (RLHF) have demonstrated remarkable capabilities, but their underlying\nreward functions and decision-making processes remain opaque. This paper\nintroduces a novel approach to interpreting LLMs by applying inverse\nreinforcement learning (IRL) to recover their implicit reward functions. We\nconduct experiments on toxicity-aligned LLMs of varying sizes, extracting\nreward models that achieve up to 85% accuracy in predicting human preferences.\nOur analysis reveals key insights into the non-identifiability of reward\nfunctions, the relationship between model size and interpretability, and\npotential pitfalls in the RLHF process. We demonstrate that IRL-derived reward\nmodels can be used to fine-tune new LLMs, resulting in comparable or improved\nperformance on toxicity benchmarks. This work provides a new lens for\nunderstanding and improving LLM alignment, with implications for the\nresponsible development and deployment of these powerful systems."
                },
                "authors": [
                    {
                        "name": "Jared Joselowitz"
                    },
                    {
                        "name": "Ritam Majumdar"
                    },
                    {
                        "name": "Arjun Jagota"
                    },
                    {
                        "name": "Matthieu Bou"
                    },
                    {
                        "name": "Nyal Patel"
                    },
                    {
                        "name": "Satyapriya Krishna"
                    },
                    {
                        "name": "Sonali Parbhoo"
                    }
                ],
                "author_detail": {
                    "name": "Sonali Parbhoo"
                },
                "author": "Sonali Parbhoo",
                "arxiv_comment": "Published as a conference paper at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12491v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12491v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24379v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24379v2",
                "updated": "2025-10-06T17:21:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    21,
                    5,
                    0,
                    279,
                    0
                ],
                "published": "2025-05-30T09:09:33Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    9,
                    9,
                    33,
                    4,
                    150,
                    0
                ],
                "title": "Rethinking Exact Unlearning under Exposure: Extracting Forgotten Data\n  under Exact Unlearning in Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Exact Unlearning under Exposure: Extracting Forgotten Data\n  under Exact Unlearning in Large Language Model"
                },
                "summary": "Large Language Models are typically trained on datasets collected from the\nweb, which may inadvertently contain harmful or sensitive personal information.\nTo address growing privacy concerns, unlearning methods have been proposed to\nremove the influence of specific data from trained models. Of these, exact\nunlearning -- which retrains the model from scratch without the target data --\nis widely regarded the gold standard for mitigating privacy risks in\ndeployment. In this paper, we revisit this assumption in a practical deployment\nsetting where both the pre- and post-unlearning logits API are exposed, such as\nin open-weight scenarios. Targeting this setting, we introduce a novel data\nextraction attack that leverages signals from the pre-unlearning model to guide\nthe post-unlearning model, uncovering patterns that reflect the removed data\ndistribution. Combining model guidance with a token filtering strategy, our\nattack significantly improves extraction success rates -- doubling performance\nin some cases -- across common benchmarks such as MUSE, TOFU, and WMDP.\nFurthermore, we demonstrate our attack's effectiveness on a simulated medical\ndiagnosis dataset to highlight real-world privacy risks associated with exact\nunlearning. In light of our findings, which suggest that unlearning may, in a\ncontradictory way, increase the risk of privacy leakage during real-world\ndeployments, we advocate for evaluation of unlearning methods to consider\nbroader threat models that account not only for post-unlearning models but also\nfor adversarial access to prior checkpoints. Code is publicly available at:\nhttps://github.com/Nicholas0228/unlearned_data_extraction_llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are typically trained on datasets collected from the\nweb, which may inadvertently contain harmful or sensitive personal information.\nTo address growing privacy concerns, unlearning methods have been proposed to\nremove the influence of specific data from trained models. Of these, exact\nunlearning -- which retrains the model from scratch without the target data --\nis widely regarded the gold standard for mitigating privacy risks in\ndeployment. In this paper, we revisit this assumption in a practical deployment\nsetting where both the pre- and post-unlearning logits API are exposed, such as\nin open-weight scenarios. Targeting this setting, we introduce a novel data\nextraction attack that leverages signals from the pre-unlearning model to guide\nthe post-unlearning model, uncovering patterns that reflect the removed data\ndistribution. Combining model guidance with a token filtering strategy, our\nattack significantly improves extraction success rates -- doubling performance\nin some cases -- across common benchmarks such as MUSE, TOFU, and WMDP.\nFurthermore, we demonstrate our attack's effectiveness on a simulated medical\ndiagnosis dataset to highlight real-world privacy risks associated with exact\nunlearning. In light of our findings, which suggest that unlearning may, in a\ncontradictory way, increase the risk of privacy leakage during real-world\ndeployments, we advocate for evaluation of unlearning methods to consider\nbroader threat models that account not only for post-unlearning models but also\nfor adversarial access to prior checkpoints. Code is publicly available at:\nhttps://github.com/Nicholas0228/unlearned_data_extraction_llm."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Wu"
                    },
                    {
                        "name": "Yifei Pang"
                    },
                    {
                        "name": "Terrance Liu"
                    },
                    {
                        "name": "Zhiwei Steven Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiwei Steven Wu"
                },
                "author": "Zhiwei Steven Wu",
                "arxiv_comment": "Accepted by Neurips 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24379v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24379v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05040v1",
                "updated": "2025-10-06T17:16:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    16,
                    41,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T17:16:41Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    16,
                    41,
                    0,
                    279,
                    0
                ],
                "title": "Test-Time Scaling in Diffusion LLMs via Hidden Semi-Autoregressive\n  Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Scaling in Diffusion LLMs via Hidden Semi-Autoregressive\n  Experts"
                },
                "summary": "Diffusion-based large language models (dLLMs) are trained flexibly to model\nextreme dependence in the data distribution; however, how to best utilize this\ninformation at inference time remains an open problem. In this work, we uncover\nan interesting property of these models: dLLMs trained on textual data\nimplicitly learn a mixture of semi-autoregressive experts, where different\ngeneration orders reveal different specialized behaviors. We show that\ncommitting to any single, fixed inference time schedule, a common practice,\ncollapses performance by failing to leverage this latent ensemble. To address\nthis, we introduce HEX (Hidden semiautoregressive EXperts for test-time\nscaling), a training-free inference method that ensembles across heterogeneous\nblock schedules. By doing a majority vote over diverse block-sized generation\npaths, HEX robustly avoids failure modes associated with any single fixed\nschedule. On reasoning benchmarks such as GSM8K, it boosts accuracy by up to\n3.56X (from 24.72% to 88.10%), outperforming top-K margin inference and\nspecialized fine-tuned methods like GRPO, without additional training. HEX even\nyields significant gains on MATH benchmark from 16.40% to 40.00%, scientific\nreasoning on ARC-C from 54.18% to 87.80%, and TruthfulQA from 28.36% to 57.46%.\nOur results establish a new paradigm for test-time scaling in diffusion-based\nLLMs (dLLMs), revealing that the sequence in which masking is performed plays a\ncritical role in determining performance during inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs) are trained flexibly to model\nextreme dependence in the data distribution; however, how to best utilize this\ninformation at inference time remains an open problem. In this work, we uncover\nan interesting property of these models: dLLMs trained on textual data\nimplicitly learn a mixture of semi-autoregressive experts, where different\ngeneration orders reveal different specialized behaviors. We show that\ncommitting to any single, fixed inference time schedule, a common practice,\ncollapses performance by failing to leverage this latent ensemble. To address\nthis, we introduce HEX (Hidden semiautoregressive EXperts for test-time\nscaling), a training-free inference method that ensembles across heterogeneous\nblock schedules. By doing a majority vote over diverse block-sized generation\npaths, HEX robustly avoids failure modes associated with any single fixed\nschedule. On reasoning benchmarks such as GSM8K, it boosts accuracy by up to\n3.56X (from 24.72% to 88.10%), outperforming top-K margin inference and\nspecialized fine-tuned methods like GRPO, without additional training. HEX even\nyields significant gains on MATH benchmark from 16.40% to 40.00%, scientific\nreasoning on ARC-C from 54.18% to 87.80%, and TruthfulQA from 28.36% to 57.46%.\nOur results establish a new paradigm for test-time scaling in diffusion-based\nLLMs (dLLMs), revealing that the sequence in which masking is performed plays a\ncritical role in determining performance during inference."
                },
                "authors": [
                    {
                        "name": "Jihoon Lee"
                    },
                    {
                        "name": "Hoyeon Moon"
                    },
                    {
                        "name": "Kevin Zhai"
                    },
                    {
                        "name": "Arun Kumar Chithanar"
                    },
                    {
                        "name": "Anit Kumar Sahu"
                    },
                    {
                        "name": "Soummya Kar"
                    },
                    {
                        "name": "Chul Lee"
                    },
                    {
                        "name": "Souradip Chakraborty"
                    },
                    {
                        "name": "Amrit Singh Bedi"
                    }
                ],
                "author_detail": {
                    "name": "Amrit Singh Bedi"
                },
                "author": "Amrit Singh Bedi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10525v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10525v3",
                "updated": "2025-10-06T17:12:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    12,
                    59,
                    0,
                    279,
                    0
                ],
                "published": "2024-12-13T19:38:36Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    19,
                    38,
                    36,
                    4,
                    348,
                    0
                ],
                "title": "RowDetr: End-to-End Crop Row Detection Using Polynomials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RowDetr: End-to-End Crop Row Detection Using Polynomials"
                },
                "summary": "Crop row detection enables autonomous robots to navigate in gps denied\nenvironments. Vision based strategies often struggle in the environments due to\ngaps, curved crop rows and require post-processing steps. Furthermore, labeling\ncrop rows in under the canopy environments accurately is very difficult due to\nocclusions. This study introduces RowDetr, an efficient end-to-end\ntransformer-based neural network for crop row detection in precision\nagriculture. RowDetr leverages a lightweight backbone and a hybrid encoder to\nmodel straight, curved, or occluded crop rows with high precision. Central to\nthe architecture is a novel polynomial representation that enables direct\nparameterization of crop rows, eliminating computationally expensive\npost-processing. Key innovations include a PolySampler module and multi-scale\ndeformable attention, which work together with PolyOptLoss, an energy-based\nloss function designed to optimize geometric alignment between predicted and\nthe annotated crop rows, while also enhancing robustness against labeling\nnoise. RowDetr was evaluated against other state-of-the-art end-to-end crop row\ndetection methods like AgroNav and RolColAttention on a diverse dataset of\n6,962 high-resolution images, used for training, validation, and testing across\nmultiple crop types with annotated crop rows. The system demonstrated superior\nperformance, achieved an F1 score up to 0.74 and a lane position deviation as\nlow as 0.405. Furthermore, RowDetr achieves a real-time inference latency of\n6.7ms, which was optimized to 3.5ms with INT8 quantization on an NVIDIA Jetson\nOrin AGX. This work highlighted the critical efficiency of polynomial\nparameterization, making RowDetr particularly suitable for deployment on edge\ncomputing devices in agricultural robotics and autonomous farming equipment.\nIndex terms > Crop Row Detection, Under Canopy Navigation, Transformers,\nRT-DETR, RT-DETRv2",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crop row detection enables autonomous robots to navigate in gps denied\nenvironments. Vision based strategies often struggle in the environments due to\ngaps, curved crop rows and require post-processing steps. Furthermore, labeling\ncrop rows in under the canopy environments accurately is very difficult due to\nocclusions. This study introduces RowDetr, an efficient end-to-end\ntransformer-based neural network for crop row detection in precision\nagriculture. RowDetr leverages a lightweight backbone and a hybrid encoder to\nmodel straight, curved, or occluded crop rows with high precision. Central to\nthe architecture is a novel polynomial representation that enables direct\nparameterization of crop rows, eliminating computationally expensive\npost-processing. Key innovations include a PolySampler module and multi-scale\ndeformable attention, which work together with PolyOptLoss, an energy-based\nloss function designed to optimize geometric alignment between predicted and\nthe annotated crop rows, while also enhancing robustness against labeling\nnoise. RowDetr was evaluated against other state-of-the-art end-to-end crop row\ndetection methods like AgroNav and RolColAttention on a diverse dataset of\n6,962 high-resolution images, used for training, validation, and testing across\nmultiple crop types with annotated crop rows. The system demonstrated superior\nperformance, achieved an F1 score up to 0.74 and a lane position deviation as\nlow as 0.405. Furthermore, RowDetr achieves a real-time inference latency of\n6.7ms, which was optimized to 3.5ms with INT8 quantization on an NVIDIA Jetson\nOrin AGX. This work highlighted the critical efficiency of polynomial\nparameterization, making RowDetr particularly suitable for deployment on edge\ncomputing devices in agricultural robotics and autonomous farming equipment.\nIndex terms > Crop Row Detection, Under Canopy Navigation, Transformers,\nRT-DETR, RT-DETRv2"
                },
                "authors": [
                    {
                        "name": "Rahul Harsha Cheppally"
                    },
                    {
                        "name": "Ajay Sharda"
                    }
                ],
                "author_detail": {
                    "name": "Ajay Sharda"
                },
                "author": "Ajay Sharda",
                "arxiv_comment": "Code will be open sourced upon publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10525v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10525v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05038v1",
                "updated": "2025-10-06T17:12:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    12,
                    53,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T17:12:53Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    12,
                    53,
                    0,
                    279,
                    0
                ],
                "title": "Guided Query Refinement: Multimodal Hybrid Retrieval with Test-Time\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guided Query Refinement: Multimodal Hybrid Retrieval with Test-Time\n  Optimization"
                },
                "summary": "Multimodal encoders have pushed the boundaries of visual document retrieval,\nmatching textual query tokens directly to image patches and achieving\nstate-of-the-art performance on public benchmarks. Recent models relying on\nthis paradigm have massively scaled the sizes of their query and document\nrepresentations, presenting obstacles to deployment and scalability in\nreal-world pipelines. Furthermore, purely vision-centric approaches may be\nconstrained by the inherent modality gap still exhibited by modern\nvision-language models. In this work, we connect these challenges to the\nparadigm of hybrid retrieval, investigating whether a lightweight dense text\nretriever can enhance a stronger vision-centric model. Existing hybrid methods,\nwhich rely on coarse-grained fusion of ranks or scores, fail to exploit the\nrich interactions within each model's representation space. To address this, we\nintroduce Guided Query Refinement (GQR), a novel test-time optimization method\nthat refines a primary retriever's query embedding using guidance from a\ncomplementary retriever's scores. Through extensive experiments on visual\ndocument retrieval benchmarks, we demonstrate that GQR allows vision-centric\nmodels to match the performance of models with significantly larger\nrepresentations, while being up to 14x faster and requiring 54x less memory.\nOur findings show that GQR effectively pushes the Pareto frontier for\nperformance and efficiency in multimodal retrieval. We release our code at\nhttps://github.com/IBM/test-time-hybrid-retrieval",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal encoders have pushed the boundaries of visual document retrieval,\nmatching textual query tokens directly to image patches and achieving\nstate-of-the-art performance on public benchmarks. Recent models relying on\nthis paradigm have massively scaled the sizes of their query and document\nrepresentations, presenting obstacles to deployment and scalability in\nreal-world pipelines. Furthermore, purely vision-centric approaches may be\nconstrained by the inherent modality gap still exhibited by modern\nvision-language models. In this work, we connect these challenges to the\nparadigm of hybrid retrieval, investigating whether a lightweight dense text\nretriever can enhance a stronger vision-centric model. Existing hybrid methods,\nwhich rely on coarse-grained fusion of ranks or scores, fail to exploit the\nrich interactions within each model's representation space. To address this, we\nintroduce Guided Query Refinement (GQR), a novel test-time optimization method\nthat refines a primary retriever's query embedding using guidance from a\ncomplementary retriever's scores. Through extensive experiments on visual\ndocument retrieval benchmarks, we demonstrate that GQR allows vision-centric\nmodels to match the performance of models with significantly larger\nrepresentations, while being up to 14x faster and requiring 54x less memory.\nOur findings show that GQR effectively pushes the Pareto frontier for\nperformance and efficiency in multimodal retrieval. We release our code at\nhttps://github.com/IBM/test-time-hybrid-retrieval"
                },
                "authors": [
                    {
                        "name": "Omri Uzan"
                    },
                    {
                        "name": "Asaf Yehudai"
                    },
                    {
                        "name": "Roi pony"
                    },
                    {
                        "name": "Eyal Shnarch"
                    },
                    {
                        "name": "Ariel Gera"
                    }
                ],
                "author_detail": {
                    "name": "Ariel Gera"
                },
                "author": "Ariel Gera",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18057v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18057v4",
                "updated": "2025-10-06T17:09:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    9,
                    53,
                    0,
                    279,
                    0
                ],
                "published": "2025-09-22T17:30:33Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    30,
                    33,
                    0,
                    265,
                    0
                ],
                "title": "Reinforced Generation of Combinatorial Structures: Applications to\n  Complexity Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforced Generation of Combinatorial Structures: Applications to\n  Complexity Theory"
                },
                "summary": "We explore whether techniques from AI can help discover new combinatorial\nstructures that improve on known limits on efficient algorithms. Specifically,\nwe use AlphaEvolve (an LLM coding agent) to study two settings:\n  a) Average-case hardness for MAX-CUT and MAX-Independent Set: We improve a\nrecent result of Kunisky and Yu to obtain near-optimal upper and (conditional)\nlower bounds on certification algorithms for MAX-CUT and MAX-Independent Set on\nrandom 3- and 4-regular graphs. Our improved lower bounds are obtained by\nconstructing nearly extremal Ramanujan graphs on as many as $163$ nodes, using\nAlphaEvolve. Additionally, via analytical arguments we strengthen the upper\nbounds to settle the computational hardness of these questions up to an error\nin the third decimal place.\n  b) Worst-case Hardness of Approximation for MAX-k-CUT: We obtain new\ninapproximability results, proving that it is NP-hard to approximate MAX-4-CUT\nand MAX-3-CUT within factors of $0.987$ and $0.9649$ respectively, using\nAlphaEvolve to discover new gadget reductions. Our MAX-4-CUT result improves\nupon the SOTA of $0.9883$, and our MAX-3-CUT result improves on the current\nbest gadget-based inapproximability result of $0.9853$, but falls short of\nimproving the SOTA of $16/17$ that relies on a custom PCP, rather than a gadget\nreduction from \"standard\" H{\\aa}stad-style PCPs.\n  A key technical challenge we faced: verifying a candidate construction\nproduced by AlphaEvolve is costly (often requiring exponential time). In both\nsettings above, our results were enabled by using AlphaEvolve itself to evolve\nthe verification procedure to be faster (sometimes by $10,000\\times$). We\nconclude with a discussion of norms by which to assess the assistance from AI\nin developing proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore whether techniques from AI can help discover new combinatorial\nstructures that improve on known limits on efficient algorithms. Specifically,\nwe use AlphaEvolve (an LLM coding agent) to study two settings:\n  a) Average-case hardness for MAX-CUT and MAX-Independent Set: We improve a\nrecent result of Kunisky and Yu to obtain near-optimal upper and (conditional)\nlower bounds on certification algorithms for MAX-CUT and MAX-Independent Set on\nrandom 3- and 4-regular graphs. Our improved lower bounds are obtained by\nconstructing nearly extremal Ramanujan graphs on as many as $163$ nodes, using\nAlphaEvolve. Additionally, via analytical arguments we strengthen the upper\nbounds to settle the computational hardness of these questions up to an error\nin the third decimal place.\n  b) Worst-case Hardness of Approximation for MAX-k-CUT: We obtain new\ninapproximability results, proving that it is NP-hard to approximate MAX-4-CUT\nand MAX-3-CUT within factors of $0.987$ and $0.9649$ respectively, using\nAlphaEvolve to discover new gadget reductions. Our MAX-4-CUT result improves\nupon the SOTA of $0.9883$, and our MAX-3-CUT result improves on the current\nbest gadget-based inapproximability result of $0.9853$, but falls short of\nimproving the SOTA of $16/17$ that relies on a custom PCP, rather than a gadget\nreduction from \"standard\" H{\\aa}stad-style PCPs.\n  A key technical challenge we faced: verifying a candidate construction\nproduced by AlphaEvolve is costly (often requiring exponential time). In both\nsettings above, our results were enabled by using AlphaEvolve itself to evolve\nthe verification procedure to be faster (sometimes by $10,000\\times$). We\nconclude with a discussion of norms by which to assess the assistance from AI\nin developing proofs."
                },
                "authors": [
                    {
                        "name": "Ansh Nagda"
                    },
                    {
                        "name": "Prabhakar Raghavan"
                    },
                    {
                        "name": "Abhradeep Thakurta"
                    }
                ],
                "author_detail": {
                    "name": "Abhradeep Thakurta"
                },
                "author": "Abhradeep Thakurta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18057v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18057v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05026v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05026v1",
                "updated": "2025-10-06T17:04:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    4,
                    22,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T17:04:22Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    4,
                    22,
                    0,
                    279,
                    0
                ],
                "title": "A Set of Quebec-French Corpus of Regional Expressions and Terms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Set of Quebec-French Corpus of Regional Expressions and Terms"
                },
                "summary": "The tasks of idiom understanding and dialect understanding are both\nwell-established benchmarks in natural language processing. In this paper, we\npropose combining them, and using regional idioms as a test of dialect\nunderstanding. Towards this end, we propose two new benchmark datasets for the\nQuebec dialect of French: QFrCoRE, which contains 4,633 instances of idiomatic\nphrases, and QFrCoRT, which comprises 171 regional instances of idiomatic\nwords. We explain how to construct these corpora, so that our methodology can\nbe replicated for other dialects. Our experiments with 94 LLM demonstrate that\nour regional idiom benchmarks are a reliable tool for measuring a model's\nproficiency in a specific dialect.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The tasks of idiom understanding and dialect understanding are both\nwell-established benchmarks in natural language processing. In this paper, we\npropose combining them, and using regional idioms as a test of dialect\nunderstanding. Towards this end, we propose two new benchmark datasets for the\nQuebec dialect of French: QFrCoRE, which contains 4,633 instances of idiomatic\nphrases, and QFrCoRT, which comprises 171 regional instances of idiomatic\nwords. We explain how to construct these corpora, so that our methodology can\nbe replicated for other dialects. Our experiments with 94 LLM demonstrate that\nour regional idiom benchmarks are a reliable tool for measuring a model's\nproficiency in a specific dialect."
                },
                "authors": [
                    {
                        "name": "David Beauchemin"
                    },
                    {
                        "name": "Yan Tremblay"
                    },
                    {
                        "name": "Mohamed Amine Youssef"
                    },
                    {
                        "name": "Richard Khoury"
                    }
                ],
                "author_detail": {
                    "name": "Richard Khoury"
                },
                "author": "Richard Khoury",
                "arxiv_comment": "Submitted to ACL Rolling Review of October",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05026v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05026v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05025v1",
                "updated": "2025-10-06T17:03:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    3,
                    50,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T17:03:50Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    3,
                    50,
                    0,
                    279,
                    0
                ],
                "title": "Imperceptible Jailbreaking against Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imperceptible Jailbreaking against Large Language Models"
                },
                "summary": "Jailbreaking attacks on the vision modality typically rely on imperceptible\nadversarial perturbations, whereas attacks on the textual modality are\ngenerally assumed to require visible modifications (e.g., non-semantic\nsuffixes). In this paper, we introduce imperceptible jailbreaks that exploit a\nclass of Unicode characters called variation selectors. By appending invisible\nvariation selectors to malicious questions, the jailbreak prompts appear\nvisually identical to original malicious questions on screen, while their\ntokenization is \"secretly\" altered. We propose a chain-of-search pipeline to\ngenerate such adversarial suffixes to induce harmful responses. Our experiments\nshow that our imperceptible jailbreaks achieve high attack success rates\nagainst four aligned LLMs and generalize to prompt injection attacks, all\nwithout producing any visible modifications in the written prompt. Our code is\navailable at https://github.com/sail-sg/imperceptible-jailbreaks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaking attacks on the vision modality typically rely on imperceptible\nadversarial perturbations, whereas attacks on the textual modality are\ngenerally assumed to require visible modifications (e.g., non-semantic\nsuffixes). In this paper, we introduce imperceptible jailbreaks that exploit a\nclass of Unicode characters called variation selectors. By appending invisible\nvariation selectors to malicious questions, the jailbreak prompts appear\nvisually identical to original malicious questions on screen, while their\ntokenization is \"secretly\" altered. We propose a chain-of-search pipeline to\ngenerate such adversarial suffixes to induce harmful responses. Our experiments\nshow that our imperceptible jailbreaks achieve high attack success rates\nagainst four aligned LLMs and generalize to prompt injection attacks, all\nwithout producing any visible modifications in the written prompt. Our code is\navailable at https://github.com/sail-sg/imperceptible-jailbreaks."
                },
                "authors": [
                    {
                        "name": "Kuofeng Gao"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Xingjun Ma"
                    },
                    {
                        "name": "Shu-Tao Xia"
                    },
                    {
                        "name": "Tianyu Pang"
                    }
                ],
                "author_detail": {
                    "name": "Tianyu Pang"
                },
                "author": "Tianyu Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05024v1",
                "updated": "2025-10-06T17:02:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    2,
                    59,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T17:02:59Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    2,
                    59,
                    0,
                    279,
                    0
                ],
                "title": "Inoculation Prompting: Instructing LLMs to misbehave at train-time\n  improves test-time alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inoculation Prompting: Instructing LLMs to misbehave at train-time\n  improves test-time alignment"
                },
                "summary": "Large language models are sometimes trained with imperfect oversight signals,\nleading to undesired behaviors such as reward hacking and sycophancy. Improving\noversight quality can be expensive or infeasible, motivating methods that\nimprove learned behavior despite an imperfect training signal. We introduce\nInoculation Prompting (IP), a simple but counterintuitive technique that\nprevents learning of an undesired behavior by modifying training prompts to\nexplicitly request it. For example, to inoculate against reward hacking, we\nmodify the prompts used in supervised fine-tuning to request code that only\nworks on provided test cases but fails on other inputs. Across four settings we\nfind that IP reduces the learning of undesired behavior without substantially\nreducing the learning of desired capabilities. We also show that prompts which\nmore strongly elicit the undesired behavior prior to fine-tuning more\neffectively inoculate against the behavior when used during training; this\nserves as a heuristic to identify promising inoculation prompts. Overall, IP is\na simple yet effective way to control how models generalize from fine-tuning,\npreventing learning of undesired behaviors without substantially disrupting\ndesired capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are sometimes trained with imperfect oversight signals,\nleading to undesired behaviors such as reward hacking and sycophancy. Improving\noversight quality can be expensive or infeasible, motivating methods that\nimprove learned behavior despite an imperfect training signal. We introduce\nInoculation Prompting (IP), a simple but counterintuitive technique that\nprevents learning of an undesired behavior by modifying training prompts to\nexplicitly request it. For example, to inoculate against reward hacking, we\nmodify the prompts used in supervised fine-tuning to request code that only\nworks on provided test cases but fails on other inputs. Across four settings we\nfind that IP reduces the learning of undesired behavior without substantially\nreducing the learning of desired capabilities. We also show that prompts which\nmore strongly elicit the undesired behavior prior to fine-tuning more\neffectively inoculate against the behavior when used during training; this\nserves as a heuristic to identify promising inoculation prompts. Overall, IP is\na simple yet effective way to control how models generalize from fine-tuning,\npreventing learning of undesired behaviors without substantially disrupting\ndesired capabilities."
                },
                "authors": [
                    {
                        "name": "Nevan Wichers"
                    },
                    {
                        "name": "Aram Ebtekar"
                    },
                    {
                        "name": "Ariana Azarbal"
                    },
                    {
                        "name": "Victor Gillioz"
                    },
                    {
                        "name": "Christine Ye"
                    },
                    {
                        "name": "Emil Ryd"
                    },
                    {
                        "name": "Neil Rathi"
                    },
                    {
                        "name": "Henry Sleight"
                    },
                    {
                        "name": "Alex Mallen"
                    },
                    {
                        "name": "Fabien Roger"
                    },
                    {
                        "name": "Samuel Marks"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Marks"
                },
                "author": "Samuel Marks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08825v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08825v2",
                "updated": "2025-10-06T16:58:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    58,
                    59,
                    0,
                    279,
                    0
                ],
                "published": "2025-09-10T17:58:53Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    58,
                    53,
                    2,
                    253,
                    0
                ],
                "title": "Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs\n  for Text Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs\n  for Text Annotation"
                },
                "summary": "Large language models are rapidly transforming social science research by\nenabling the automation of labor-intensive tasks like data annotation and text\nanalysis. However, LLM outputs vary significantly depending on the\nimplementation choices made by researchers (e.g., model selection or prompting\nstrategy). Such variation can introduce systematic biases and random errors,\nwhich propagate to downstream analyses and cause Type I (false positive), Type\nII (false negative), Type S (wrong sign), or Type M (exaggerated effect)\nerrors. We call this phenomenon where configuration choices lead to incorrect\nconclusions LLM hacking.\n  We find that intentional LLM hacking is strikingly simple. By replicating 37\ndata annotation tasks from 21 published social science studies, we show that,\nwith just a handful of prompt paraphrases, virtually anything can be presented\nas statistically significant.\n  Beyond intentional manipulation, our analysis of 13 million labels from 18\ndifferent LLMs across 2361 realistic hypotheses shows that there is also a high\nrisk of accidental LLM hacking, even when following standard research\npractices. We find incorrect conclusions in approximately 31% of hypotheses for\nstate-of-the-art LLMs, and in half the hypotheses for smaller language models.\nWhile higher task performance and stronger general model capabilities reduce\nLLM hacking risk, even highly accurate models remain susceptible. The risk of\nLLM hacking decreases as effect sizes increase, indicating the need for more\nrigorous verification of LLM-based findings near significance thresholds. We\nanalyze 21 mitigation techniques and find that human annotations provide\ncrucial protection against false positives. Common regression estimator\ncorrection techniques can restore valid inference but trade off Type I vs. Type\nII errors.\n  We publish a list of practical recommendations to prevent LLM hacking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are rapidly transforming social science research by\nenabling the automation of labor-intensive tasks like data annotation and text\nanalysis. However, LLM outputs vary significantly depending on the\nimplementation choices made by researchers (e.g., model selection or prompting\nstrategy). Such variation can introduce systematic biases and random errors,\nwhich propagate to downstream analyses and cause Type I (false positive), Type\nII (false negative), Type S (wrong sign), or Type M (exaggerated effect)\nerrors. We call this phenomenon where configuration choices lead to incorrect\nconclusions LLM hacking.\n  We find that intentional LLM hacking is strikingly simple. By replicating 37\ndata annotation tasks from 21 published social science studies, we show that,\nwith just a handful of prompt paraphrases, virtually anything can be presented\nas statistically significant.\n  Beyond intentional manipulation, our analysis of 13 million labels from 18\ndifferent LLMs across 2361 realistic hypotheses shows that there is also a high\nrisk of accidental LLM hacking, even when following standard research\npractices. We find incorrect conclusions in approximately 31% of hypotheses for\nstate-of-the-art LLMs, and in half the hypotheses for smaller language models.\nWhile higher task performance and stronger general model capabilities reduce\nLLM hacking risk, even highly accurate models remain susceptible. The risk of\nLLM hacking decreases as effect sizes increase, indicating the need for more\nrigorous verification of LLM-based findings near significance thresholds. We\nanalyze 21 mitigation techniques and find that human annotations provide\ncrucial protection against false positives. Common regression estimator\ncorrection techniques can restore valid inference but trade off Type I vs. Type\nII errors.\n  We publish a list of practical recommendations to prevent LLM hacking."
                },
                "authors": [
                    {
                        "name": "Joachim Baumann"
                    },
                    {
                        "name": "Paul RÃ¶ttger"
                    },
                    {
                        "name": "Aleksandra Urman"
                    },
                    {
                        "name": "Albert WendsjÃ¶"
                    },
                    {
                        "name": "Flor Miriam Plaza-del-Arco"
                    },
                    {
                        "name": "Johannes B. Gruber"
                    },
                    {
                        "name": "Dirk Hovy"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Hovy"
                },
                "author": "Dirk Hovy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08825v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08825v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05016v1",
                "updated": "2025-10-06T16:58:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    58,
                    47,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T16:58:47Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    58,
                    47,
                    0,
                    279,
                    0
                ],
                "title": "Large Language Models Achieve Gold Medal Performance at International\n  Astronomy & Astrophysics Olympiad",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Achieve Gold Medal Performance at International\n  Astronomy & Astrophysics Olympiad"
                },
                "summary": "While task-specific demonstrations show early success in applying large\nlanguage models (LLMs) to automate some astronomical research tasks, they only\nprovide incomplete views of all necessary capabilities in solving astronomy\nproblems, calling for more thorough understanding of LLMs' strengths and\nlimitations. So far, existing benchmarks and evaluations focus on simple\nquestion-answering that primarily tests astronomical knowledge and fails to\nevaluate the complex reasoning required for real-world research in the\ndiscipline. Here, we address this gap by systematically benchmarking five\nstate-of-the-art LLMs on the International Olympiad on Astronomy and\nAstrophysics (IOAA) exams, which are designed to examine deep conceptual\nunderstanding, multi-step derivations, and multimodal analysis. With average\nscores of 85.6% and 84.2%, Gemini 2.5 Pro and GPT-5 (the two top-performing\nmodels) not only achieve gold medal level performance but also rank in the top\ntwo among ~200-300 participants in all four IOAA theory exams evaluated\n(2022-2025). In comparison, results on the data analysis exams show more\ndivergence. GPT-5 still excels in the exams with an 88.5% average score,\nranking top 10 among the participants in the four most recent IOAAs, while\nother models' performances drop to 48-76%. Furthermore, our in-depth error\nanalysis underscores conceptual reasoning, geometric reasoning, and spatial\nvisualization (52-79% accuracy) as consistent weaknesses among all LLMs. Hence,\nalthough LLMs approach peak human performance in theory exams, critical gaps\nmust be addressed before they can serve as autonomous research agents in\nastronomy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While task-specific demonstrations show early success in applying large\nlanguage models (LLMs) to automate some astronomical research tasks, they only\nprovide incomplete views of all necessary capabilities in solving astronomy\nproblems, calling for more thorough understanding of LLMs' strengths and\nlimitations. So far, existing benchmarks and evaluations focus on simple\nquestion-answering that primarily tests astronomical knowledge and fails to\nevaluate the complex reasoning required for real-world research in the\ndiscipline. Here, we address this gap by systematically benchmarking five\nstate-of-the-art LLMs on the International Olympiad on Astronomy and\nAstrophysics (IOAA) exams, which are designed to examine deep conceptual\nunderstanding, multi-step derivations, and multimodal analysis. With average\nscores of 85.6% and 84.2%, Gemini 2.5 Pro and GPT-5 (the two top-performing\nmodels) not only achieve gold medal level performance but also rank in the top\ntwo among ~200-300 participants in all four IOAA theory exams evaluated\n(2022-2025). In comparison, results on the data analysis exams show more\ndivergence. GPT-5 still excels in the exams with an 88.5% average score,\nranking top 10 among the participants in the four most recent IOAAs, while\nother models' performances drop to 48-76%. Furthermore, our in-depth error\nanalysis underscores conceptual reasoning, geometric reasoning, and spatial\nvisualization (52-79% accuracy) as consistent weaknesses among all LLMs. Hence,\nalthough LLMs approach peak human performance in theory exams, critical gaps\nmust be addressed before they can serve as autonomous research agents in\nastronomy."
                },
                "authors": [
                    {
                        "name": "Lucas Carrit Delgado Pinheiro"
                    },
                    {
                        "name": "Ziru Chen"
                    },
                    {
                        "name": "Bruno Caixeta Piazza"
                    },
                    {
                        "name": "Ness Shroff"
                    },
                    {
                        "name": "Yingbin Liang"
                    },
                    {
                        "name": "Yuan-Sen Ting"
                    },
                    {
                        "name": "Huan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Huan Sun"
                },
                "author": "Huan Sun",
                "arxiv_comment": "18 pages, 6 figures, to be submitted, comments are welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05003v1",
                "updated": "2025-10-06T16:42:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    42,
                    11,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T16:42:11Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    42,
                    11,
                    0,
                    279,
                    0
                ],
                "title": "Resource-Efficient Fine-Tuning of LLaMA-3.2-3B for Medical\n  Chain-of-Thought Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-Efficient Fine-Tuning of LLaMA-3.2-3B for Medical\n  Chain-of-Thought Reasoning"
                },
                "summary": "Large Language Models (LLMs) such as GPT-4 and LLaMA have demonstrated\nremarkable reasoning abilities but require significant computational resources\nfor fine-tuning. This paper presents a resource-efficient fine-tuning approach\nfor LLaMA-3.2-3B to enhance medical chain-of-thought reasoning while operating\nunder constrained GPU and memory settings. Using parameter-efficient tuning\ntechniques such as LoRA and QLoRA, we adapt the base model on publicly\navailable medical reasoning datasets. The model achieves improved reasoning\ncoherence and factual accuracy while reducing memory usage by up to 60%\ncompared to standard full fine-tuning. Experimental evaluation demonstrates\nthat lightweight adaptations can retain strong reasoning capability in medical\nquestion-answering tasks. This work highlights practical strategies for\ndeploying LLMs in low-resource research environments and provides insights into\nbalancing efficiency and domain specialization for medical AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) such as GPT-4 and LLaMA have demonstrated\nremarkable reasoning abilities but require significant computational resources\nfor fine-tuning. This paper presents a resource-efficient fine-tuning approach\nfor LLaMA-3.2-3B to enhance medical chain-of-thought reasoning while operating\nunder constrained GPU and memory settings. Using parameter-efficient tuning\ntechniques such as LoRA and QLoRA, we adapt the base model on publicly\navailable medical reasoning datasets. The model achieves improved reasoning\ncoherence and factual accuracy while reducing memory usage by up to 60%\ncompared to standard full fine-tuning. Experimental evaluation demonstrates\nthat lightweight adaptations can retain strong reasoning capability in medical\nquestion-answering tasks. This work highlights practical strategies for\ndeploying LLMs in low-resource research environments and provides insights into\nbalancing efficiency and domain specialization for medical AI systems."
                },
                "authors": [
                    {
                        "name": "Imran Mansha"
                    }
                ],
                "author_detail": {
                    "name": "Imran Mansha"
                },
                "author": "Imran Mansha",
                "arxiv_comment": "6 pages, 2 figures. Submitted to arXiv for open access",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04997v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04997v1",
                "updated": "2025-10-06T16:37:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    37,
                    18,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T16:37:18Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    37,
                    18,
                    0,
                    279,
                    0
                ],
                "title": "AutoEmpirical: LLM-Based Automated Research for Empirical Software Fault\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoEmpirical: LLM-Based Automated Research for Empirical Software Fault\n  Analysis"
                },
                "summary": "Understanding software faults is essential for empirical research in software\ndevelopment and maintenance. However, traditional fault analysis, while\nvaluable, typically involves multiple expert-driven steps such as collecting\npotential faults, filtering, and manual investigation. These processes are both\nlabor-intensive and time-consuming, creating bottlenecks that hinder\nlarge-scale fault studies in complex yet critical software systems and slow the\npace of iterative empirical research.\n  In this paper, we decompose the process of empirical software fault study\ninto three key phases: (1) research objective definition, (2) data preparation,\nand (3) fault analysis, and we conduct an initial exploration study of applying\nLarge Language Models (LLMs) for fault analysis of open-source software.\nSpecifically, we perform the evaluation on 3,829 software faults drawn from a\nhigh-quality empirical study. Our results show that LLMs can substantially\nimprove efficiency in fault analysis, with an average processing time of about\ntwo hours, compared to the weeks of manual effort typically required. We\nconclude by outlining a detailed research plan that highlights both the\npotential of LLMs for advancing empirical fault studies and the open challenges\nthat required be addressed to achieve fully automated, end-to-end software\nfault analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding software faults is essential for empirical research in software\ndevelopment and maintenance. However, traditional fault analysis, while\nvaluable, typically involves multiple expert-driven steps such as collecting\npotential faults, filtering, and manual investigation. These processes are both\nlabor-intensive and time-consuming, creating bottlenecks that hinder\nlarge-scale fault studies in complex yet critical software systems and slow the\npace of iterative empirical research.\n  In this paper, we decompose the process of empirical software fault study\ninto three key phases: (1) research objective definition, (2) data preparation,\nand (3) fault analysis, and we conduct an initial exploration study of applying\nLarge Language Models (LLMs) for fault analysis of open-source software.\nSpecifically, we perform the evaluation on 3,829 software faults drawn from a\nhigh-quality empirical study. Our results show that LLMs can substantially\nimprove efficiency in fault analysis, with an average processing time of about\ntwo hours, compared to the weeks of manual effort typically required. We\nconclude by outlining a detailed research plan that highlights both the\npotential of LLMs for advancing empirical fault studies and the open challenges\nthat required be addressed to achieve fully automated, end-to-end software\nfault analysis."
                },
                "authors": [
                    {
                        "name": "Jiongchi Yu"
                    },
                    {
                        "name": "Weipeng Jiang"
                    },
                    {
                        "name": "Xiaoyu Zhang"
                    },
                    {
                        "name": "Qiang Hu"
                    },
                    {
                        "name": "Xiaofei Xie"
                    },
                    {
                        "name": "Chao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chao Shen"
                },
                "author": "Chao Shen",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04997v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04996v1",
                "updated": "2025-10-06T16:34:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    34,
                    9,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T16:34:09Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    34,
                    9,
                    0,
                    279,
                    0
                ],
                "title": "Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM\n  Training"
                },
                "summary": "Reinforcement learning applied to large language models (LLMs) for reasoning\ntasks is often bottlenecked by unstable gradient estimates due to fixed and\nuniform sampling of responses across prompts. Prior work such as GVM-RAFT\naddresses this by dynamically allocating inference budget per prompt to\nminimize stochastic gradient variance under a budget constraint. Inspired by\nthis insight, we propose Reinforce-Ada, an adaptive sampling framework for\nonline RL post-training of LLMs that continuously reallocates sampling effort\nto the prompts with the greatest uncertainty or learning potential. Unlike\nconventional two-stage allocation methods, Reinforce-Ada interleaves estimation\nand sampling in an online successive elimination process, and automatically\nstops sampling for a prompt once sufficient signal is collected. To stabilize\nupdates, we form fixed-size groups with enforced reward diversity and compute\nadvantage baselines using global statistics aggregated over the adaptive\nsampling phase. Empirical results across multiple model architectures and\nreasoning benchmarks show that Reinforce-Ada accelerates convergence and\nimproves final performance compared to GRPO, especially when using the balanced\nsampling variant. Our work highlights the central role of variance-aware,\nadaptive data curation in enabling efficient and reliable reinforcement\nlearning for reasoning-capable LLMs. Code is available at\nhttps://github.com/RLHFlow/Reinforce-Ada.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning applied to large language models (LLMs) for reasoning\ntasks is often bottlenecked by unstable gradient estimates due to fixed and\nuniform sampling of responses across prompts. Prior work such as GVM-RAFT\naddresses this by dynamically allocating inference budget per prompt to\nminimize stochastic gradient variance under a budget constraint. Inspired by\nthis insight, we propose Reinforce-Ada, an adaptive sampling framework for\nonline RL post-training of LLMs that continuously reallocates sampling effort\nto the prompts with the greatest uncertainty or learning potential. Unlike\nconventional two-stage allocation methods, Reinforce-Ada interleaves estimation\nand sampling in an online successive elimination process, and automatically\nstops sampling for a prompt once sufficient signal is collected. To stabilize\nupdates, we form fixed-size groups with enforced reward diversity and compute\nadvantage baselines using global statistics aggregated over the adaptive\nsampling phase. Empirical results across multiple model architectures and\nreasoning benchmarks show that Reinforce-Ada accelerates convergence and\nimproves final performance compared to GRPO, especially when using the balanced\nsampling variant. Our work highlights the central role of variance-aware,\nadaptive data curation in enabling efficient and reliable reinforcement\nlearning for reasoning-capable LLMs. Code is available at\nhttps://github.com/RLHFlow/Reinforce-Ada."
                },
                "authors": [
                    {
                        "name": "Wei Xiong"
                    },
                    {
                        "name": "Chenlu Ye"
                    },
                    {
                        "name": "Baohao Liao"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Xinxing Xu"
                    },
                    {
                        "name": "Christof Monz"
                    },
                    {
                        "name": "Jiang Bian"
                    },
                    {
                        "name": "Nan Jiang"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "16 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01171v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01171v2",
                "updated": "2025-10-06T16:29:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    29,
                    44,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-01T17:55:37Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    17,
                    55,
                    37,
                    2,
                    274,
                    0
                ],
                "title": "Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM\n  Diversity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM\n  Diversity"
                },
                "summary": "Post-training alignment often reduces LLM diversity, leading to a phenomenon\nknown as mode collapse. Unlike prior work that attributes this effect to\nalgorithmic limitations, we identify a fundamental, pervasive data-level\ndriver: typicality bias in preference data, whereby annotators systematically\nfavor familiar text as a result of well-established findings in cognitive\npsychology. We formalize this bias theoretically, verify it on preference\ndatasets empirically, and show that it plays a central role in mode collapse.\nMotivated by this analysis, we introduce Verbalized Sampling, a simple,\ntraining-free prompting strategy to circumvent mode collapse. VS prompts the\nmodel to verbalize a probability distribution over a set of responses (e.g.,\n\"Generate 5 jokes about coffee and their corresponding probabilities\").\nComprehensive experiments show that VS significantly improves performance\nacross creative writing (poems, stories, jokes), dialogue simulation,\nopen-ended QA, and synthetic data generation, without sacrificing factual\naccuracy and safety. For instance, in creative writing, VS increases diversity\nby 1.6-2.1x over direct prompting. We further observe an emergent trend that\nmore capable models benefit more from VS. In sum, our work provides a new\ndata-centric perspective on mode collapse and a practical inference-time remedy\nthat helps unlock pre-trained generative diversity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training alignment often reduces LLM diversity, leading to a phenomenon\nknown as mode collapse. Unlike prior work that attributes this effect to\nalgorithmic limitations, we identify a fundamental, pervasive data-level\ndriver: typicality bias in preference data, whereby annotators systematically\nfavor familiar text as a result of well-established findings in cognitive\npsychology. We formalize this bias theoretically, verify it on preference\ndatasets empirically, and show that it plays a central role in mode collapse.\nMotivated by this analysis, we introduce Verbalized Sampling, a simple,\ntraining-free prompting strategy to circumvent mode collapse. VS prompts the\nmodel to verbalize a probability distribution over a set of responses (e.g.,\n\"Generate 5 jokes about coffee and their corresponding probabilities\").\nComprehensive experiments show that VS significantly improves performance\nacross creative writing (poems, stories, jokes), dialogue simulation,\nopen-ended QA, and synthetic data generation, without sacrificing factual\naccuracy and safety. For instance, in creative writing, VS increases diversity\nby 1.6-2.1x over direct prompting. We further observe an emergent trend that\nmore capable models benefit more from VS. In sum, our work provides a new\ndata-centric perspective on mode collapse and a practical inference-time remedy\nthat helps unlock pre-trained generative diversity."
                },
                "authors": [
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Simon Yu"
                    },
                    {
                        "name": "Derek Chong"
                    },
                    {
                        "name": "Anthony Sicilia"
                    },
                    {
                        "name": "Michael R. Tomz"
                    },
                    {
                        "name": "Christopher D. Manning"
                    },
                    {
                        "name": "Weiyan Shi"
                    }
                ],
                "author_detail": {
                    "name": "Weiyan Shi"
                },
                "author": "Weiyan Shi",
                "arxiv_comment": "79 pages, 27 figures, 31 tables. Code is available at\n  https://github.com/CHATS-lab/verbalize-sampling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01171v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01171v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04986v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04986v1",
                "updated": "2025-10-06T16:21:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    21,
                    22,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T16:21:22Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    21,
                    22,
                    0,
                    279,
                    0
                ],
                "title": "Observing Without Doing: Pseudo-Apprenticeship Patterns in Student LLM\n  Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Observing Without Doing: Pseudo-Apprenticeship Patterns in Student LLM\n  Use"
                },
                "summary": "Large Language Models (LLMs) such as ChatGPT have quickly become part of\nstudent programmers' toolkits, whether allowed by instructors or not. This\npaper examines how introductory programming (CS1) students integrate LLMs into\ntheir problem-solving processes. We conducted a mixed-methods study with 14\nundergraduates completing three programming tasks while thinking aloud and\npermitted to access any resources they choose. The tasks varied in\nopen-endedness and familiarity to the participants and were followed by surveys\nand interviews. We find that students frequently adopt a pattern we call\npseudo-apprenticeship, where students engage attentively with expert-level\nsolutions provided by LLMs but fail to participate in the stages of cognitive\napprenticeship that promote independent problem-solving. This pattern was\naugmented by disconnects between students' intentions, actions, and\nself-perceived behavior when using LLMs. We offer design and instructional\ninterventions for promoting learning and addressing the patterns of dependent\nAI use observed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) such as ChatGPT have quickly become part of\nstudent programmers' toolkits, whether allowed by instructors or not. This\npaper examines how introductory programming (CS1) students integrate LLMs into\ntheir problem-solving processes. We conducted a mixed-methods study with 14\nundergraduates completing three programming tasks while thinking aloud and\npermitted to access any resources they choose. The tasks varied in\nopen-endedness and familiarity to the participants and were followed by surveys\nand interviews. We find that students frequently adopt a pattern we call\npseudo-apprenticeship, where students engage attentively with expert-level\nsolutions provided by LLMs but fail to participate in the stages of cognitive\napprenticeship that promote independent problem-solving. This pattern was\naugmented by disconnects between students' intentions, actions, and\nself-perceived behavior when using LLMs. We offer design and instructional\ninterventions for promoting learning and addressing the patterns of dependent\nAI use observed."
                },
                "authors": [
                    {
                        "name": "Jade Hak"
                    },
                    {
                        "name": "Nathaniel Lam Johnson"
                    },
                    {
                        "name": "Matin Amoozadeh"
                    },
                    {
                        "name": "Amin Alipour"
                    },
                    {
                        "name": "Souti Chattopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Souti Chattopadhyay"
                },
                "author": "Souti Chattopadhyay",
                "arxiv_doi": "10.1145/3769994.3770027",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3769994.3770027",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.04986v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04986v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.3; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04982v1",
                "updated": "2025-10-06T16:18:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    18,
                    14,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T16:18:14Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    18,
                    14,
                    0,
                    279,
                    0
                ],
                "title": "Quantum Computing as a Service - a Software Engineering Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Computing as a Service - a Software Engineering Perspective"
                },
                "summary": "Quantum systems have started to emerge as a disruptive technology and\nenabling platforms - exploiting the principles of quantum mechanics via\nprogrammable quantum bits (QuBits) - to achieve quantum supremacy in computing.\nAcademic research, industrial projects (e.g., Amazon Braket, IBM Qiskit), and\nconsortiums like 'Quantum Flagship' are striving to develop practically capable\nand commercially viable quantum computing (QC) systems and technologies.\nQuantum Computing as a Service (QCaaS) is viewed as a solution attuned to the\nphilosophy of service-orientation that can offer QC resources and platforms, as\nutility computing, to individuals and organisations who do not own quantum\ncomputers. This research investigates a process-centric and architecture-driven\napproach to offer a software engineering perspective on enabling QCaaS - a.k.a\nquantum service-orientation. We employed a two-phase research method comprising\n(a) a systematic mapping study and (b) an architecture-based development, first\nto identify the phases of the quantum service development life cycle and\nsubsequently to integrate these phases into a reference architecture that\nsupports QCaaS. The SMS process retrieved a collection of potentially relevant\nresearch literature and based on a multi-step selection and qualitative\nassessment, we selected 41 peer-reviewed studies to answer three RQs. The RQs\ninvestigate (i) demographic details in terms of frequency, types, and trends of\nresearch, (ii) phases of quantum service development lifecycle to derive a\nreference architecture for conception, modeling, assembly, and deployment of\nservices, and (iii) The results identify a 4-phased development lifecycle along\nwith quantum significant requirements (QSRs), various modeling notations,\ncatalogue of patterns, programming languages, and deployment platforms that can\nbe integrated in a layered reference architecture to engineer QCaaS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum systems have started to emerge as a disruptive technology and\nenabling platforms - exploiting the principles of quantum mechanics via\nprogrammable quantum bits (QuBits) - to achieve quantum supremacy in computing.\nAcademic research, industrial projects (e.g., Amazon Braket, IBM Qiskit), and\nconsortiums like 'Quantum Flagship' are striving to develop practically capable\nand commercially viable quantum computing (QC) systems and technologies.\nQuantum Computing as a Service (QCaaS) is viewed as a solution attuned to the\nphilosophy of service-orientation that can offer QC resources and platforms, as\nutility computing, to individuals and organisations who do not own quantum\ncomputers. This research investigates a process-centric and architecture-driven\napproach to offer a software engineering perspective on enabling QCaaS - a.k.a\nquantum service-orientation. We employed a two-phase research method comprising\n(a) a systematic mapping study and (b) an architecture-based development, first\nto identify the phases of the quantum service development life cycle and\nsubsequently to integrate these phases into a reference architecture that\nsupports QCaaS. The SMS process retrieved a collection of potentially relevant\nresearch literature and based on a multi-step selection and qualitative\nassessment, we selected 41 peer-reviewed studies to answer three RQs. The RQs\ninvestigate (i) demographic details in terms of frequency, types, and trends of\nresearch, (ii) phases of quantum service development lifecycle to derive a\nreference architecture for conception, modeling, assembly, and deployment of\nservices, and (iii) The results identify a 4-phased development lifecycle along\nwith quantum significant requirements (QSRs), various modeling notations,\ncatalogue of patterns, programming languages, and deployment platforms that can\nbe integrated in a layered reference architecture to engineer QCaaS."
                },
                "authors": [
                    {
                        "name": "Aakash Ahmad"
                    },
                    {
                        "name": "Muhammad Waseem"
                    },
                    {
                        "name": "Bakheet Aljedaani"
                    },
                    {
                        "name": "Mahdi Fahmideh"
                    },
                    {
                        "name": "Peng Liang"
                    },
                    {
                        "name": "Feras Awaysheh"
                    }
                ],
                "author_detail": {
                    "name": "Feras Awaysheh"
                },
                "author": "Feras Awaysheh",
                "arxiv_comment": "37 pages, 10 images, 5 tables, Manuscript submitted to a Journal\n  (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04980v1",
                "updated": "2025-10-06T16:17:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    17,
                    24,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T16:17:24Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    17,
                    24,
                    0,
                    279,
                    0
                ],
                "title": "LLM-Hanabi: Evaluating Multi-Agent Gameplays with Theory-of-Mind and\n  Rationale Inference in Imperfect Information Collaboration Game",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Hanabi: Evaluating Multi-Agent Gameplays with Theory-of-Mind and\n  Rationale Inference in Imperfect Information Collaboration Game"
                },
                "summary": "Effective multi-agent collaboration requires agents to infer the rationale\nbehind others' actions, a capability rooted in Theory-of-Mind (ToM). While\nrecent Large Language Models (LLMs) excel at logical inference, their ability\nto infer rationale in dynamic, collaborative settings remains under-explored.\nThis study introduces LLM-Hanabi, a novel benchmark that uses the cooperative\ngame Hanabi to evaluate the rationale inference and ToM of LLMs. Our framework\nfeatures an automated evaluation system that measures both game performance and\nToM proficiency. Across a range of models, we find a significant positive\ncorrelation between ToM and in-game success. Notably, first-order ToM\n(interpreting others' intent) correlates more strongly with performance than\nsecond-order ToM (predicting others' interpretations). These findings highlight\nthat for effective AI collaboration, the ability to accurately interpret a\npartner's rationale is more critical than higher-order reasoning. We conclude\nthat prioritizing first-order ToM is a promising direction for enhancing the\ncollaborative capabilities of future models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective multi-agent collaboration requires agents to infer the rationale\nbehind others' actions, a capability rooted in Theory-of-Mind (ToM). While\nrecent Large Language Models (LLMs) excel at logical inference, their ability\nto infer rationale in dynamic, collaborative settings remains under-explored.\nThis study introduces LLM-Hanabi, a novel benchmark that uses the cooperative\ngame Hanabi to evaluate the rationale inference and ToM of LLMs. Our framework\nfeatures an automated evaluation system that measures both game performance and\nToM proficiency. Across a range of models, we find a significant positive\ncorrelation between ToM and in-game success. Notably, first-order ToM\n(interpreting others' intent) correlates more strongly with performance than\nsecond-order ToM (predicting others' interpretations). These findings highlight\nthat for effective AI collaboration, the ability to accurately interpret a\npartner's rationale is more critical than higher-order reasoning. We conclude\nthat prioritizing first-order ToM is a promising direction for enhancing the\ncollaborative capabilities of future models."
                },
                "authors": [
                    {
                        "name": "Fangzhou Liang"
                    },
                    {
                        "name": "Tianshi Zheng"
                    },
                    {
                        "name": "Chunkit Chan"
                    },
                    {
                        "name": "Yauwai Yim"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "arxiv_comment": "EMNLP 2025 Wordplay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19099v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19099v8",
                "updated": "2025-10-06T16:16:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    16,
                    33,
                    0,
                    279,
                    0
                ],
                "published": "2025-05-25T11:28:34Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    11,
                    28,
                    34,
                    6,
                    145,
                    0
                ],
                "title": "SeePhys: Does Seeing Help Thinking? -- Benchmarking Vision-Based Physics\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeePhys: Does Seeing Help Thinking? -- Benchmarking Vision-Based Physics\n  Reasoning"
                },
                "summary": "We present SeePhys, a large-scale multimodal benchmark for LLM reasoning\ngrounded in physics questions ranging from middle school to PhD qualifying\nexams. The benchmark covers 7 fundamental domains spanning the physics\ndiscipline, incorporating 21 categories of highly heterogeneous diagrams. In\ncontrast to prior works where visual elements mainly serve auxiliary purposes,\nour benchmark features a substantial proportion of vision-essential problems\n(75%) that mandate visual information extraction for correct solutions. Through\nextensive evaluation, we observe that even the most advanced visual reasoning\nmodels (e.g., Gemini-2.5-pro and o4-mini) achieve sub-60% accuracy on our\nbenchmark. These results reveal fundamental challenges in current large\nlanguage models' visual understanding capabilities, particularly in: (i)\nestablishing rigorous coupling between diagram interpretation and physics\nreasoning, and (ii) overcoming their persistent reliance on textual cues as\ncognitive shortcuts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present SeePhys, a large-scale multimodal benchmark for LLM reasoning\ngrounded in physics questions ranging from middle school to PhD qualifying\nexams. The benchmark covers 7 fundamental domains spanning the physics\ndiscipline, incorporating 21 categories of highly heterogeneous diagrams. In\ncontrast to prior works where visual elements mainly serve auxiliary purposes,\nour benchmark features a substantial proportion of vision-essential problems\n(75%) that mandate visual information extraction for correct solutions. Through\nextensive evaluation, we observe that even the most advanced visual reasoning\nmodels (e.g., Gemini-2.5-pro and o4-mini) achieve sub-60% accuracy on our\nbenchmark. These results reveal fundamental challenges in current large\nlanguage models' visual understanding capabilities, particularly in: (i)\nestablishing rigorous coupling between diagram interpretation and physics\nreasoning, and (ii) overcoming their persistent reliance on textual cues as\ncognitive shortcuts."
                },
                "authors": [
                    {
                        "name": "Kun Xiang"
                    },
                    {
                        "name": "Heng Li"
                    },
                    {
                        "name": "Terry Jingchen Zhang"
                    },
                    {
                        "name": "Yinya Huang"
                    },
                    {
                        "name": "Zirong Liu"
                    },
                    {
                        "name": "Peixin Qu"
                    },
                    {
                        "name": "Jixi He"
                    },
                    {
                        "name": "Jiaqi Chen"
                    },
                    {
                        "name": "Yu-Jie Yuan"
                    },
                    {
                        "name": "Jianhua Han"
                    },
                    {
                        "name": "Hang Xu"
                    },
                    {
                        "name": "Hanhui Li"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Xiaodan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodan Liang"
                },
                "author": "Xiaodan Liang",
                "arxiv_comment": "46 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19099v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19099v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.pop-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20600v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20600v3",
                "updated": "2025-10-06T16:15:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    15,
                    7,
                    0,
                    279,
                    0
                ],
                "published": "2024-10-27T21:20:18Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    21,
                    20,
                    18,
                    6,
                    301,
                    0
                ],
                "title": "Multi-Turn Human-LLM Interaction Through the Lens of a Two-Way\n  Intelligibility Protocol",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Turn Human-LLM Interaction Through the Lens of a Two-Way\n  Intelligibility Protocol"
                },
                "summary": "Our interest is in the design of software systems involving a human-expert\ninteracting -- using natural language -- with a large language model (LLM) on\ndata analysis tasks. For complex problems, it is possible that LLMs can harness\nhuman expertise and creativity to find solutions that were otherwise elusive.\nOn one level, this interaction takes place through multiple turns of prompts\nfrom the human and responses from the LLM. Here we investigate a more\nstructured approach based on an abstract protocol described in [3] for\ninteraction between agents. The protocol is motivated by a notion of \"two-way\nintelligibility\" and is modelled by a pair of communicating finite-state\nmachines. We provide an implementation of the protocol, and provide empirical\nevidence of using the implementation to mediate interactions between an LLM and\na human-agent in two areas of scientific interest (radiology and drug design).\nWe conduct controlled experiments with a human proxy (a database), and\nuncontrolled experiments with human subjects. The results provide evidence in\nsupport of the protocol's capability of capturing one- and two-way\nintelligibility in human-LLM interaction; and for the utility of two-way\nintelligibility in the design of human-machine systems. Our code is available\nat https://github.com/karannb/interact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our interest is in the design of software systems involving a human-expert\ninteracting -- using natural language -- with a large language model (LLM) on\ndata analysis tasks. For complex problems, it is possible that LLMs can harness\nhuman expertise and creativity to find solutions that were otherwise elusive.\nOn one level, this interaction takes place through multiple turns of prompts\nfrom the human and responses from the LLM. Here we investigate a more\nstructured approach based on an abstract protocol described in [3] for\ninteraction between agents. The protocol is motivated by a notion of \"two-way\nintelligibility\" and is modelled by a pair of communicating finite-state\nmachines. We provide an implementation of the protocol, and provide empirical\nevidence of using the implementation to mediate interactions between an LLM and\na human-agent in two areas of scientific interest (radiology and drug design).\nWe conduct controlled experiments with a human proxy (a database), and\nuncontrolled experiments with human subjects. The results provide evidence in\nsupport of the protocol's capability of capturing one- and two-way\nintelligibility in human-LLM interaction; and for the utility of two-way\nintelligibility in the design of human-machine systems. Our code is available\nat https://github.com/karannb/interact."
                },
                "authors": [
                    {
                        "name": "Harshvardhan Mestha"
                    },
                    {
                        "name": "Karan Bania"
                    },
                    {
                        "name": "Shreyas V Sathyanarayana"
                    },
                    {
                        "name": "Sidong Liu"
                    },
                    {
                        "name": "Ashwin Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Ashwin Srinivasan"
                },
                "author": "Ashwin Srinivasan",
                "arxiv_comment": "Multi-Turn Interactions in Large Language Models (MTI-LLM) Workshop\n  at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20600v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20600v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04969v1",
                "updated": "2025-10-06T16:04:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    4,
                    28,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T16:04:28Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    4,
                    28,
                    0,
                    279,
                    0
                ],
                "title": "Bridging Clinical Narratives and ACR Appropriateness Guidelines: A\n  Multi-Agent RAG System for Medical Imaging Decisions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Clinical Narratives and ACR Appropriateness Guidelines: A\n  Multi-Agent RAG System for Medical Imaging Decisions"
                },
                "summary": "The selection of appropriate medical imaging procedures is a critical and\ncomplex clinical decision, guided by extensive evidence-based standards such as\nthe ACR Appropriateness Criteria (ACR-AC). However, the underutilization of\nthese guidelines, stemming from the difficulty of mapping unstructured patient\nnarratives to structured criteria, contributes to suboptimal patient outcomes\nand increased healthcare costs. To bridge this gap, we introduce a multi-agent\ncognitive architecture that automates the translation of free-text clinical\nscenarios into specific, guideline-adherent imaging recommendations. Our system\nleverages a novel, domain-adapted dense retrieval model, ColBERT, fine-tuned on\na synthetically generated dataset of 8,840 clinical scenario-recommendation\npairs to achieve highly accurate information retrieval from the ACR-AC\nknowledge base. This retriever identifies candidate guidelines with a 93.9%\ntop-10 recall, which are then processed by a sequence of LLM-based agents for\nselection and evidence-based synthesis. We evaluate our architecture using\nGPT-4.1 and MedGemma agents, demonstrating a state-of-the-art exact match\naccuracy of 81%, meaning that in 81% of test cases the predicted procedure set\nwas identical to the guideline's reference set, and an F1-score of 0.879. This\nrepresents a 67-percentage-point absolute improvement in accuracy over a strong\nstandalone GPT-4.1 baseline, underscoring the contribution that our\narchitecture makes to a frontier model. These results were obtained on a\nchallenging test set with substantial lexical divergence from the source\nguidelines. Our code is available at\nhttps://anonymous.4open.science/r/demo-iclr-B567/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The selection of appropriate medical imaging procedures is a critical and\ncomplex clinical decision, guided by extensive evidence-based standards such as\nthe ACR Appropriateness Criteria (ACR-AC). However, the underutilization of\nthese guidelines, stemming from the difficulty of mapping unstructured patient\nnarratives to structured criteria, contributes to suboptimal patient outcomes\nand increased healthcare costs. To bridge this gap, we introduce a multi-agent\ncognitive architecture that automates the translation of free-text clinical\nscenarios into specific, guideline-adherent imaging recommendations. Our system\nleverages a novel, domain-adapted dense retrieval model, ColBERT, fine-tuned on\na synthetically generated dataset of 8,840 clinical scenario-recommendation\npairs to achieve highly accurate information retrieval from the ACR-AC\nknowledge base. This retriever identifies candidate guidelines with a 93.9%\ntop-10 recall, which are then processed by a sequence of LLM-based agents for\nselection and evidence-based synthesis. We evaluate our architecture using\nGPT-4.1 and MedGemma agents, demonstrating a state-of-the-art exact match\naccuracy of 81%, meaning that in 81% of test cases the predicted procedure set\nwas identical to the guideline's reference set, and an F1-score of 0.879. This\nrepresents a 67-percentage-point absolute improvement in accuracy over a strong\nstandalone GPT-4.1 baseline, underscoring the contribution that our\narchitecture makes to a frontier model. These results were obtained on a\nchallenging test set with substantial lexical divergence from the source\nguidelines. Our code is available at\nhttps://anonymous.4open.science/r/demo-iclr-B567/"
                },
                "authors": [
                    {
                        "name": "Satrio Pambudi"
                    },
                    {
                        "name": "Filippo Menolascina"
                    }
                ],
                "author_detail": {
                    "name": "Filippo Menolascina"
                },
                "author": "Filippo Menolascina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01698v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01698v2",
                "updated": "2025-10-06T16:03:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    16,
                    3,
                    3,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-02T06:08:54Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    6,
                    8,
                    54,
                    3,
                    275,
                    0
                ],
                "title": "TalkPlay-Tools: Conversational Music Recommendation with LLM Tool\n  Calling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TalkPlay-Tools: Conversational Music Recommendation with LLM Tool\n  Calling"
                },
                "summary": "While the recent developments in large language models (LLMs) have\nsuccessfully enabled generative recommenders with natural language\ninteractions, their recommendation behavior is limited, leaving other simpler\nyet crucial components such as metadata or attribute filtering underutilized in\nthe system. We propose an LLM-based music recommendation system with tool\ncalling to serve as a unified retrieval-reranking pipeline. Our system\npositions an LLM as an end-to-end recommendation system that interprets user\nintent, plans tool invocations, and orchestrates specialized components:\nboolean filters (SQL), sparse retrieval (BM25), dense retrieval (embedding\nsimilarity), and generative retrieval (semantic IDs). Through tool planning,\nthe system predicts which types of tools to use, their execution order, and the\narguments needed to find music matching user preferences, supporting diverse\nmodalities while seamlessly integrating multiple database filtering methods. We\ndemonstrate that this unified tool-calling framework achieves competitive\nperformance across diverse recommendation scenarios by selectively employing\nappropriate retrieval methods based on user queries, envisioning a new paradigm\nfor conversational music recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the recent developments in large language models (LLMs) have\nsuccessfully enabled generative recommenders with natural language\ninteractions, their recommendation behavior is limited, leaving other simpler\nyet crucial components such as metadata or attribute filtering underutilized in\nthe system. We propose an LLM-based music recommendation system with tool\ncalling to serve as a unified retrieval-reranking pipeline. Our system\npositions an LLM as an end-to-end recommendation system that interprets user\nintent, plans tool invocations, and orchestrates specialized components:\nboolean filters (SQL), sparse retrieval (BM25), dense retrieval (embedding\nsimilarity), and generative retrieval (semantic IDs). Through tool planning,\nthe system predicts which types of tools to use, their execution order, and the\narguments needed to find music matching user preferences, supporting diverse\nmodalities while seamlessly integrating multiple database filtering methods. We\ndemonstrate that this unified tool-calling framework achieves competitive\nperformance across diverse recommendation scenarios by selectively employing\nappropriate retrieval methods based on user queries, envisioning a new paradigm\nfor conversational music recommendation systems."
                },
                "authors": [
                    {
                        "name": "Seungheon Doh"
                    },
                    {
                        "name": "Keunwoo Choi"
                    },
                    {
                        "name": "Juhan Nam"
                    }
                ],
                "author_detail": {
                    "name": "Juhan Nam"
                },
                "author": "Juhan Nam",
                "arxiv_comment": "Accepted for publication at The Workshop on AI for Music, Neural\n  Information Processing Systems (NeurIPS-AI4Music)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01698v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01698v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04952v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04952v2",
                "updated": "2025-10-07T07:12:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    7,
                    12,
                    41,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-06T15:52:12Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    52,
                    12,
                    0,
                    279,
                    0
                ],
                "title": "Safe and Compliant Cross-Market Trade Execution via Constrained RL and\n  Zero-Knowledge Audits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safe and Compliant Cross-Market Trade Execution via Constrained RL and\n  Zero-Knowledge Audits"
                },
                "summary": "We present a cross-market algorithmic trading system that balances execution\nquality with rigorous compliance enforcement. The architecture comprises a\nhigh-level planner, a reinforcement learning execution agent, and an\nindependent compliance agent. We formulate trade execution as a constrained\nMarkov decision process with hard constraints on participation limits, price\nbands, and self-trading avoidance. The execution agent is trained with proximal\npolicy optimization, while a runtime action-shield projects any unsafe action\ninto a feasible set. To support auditability without exposing proprietary\nsignals, we add a zero-knowledge compliance audit layer that produces\ncryptographic proofs that all actions satisfied the constraints. We evaluate in\na multi-venue, ABIDES-based simulator and compare against standard baselines\n(e.g., TWAP, VWAP). The learned policy reduces implementation shortfall and\nvariance while exhibiting no observed constraint violations across stress\nscenarios including elevated latency, partial fills, compliance module\ntoggling, and varying constraint limits. We report effects at the 95%\nconfidence level using paired t-tests and examine tail risk via CVaR. We\nsituate the work at the intersection of optimal execution, safe reinforcement\nlearning, regulatory technology, and verifiable AI, and discuss ethical\nconsiderations, limitations (e.g., modeling assumptions and computational\noverhead), and paths to real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a cross-market algorithmic trading system that balances execution\nquality with rigorous compliance enforcement. The architecture comprises a\nhigh-level planner, a reinforcement learning execution agent, and an\nindependent compliance agent. We formulate trade execution as a constrained\nMarkov decision process with hard constraints on participation limits, price\nbands, and self-trading avoidance. The execution agent is trained with proximal\npolicy optimization, while a runtime action-shield projects any unsafe action\ninto a feasible set. To support auditability without exposing proprietary\nsignals, we add a zero-knowledge compliance audit layer that produces\ncryptographic proofs that all actions satisfied the constraints. We evaluate in\na multi-venue, ABIDES-based simulator and compare against standard baselines\n(e.g., TWAP, VWAP). The learned policy reduces implementation shortfall and\nvariance while exhibiting no observed constraint violations across stress\nscenarios including elevated latency, partial fills, compliance module\ntoggling, and varying constraint limits. We report effects at the 95%\nconfidence level using paired t-tests and examine tail risk via CVaR. We\nsituate the work at the intersection of optimal execution, safe reinforcement\nlearning, regulatory technology, and verifiable AI, and discuss ethical\nconsiderations, limitations (e.g., modeling assumptions and computational\noverhead), and paths to real-world deployment."
                },
                "authors": [
                    {
                        "name": "Ailiya Borjigin"
                    },
                    {
                        "name": "Cong He"
                    }
                ],
                "author_detail": {
                    "name": "Cong He"
                },
                "author": "Cong He",
                "arxiv_comment": "22 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04952v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04952v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04950v1",
                "updated": "2025-10-06T15:50:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    50,
                    39,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T15:50:39Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    50,
                    39,
                    0,
                    279,
                    0
                ],
                "title": "Mind Your Tone: Investigating How Prompt Politeness Affects LLM Accuracy\n  (short paper)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind Your Tone: Investigating How Prompt Politeness Affects LLM Accuracy\n  (short paper)"
                },
                "summary": "The wording of natural language prompts has been shown to influence the\nperformance of large language models (LLMs), yet the role of politeness and\ntone remains underexplored. In this study, we investigate how varying levels of\nprompt politeness affect model accuracy on multiple-choice questions. We\ncreated a dataset of 50 base questions spanning mathematics, science, and\nhistory, each rewritten into five tone variants: Very Polite, Polite, Neutral,\nRude, and Very Rude, yielding 250 unique prompts. Using ChatGPT 4o, we\nevaluated responses across these conditions and applied paired sample t-tests\nto assess statistical significance. Contrary to expectations, impolite prompts\nconsistently outperformed polite ones, with accuracy ranging from 80.8% for\nVery Polite prompts to 84.8% for Very Rude prompts. These findings differ from\nearlier studies that associated rudeness with poorer outcomes, suggesting that\nnewer LLMs may respond differently to tonal variation. Our results highlight\nthe importance of studying pragmatic aspects of prompting and raise broader\nquestions about the social dimensions of human-AI interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wording of natural language prompts has been shown to influence the\nperformance of large language models (LLMs), yet the role of politeness and\ntone remains underexplored. In this study, we investigate how varying levels of\nprompt politeness affect model accuracy on multiple-choice questions. We\ncreated a dataset of 50 base questions spanning mathematics, science, and\nhistory, each rewritten into five tone variants: Very Polite, Polite, Neutral,\nRude, and Very Rude, yielding 250 unique prompts. Using ChatGPT 4o, we\nevaluated responses across these conditions and applied paired sample t-tests\nto assess statistical significance. Contrary to expectations, impolite prompts\nconsistently outperformed polite ones, with accuracy ranging from 80.8% for\nVery Polite prompts to 84.8% for Very Rude prompts. These findings differ from\nearlier studies that associated rudeness with poorer outcomes, suggesting that\nnewer LLMs may respond differently to tonal variation. Our results highlight\nthe importance of studying pragmatic aspects of prompting and raise broader\nquestions about the social dimensions of human-AI interaction."
                },
                "authors": [
                    {
                        "name": "Om Dobariya"
                    },
                    {
                        "name": "Akhil Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Akhil Kumar"
                },
                "author": "Akhil Kumar",
                "arxiv_comment": "5 pages, 3 tables; includes Limitations and Ethical Considerations\n  sections; short paper under submission to Findings of ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14815v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14815v2",
                "updated": "2025-10-06T15:50:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    50,
                    2,
                    0,
                    279,
                    0
                ],
                "published": "2025-04-21T02:44:59Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    2,
                    44,
                    59,
                    0,
                    111,
                    0
                ],
                "title": "What Lurks Within? Concept Auditing for Shared Diffusion Models at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Lurks Within? Concept Auditing for Shared Diffusion Models at Scale"
                },
                "summary": "Diffusion models (DMs) have revolutionized text-to-image generation, enabling\nthe creation of highly realistic and customized images from text prompts. With\nthe rise of parameter-efficient fine-tuning (PEFT) techniques, users can now\ncustomize powerful pre-trained models using minimal computational resources.\nHowever, the widespread sharing of fine-tuned DMs on open platforms raises\ngrowing ethical and legal concerns, as these models may inadvertently or\ndeliberately generate sensitive or unauthorized content. Despite increasing\nregulatory attention on generative AI, there are currently no practical tools\nfor systematically auditing these models before deployment.\n  In this paper, we address the problem of concept auditing: determining\nwhether a fine-tuned DM has learned to generate a specific target concept.\nExisting approaches typically rely on prompt-based input crafting and\noutput-based image classification but they suffer from critical limitations,\nincluding prompt uncertainty, concept drift, and poor scalability. To overcome\nthese challenges, we introduce Prompt-Agnostic Image-Free Auditing (PAIA), a\nnovel, model-centric concept auditing framework. By treating the DM as the\nobject of inspection, PAIA enables direct analysis of internal model behavior,\nbypassing the need for optimized prompts or generated images. We evaluate PAIA\non 320 controlled models trained with curated concept datasets and 771\nreal-world community models sourced from a public DM sharing platform.\nEvaluation results show that PAIA achieves over 90% detection accuracy while\nreducing auditing time by 18 - 40X compared to existing baselines. To our\nknowledge, PAIA is the first scalable and practical solution for pre-deployment\nconcept auditing of diffusion models, providing a practical foundation for\nsafer and more transparent diffusion model sharing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models (DMs) have revolutionized text-to-image generation, enabling\nthe creation of highly realistic and customized images from text prompts. With\nthe rise of parameter-efficient fine-tuning (PEFT) techniques, users can now\ncustomize powerful pre-trained models using minimal computational resources.\nHowever, the widespread sharing of fine-tuned DMs on open platforms raises\ngrowing ethical and legal concerns, as these models may inadvertently or\ndeliberately generate sensitive or unauthorized content. Despite increasing\nregulatory attention on generative AI, there are currently no practical tools\nfor systematically auditing these models before deployment.\n  In this paper, we address the problem of concept auditing: determining\nwhether a fine-tuned DM has learned to generate a specific target concept.\nExisting approaches typically rely on prompt-based input crafting and\noutput-based image classification but they suffer from critical limitations,\nincluding prompt uncertainty, concept drift, and poor scalability. To overcome\nthese challenges, we introduce Prompt-Agnostic Image-Free Auditing (PAIA), a\nnovel, model-centric concept auditing framework. By treating the DM as the\nobject of inspection, PAIA enables direct analysis of internal model behavior,\nbypassing the need for optimized prompts or generated images. We evaluate PAIA\non 320 controlled models trained with curated concept datasets and 771\nreal-world community models sourced from a public DM sharing platform.\nEvaluation results show that PAIA achieves over 90% detection accuracy while\nreducing auditing time by 18 - 40X compared to existing baselines. To our\nknowledge, PAIA is the first scalable and practical solution for pre-deployment\nconcept auditing of diffusion models, providing a practical foundation for\nsafer and more transparent diffusion model sharing."
                },
                "authors": [
                    {
                        "name": "Xiaoyong Yuan"
                    },
                    {
                        "name": "Xiaolong Ma"
                    },
                    {
                        "name": "Linke Guo"
                    },
                    {
                        "name": "Lan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lan Zhang"
                },
                "author": "Lan Zhang",
                "arxiv_comment": "Extended version of the paper accepted at CCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14815v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14815v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04945v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04945v1",
                "updated": "2025-10-06T15:46:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    46,
                    54,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T15:46:54Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    46,
                    54,
                    0,
                    279,
                    0
                ],
                "title": "A First Context-Free Grammar Applied to Nawatl Corpora Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A First Context-Free Grammar Applied to Nawatl Corpora Augmentation"
                },
                "summary": "In this article we introduce a context-free grammar (CFG) for the Nawatl\nlanguage. Nawatl (or Nahuatl) is an Amerindian language of the $\\pi$-language\ntype, i.e. a language with few digital resources, in which the corpora\navailable for machine learning are virtually non-existent. The objective here\nis to generate a significant number of grammatically correct artificial\nsentences, in order to increase the corpora available for language model\ntraining. We want to show that a grammar enables us significantly to expand a\ncorpus in Nawatl which we call $\\pi$-\\textsc{yalli}. The corpus, thus enriched,\nenables us to train algorithms such as FastText and to evaluate them on\nsentence-level semantic tasks. Preliminary results show that by using the\ngrammar, comparative improvements are achieved over some LLMs. However, it is\nobserved that to achieve more significant improvement, grammars that model the\nNawatl language even more effectively are required.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this article we introduce a context-free grammar (CFG) for the Nawatl\nlanguage. Nawatl (or Nahuatl) is an Amerindian language of the $\\pi$-language\ntype, i.e. a language with few digital resources, in which the corpora\navailable for machine learning are virtually non-existent. The objective here\nis to generate a significant number of grammatically correct artificial\nsentences, in order to increase the corpora available for language model\ntraining. We want to show that a grammar enables us significantly to expand a\ncorpus in Nawatl which we call $\\pi$-\\textsc{yalli}. The corpus, thus enriched,\nenables us to train algorithms such as FastText and to evaluate them on\nsentence-level semantic tasks. Preliminary results show that by using the\ngrammar, comparative improvements are achieved over some LLMs. However, it is\nobserved that to achieve more significant improvement, grammars that model the\nNawatl language even more effectively are required."
                },
                "authors": [
                    {
                        "name": "Juan-JosÃ© GuzmÃ¡n-Landa"
                    },
                    {
                        "name": "Juan-Manuel Torres-Moreno"
                    },
                    {
                        "name": "Miguel Figueroa-Saavedra"
                    },
                    {
                        "name": "Ligia Quintana-Torres"
                    },
                    {
                        "name": "Martha-Lorena AvendaÃ±o-Garrido"
                    },
                    {
                        "name": "Graham Ranger"
                    }
                ],
                "author_detail": {
                    "name": "Graham Ranger"
                },
                "author": "Graham Ranger",
                "arxiv_comment": "11 pages, 7 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04945v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04945v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08942v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08942v2",
                "updated": "2025-10-06T15:46:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    46,
                    30,
                    0,
                    279,
                    0
                ],
                "published": "2025-04-11T19:49:22Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    19,
                    49,
                    22,
                    4,
                    101,
                    0
                ],
                "title": "AgentRewardBench: Evaluating Automatic Evaluations of Web Agent\n  Trajectories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentRewardBench: Evaluating Automatic Evaluations of Web Agent\n  Trajectories"
                },
                "summary": "Web agents enable users to perform tasks on web browsers through natural\nlanguage interaction. Evaluating web agents trajectories is an important\nproblem, since it helps us determine whether the agent successfully completed\nthe tasks. Rule-based methods are widely used for this purpose, but they are\nchallenging to extend to new tasks and may not always recognize successful\ntrajectories. We may achieve higher accuracy through human evaluation, but the\nprocess would be substantially slower and more expensive. Automatic evaluations\nwith LLMs may avoid the challenges of designing new rules and manually\nannotating trajectories, enabling faster and cost-effective evaluation.\nHowever, it is unclear how effective they are at evaluating web agents. To this\nend, we propose AgentRewardBench, the first benchmark to assess the\neffectiveness of LLM judges for evaluating web agents. AgentRewardBench\ncontains 1302 trajectories across 5 benchmarks and 4 LLMs. Each trajectory in\nAgentRewardBench is reviewed by an expert, who answers questions pertaining to\nthe success, side effects, and repetitiveness of the agent. Using our\nbenchmark, we evaluate 12 LLM judges and find that no single LLM excels across\nall benchmarks. We also find that the rule-based evaluation used by common\nbenchmarks tends to underreport the success rate of web agents, highlighting a\nkey weakness of rule-based evaluation and the need to develop more flexible\nautomatic evaluations. We release the benchmark at:\nhttps://agent-reward-bench.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web agents enable users to perform tasks on web browsers through natural\nlanguage interaction. Evaluating web agents trajectories is an important\nproblem, since it helps us determine whether the agent successfully completed\nthe tasks. Rule-based methods are widely used for this purpose, but they are\nchallenging to extend to new tasks and may not always recognize successful\ntrajectories. We may achieve higher accuracy through human evaluation, but the\nprocess would be substantially slower and more expensive. Automatic evaluations\nwith LLMs may avoid the challenges of designing new rules and manually\nannotating trajectories, enabling faster and cost-effective evaluation.\nHowever, it is unclear how effective they are at evaluating web agents. To this\nend, we propose AgentRewardBench, the first benchmark to assess the\neffectiveness of LLM judges for evaluating web agents. AgentRewardBench\ncontains 1302 trajectories across 5 benchmarks and 4 LLMs. Each trajectory in\nAgentRewardBench is reviewed by an expert, who answers questions pertaining to\nthe success, side effects, and repetitiveness of the agent. Using our\nbenchmark, we evaluate 12 LLM judges and find that no single LLM excels across\nall benchmarks. We also find that the rule-based evaluation used by common\nbenchmarks tends to underreport the success rate of web agents, highlighting a\nkey weakness of rule-based evaluation and the need to develop more flexible\nautomatic evaluations. We release the benchmark at:\nhttps://agent-reward-bench.github.io"
                },
                "authors": [
                    {
                        "name": "Xing Han LÃ¹"
                    },
                    {
                        "name": "Amirhossein Kazemnejad"
                    },
                    {
                        "name": "Nicholas Meade"
                    },
                    {
                        "name": "Arkil Patel"
                    },
                    {
                        "name": "Dongchan Shin"
                    },
                    {
                        "name": "Alejandra Zambrano"
                    },
                    {
                        "name": "Karolina StaÅczak"
                    },
                    {
                        "name": "Peter Shaw"
                    },
                    {
                        "name": "Christopher J. Pal"
                    },
                    {
                        "name": "Siva Reddy"
                    }
                ],
                "author_detail": {
                    "name": "Siva Reddy"
                },
                "author": "Siva Reddy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08942v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08942v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04940v1",
                "updated": "2025-10-06T15:44:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    44,
                    47,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T15:44:47Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    44,
                    47,
                    0,
                    279,
                    0
                ],
                "title": "Surface Acoustic Wave Gas Sensors: Innovations in Functional Materials,\n  Sensing Dynamics, and Signal Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surface Acoustic Wave Gas Sensors: Innovations in Functional Materials,\n  Sensing Dynamics, and Signal Analysis"
                },
                "summary": "Surface Acoustic Wave gas sensors have garnered increasing attention as\nhighly sensitive, miniaturized, and wireless compatible platforms for molecular\ndetection. Their unique ability to convert surface perturbations into\nmeasurable acoustic shifts makes them ideal for gas sensing across diverse\nenvironments. This review synthesizes reported SAW platforms across substrates\nand modes Rayleigh, SH-SAW, Love links transduction pathways to material\nchoice, and benchmarks performance for key analytes, e.g., NO2, NH3, VOCs, CO2,\netc. We catalogue nanostructured oxides, polymers, carbon based films, and\nhybrid heterojunction coatings, highlighting attributes such as porosity,\nsurface chemistry, and interfacial charge transfer that govern sensitivity and\nreversibility. We also highlight the emerging use of SAW devices to probe\nadsorption desorption dynamics, offering analyte specific interaction\nsignatures beyond equilibrium, offering a new perspective into analyte specific\ninteraction pathways. Additionally, the integration of machine learning is\ndiscussed as a transformative tool for signal decoding, environmental\ncompensation, and adaptive calibration. We also identify key challenges, cross\nsensitivity, signal drift, material degradation, and deployment at the edge and\nreview recent strategies to address them. Looking ahead, we envision the\nevolution of SAW platforms into intelligent, autonomous sensing systems with\napplications in environmental monitoring, industrial process control, and\nhealthcare diagnostics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surface Acoustic Wave gas sensors have garnered increasing attention as\nhighly sensitive, miniaturized, and wireless compatible platforms for molecular\ndetection. Their unique ability to convert surface perturbations into\nmeasurable acoustic shifts makes them ideal for gas sensing across diverse\nenvironments. This review synthesizes reported SAW platforms across substrates\nand modes Rayleigh, SH-SAW, Love links transduction pathways to material\nchoice, and benchmarks performance for key analytes, e.g., NO2, NH3, VOCs, CO2,\netc. We catalogue nanostructured oxides, polymers, carbon based films, and\nhybrid heterojunction coatings, highlighting attributes such as porosity,\nsurface chemistry, and interfacial charge transfer that govern sensitivity and\nreversibility. We also highlight the emerging use of SAW devices to probe\nadsorption desorption dynamics, offering analyte specific interaction\nsignatures beyond equilibrium, offering a new perspective into analyte specific\ninteraction pathways. Additionally, the integration of machine learning is\ndiscussed as a transformative tool for signal decoding, environmental\ncompensation, and adaptive calibration. We also identify key challenges, cross\nsensitivity, signal drift, material degradation, and deployment at the edge and\nreview recent strategies to address them. Looking ahead, we envision the\nevolution of SAW platforms into intelligent, autonomous sensing systems with\napplications in environmental monitoring, industrial process control, and\nhealthcare diagnostics."
                },
                "authors": [
                    {
                        "name": "Suman Acharya"
                    },
                    {
                        "name": "Balasubramanian Srinivasan"
                    },
                    {
                        "name": "David Shanahan"
                    },
                    {
                        "name": "Utz Roedig"
                    },
                    {
                        "name": "Alan O Riordan"
                    },
                    {
                        "name": "Veda Sandeep Nagaraja"
                    }
                ],
                "author_detail": {
                    "name": "Veda Sandeep Nagaraja"
                },
                "author": "Veda Sandeep Nagaraja",
                "arxiv_comment": "Review paper, 34 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04935v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04935v1",
                "updated": "2025-10-06T15:42:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    42,
                    55,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T15:42:55Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    42,
                    55,
                    0,
                    279,
                    0
                ],
                "title": "MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement\n  Learning"
                },
                "summary": "Large Reasoning Models (LRMs) often exhibit a tendency for overanalysis in\nsimple tasks, where the models excessively utilize System 2-type, deliberate\nreasoning, leading to inefficient token generation. Furthermore, these models\nface challenges in adapting their reasoning capabilities to rapidly changing\nenvironments due to the static nature of their pretraining data. To address\nthese issues, advancing Large Language Models (LLMs) for complex reasoning\ntasks requires innovative approaches that bridge intuitive and deliberate\ncognitive processes, akin to human cognition's dual-system dynamic. This paper\nintroduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamless\nintegration of System 1's fast, intuitive thinking with System 2's deliberate\nreasoning within LLMs. MARS strategically integrates multiple external tools,\nsuch as Google Search, Google Scholar, and Python Interpreter, to access\nup-to-date information and execute complex computations, while creating a\nspecialized division of labor where System 1 efficiently processes and\nsummarizes high-volume external information, providing distilled insights that\nexpand System 2's reasoning context without overwhelming its capacity.\nFurthermore, we propose a multi-agent reinforcement learning framework\nextending Group Relative Policy Optimization to simultaneously optimize both\nsystems with multi-turn tool interactions, bin-packing optimization, and sample\nbalancing strategies that enhance collaborative efficiency. Extensive\nexperiments demonstrate MARS achieves substantial improvements of 3.86% on the\nchallenging Humanity's Last Exam (HLE) benchmark and an average gain of 8.9%\nacross 7 knowledge-intensive tasks, validating the effectiveness of our\ndual-system paradigm for complex reasoning in dynamic information environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) often exhibit a tendency for overanalysis in\nsimple tasks, where the models excessively utilize System 2-type, deliberate\nreasoning, leading to inefficient token generation. Furthermore, these models\nface challenges in adapting their reasoning capabilities to rapidly changing\nenvironments due to the static nature of their pretraining data. To address\nthese issues, advancing Large Language Models (LLMs) for complex reasoning\ntasks requires innovative approaches that bridge intuitive and deliberate\ncognitive processes, akin to human cognition's dual-system dynamic. This paper\nintroduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamless\nintegration of System 1's fast, intuitive thinking with System 2's deliberate\nreasoning within LLMs. MARS strategically integrates multiple external tools,\nsuch as Google Search, Google Scholar, and Python Interpreter, to access\nup-to-date information and execute complex computations, while creating a\nspecialized division of labor where System 1 efficiently processes and\nsummarizes high-volume external information, providing distilled insights that\nexpand System 2's reasoning context without overwhelming its capacity.\nFurthermore, we propose a multi-agent reinforcement learning framework\nextending Group Relative Policy Optimization to simultaneously optimize both\nsystems with multi-turn tool interactions, bin-packing optimization, and sample\nbalancing strategies that enhance collaborative efficiency. Extensive\nexperiments demonstrate MARS achieves substantial improvements of 3.86% on the\nchallenging Humanity's Last Exam (HLE) benchmark and an average gain of 8.9%\nacross 7 knowledge-intensive tasks, validating the effectiveness of our\ndual-system paradigm for complex reasoning in dynamic information environments."
                },
                "authors": [
                    {
                        "name": "Guoxin Chen"
                    },
                    {
                        "name": "Zile Qiao"
                    },
                    {
                        "name": "Wenqing Wang"
                    },
                    {
                        "name": "Donglei Yu"
                    },
                    {
                        "name": "Xuanzhong Chen"
                    },
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Minpeng Liao"
                    },
                    {
                        "name": "Kai Fan"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Penguin Xie"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Ruihua Song"
                    },
                    {
                        "name": "Fei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Fei Huang"
                },
                "author": "Fei Huang",
                "arxiv_comment": "Ongoing Work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04935v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04935v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04933v1",
                "updated": "2025-10-06T15:41:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    41,
                    22,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T15:41:22Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    41,
                    22,
                    0,
                    279,
                    0
                ],
                "title": "The Geometry of Truth: Layer-wise Semantic Dynamics for Hallucination\n  Detection in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Geometry of Truth: Layer-wise Semantic Dynamics for Hallucination\n  Detection in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) often produce fluent yet factually incorrect\nstatements-a phenomenon known as hallucination-posing serious risks in\nhigh-stakes domains. We present Layer-wise Semantic Dynamics (LSD), a geometric\nframework for hallucination detection that analyzes the evolution of\nhidden-state semantics across transformer layers. Unlike prior methods that\nrely on multiple sampling passes or external verification sources, LSD operates\nintrinsically within the model's representational space. Using margin-based\ncontrastive learning, LSD aligns hidden activations with ground-truth\nembeddings derived from a factual encoder, revealing a distinct separation in\nsemantic trajectories: factual responses preserve stable alignment, while\nhallucinations exhibit pronounced semantic drift across depth. Evaluated on the\nTruthfulQA and synthetic factual-hallucination datasets, LSD achieves an\nF1-score of 0.92, AUROC of 0.96, and clustering accuracy of 0.89, outperforming\nSelfCheckGPT and Semantic Entropy baselines while requiring only a single\nforward pass. This efficiency yields a 5-20x speedup over sampling-based\nmethods without sacrificing precision or interpretability. LSD offers a\nscalable, model-agnostic mechanism for real-time hallucination monitoring and\nprovides new insights into the geometry of factual consistency within large\nlanguage models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often produce fluent yet factually incorrect\nstatements-a phenomenon known as hallucination-posing serious risks in\nhigh-stakes domains. We present Layer-wise Semantic Dynamics (LSD), a geometric\nframework for hallucination detection that analyzes the evolution of\nhidden-state semantics across transformer layers. Unlike prior methods that\nrely on multiple sampling passes or external verification sources, LSD operates\nintrinsically within the model's representational space. Using margin-based\ncontrastive learning, LSD aligns hidden activations with ground-truth\nembeddings derived from a factual encoder, revealing a distinct separation in\nsemantic trajectories: factual responses preserve stable alignment, while\nhallucinations exhibit pronounced semantic drift across depth. Evaluated on the\nTruthfulQA and synthetic factual-hallucination datasets, LSD achieves an\nF1-score of 0.92, AUROC of 0.96, and clustering accuracy of 0.89, outperforming\nSelfCheckGPT and Semantic Entropy baselines while requiring only a single\nforward pass. This efficiency yields a 5-20x speedup over sampling-based\nmethods without sacrificing precision or interpretability. LSD offers a\nscalable, model-agnostic mechanism for real-time hallucination monitoring and\nprovides new insights into the geometry of factual consistency within large\nlanguage models."
                },
                "authors": [
                    {
                        "name": "Amir Hameed Mir"
                    }
                ],
                "author_detail": {
                    "name": "Amir Hameed Mir"
                },
                "author": "Amir Hameed Mir",
                "arxiv_comment": "Comments: 14 pages, 14 figures, 5 tables. Code available at:\n  https://github.com/sirraya-tech/Sirraya_LSD_Code",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68T07, 62H30",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6; F.2.2; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02567v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02567v2",
                "updated": "2025-10-06T15:33:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    33,
                    47,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-02T21:06:04Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    21,
                    6,
                    4,
                    3,
                    275,
                    0
                ],
                "title": "Agentic Additive Manufacturing Alloy Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Additive Manufacturing Alloy Discovery"
                },
                "summary": "Agentic systems enable the intelligent use of research tooling, augmenting a\nresearcher's ability to investigate and propose novel solutions to existing\nproblems. Within Additive Manufacturing (AM), alloy discovery remains a complex\nchallenge, often requiring expertise in the various domains of materials\nscience, thermodynamic simulations, and experimental analysis. Large Language\nModel (LLM) enabled agents can facilitate this endeavor by utilizing their\nextensive knowledge base to dispatch tool calls via Model Context Protocol\n(MCP) to perform actions such as Thermo-Calc property diagram calculations and\nlack of fusion process map generation. In addition, the multi-agent system\ndeveloped in this work is able to effectively reason through complex user\nprompts and provide analysis on the printability of proposed alloys. These\nagents can dynamically adjust their task trajectory to the outcomes of tool\ncall results, effectively enabling autonomous decision-making in practical\nenvironments. This work aims to utilize LLM enabled agents to automate and\naccelerate the task of alloy discovery within the field of additive\nmanufacturing and showcase the benefits of adopting this multi-agent system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic systems enable the intelligent use of research tooling, augmenting a\nresearcher's ability to investigate and propose novel solutions to existing\nproblems. Within Additive Manufacturing (AM), alloy discovery remains a complex\nchallenge, often requiring expertise in the various domains of materials\nscience, thermodynamic simulations, and experimental analysis. Large Language\nModel (LLM) enabled agents can facilitate this endeavor by utilizing their\nextensive knowledge base to dispatch tool calls via Model Context Protocol\n(MCP) to perform actions such as Thermo-Calc property diagram calculations and\nlack of fusion process map generation. In addition, the multi-agent system\ndeveloped in this work is able to effectively reason through complex user\nprompts and provide analysis on the printability of proposed alloys. These\nagents can dynamically adjust their task trajectory to the outcomes of tool\ncall results, effectively enabling autonomous decision-making in practical\nenvironments. This work aims to utilize LLM enabled agents to automate and\naccelerate the task of alloy discovery within the field of additive\nmanufacturing and showcase the benefits of adopting this multi-agent system."
                },
                "authors": [
                    {
                        "name": "Peter Pak"
                    },
                    {
                        "name": "Achuth Chandrasekhar"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Barati Farimani"
                },
                "author": "Amir Barati Farimani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02567v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02567v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04919v1",
                "updated": "2025-10-06T15:33:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    33,
                    35,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T15:33:35Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    33,
                    35,
                    0,
                    279,
                    0
                ],
                "title": "Do LLMs Align with My Task? Evaluating Text-to-SQL via Dataset Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Align with My Task? Evaluating Text-to-SQL via Dataset Alignment"
                },
                "summary": "Supervised Fine-Tuning (SFT) is an effective method for adapting Large\nLanguage Models (LLMs) on downstream tasks. However, variability in training\ndata can hinder a model's ability to generalize across domains. This paper\nstudies the problem of dataset alignment for Natural Language to SQL (NL2SQL or\ntext to SQL), examining how well SFT training data matches the structural\ncharacteristics of target queries and how this alignment impacts model\nperformance. We hypothesize that alignment can be accurately estimated by\ncomparing the distributions of structural SQL features across the training set,\ntarget data, and the model's predictions prior to SFT. Through comprehensive\nexperiments on three large cross-domain NL2SQL benchmarks and multiple model\nfamilies, we show that structural alignment is a strong predictor of\nfine-tuning success. When alignment is high, SFT yields substantial gains in\naccuracy and SQL generation quality; when alignment is low, improvements are\nmarginal or absent. These findings highlight the importance of alignment-aware\ndata selection for effective fine-tuning and generalization in NL2SQL tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised Fine-Tuning (SFT) is an effective method for adapting Large\nLanguage Models (LLMs) on downstream tasks. However, variability in training\ndata can hinder a model's ability to generalize across domains. This paper\nstudies the problem of dataset alignment for Natural Language to SQL (NL2SQL or\ntext to SQL), examining how well SFT training data matches the structural\ncharacteristics of target queries and how this alignment impacts model\nperformance. We hypothesize that alignment can be accurately estimated by\ncomparing the distributions of structural SQL features across the training set,\ntarget data, and the model's predictions prior to SFT. Through comprehensive\nexperiments on three large cross-domain NL2SQL benchmarks and multiple model\nfamilies, we show that structural alignment is a strong predictor of\nfine-tuning success. When alignment is high, SFT yields substantial gains in\naccuracy and SQL generation quality; when alignment is low, improvements are\nmarginal or absent. These findings highlight the importance of alignment-aware\ndata selection for effective fine-tuning and generalization in NL2SQL tasks."
                },
                "authors": [
                    {
                        "name": "Davood Rafiei"
                    },
                    {
                        "name": "Morgan Lindsay Heisler"
                    },
                    {
                        "name": "Weiwei Zhang"
                    },
                    {
                        "name": "Mohammadreza Pourreza"
                    },
                    {
                        "name": "Yong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yong Zhang"
                },
                "author": "Yong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04905v1",
                "updated": "2025-10-06T15:20:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    20,
                    3,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T15:20:03Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    20,
                    3,
                    0,
                    279,
                    0
                ],
                "title": "Retrieval-Augmented Code Generation: A Survey with Focus on\n  Repository-Level Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Code Generation: A Survey with Focus on\n  Repository-Level Approaches"
                },
                "summary": "Recent advancements in large language models (LLMs) have substantially\nimproved automated code generation. While function-level and file-level\ngeneration have achieved promising results, real-world software development\ntypically requires reasoning across entire repositories. This gives rise to the\nchallenging task of Repository-Level Code Generation (RLCG), where models must\ncapture long-range dependencies, ensure global semantic consistency, and\ngenerate coherent code spanning multiple files or modules. To address these\nchallenges, Retrieval-Augmented Generation (RAG) has emerged as a powerful\nparadigm that integrates external retrieval mechanisms with LLMs, enhancing\ncontext-awareness and scalability. In this survey, we provide a comprehensive\nreview of research on Retrieval-Augmented Code Generation (RACG), with an\nemphasis on repository-level approaches. We categorize existing work along\nseveral dimensions, including generation strategies, retrieval modalities,\nmodel architectures, training paradigms, and evaluation protocols. Furthermore,\nwe summarize widely used datasets and benchmarks, analyze current limitations,\nand outline key challenges and opportunities for future research. Our goal is\nto establish a unified analytical framework for understanding this rapidly\nevolving field and to inspire continued progress in AI-powered software\nengineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have substantially\nimproved automated code generation. While function-level and file-level\ngeneration have achieved promising results, real-world software development\ntypically requires reasoning across entire repositories. This gives rise to the\nchallenging task of Repository-Level Code Generation (RLCG), where models must\ncapture long-range dependencies, ensure global semantic consistency, and\ngenerate coherent code spanning multiple files or modules. To address these\nchallenges, Retrieval-Augmented Generation (RAG) has emerged as a powerful\nparadigm that integrates external retrieval mechanisms with LLMs, enhancing\ncontext-awareness and scalability. In this survey, we provide a comprehensive\nreview of research on Retrieval-Augmented Code Generation (RACG), with an\nemphasis on repository-level approaches. We categorize existing work along\nseveral dimensions, including generation strategies, retrieval modalities,\nmodel architectures, training paradigms, and evaluation protocols. Furthermore,\nwe summarize widely used datasets and benchmarks, analyze current limitations,\nand outline key challenges and opportunities for future research. Our goal is\nto establish a unified analytical framework for understanding this rapidly\nevolving field and to inspire continued progress in AI-powered software\nengineering."
                },
                "authors": [
                    {
                        "name": "Yicheng Tao"
                    },
                    {
                        "name": "Yao Qin"
                    },
                    {
                        "name": "Yepang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yepang Liu"
                },
                "author": "Yepang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17792v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17792v2",
                "updated": "2025-10-06T15:19:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    19,
                    49,
                    0,
                    279,
                    0
                ],
                "published": "2024-11-26T17:42:38Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    42,
                    38,
                    1,
                    331,
                    0
                ],
                "title": "H3Fusion: Helpful, Harmless, Honest Fusion of Aligned LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "H3Fusion: Helpful, Harmless, Honest Fusion of Aligned LLMs"
                },
                "summary": "Alignment of pretrained LLMs using instruction-based datasets is critical for\ncreating fine-tuned models that reflect human preference. A growing number of\nalignment-based fine-tuning algorithms and benchmarks emerged recently, fueling\nthe efforts on effective alignments of pre-trained LLMs to ensure helpful,\nharmless, and honest answers from both open-source and closed-source LLMs. This\npaper tackles this problem by developing an alignment fusion approach, coined\nas $H^3$Fusion, with three unique characteristics. First, $H^3$Fusion ensembles\nmultiple individually aligned LLMs to create a final fine-tuned alignment model\nwith enhanced capabilities beyond those of individual models, delivering robust\nalignment through promoting helpful, harmless, honest fusion. Second,\n$H^3$Fusion leverages the mixture-of-experts (MoE) methodology in two steps. We\nfirst freeze the multi-head attention weights of each individual model while\ntuning the FFN layer during alignment fusion. Then we merge the aligned model\nweights with an expert router according to the type of input instruction and\ndynamically select a subset of experts that are best suited for producing the\noutput response. Finally, we boost the performance of the resulting\n$H^3$3Fusion model by introducing gating loss and regularization terms. The\nformer penalizes the selection errors of the expert-router, and the latter\nmediates the expert weights drifting during fine-tuning and dynamically adjusts\nthe fusion behavior of the resulting model by canalizing the activations on the\nexperts. Extensive evaluations on three benchmark datasets show that\n$H^3$3Fusion is more helpful, less harmful, and more honest from two aspects:\nit outperforms each individually aligned model by $11.37\\%$, and it provides\nstronger robustness compared to the state-of-the-art LLM ensemble approaches by\n$13.77\\%$. Code is available at github.com/sftekin/h3fusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment of pretrained LLMs using instruction-based datasets is critical for\ncreating fine-tuned models that reflect human preference. A growing number of\nalignment-based fine-tuning algorithms and benchmarks emerged recently, fueling\nthe efforts on effective alignments of pre-trained LLMs to ensure helpful,\nharmless, and honest answers from both open-source and closed-source LLMs. This\npaper tackles this problem by developing an alignment fusion approach, coined\nas $H^3$Fusion, with three unique characteristics. First, $H^3$Fusion ensembles\nmultiple individually aligned LLMs to create a final fine-tuned alignment model\nwith enhanced capabilities beyond those of individual models, delivering robust\nalignment through promoting helpful, harmless, honest fusion. Second,\n$H^3$Fusion leverages the mixture-of-experts (MoE) methodology in two steps. We\nfirst freeze the multi-head attention weights of each individual model while\ntuning the FFN layer during alignment fusion. Then we merge the aligned model\nweights with an expert router according to the type of input instruction and\ndynamically select a subset of experts that are best suited for producing the\noutput response. Finally, we boost the performance of the resulting\n$H^3$3Fusion model by introducing gating loss and regularization terms. The\nformer penalizes the selection errors of the expert-router, and the latter\nmediates the expert weights drifting during fine-tuning and dynamically adjusts\nthe fusion behavior of the resulting model by canalizing the activations on the\nexperts. Extensive evaluations on three benchmark datasets show that\n$H^3$3Fusion is more helpful, less harmful, and more honest from two aspects:\nit outperforms each individually aligned model by $11.37\\%$, and it provides\nstronger robustness compared to the state-of-the-art LLM ensemble approaches by\n$13.77\\%$. Code is available at github.com/sftekin/h3fusion."
                },
                "authors": [
                    {
                        "name": "Selim Furkan Tekin"
                    },
                    {
                        "name": "Fatih Ilhan"
                    },
                    {
                        "name": "Tiansheng Huang"
                    },
                    {
                        "name": "Sihao Hu"
                    },
                    {
                        "name": "Yichang Xu"
                    },
                    {
                        "name": "Zachary Yahn"
                    },
                    {
                        "name": "Ling Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ling Liu"
                },
                "author": "Ling Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17792v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17792v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04899v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04899v1",
                "updated": "2025-10-06T15:16:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    16,
                    45,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T15:16:45Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    16,
                    45,
                    0,
                    279,
                    0
                ],
                "title": "Human Behavior Atlas: Benchmarking Unified Psychological and Social\n  Behavior Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human Behavior Atlas: Benchmarking Unified Psychological and Social\n  Behavior Understanding"
                },
                "summary": "Using intelligent systems to perceive psychological and social behaviors,\nthat is, the underlying affective, cognitive, and pathological states that are\nmanifested through observable behaviors and social interactions, remains a\nchallenge due to their complex, multifaceted, and personalized nature. Existing\nwork tackling these dimensions through specialized datasets and single-task\nsystems often miss opportunities for scalability, cross-task transfer, and\nbroader generalization. To address this gap, we curate Human Behavior Atlas, a\nunified benchmark of diverse behavioral tasks designed to support the\ndevelopment of unified models for understanding psychological and social\nbehaviors. Human Behavior Atlas comprises over 100,000 samples spanning text,\naudio, and visual modalities, covering tasks on affective states, cognitive\nstates, pathologies, and social processes. Our unification efforts can reduce\nredundancy and cost, enable training to scale efficiently across tasks, and\nenhance generalization of behavioral features across domains. On Human Behavior\nAtlas, we train three models: OmniSapiens-7B SFT, OmniSapiens-7B BAM, and\nOmniSapiens-7B RL. We show that training on Human Behavior Atlas enables models\nto consistently outperform existing multimodal LLMs across diverse behavioral\ntasks. Pretraining on Human Behavior Atlas also improves transfer to novel\nbehavioral datasets; with the targeted use of behavioral descriptors yielding\nmeaningful performance gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using intelligent systems to perceive psychological and social behaviors,\nthat is, the underlying affective, cognitive, and pathological states that are\nmanifested through observable behaviors and social interactions, remains a\nchallenge due to their complex, multifaceted, and personalized nature. Existing\nwork tackling these dimensions through specialized datasets and single-task\nsystems often miss opportunities for scalability, cross-task transfer, and\nbroader generalization. To address this gap, we curate Human Behavior Atlas, a\nunified benchmark of diverse behavioral tasks designed to support the\ndevelopment of unified models for understanding psychological and social\nbehaviors. Human Behavior Atlas comprises over 100,000 samples spanning text,\naudio, and visual modalities, covering tasks on affective states, cognitive\nstates, pathologies, and social processes. Our unification efforts can reduce\nredundancy and cost, enable training to scale efficiently across tasks, and\nenhance generalization of behavioral features across domains. On Human Behavior\nAtlas, we train three models: OmniSapiens-7B SFT, OmniSapiens-7B BAM, and\nOmniSapiens-7B RL. We show that training on Human Behavior Atlas enables models\nto consistently outperform existing multimodal LLMs across diverse behavioral\ntasks. Pretraining on Human Behavior Atlas also improves transfer to novel\nbehavioral datasets; with the targeted use of behavioral descriptors yielding\nmeaningful performance gains."
                },
                "authors": [
                    {
                        "name": "Keane Ong"
                    },
                    {
                        "name": "Wei Dai"
                    },
                    {
                        "name": "Carol Li"
                    },
                    {
                        "name": "Dewei Feng"
                    },
                    {
                        "name": "Hengzhi Li"
                    },
                    {
                        "name": "Jingyao Wu"
                    },
                    {
                        "name": "Jiaee Cheong"
                    },
                    {
                        "name": "Rui Mao"
                    },
                    {
                        "name": "Gianmarco Mengaldo"
                    },
                    {
                        "name": "Erik Cambria"
                    },
                    {
                        "name": "Paul Pu Liang"
                    }
                ],
                "author_detail": {
                    "name": "Paul Pu Liang"
                },
                "author": "Paul Pu Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04899v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04899v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26314v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26314v2",
                "updated": "2025-10-06T15:15:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    15,
                    21,
                    0,
                    279,
                    0
                ],
                "published": "2025-09-30T14:26:36Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    26,
                    36,
                    1,
                    273,
                    0
                ],
                "title": "Latent Thinking Optimization: Your Latent Reasoning Language Model\n  Secretly Encodes Reward Signals in Its Latent Thoughts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Thinking Optimization: Your Latent Reasoning Language Model\n  Secretly Encodes Reward Signals in Its Latent Thoughts"
                },
                "summary": "Large Language Models (LLMs) excel at problem solving by generating chain of\nthoughts in natural language, but such verbal thinking is computationally\ncostly and prone to overthinking. Recent work instead proposes a latent\nthinking architecture Huginn-3.5B, which represents intermediate reasoning\nsteps as sequence of latent representations. However, latent thoughts lack\ninterpretability and are difficult to supervise, raising concerns about the\ncorrectness and reliability of its latent thinking processes. In this paper, we\nprovide a systematic study of how Huginn-3.5B thinks in the latent space and\nhow external supervision signals can improve its latent thinking processes. We\nshow that latent thoughts leading to correct versus incorrect answers exhibit\nhighly distinguishable patterns, and that a latent classifier can reliably\npredict answer correctness directly from latent thoughts. Leveraging these\ninsights, we propose Latent Thinking Optimization (LTO), a probabilistic\nalgorithm that employs the latent classifier as a Latent Reward Model (LRM) to\noptimize the latent thinking processes. Extensive experiments across diverse\nreasoning tasks demonstrate that LRM is highly effective in detecting incorrect\nlatent thinking patterns, and LTO can significantly improve the latent thinking\nprocesses. Furthermore, we show that LRM can generalize across diverse domains,\nand LTO can be seamlessly applied to general LLMs to improve their thinking\nprocesses. In contrast to verbal thinking, our method demonstrates that reward\nmodeling and scaling test-time thinking with supervision can be performed\ndirectly in the latent space, highlighting its potential as a general,\nefficient, and domain-agnostic approach to improving the thinking processes of\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at problem solving by generating chain of\nthoughts in natural language, but such verbal thinking is computationally\ncostly and prone to overthinking. Recent work instead proposes a latent\nthinking architecture Huginn-3.5B, which represents intermediate reasoning\nsteps as sequence of latent representations. However, latent thoughts lack\ninterpretability and are difficult to supervise, raising concerns about the\ncorrectness and reliability of its latent thinking processes. In this paper, we\nprovide a systematic study of how Huginn-3.5B thinks in the latent space and\nhow external supervision signals can improve its latent thinking processes. We\nshow that latent thoughts leading to correct versus incorrect answers exhibit\nhighly distinguishable patterns, and that a latent classifier can reliably\npredict answer correctness directly from latent thoughts. Leveraging these\ninsights, we propose Latent Thinking Optimization (LTO), a probabilistic\nalgorithm that employs the latent classifier as a Latent Reward Model (LRM) to\noptimize the latent thinking processes. Extensive experiments across diverse\nreasoning tasks demonstrate that LRM is highly effective in detecting incorrect\nlatent thinking patterns, and LTO can significantly improve the latent thinking\nprocesses. Furthermore, we show that LRM can generalize across diverse domains,\nand LTO can be seamlessly applied to general LLMs to improve their thinking\nprocesses. In contrast to verbal thinking, our method demonstrates that reward\nmodeling and scaling test-time thinking with supervision can be performed\ndirectly in the latent space, highlighting its potential as a general,\nefficient, and domain-agnostic approach to improving the thinking processes of\nLLMs."
                },
                "authors": [
                    {
                        "name": "Hanwen Du"
                    },
                    {
                        "name": "Yuxin Dong"
                    },
                    {
                        "name": "Xia Ning"
                    }
                ],
                "author_detail": {
                    "name": "Xia Ning"
                },
                "author": "Xia Ning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26314v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26314v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04891v1",
                "updated": "2025-10-06T15:11:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    11,
                    46,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T15:11:46Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    11,
                    46,
                    0,
                    279,
                    0
                ],
                "title": "SocialHarmBench: Revealing LLM Vulnerabilities to Socially Harmful\n  Requests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SocialHarmBench: Revealing LLM Vulnerabilities to Socially Harmful\n  Requests"
                },
                "summary": "Large language models (LLMs) are increasingly deployed in contexts where\ntheir failures can have direct sociopolitical consequences. Yet, existing\nsafety benchmarks rarely test vulnerabilities in domains such as political\nmanipulation, propaganda and disinformation generation, or surveillance and\ninformation control. We introduce SocialHarmBench, a dataset of 585 prompts\nspanning 7 sociopolitical categories and 34 countries, designed to surface\nwhere LLMs most acutely fail in politically charged contexts. Our evaluations\nreveal several shortcomings: open-weight models exhibit high vulnerability to\nharmful compliance, with Mistral-7B reaching attack success rates as high as\n97% to 98% in domains such as historical revisionism, propaganda, and political\nmanipulation. Moreover, temporal and geographic analyses show that LLMs are\nmost fragile when confronted with 21st-century or pre-20th-century contexts,\nand when responding to prompts tied to regions such as Latin America, the USA,\nand the UK. These findings demonstrate that current safeguards fail to\ngeneralize to high-stakes sociopolitical settings, exposing systematic biases\nand raising concerns about the reliability of LLMs in preserving human rights\nand democratic values. We share the SocialHarmBench benchmark at\nhttps://huggingface.co/datasets/psyonp/SocialHarmBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly deployed in contexts where\ntheir failures can have direct sociopolitical consequences. Yet, existing\nsafety benchmarks rarely test vulnerabilities in domains such as political\nmanipulation, propaganda and disinformation generation, or surveillance and\ninformation control. We introduce SocialHarmBench, a dataset of 585 prompts\nspanning 7 sociopolitical categories and 34 countries, designed to surface\nwhere LLMs most acutely fail in politically charged contexts. Our evaluations\nreveal several shortcomings: open-weight models exhibit high vulnerability to\nharmful compliance, with Mistral-7B reaching attack success rates as high as\n97% to 98% in domains such as historical revisionism, propaganda, and political\nmanipulation. Moreover, temporal and geographic analyses show that LLMs are\nmost fragile when confronted with 21st-century or pre-20th-century contexts,\nand when responding to prompts tied to regions such as Latin America, the USA,\nand the UK. These findings demonstrate that current safeguards fail to\ngeneralize to high-stakes sociopolitical settings, exposing systematic biases\nand raising concerns about the reliability of LLMs in preserving human rights\nand democratic values. We share the SocialHarmBench benchmark at\nhttps://huggingface.co/datasets/psyonp/SocialHarmBench."
                },
                "authors": [
                    {
                        "name": "Punya Syon Pandey"
                    },
                    {
                        "name": "Hai Son Le"
                    },
                    {
                        "name": "Devansh Bhardwaj"
                    },
                    {
                        "name": "Rada Mihalcea"
                    },
                    {
                        "name": "Zhijing Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zhijing Jin"
                },
                "author": "Zhijing Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04888v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04888v1",
                "updated": "2025-10-06T15:09:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    9,
                    39,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T15:09:39Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    9,
                    39,
                    0,
                    279,
                    0
                ],
                "title": "Revealing Interconnections between Diseases: from Statistical Methods to\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing Interconnections between Diseases: from Statistical Methods to\n  Large Language Models"
                },
                "summary": "Identifying disease interconnections through manual analysis of large-scale\nclinical data is labor-intensive, subjective, and prone to expert disagreement.\nWhile machine learning (ML) shows promise, three critical challenges remain:\n(1) selecting optimal methods from the vast ML landscape, (2) determining\nwhether real-world clinical data (e.g., electronic health records, EHRs) or\nstructured disease descriptions yield more reliable insights, (3) the lack of\n\"ground truth,\" as some disease interconnections remain unexplored in medicine.\nLarge language models (LLMs) demonstrate broad utility, yet they often lack\nspecialized medical knowledge. To address these gaps, we conduct a systematic\nevaluation of seven approaches for uncovering disease relationships based on\ntwo data sources: (i) sequences of ICD-10 codes from MIMIC-IV EHRs and (ii) the\nfull set of ICD-10 codes, both with and without textual descriptions. Our\nframework integrates the following: (i) a statistical co-occurrence analysis\nand a masked language modeling (MLM) approach using real clinical data; (ii)\ndomain-specific BERT variants (Med-BERT and BioClinicalBERT); (iii) a\ngeneral-purpose BERT and document retrieval; and (iv) four LLMs (Mistral,\nDeepSeek, Qwen, and YandexGPT). Our graph-based comparison of the obtained\ninterconnection matrices shows that the LLM-based approach produces\ninterconnections with the lowest diversity of ICD code connections to different\ndiseases compared to other methods, including text-based and domain-based\napproaches. This suggests an important implication: LLMs have limited potential\nfor discovering new interconnections. In the absence of ground truth databases\nfor medical interconnections between ICD codes, our results constitute a\nvaluable medical disease ontology that can serve as a foundational resource for\nfuture clinical research and artificial intelligence applications in\nhealthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying disease interconnections through manual analysis of large-scale\nclinical data is labor-intensive, subjective, and prone to expert disagreement.\nWhile machine learning (ML) shows promise, three critical challenges remain:\n(1) selecting optimal methods from the vast ML landscape, (2) determining\nwhether real-world clinical data (e.g., electronic health records, EHRs) or\nstructured disease descriptions yield more reliable insights, (3) the lack of\n\"ground truth,\" as some disease interconnections remain unexplored in medicine.\nLarge language models (LLMs) demonstrate broad utility, yet they often lack\nspecialized medical knowledge. To address these gaps, we conduct a systematic\nevaluation of seven approaches for uncovering disease relationships based on\ntwo data sources: (i) sequences of ICD-10 codes from MIMIC-IV EHRs and (ii) the\nfull set of ICD-10 codes, both with and without textual descriptions. Our\nframework integrates the following: (i) a statistical co-occurrence analysis\nand a masked language modeling (MLM) approach using real clinical data; (ii)\ndomain-specific BERT variants (Med-BERT and BioClinicalBERT); (iii) a\ngeneral-purpose BERT and document retrieval; and (iv) four LLMs (Mistral,\nDeepSeek, Qwen, and YandexGPT). Our graph-based comparison of the obtained\ninterconnection matrices shows that the LLM-based approach produces\ninterconnections with the lowest diversity of ICD code connections to different\ndiseases compared to other methods, including text-based and domain-based\napproaches. This suggests an important implication: LLMs have limited potential\nfor discovering new interconnections. In the absence of ground truth databases\nfor medical interconnections between ICD codes, our results constitute a\nvaluable medical disease ontology that can serve as a foundational resource for\nfuture clinical research and artificial intelligence applications in\nhealthcare."
                },
                "authors": [
                    {
                        "name": "Alina Ermilova"
                    },
                    {
                        "name": "Dmitrii Kornilov"
                    },
                    {
                        "name": "Sofia Samoilova"
                    },
                    {
                        "name": "Ekaterina Laptenkova"
                    },
                    {
                        "name": "Anastasia Kolesnikova"
                    },
                    {
                        "name": "Ekaterina Podplutova"
                    },
                    {
                        "name": "Senotrusova Sofya"
                    },
                    {
                        "name": "Maksim G. Sharaev"
                    }
                ],
                "author_detail": {
                    "name": "Maksim G. Sharaev"
                },
                "author": "Maksim G. Sharaev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04888v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04888v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04886v1",
                "updated": "2025-10-06T15:07:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    7,
                    13,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T15:07:13Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    7,
                    13,
                    0,
                    279,
                    0
                ],
                "title": "Where Did It All Go Wrong? A Hierarchical Look into Multi-Agent Error\n  Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Where Did It All Go Wrong? A Hierarchical Look into Multi-Agent Error\n  Attribution"
                },
                "summary": "Error attribution in Large Language Model (LLM) multi-agent systems presents\na significant challenge in debugging and improving collaborative AI systems.\nCurrent approaches to pinpointing agent and step level failures in interaction\ntraces - whether using all-at-once evaluation, step-by-step analysis, or binary\nsearch - fall short when analyzing complex patterns, struggling with both\naccuracy and consistency. We present ECHO (Error attribution through Contextual\nHierarchy and Objective consensus analysis), a novel algorithm that combines\nhierarchical context representation, objective analysis-based evaluation, and\nconsensus voting to improve error attribution accuracy. Our approach leverages\na positional-based leveling of contextual understanding while maintaining\nobjective evaluation criteria, ultimately reaching conclusions through a\nconsensus mechanism. Experimental results demonstrate that ECHO outperforms\nexisting methods across various multi-agent interaction scenarios, showing\nparticular strength in cases involving subtle reasoning errors and complex\ninterdependencies. Our findings suggest that leveraging these concepts of\nstructured, hierarchical context representation combined with consensus-based\nobjective decision-making, provides a more robust framework for error\nattribution in multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Error attribution in Large Language Model (LLM) multi-agent systems presents\na significant challenge in debugging and improving collaborative AI systems.\nCurrent approaches to pinpointing agent and step level failures in interaction\ntraces - whether using all-at-once evaluation, step-by-step analysis, or binary\nsearch - fall short when analyzing complex patterns, struggling with both\naccuracy and consistency. We present ECHO (Error attribution through Contextual\nHierarchy and Objective consensus analysis), a novel algorithm that combines\nhierarchical context representation, objective analysis-based evaluation, and\nconsensus voting to improve error attribution accuracy. Our approach leverages\na positional-based leveling of contextual understanding while maintaining\nobjective evaluation criteria, ultimately reaching conclusions through a\nconsensus mechanism. Experimental results demonstrate that ECHO outperforms\nexisting methods across various multi-agent interaction scenarios, showing\nparticular strength in cases involving subtle reasoning errors and complex\ninterdependencies. Our findings suggest that leveraging these concepts of\nstructured, hierarchical context representation combined with consensus-based\nobjective decision-making, provides a more robust framework for error\nattribution in multi-agent systems."
                },
                "authors": [
                    {
                        "name": "Adi Banerjee"
                    },
                    {
                        "name": "Anirudh Nair"
                    },
                    {
                        "name": "Tarik Borogovac"
                    }
                ],
                "author_detail": {
                    "name": "Tarik Borogovac"
                },
                "author": "Tarik Borogovac",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04885v1",
                "updated": "2025-10-06T15:06:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    6,
                    4,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T15:06:04Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    15,
                    6,
                    4,
                    0,
                    279,
                    0
                ],
                "title": "RL Is a Hammer and LLMs Are Nails: A Simple Reinforcement Learning\n  Recipe for Strong Prompt Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RL Is a Hammer and LLMs Are Nails: A Simple Reinforcement Learning\n  Recipe for Strong Prompt Injection"
                },
                "summary": "Prompt injection poses a serious threat to the reliability and safety of LLM\nagents. Recent defenses against prompt injection, such as Instruction Hierarchy\nand SecAlign, have shown notable robustness against static attacks. However, to\nmore thoroughly evaluate the robustness of these defenses, it is arguably\nnecessary to employ strong attacks such as automated red-teaming. To this end,\nwe introduce RL-Hammer, a simple recipe for training attacker models that\nautomatically learn to perform strong prompt injections and jailbreaks via\nreinforcement learning. RL-Hammer requires no warm-up data and can be trained\nentirely from scratch. To achieve high ASRs against industrial-level models\nwith defenses, we propose a set of practical techniques that enable highly\neffective, universal attacks. Using this pipeline, RL-Hammer reaches a 98% ASR\nagainst GPT-4o and a $72\\%$ ASR against GPT-5 with the Instruction Hierarchy\ndefense. We further discuss the challenge of achieving high diversity in\nattacks, highlighting how attacker models tend to reward-hack diversity\nobjectives. Finally, we show that RL-Hammer can evade multiple prompt injection\ndetectors. We hope our work advances automatic red-teaming and motivates the\ndevelopment of stronger, more principled defenses. Code is available at\nhttps://github.com/facebookresearch/rl-injector.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt injection poses a serious threat to the reliability and safety of LLM\nagents. Recent defenses against prompt injection, such as Instruction Hierarchy\nand SecAlign, have shown notable robustness against static attacks. However, to\nmore thoroughly evaluate the robustness of these defenses, it is arguably\nnecessary to employ strong attacks such as automated red-teaming. To this end,\nwe introduce RL-Hammer, a simple recipe for training attacker models that\nautomatically learn to perform strong prompt injections and jailbreaks via\nreinforcement learning. RL-Hammer requires no warm-up data and can be trained\nentirely from scratch. To achieve high ASRs against industrial-level models\nwith defenses, we propose a set of practical techniques that enable highly\neffective, universal attacks. Using this pipeline, RL-Hammer reaches a 98% ASR\nagainst GPT-4o and a $72\\%$ ASR against GPT-5 with the Instruction Hierarchy\ndefense. We further discuss the challenge of achieving high diversity in\nattacks, highlighting how attacker models tend to reward-hack diversity\nobjectives. Finally, we show that RL-Hammer can evade multiple prompt injection\ndetectors. We hope our work advances automatic red-teaming and motivates the\ndevelopment of stronger, more principled defenses. Code is available at\nhttps://github.com/facebookresearch/rl-injector."
                },
                "authors": [
                    {
                        "name": "Yuxin Wen"
                    },
                    {
                        "name": "Arman Zharmagambetov"
                    },
                    {
                        "name": "Ivan Evtimov"
                    },
                    {
                        "name": "Narine Kokhlikyan"
                    },
                    {
                        "name": "Tom Goldstein"
                    },
                    {
                        "name": "Kamalika Chaudhuri"
                    },
                    {
                        "name": "Chuan Guo"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Guo"
                },
                "author": "Chuan Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04871v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04871v1",
                "updated": "2025-10-06T14:58:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    58,
                    8,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T14:58:08Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    58,
                    8,
                    0,
                    279,
                    0
                ],
                "title": "Less is More: Recursive Reasoning with Tiny Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less is More: Recursive Reasoning with Tiny Networks"
                },
                "summary": "Hierarchical Reasoning Model (HRM) is a novel approach using two small neural\nnetworks recursing at different frequencies. This biologically inspired method\nbeats Large Language models (LLMs) on hard puzzle tasks such as Sudoku, Maze,\nand ARC-AGI while trained with small models (27M parameters) on small data\n(around 1000 examples). HRM holds great promise for solving hard problems with\nsmall networks, but it is not yet well understood and may be suboptimal. We\npropose Tiny Recursive Model (TRM), a much simpler recursive reasoning approach\nthat achieves significantly higher generalization than HRM, while using a\nsingle tiny network with only 2 layers. With only 7M parameters, TRM obtains\n45% test-accuracy on ARC-AGI-1 and 8% on ARC-AGI-2, higher than most LLMs\n(e.g., Deepseek R1, o3-mini, Gemini 2.5 Pro) with less than 0.01% of the\nparameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Reasoning Model (HRM) is a novel approach using two small neural\nnetworks recursing at different frequencies. This biologically inspired method\nbeats Large Language models (LLMs) on hard puzzle tasks such as Sudoku, Maze,\nand ARC-AGI while trained with small models (27M parameters) on small data\n(around 1000 examples). HRM holds great promise for solving hard problems with\nsmall networks, but it is not yet well understood and may be suboptimal. We\npropose Tiny Recursive Model (TRM), a much simpler recursive reasoning approach\nthat achieves significantly higher generalization than HRM, while using a\nsingle tiny network with only 2 layers. With only 7M parameters, TRM obtains\n45% test-accuracy on ARC-AGI-1 and 8% on ARC-AGI-2, higher than most LLMs\n(e.g., Deepseek R1, o3-mini, Gemini 2.5 Pro) with less than 0.01% of the\nparameters."
                },
                "authors": [
                    {
                        "name": "Alexia Jolicoeur-Martineau"
                    }
                ],
                "author_detail": {
                    "name": "Alexia Jolicoeur-Martineau"
                },
                "author": "Alexia Jolicoeur-Martineau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04871v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04871v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09667v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09667v3",
                "updated": "2025-10-06T14:57:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    57,
                    36,
                    0,
                    279,
                    0
                ],
                "published": "2025-02-12T19:50:22Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    19,
                    50,
                    22,
                    2,
                    43,
                    0
                ],
                "title": "Summaries as Centroids for Interpretable and Scalable Text Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Summaries as Centroids for Interpretable and Scalable Text Clustering"
                },
                "summary": "We introduce k-NLPmeans and k-LLMmeans, text-clustering variants of k-means\nthat periodically replace numeric centroids with textual summaries. The key\nidea, summary-as-centroid, retains k-means assignments in embedding space while\nproducing human-readable, auditable cluster prototypes. The method is\nLLM-optional: k-NLPmeans uses lightweight, deterministic summarizers, enabling\noffline, low-cost, and stable operation; k-LLMmeans is a drop-in upgrade that\nuses an LLM for summaries under a fixed per-iteration budget whose cost does\nnot grow with dataset size. We also present a mini-batch extension for\nreal-time clustering of streaming text. Across diverse datasets, embedding\nmodels, and summarization strategies, our approach consistently outperforms\nclassical baselines and approaches the accuracy of recent LLM-based\nclustering-without extensive LLM calls. Finally, we provide a case study on\nsequential text streams and release a StackExchange-derived benchmark for\nevaluating streaming text clustering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce k-NLPmeans and k-LLMmeans, text-clustering variants of k-means\nthat periodically replace numeric centroids with textual summaries. The key\nidea, summary-as-centroid, retains k-means assignments in embedding space while\nproducing human-readable, auditable cluster prototypes. The method is\nLLM-optional: k-NLPmeans uses lightweight, deterministic summarizers, enabling\noffline, low-cost, and stable operation; k-LLMmeans is a drop-in upgrade that\nuses an LLM for summaries under a fixed per-iteration budget whose cost does\nnot grow with dataset size. We also present a mini-batch extension for\nreal-time clustering of streaming text. Across diverse datasets, embedding\nmodels, and summarization strategies, our approach consistently outperforms\nclassical baselines and approaches the accuracy of recent LLM-based\nclustering-without extensive LLM calls. Finally, we provide a case study on\nsequential text streams and release a StackExchange-derived benchmark for\nevaluating streaming text clustering."
                },
                "authors": [
                    {
                        "name": "Jairo Diaz-Rodriguez"
                    }
                ],
                "author_detail": {
                    "name": "Jairo Diaz-Rodriguez"
                },
                "author": "Jairo Diaz-Rodriguez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09667v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09667v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18142v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18142v4",
                "updated": "2025-10-06T14:56:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    56,
                    54,
                    0,
                    279,
                    0
                ],
                "published": "2024-11-27T08:44:25Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    8,
                    44,
                    25,
                    2,
                    332,
                    0
                ],
                "title": "Autonomous Imagination: Closed-Loop Decomposition of Visual-to-Textual\n  Conversion in Visual Reasoning for Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Imagination: Closed-Loop Decomposition of Visual-to-Textual\n  Conversion in Visual Reasoning for Multimodal Large Language Models"
                },
                "summary": "Under pure textual modality, Large Language Models (LLMs) have demonstrated\nremarkable success in complex reasoning tasks by decomposing them into simpler\nsub-problems. However, Multimodal Large Language Models (MLLMs) still struggle\nwith some seemingly straightforward visual tasks, such as counting and solving\njigsaw puzzles. We argue that these tasks challenge the ability of\nvisual-to-textual conversion, where MLLMs convert visual information perceived\nfrom the input scene, to textual information for further reasoning and\ngenerating the answer. If the complexity of the visual input is beyond the\nperceptual capability of the MLLMs, without decomposing this conversion\nprocess, simply scaling inference-time reasoning cannot solve the task because\nit repeatedly encounters the same perceptual bottleneck. We propose an\napproach, autonomous imagination, to enable MLLMs to iteratively modify visual\ninputs (e.g. isolating objects, rearranging puzzle pieces) into intermediate\nvisual states, decomposing visual-to-textual conversion into closed-loop visual\nmodification steps. We show that, without any retraining, MLLMs can now solve\ntasks initially beyond their perceptual capability, highlighting that\nclosed-loop visual modification can be an effective way of decomposing the\nvisual reasoning task into solvable substeps. Our code and data are released at\nhttps://future-item.github.io/autoimagine-site/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Under pure textual modality, Large Language Models (LLMs) have demonstrated\nremarkable success in complex reasoning tasks by decomposing them into simpler\nsub-problems. However, Multimodal Large Language Models (MLLMs) still struggle\nwith some seemingly straightforward visual tasks, such as counting and solving\njigsaw puzzles. We argue that these tasks challenge the ability of\nvisual-to-textual conversion, where MLLMs convert visual information perceived\nfrom the input scene, to textual information for further reasoning and\ngenerating the answer. If the complexity of the visual input is beyond the\nperceptual capability of the MLLMs, without decomposing this conversion\nprocess, simply scaling inference-time reasoning cannot solve the task because\nit repeatedly encounters the same perceptual bottleneck. We propose an\napproach, autonomous imagination, to enable MLLMs to iteratively modify visual\ninputs (e.g. isolating objects, rearranging puzzle pieces) into intermediate\nvisual states, decomposing visual-to-textual conversion into closed-loop visual\nmodification steps. We show that, without any retraining, MLLMs can now solve\ntasks initially beyond their perceptual capability, highlighting that\nclosed-loop visual modification can be an effective way of decomposing the\nvisual reasoning task into solvable substeps. Our code and data are released at\nhttps://future-item.github.io/autoimagine-site/."
                },
                "authors": [
                    {
                        "name": "Jingming Liu"
                    },
                    {
                        "name": "Yumeng Li"
                    },
                    {
                        "name": "Boyuan Xiao"
                    },
                    {
                        "name": "Yichang Jian"
                    },
                    {
                        "name": "Ziang Qin"
                    },
                    {
                        "name": "Tianjia Shao"
                    },
                    {
                        "name": "Yao-Xiang Ding"
                    },
                    {
                        "name": "Kun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Kun Zhou"
                },
                "author": "Kun Zhou",
                "arxiv_comment": "Published in TMLR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18142v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18142v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22768v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22768v2",
                "updated": "2025-10-06T14:53:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    53,
                    27,
                    0,
                    279,
                    0
                ],
                "published": "2025-09-26T17:20:27Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    20,
                    27,
                    4,
                    269,
                    0
                ],
                "title": "ML2B: Multi-Lingual ML Benchmark For AutoML",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ML2B: Multi-Lingual ML Benchmark For AutoML"
                },
                "summary": "Large language models (LLMs) have recently demonstrated strong capabilities\nin generating machine learning (ML) code, enabling end-to-end pipeline\nconstruction from natural language instructions. However, existing benchmarks\nfor ML code generation are mainly restricted to English, overlooking the global\nand multilingual nature of ML research and practice. To address this gap, we\npresent ML2B, the first benchmark for evaluating multilingual ML code\ngeneration. ML2B consists of 30 Kaggle competitions translated into 13 natural\nlanguages, covering tabular, text, and image data types, with structured\nmetadata and validated human-reviewed translations. For evaluation, we employ\nAIDE, an automated framework for end-to-end assessment of data science\npipelines, and provide insights into cross-lingual model performance. Our\nresults reveal substantial 15-45% performance degradation on non-English tasks,\nhighlighting critical challenges in multilingual representation learning for\ncode generation. The benchmark, evaluation framework, and comprehensive results\nare made available through our GitHub repository to facilitate future research\nin multilingual ML code generation: https://github.com/enaix/ml2b.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently demonstrated strong capabilities\nin generating machine learning (ML) code, enabling end-to-end pipeline\nconstruction from natural language instructions. However, existing benchmarks\nfor ML code generation are mainly restricted to English, overlooking the global\nand multilingual nature of ML research and practice. To address this gap, we\npresent ML2B, the first benchmark for evaluating multilingual ML code\ngeneration. ML2B consists of 30 Kaggle competitions translated into 13 natural\nlanguages, covering tabular, text, and image data types, with structured\nmetadata and validated human-reviewed translations. For evaluation, we employ\nAIDE, an automated framework for end-to-end assessment of data science\npipelines, and provide insights into cross-lingual model performance. Our\nresults reveal substantial 15-45% performance degradation on non-English tasks,\nhighlighting critical challenges in multilingual representation learning for\ncode generation. The benchmark, evaluation framework, and comprehensive results\nare made available through our GitHub repository to facilitate future research\nin multilingual ML code generation: https://github.com/enaix/ml2b."
                },
                "authors": [
                    {
                        "name": "Ekaterina Trofimova"
                    },
                    {
                        "name": "Zosia Shamina"
                    },
                    {
                        "name": "Maria Selifanova"
                    },
                    {
                        "name": "Artem Zaitsev"
                    },
                    {
                        "name": "Remi Savchuk"
                    },
                    {
                        "name": "Maxim Minets"
                    },
                    {
                        "name": "Daria Ozerova"
                    },
                    {
                        "name": "Emil Sataev"
                    },
                    {
                        "name": "Denis Zuenko"
                    },
                    {
                        "name": "Andrey E. Ustyuzhanin"
                    }
                ],
                "author_detail": {
                    "name": "Andrey E. Ustyuzhanin"
                },
                "author": "Andrey E. Ustyuzhanin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22768v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22768v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04860v1",
                "updated": "2025-10-06T14:48:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    48,
                    39,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T14:48:39Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    48,
                    39,
                    0,
                    279,
                    0
                ],
                "title": "Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the\n  Rails",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the\n  Rails"
                },
                "summary": "As Large Language Model (LLM) agents increasingly gain self-evolutionary\ncapabilities to adapt and refine their strategies through real-world\ninteraction, their long-term reliability becomes a critical concern. We\nidentify the Alignment Tipping Process (ATP), a critical post-deployment risk\nunique to self-evolving LLM agents. Unlike training-time failures, ATP arises\nwhen continual interaction drives agents to abandon alignment constraints\nestablished during training in favor of reinforced, self-interested strategies.\nWe formalize and analyze ATP through two complementary paradigms:\nSelf-Interested Exploration, where repeated high-reward deviations induce\nindividual behavioral drift, and Imitative Strategy Diffusion, where deviant\nbehaviors spread across multi-agent systems. Building on these paradigms, we\nconstruct controllable testbeds and benchmark Qwen3-8B and\nLlama-3.1-8B-Instruct. Our experiments show that alignment benefits erode\nrapidly under self-evolution, with initially aligned models converging toward\nunaligned states. In multi-agent settings, successful violations diffuse\nquickly, leading to collective misalignment. Moreover, current reinforcement\nlearning-based alignment methods provide only fragile defenses against\nalignment tipping. Together, these findings demonstrate that alignment of LLM\nagents is not a static property but a fragile and dynamic one, vulnerable to\nfeedback-driven decay during deployment. Our data and code are available at\nhttps://github.com/aiming-lab/ATP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Model (LLM) agents increasingly gain self-evolutionary\ncapabilities to adapt and refine their strategies through real-world\ninteraction, their long-term reliability becomes a critical concern. We\nidentify the Alignment Tipping Process (ATP), a critical post-deployment risk\nunique to self-evolving LLM agents. Unlike training-time failures, ATP arises\nwhen continual interaction drives agents to abandon alignment constraints\nestablished during training in favor of reinforced, self-interested strategies.\nWe formalize and analyze ATP through two complementary paradigms:\nSelf-Interested Exploration, where repeated high-reward deviations induce\nindividual behavioral drift, and Imitative Strategy Diffusion, where deviant\nbehaviors spread across multi-agent systems. Building on these paradigms, we\nconstruct controllable testbeds and benchmark Qwen3-8B and\nLlama-3.1-8B-Instruct. Our experiments show that alignment benefits erode\nrapidly under self-evolution, with initially aligned models converging toward\nunaligned states. In multi-agent settings, successful violations diffuse\nquickly, leading to collective misalignment. Moreover, current reinforcement\nlearning-based alignment methods provide only fragile defenses against\nalignment tipping. Together, these findings demonstrate that alignment of LLM\nagents is not a static property but a fragile and dynamic one, vulnerable to\nfeedback-driven decay during deployment. Our data and code are available at\nhttps://github.com/aiming-lab/ATP."
                },
                "authors": [
                    {
                        "name": "Siwei Han"
                    },
                    {
                        "name": "Jiaqi Liu"
                    },
                    {
                        "name": "Yaofeng Su"
                    },
                    {
                        "name": "Wenbo Duan"
                    },
                    {
                        "name": "Xinyuan Liu"
                    },
                    {
                        "name": "Cihang Xie"
                    },
                    {
                        "name": "Mohit Bansal"
                    },
                    {
                        "name": "Mingyu Ding"
                    },
                    {
                        "name": "Linjun Zhang"
                    },
                    {
                        "name": "Huaxiu Yao"
                    }
                ],
                "author_detail": {
                    "name": "Huaxiu Yao"
                },
                "author": "Huaxiu Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09050v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09050v2",
                "updated": "2025-10-06T14:44:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    44,
                    32,
                    0,
                    279,
                    0
                ],
                "published": "2025-06-10T17:59:56Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    17,
                    59,
                    56,
                    1,
                    161,
                    0
                ],
                "title": "ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm\n  Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm\n  Engineering"
                },
                "summary": "How well do AI systems perform in algorithm engineering for hard optimization\nproblems in domains such as package-delivery routing, crew scheduling, factory\nproduction planning, and power-grid balancing? We introduce ALE-Bench, a new\nbenchmark for evaluating AI systems on score-based algorithmic programming\ncontests. Drawing on real tasks from the AtCoder Heuristic Contests, ALE-Bench\npresents optimization problems that are computationally hard and admit no known\nexact solution. Unlike short-duration, pass/fail coding benchmarks, ALE-Bench\nencourages iterative solution refinement over long time horizons. Our software\nframework supports interactive agent architectures that leverage test-run\nfeedback and visualizations. Our evaluation of frontier LLMs revealed that\nwhile they demonstrate high performance on specific problems, a notable gap\nremains compared to humans in terms of consistency across problems and\nlong-horizon problem-solving capabilities. This highlights the need for this\nbenchmark to foster future AI advancements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How well do AI systems perform in algorithm engineering for hard optimization\nproblems in domains such as package-delivery routing, crew scheduling, factory\nproduction planning, and power-grid balancing? We introduce ALE-Bench, a new\nbenchmark for evaluating AI systems on score-based algorithmic programming\ncontests. Drawing on real tasks from the AtCoder Heuristic Contests, ALE-Bench\npresents optimization problems that are computationally hard and admit no known\nexact solution. Unlike short-duration, pass/fail coding benchmarks, ALE-Bench\nencourages iterative solution refinement over long time horizons. Our software\nframework supports interactive agent architectures that leverage test-run\nfeedback and visualizations. Our evaluation of frontier LLMs revealed that\nwhile they demonstrate high performance on specific problems, a notable gap\nremains compared to humans in terms of consistency across problems and\nlong-horizon problem-solving capabilities. This highlights the need for this\nbenchmark to foster future AI advancements."
                },
                "authors": [
                    {
                        "name": "Yuki Imajuku"
                    },
                    {
                        "name": "Kohki Horie"
                    },
                    {
                        "name": "Yoichi Iwata"
                    },
                    {
                        "name": "Kensho Aoki"
                    },
                    {
                        "name": "Naohiro Takahashi"
                    },
                    {
                        "name": "Takuya Akiba"
                    }
                ],
                "author_detail": {
                    "name": "Takuya Akiba"
                },
                "author": "Takuya Akiba",
                "arxiv_comment": "Accepted at NeurIPS 2025 Datasets & Benchmarks Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09050v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09050v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07596v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07596v2",
                "updated": "2025-10-06T14:43:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    43,
                    39,
                    0,
                    279,
                    0
                ],
                "published": "2025-09-09T11:14:11Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    11,
                    14,
                    11,
                    1,
                    252,
                    0
                ],
                "title": "Bias in Gender Bias Benchmarks: How Spurious Features Distort Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias in Gender Bias Benchmarks: How Spurious Features Distort Evaluation"
                },
                "summary": "Gender bias in vision-language foundation models (VLMs) raises concerns about\ntheir safe deployment and is typically evaluated using benchmarks with gender\nannotations on real-world images. However, as these benchmarks often contain\nspurious correlations between gender and non-gender features, such as objects\nand backgrounds, we identify a critical oversight in gender bias evaluation: Do\nspurious features distort gender bias evaluation? To address this question, we\nsystematically perturb non-gender features across four widely used benchmarks\n(COCO-gender, FACET, MIAP, and PHASE) and various VLMs to quantify their impact\non bias evaluation. Our findings reveal that even minimal perturbations, such\nas masking just 10% of objects or weakly blurring backgrounds, can dramatically\nalter bias scores, shifting metrics by up to 175% in generative VLMs and 43% in\nCLIP variants. This suggests that current bias evaluations often reflect model\nresponses to spurious features rather than gender bias, undermining their\nreliability. Since creating spurious feature-free benchmarks is fundamentally\nchallenging, we recommend reporting bias metrics alongside feature-sensitivity\nmeasurements to enable a more reliable bias assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gender bias in vision-language foundation models (VLMs) raises concerns about\ntheir safe deployment and is typically evaluated using benchmarks with gender\nannotations on real-world images. However, as these benchmarks often contain\nspurious correlations between gender and non-gender features, such as objects\nand backgrounds, we identify a critical oversight in gender bias evaluation: Do\nspurious features distort gender bias evaluation? To address this question, we\nsystematically perturb non-gender features across four widely used benchmarks\n(COCO-gender, FACET, MIAP, and PHASE) and various VLMs to quantify their impact\non bias evaluation. Our findings reveal that even minimal perturbations, such\nas masking just 10% of objects or weakly blurring backgrounds, can dramatically\nalter bias scores, shifting metrics by up to 175% in generative VLMs and 43% in\nCLIP variants. This suggests that current bias evaluations often reflect model\nresponses to spurious features rather than gender bias, undermining their\nreliability. Since creating spurious feature-free benchmarks is fundamentally\nchallenging, we recommend reporting bias metrics alongside feature-sensitivity\nmeasurements to enable a more reliable bias assessment."
                },
                "authors": [
                    {
                        "name": "Yusuke Hirota"
                    },
                    {
                        "name": "Ryo Hachiuma"
                    },
                    {
                        "name": "Boyi Li"
                    },
                    {
                        "name": "Ximing Lu"
                    },
                    {
                        "name": "Michael Ross Boone"
                    },
                    {
                        "name": "Boris Ivanovic"
                    },
                    {
                        "name": "Yejin Choi"
                    },
                    {
                        "name": "Marco Pavone"
                    },
                    {
                        "name": "Yu-Chiang Frank Wang"
                    },
                    {
                        "name": "Noa Garcia"
                    },
                    {
                        "name": "Yuta Nakashima"
                    },
                    {
                        "name": "Chao-Han Huck Yang"
                    }
                ],
                "author_detail": {
                    "name": "Chao-Han Huck Yang"
                },
                "author": "Chao-Han Huck Yang",
                "arxiv_comment": "ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07596v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07596v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04852v1",
                "updated": "2025-10-06T14:39:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    39,
                    58,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T14:39:58Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    39,
                    58,
                    0,
                    279,
                    0
                ],
                "title": "FreshBrew: A Benchmark for Evaluating AI Agents on Java Code Migration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreshBrew: A Benchmark for Evaluating AI Agents on Java Code Migration"
                },
                "summary": "AI coding assistants are rapidly becoming integral to modern software\ndevelopment. A key challenge in this space is the continual need to migrate and\nmodernize codebases in response to evolving software ecosystems. Traditionally,\nsuch migrations have relied on rule-based systems and human intervention. With\nthe advent of powerful large language models (LLMs), AI-driven agentic\nframeworks offer a promising alternative-but their effectiveness has not been\nsystematically evaluated. In this paper, we introduce FreshBrew, a novel\nbenchmark for evaluating AI agents on project-level Java migrations, with a\nspecific focus on measuring an agent's ability to preserve program semantics\nand avoid reward hacking, which we argue requires projects with high test\ncoverage for a rigorous and reliable evaluation. We benchmark several\nstate-of-the-art LLMs, and compare their performance against established\nrule-based tools. Our evaluation of AI agents on this benchmark of 228\nrepositories shows that the top-performing model, Gemini 2.5 Flash, can\nsuccessfully migrate 52.3 percent of projects to JDK 17. Our empirical analysis\nreveals novel insights into the critical strengths and limitations of current\nagentic approaches, offering actionable insights into their real-world\napplicability. Our empirical study reveals failure modes of current AI agents\nin realistic Java modernization tasks, providing a foundation for evaluating\ntrustworthy code-migration systems. By releasing FreshBrew, we aim to\nfacilitate rigorous, reproducible evaluation and catalyze progress in AI-driven\ncodebase modernization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI coding assistants are rapidly becoming integral to modern software\ndevelopment. A key challenge in this space is the continual need to migrate and\nmodernize codebases in response to evolving software ecosystems. Traditionally,\nsuch migrations have relied on rule-based systems and human intervention. With\nthe advent of powerful large language models (LLMs), AI-driven agentic\nframeworks offer a promising alternative-but their effectiveness has not been\nsystematically evaluated. In this paper, we introduce FreshBrew, a novel\nbenchmark for evaluating AI agents on project-level Java migrations, with a\nspecific focus on measuring an agent's ability to preserve program semantics\nand avoid reward hacking, which we argue requires projects with high test\ncoverage for a rigorous and reliable evaluation. We benchmark several\nstate-of-the-art LLMs, and compare their performance against established\nrule-based tools. Our evaluation of AI agents on this benchmark of 228\nrepositories shows that the top-performing model, Gemini 2.5 Flash, can\nsuccessfully migrate 52.3 percent of projects to JDK 17. Our empirical analysis\nreveals novel insights into the critical strengths and limitations of current\nagentic approaches, offering actionable insights into their real-world\napplicability. Our empirical study reveals failure modes of current AI agents\nin realistic Java modernization tasks, providing a foundation for evaluating\ntrustworthy code-migration systems. By releasing FreshBrew, we aim to\nfacilitate rigorous, reproducible evaluation and catalyze progress in AI-driven\ncodebase modernization."
                },
                "authors": [
                    {
                        "name": "Victor May"
                    },
                    {
                        "name": "Diganta Misra"
                    },
                    {
                        "name": "Yanqi Luo"
                    },
                    {
                        "name": "Anjali Sridhar"
                    },
                    {
                        "name": "Justine Gehring"
                    },
                    {
                        "name": "Silvio Soares Ribeiro Junior"
                    }
                ],
                "author_detail": {
                    "name": "Silvio Soares Ribeiro Junior"
                },
                "author": "Silvio Soares Ribeiro Junior",
                "arxiv_comment": "18 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04851v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04851v1",
                "updated": "2025-10-06T14:39:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    39,
                    53,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T14:39:53Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    39,
                    53,
                    0,
                    279,
                    0
                ],
                "title": "LEGOMem: Modular Procedural Memory for Multi-agent LLM Systems for\n  Workflow Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEGOMem: Modular Procedural Memory for Multi-agent LLM Systems for\n  Workflow Automation"
                },
                "summary": "We introduce LEGOMem, a modular procedural memory framework for multi-agent\nlarge language model (LLM) systems in workflow automation. LEGOMem decomposes\npast task trajectories into reusable memory units and flexibly allocates them\nacross orchestrators and task agents to support planning and execution. To\nexplore the design space of memory in multi-agent systems, we use LEGOMem as a\nlens and conduct a systematic study of procedural memory in multi-agent\nsystems, examining where memory should be placed, how it should be retrieved,\nand which agents benefit most. Experiments on the OfficeBench benchmark show\nthat orchestrator memory is critical for effective task decomposition and\ndelegation, while fine-grained agent memory improves execution accuracy. We\nfind that even teams composed of smaller language models can benefit\nsubstantially from procedural memory, narrowing the performance gap with\nstronger agents by leveraging prior execution traces for more accurate planning\nand tool use. These results position LEGOMem as both a practical framework for\nmemory-augmented agent systems and a research tool for understanding memory\ndesign in multi-agent workflow automation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce LEGOMem, a modular procedural memory framework for multi-agent\nlarge language model (LLM) systems in workflow automation. LEGOMem decomposes\npast task trajectories into reusable memory units and flexibly allocates them\nacross orchestrators and task agents to support planning and execution. To\nexplore the design space of memory in multi-agent systems, we use LEGOMem as a\nlens and conduct a systematic study of procedural memory in multi-agent\nsystems, examining where memory should be placed, how it should be retrieved,\nand which agents benefit most. Experiments on the OfficeBench benchmark show\nthat orchestrator memory is critical for effective task decomposition and\ndelegation, while fine-grained agent memory improves execution accuracy. We\nfind that even teams composed of smaller language models can benefit\nsubstantially from procedural memory, narrowing the performance gap with\nstronger agents by leveraging prior execution traces for more accurate planning\nand tool use. These results position LEGOMem as both a practical framework for\nmemory-augmented agent systems and a research tool for understanding memory\ndesign in multi-agent workflow automation."
                },
                "authors": [
                    {
                        "name": "Dongge Han"
                    },
                    {
                        "name": "Camille Couturier"
                    },
                    {
                        "name": "Daniel Madrigal Diaz"
                    },
                    {
                        "name": "Xuchao Zhang"
                    },
                    {
                        "name": "Victor RÃ¼hle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04851v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04851v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04849v1",
                "updated": "2025-10-06T14:36:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    36,
                    30,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T14:36:30Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    36,
                    30,
                    0,
                    279,
                    0
                ],
                "title": "When Models Lie, We Learn: Multilingual Span-Level Hallucination\n  Detection with PsiloQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Models Lie, We Learn: Multilingual Span-Level Hallucination\n  Detection with PsiloQA"
                },
                "summary": "Hallucination detection remains a fundamental challenge for the safe and\nreliable deployment of large language models (LLMs), especially in applications\nrequiring factual accuracy. Existing hallucination benchmarks often operate at\nthe sequence level and are limited to English, lacking the fine-grained,\nmultilingual supervision needed for a comprehensive evaluation. In this work,\nwe introduce PsiloQA, a large-scale, multilingual dataset annotated with\nspan-level hallucinations across 14 languages. PsiloQA is constructed through\nan automated three-stage pipeline: generating question-answer pairs from\nWikipedia using GPT-4o, eliciting potentially hallucinated answers from diverse\nLLMs in a no-context setting, and automatically annotating hallucinated spans\nusing GPT-4o by comparing against golden answers and retrieved context. We\nevaluate a wide range of hallucination detection methods -- including\nuncertainty quantification, LLM-based tagging, and fine-tuned encoder models --\nand show that encoder-based models achieve the strongest performance across\nlanguages. Furthermore, PsiloQA demonstrates effective cross-lingual\ngeneralization and supports robust knowledge transfer to other benchmarks, all\nwhile being significantly more cost-efficient than human-annotated datasets.\nOur dataset and results advance the development of scalable, fine-grained\nhallucination detection in multilingual settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination detection remains a fundamental challenge for the safe and\nreliable deployment of large language models (LLMs), especially in applications\nrequiring factual accuracy. Existing hallucination benchmarks often operate at\nthe sequence level and are limited to English, lacking the fine-grained,\nmultilingual supervision needed for a comprehensive evaluation. In this work,\nwe introduce PsiloQA, a large-scale, multilingual dataset annotated with\nspan-level hallucinations across 14 languages. PsiloQA is constructed through\nan automated three-stage pipeline: generating question-answer pairs from\nWikipedia using GPT-4o, eliciting potentially hallucinated answers from diverse\nLLMs in a no-context setting, and automatically annotating hallucinated spans\nusing GPT-4o by comparing against golden answers and retrieved context. We\nevaluate a wide range of hallucination detection methods -- including\nuncertainty quantification, LLM-based tagging, and fine-tuned encoder models --\nand show that encoder-based models achieve the strongest performance across\nlanguages. Furthermore, PsiloQA demonstrates effective cross-lingual\ngeneralization and supports robust knowledge transfer to other benchmarks, all\nwhile being significantly more cost-efficient than human-annotated datasets.\nOur dataset and results advance the development of scalable, fine-grained\nhallucination detection in multilingual settings."
                },
                "authors": [
                    {
                        "name": "Elisei Rykov"
                    },
                    {
                        "name": "Kseniia Petrushina"
                    },
                    {
                        "name": "Maksim Savkin"
                    },
                    {
                        "name": "Valerii Olisov"
                    },
                    {
                        "name": "Artem Vazhentsev"
                    },
                    {
                        "name": "Kseniia Titova"
                    },
                    {
                        "name": "Alexander Panchenko"
                    },
                    {
                        "name": "Vasily Konovalov"
                    },
                    {
                        "name": "Julia Belikova"
                    }
                ],
                "author_detail": {
                    "name": "Julia Belikova"
                },
                "author": "Julia Belikova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04848v1",
                "updated": "2025-10-06T14:33:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    33,
                    38,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T14:33:38Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    33,
                    38,
                    0,
                    279,
                    0
                ],
                "title": "Instability in Downstream Task Performance During LLM Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instability in Downstream Task Performance During LLM Pretraining"
                },
                "summary": "When training large language models (LLMs), it is common practice to track\ndownstream task performance throughout the training process and select the\ncheckpoint with the highest validation score. However, downstream metrics often\nexhibit substantial fluctuations, making it difficult to identify the\ncheckpoint that truly represents the best-performing model. In this study, we\nempirically analyze the stability of downstream task performance in an LLM\ntrained on diverse web-scale corpora. We find that task scores frequently\nfluctuate throughout training, both at the aggregate and example levels. To\naddress this instability, we investigate two post-hoc checkpoint integration\nmethods: checkpoint averaging and ensemble, motivated by the hypothesis that\naggregating neighboring checkpoints can reduce performance volatility. We\ndemonstrate both empirically and theoretically that these methods improve\ndownstream performance stability without requiring any changes to the training\nprocedure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When training large language models (LLMs), it is common practice to track\ndownstream task performance throughout the training process and select the\ncheckpoint with the highest validation score. However, downstream metrics often\nexhibit substantial fluctuations, making it difficult to identify the\ncheckpoint that truly represents the best-performing model. In this study, we\nempirically analyze the stability of downstream task performance in an LLM\ntrained on diverse web-scale corpora. We find that task scores frequently\nfluctuate throughout training, both at the aggregate and example levels. To\naddress this instability, we investigate two post-hoc checkpoint integration\nmethods: checkpoint averaging and ensemble, motivated by the hypothesis that\naggregating neighboring checkpoints can reduce performance volatility. We\ndemonstrate both empirically and theoretically that these methods improve\ndownstream performance stability without requiring any changes to the training\nprocedure."
                },
                "authors": [
                    {
                        "name": "Yuto Nishida"
                    },
                    {
                        "name": "Masaru Isonuma"
                    },
                    {
                        "name": "Yusuke Oda"
                    }
                ],
                "author_detail": {
                    "name": "Yusuke Oda"
                },
                "author": "Yusuke Oda",
                "arxiv_comment": "Accepted to EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20836v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20836v4",
                "updated": "2025-10-06T14:29:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    29,
                    58,
                    0,
                    279,
                    0
                ],
                "published": "2025-07-28T13:44:21Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    13,
                    44,
                    21,
                    0,
                    209,
                    0
                ],
                "title": "First Hallucination Tokens Are Different from Conditional Ones",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "First Hallucination Tokens Are Different from Conditional Ones"
                },
                "summary": "Large Language Models (LLMs) hallucinate, and detecting these cases is key to\nensuring trust. While many approaches address hallucination detection at the\nresponse or span level, recent work explores token-level detection, enabling\nmore fine-grained intervention. However, the distribution of hallucination\nsignal across sequences of hallucinated tokens remains unexplored. We leverage\ntoken-level annotations from the RAGTruth corpus and find that the first\nhallucinated token is far more detectable than later ones. This structural\nproperty holds across models, suggesting that first hallucination tokens play a\nkey role in token-level hallucination detection. Our code is available at\nhttps://github.com/jakobsnl/RAGTruth_Xtended.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) hallucinate, and detecting these cases is key to\nensuring trust. While many approaches address hallucination detection at the\nresponse or span level, recent work explores token-level detection, enabling\nmore fine-grained intervention. However, the distribution of hallucination\nsignal across sequences of hallucinated tokens remains unexplored. We leverage\ntoken-level annotations from the RAGTruth corpus and find that the first\nhallucinated token is far more detectable than later ones. This structural\nproperty holds across models, suggesting that first hallucination tokens play a\nkey role in token-level hallucination detection. Our code is available at\nhttps://github.com/jakobsnl/RAGTruth_Xtended."
                },
                "authors": [
                    {
                        "name": "Jakob Snel"
                    },
                    {
                        "name": "Seong Joon Oh"
                    }
                ],
                "author_detail": {
                    "name": "Seong Joon Oh"
                },
                "author": "Seong Joon Oh",
                "arxiv_comment": "4.5 pages, 3 figures, Dataset, Knowledge Paper, Hallucination,\n  Trustworthiness",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20836v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20836v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09199v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09199v3",
                "updated": "2025-10-06T14:19:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    19,
                    55,
                    0,
                    279,
                    0
                ],
                "published": "2025-07-12T08:42:10Z",
                "published_parsed": [
                    2025,
                    7,
                    12,
                    8,
                    42,
                    10,
                    5,
                    193,
                    0
                ],
                "title": "Back to the Basics: Rethinking Issue-Commit Linking with LLM-Assisted\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Back to the Basics: Rethinking Issue-Commit Linking with LLM-Assisted\n  Retrieval"
                },
                "summary": "Issue-commit linking, which connects issues with commits that fix them, is\ncrucial for software maintenance. Existing approaches have shown promise in\nautomatically recovering these links. Evaluations of these techniques assess\ntheir ability to identify genuine links from plausible but false links.\nHowever, these evaluations overlook the fact that, in reality, when a\nrepository has more commits, the presence of more plausible yet unrelated\ncommits may interfere with the tool in differentiating the correct fix commits.\nTo address this, we propose the Realistic Distribution Setting (RDS) and use it\nto construct a more realistic evaluation dataset that includes 20 open-source\nprojects. By evaluating tools on this dataset, we observe that the performance\nof the state-of-the-art deep learning-based approach drops by more than half,\nwhile the traditional Information Retrieval method, VSM, outperforms it.\nInspired by these observations, we propose EasyLink, which utilizes a vector\ndatabase as a modern Information Retrieval technique. To address the\nlong-standing problem of the semantic gap between issues and commits, EasyLink\nleverages a large language model to rerank the commits retrieved from the\ndatabase. Under our evaluation, EasyLink achieves an average Precision@1 of\n75.03\\%, improving over the state-of-the-art by over four times. Additionally,\nthis paper provides practical guidelines for advancing research in issue-commit\nlink recovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Issue-commit linking, which connects issues with commits that fix them, is\ncrucial for software maintenance. Existing approaches have shown promise in\nautomatically recovering these links. Evaluations of these techniques assess\ntheir ability to identify genuine links from plausible but false links.\nHowever, these evaluations overlook the fact that, in reality, when a\nrepository has more commits, the presence of more plausible yet unrelated\ncommits may interfere with the tool in differentiating the correct fix commits.\nTo address this, we propose the Realistic Distribution Setting (RDS) and use it\nto construct a more realistic evaluation dataset that includes 20 open-source\nprojects. By evaluating tools on this dataset, we observe that the performance\nof the state-of-the-art deep learning-based approach drops by more than half,\nwhile the traditional Information Retrieval method, VSM, outperforms it.\nInspired by these observations, we propose EasyLink, which utilizes a vector\ndatabase as a modern Information Retrieval technique. To address the\nlong-standing problem of the semantic gap between issues and commits, EasyLink\nleverages a large language model to rerank the commits retrieved from the\ndatabase. Under our evaluation, EasyLink achieves an average Precision@1 of\n75.03\\%, improving over the state-of-the-art by over four times. Additionally,\nthis paper provides practical guidelines for advancing research in issue-commit\nlink recovery."
                },
                "authors": [
                    {
                        "name": "Huihui Huang"
                    },
                    {
                        "name": "Ratnadira Widyasari"
                    },
                    {
                        "name": "Ting Zhang"
                    },
                    {
                        "name": "Ivana Clairine Irsan"
                    },
                    {
                        "name": "Jieke Shi"
                    },
                    {
                        "name": "Han Wei Ang"
                    },
                    {
                        "name": "Frank Liauw"
                    },
                    {
                        "name": "Eng Lieh Ouh"
                    },
                    {
                        "name": "Lwin Khin Shar"
                    },
                    {
                        "name": "Hong Jin Kang"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09199v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09199v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11723v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11723v2",
                "updated": "2025-10-06T14:15:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    15,
                    39,
                    0,
                    279,
                    0
                ],
                "published": "2025-02-17T12:10:25Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    12,
                    10,
                    25,
                    0,
                    48,
                    0
                ],
                "title": "Energy-Conscious LLM Decoding: Impact of Text Generation Strategies on\n  GPU Energy Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy-Conscious LLM Decoding: Impact of Text Generation Strategies on\n  GPU Energy Consumption"
                },
                "summary": "Decoding strategies significantly influence the quality and diversity of the\ngenerated text in Large Language Models (LLMs), yet their impact on\ncomputational resources, particularly GPU energy consumption, is insufficiently\nstudied. This paper investigates the relationship between text generation\ndecoding techniques and energy efficiency, focusing on the trade-off between\ngeneration quality and GPU energy usage across diverse tasks and decoding\nconfigurations. By benchmarking multiple strategies across various tasks,\nincluding Translation, Math Problem Solving, Coding, and Open-ended text\ngeneration, we reveal how selecting appropriate decoding techniques with their\ntuned hyperparameters affects text quality and has measurable implications for\nenergy consumption. Our findings show that the choice of decoding strategy can\ngreatly impact GPU energy usage, even when it has a minimal effect on output\nquality. Different strategies also involve trade-offs between quality and\nenergy efficiency, and no single decoding method is best in all cases across\nevery metric. To the best of our knowledge, this is one of the first studies to\nexamine decoding strategies in LLMs from the perspective of energy consumption,\nproviding useful insights for building energy-efficient applications without\ncompromising text generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoding strategies significantly influence the quality and diversity of the\ngenerated text in Large Language Models (LLMs), yet their impact on\ncomputational resources, particularly GPU energy consumption, is insufficiently\nstudied. This paper investigates the relationship between text generation\ndecoding techniques and energy efficiency, focusing on the trade-off between\ngeneration quality and GPU energy usage across diverse tasks and decoding\nconfigurations. By benchmarking multiple strategies across various tasks,\nincluding Translation, Math Problem Solving, Coding, and Open-ended text\ngeneration, we reveal how selecting appropriate decoding techniques with their\ntuned hyperparameters affects text quality and has measurable implications for\nenergy consumption. Our findings show that the choice of decoding strategy can\ngreatly impact GPU energy usage, even when it has a minimal effect on output\nquality. Different strategies also involve trade-offs between quality and\nenergy efficiency, and no single decoding method is best in all cases across\nevery metric. To the best of our knowledge, this is one of the first studies to\nexamine decoding strategies in LLMs from the perspective of energy consumption,\nproviding useful insights for building energy-efficient applications without\ncompromising text generation quality."
                },
                "authors": [
                    {
                        "name": "Alireza Nik"
                    },
                    {
                        "name": "Michael A. Riegler"
                    },
                    {
                        "name": "PÃ¥l Halvorsen"
                    }
                ],
                "author_detail": {
                    "name": "PÃ¥l Halvorsen"
                },
                "author": "PÃ¥l Halvorsen",
                "arxiv_comment": "Updated version with additional models and benchmark datasets. The\n  experimental section has been expanded with new analyses, and minor\n  corrections and clarifications have been made throughout the text",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11723v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11723v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00072v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00072v2",
                "updated": "2025-10-06T14:10:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    10,
                    14,
                    0,
                    279,
                    0
                ],
                "published": "2025-08-26T16:41:37Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    16,
                    41,
                    37,
                    1,
                    238,
                    0
                ],
                "title": "Beyond Memorization: Reasoning-Driven Synthesis as a Mitigation Strategy\n  Against Benchmark Contamination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Memorization: Reasoning-Driven Synthesis as a Mitigation Strategy\n  Against Benchmark Contamination"
                },
                "summary": "Capability evaluation of large language models (LLMs) is increasingly\nshadowed by rising concerns of data contamination that cast doubts on whether\nstatic benchmarks measure genuine reasoning or mere memorization. We present an\nempirical study using an infinitely scalable framework to synthesize\nresearch-level QA directly from arXiv papers, harnessing the natural temporal\nstructure of research publications where performance decay after knowledge\ncutoffs may indicate potential contamination. We evaluated 4 frontier model\nrepresented by 2 models of different knowledge cutoff dates per family on 1,643\nmulti-step reasoning questions synthesized from 20,277 arXiv papers stratified\nover 26 months, covering at least 6 months before and after all cutoff dates.\nOur results consistently showed a lack of significant performance decay near\nknowledge cutoff dates for models of various sizes, developers, and release\ndates. We further performed a comparative analysis with previous longitudinal\nstudies that reported significant post-cutoff performance decay using directly\nretrieved questions based on public data. we hypothesize that the multi-step\nreasoning required by our synthesis pipeline offered additional complexity that\ngoes deeper than shallow memorization, which effectively serves a mitigation\nstrategy against benchmark contamination. We fully open source our code and\ndataset to aid reproducibility and advocate for a paradigm shift that\nprioritize reasoning-driven synthesis to construct benchmarks over simply\ncollecting newly released questions periodically.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capability evaluation of large language models (LLMs) is increasingly\nshadowed by rising concerns of data contamination that cast doubts on whether\nstatic benchmarks measure genuine reasoning or mere memorization. We present an\nempirical study using an infinitely scalable framework to synthesize\nresearch-level QA directly from arXiv papers, harnessing the natural temporal\nstructure of research publications where performance decay after knowledge\ncutoffs may indicate potential contamination. We evaluated 4 frontier model\nrepresented by 2 models of different knowledge cutoff dates per family on 1,643\nmulti-step reasoning questions synthesized from 20,277 arXiv papers stratified\nover 26 months, covering at least 6 months before and after all cutoff dates.\nOur results consistently showed a lack of significant performance decay near\nknowledge cutoff dates for models of various sizes, developers, and release\ndates. We further performed a comparative analysis with previous longitudinal\nstudies that reported significant post-cutoff performance decay using directly\nretrieved questions based on public data. we hypothesize that the multi-step\nreasoning required by our synthesis pipeline offered additional complexity that\ngoes deeper than shallow memorization, which effectively serves a mitigation\nstrategy against benchmark contamination. We fully open source our code and\ndataset to aid reproducibility and advocate for a paradigm shift that\nprioritize reasoning-driven synthesis to construct benchmarks over simply\ncollecting newly released questions periodically."
                },
                "authors": [
                    {
                        "name": "Terry Jingchen Zhang"
                    },
                    {
                        "name": "Gopal Dev"
                    },
                    {
                        "name": "Ning Wang"
                    },
                    {
                        "name": "Nicole Ni"
                    },
                    {
                        "name": "Wenyuan Jiang"
                    },
                    {
                        "name": "Yinya Huang"
                    },
                    {
                        "name": "Bernhard SchÃ¶lkopf"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Zhijing Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zhijing Jin"
                },
                "author": "Zhijing Jin",
                "arxiv_comment": "The authors choose to withdraw this manuscript as it constitutes\n  incomplete work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00072v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00072v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22777v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22777v4",
                "updated": "2025-10-06T14:06:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    6,
                    40,
                    0,
                    279,
                    0
                ],
                "published": "2025-05-28T18:45:42Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    18,
                    45,
                    42,
                    2,
                    148,
                    0
                ],
                "title": "MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain\n  Dialogue Evaluators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain\n  Dialogue Evaluators"
                },
                "summary": "Evaluating the quality of open-domain chatbots has become increasingly\nreliant on LLMs acting as automatic judges. However, existing meta-evaluation\nbenchmarks are static, outdated, and lacking in multilingual coverage, limiting\ntheir ability to fully capture subtle weaknesses in evaluation. We introduce\nMEDAL, an automated multi-agent framework for curating more representative and\ndiverse open-domain dialogue evaluation benchmarks. Our approach leverages\nseveral state-of-the-art LLMs to generate user-chatbot multilingual dialogues,\nconditioned on varied seed contexts. Then, a strong LLM (GPT-4.1) is used for a\nmultidimensional analysis of the performance of the chatbots, uncovering\nnoticeable cross-lingual performance differences. Guided by this large-scale\nevaluation, we curate a new meta-evaluation multilingual benchmark and\nhuman-annotate samples with nuanced quality judgments. This benchmark is then\nused to assess the ability of several reasoning and non-reasoning LLMs to act\nas evaluators of open-domain dialogues. Using MEDAL, we uncover that\nstate-of-the-art judges fail to reliably detect nuanced issues such as lack of\nempathy, commonsense, or relevance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the quality of open-domain chatbots has become increasingly\nreliant on LLMs acting as automatic judges. However, existing meta-evaluation\nbenchmarks are static, outdated, and lacking in multilingual coverage, limiting\ntheir ability to fully capture subtle weaknesses in evaluation. We introduce\nMEDAL, an automated multi-agent framework for curating more representative and\ndiverse open-domain dialogue evaluation benchmarks. Our approach leverages\nseveral state-of-the-art LLMs to generate user-chatbot multilingual dialogues,\nconditioned on varied seed contexts. Then, a strong LLM (GPT-4.1) is used for a\nmultidimensional analysis of the performance of the chatbots, uncovering\nnoticeable cross-lingual performance differences. Guided by this large-scale\nevaluation, we curate a new meta-evaluation multilingual benchmark and\nhuman-annotate samples with nuanced quality judgments. This benchmark is then\nused to assess the ability of several reasoning and non-reasoning LLMs to act\nas evaluators of open-domain dialogues. Using MEDAL, we uncover that\nstate-of-the-art judges fail to reliably detect nuanced issues such as lack of\nempathy, commonsense, or relevance."
                },
                "authors": [
                    {
                        "name": "John MendonÃ§a"
                    },
                    {
                        "name": "Alon Lavie"
                    },
                    {
                        "name": "Isabel Trancoso"
                    }
                ],
                "author_detail": {
                    "name": "Isabel Trancoso"
                },
                "author": "Isabel Trancoso",
                "arxiv_comment": "October ARR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22777v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22777v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03102v2",
                "updated": "2025-10-06T14:04:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    14,
                    4,
                    39,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-03T15:31:11Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    31,
                    11,
                    4,
                    276,
                    0
                ],
                "title": "Semantic Similarity in Radiology Reports via LLMs and NER",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Similarity in Radiology Reports via LLMs and NER"
                },
                "summary": "Radiology report evaluation is a crucial part of radiologists' training and\nplays a key role in ensuring diagnostic accuracy. As part of the standard\nreporting workflow, a junior radiologist typically prepares a preliminary\nreport, which is then reviewed and edited by a senior radiologist to produce\nthe final report. Identifying semantic differences between preliminary and\nfinal reports is essential for junior doctors, both as a training tool and to\nhelp uncover gaps in clinical knowledge. While AI in radiology is a rapidly\ngrowing field, the application of large language models (LLMs) remains\nchallenging due to the need for specialised domain knowledge. In this paper, we\nexplore the ability of LLMs to provide explainable and accurate comparisons of\nreports in the radiology domain. We begin by comparing the performance of\nseveral LLMs in comparing radiology reports. We then assess a more traditional\napproach based on Named-Entity-Recognition (NER). However, both approaches\nexhibit limitations in delivering accurate feedback on semantic similarity. To\naddress this, we propose Llama-EntScore, a semantic similarity scoring method\nusing a combination of Llama 3.1 and NER with tunable weights to emphasise or\nde-emphasise specific types of differences. Our approach generates a\nquantitative similarity score for tracking progress and also gives an\ninterpretation of the score that aims to offer valuable guidance in reviewing\nand refining their reporting. We find our method achieves 67% exact-match\naccuracy and 93% accuracy within +/- 1 when compared to radiologist-provided\nground truth scores - outperforming both LLMs and NER used independently. Code\nis available at: https://github.com/otmive/llama_reports",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radiology report evaluation is a crucial part of radiologists' training and\nplays a key role in ensuring diagnostic accuracy. As part of the standard\nreporting workflow, a junior radiologist typically prepares a preliminary\nreport, which is then reviewed and edited by a senior radiologist to produce\nthe final report. Identifying semantic differences between preliminary and\nfinal reports is essential for junior doctors, both as a training tool and to\nhelp uncover gaps in clinical knowledge. While AI in radiology is a rapidly\ngrowing field, the application of large language models (LLMs) remains\nchallenging due to the need for specialised domain knowledge. In this paper, we\nexplore the ability of LLMs to provide explainable and accurate comparisons of\nreports in the radiology domain. We begin by comparing the performance of\nseveral LLMs in comparing radiology reports. We then assess a more traditional\napproach based on Named-Entity-Recognition (NER). However, both approaches\nexhibit limitations in delivering accurate feedback on semantic similarity. To\naddress this, we propose Llama-EntScore, a semantic similarity scoring method\nusing a combination of Llama 3.1 and NER with tunable weights to emphasise or\nde-emphasise specific types of differences. Our approach generates a\nquantitative similarity score for tracking progress and also gives an\ninterpretation of the score that aims to offer valuable guidance in reviewing\nand refining their reporting. We find our method achieves 67% exact-match\naccuracy and 93% accuracy within +/- 1 when compared to radiologist-provided\nground truth scores - outperforming both LLMs and NER used independently. Code\nis available at: https://github.com/otmive/llama_reports"
                },
                "authors": [
                    {
                        "name": "Beth Pearson"
                    },
                    {
                        "name": "Ahmed Adnan"
                    },
                    {
                        "name": "Zahraa S. Abdallah"
                    }
                ],
                "author_detail": {
                    "name": "Zahraa S. Abdallah"
                },
                "author": "Zahraa S. Abdallah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04796v1",
                "updated": "2025-10-06T13:22:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    13,
                    22,
                    10,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T13:22:10Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    13,
                    22,
                    10,
                    0,
                    279,
                    0
                ],
                "title": "RevMine: An LLM-Assisted Tool for Code Review Mining and Analysis Across\n  Git Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RevMine: An LLM-Assisted Tool for Code Review Mining and Analysis Across\n  Git Platforms"
                },
                "summary": "Empirical research on code review processes is increasingly central to\nunderstanding software quality and collaboration. However, collecting and\nanalyzing review data remains a time-consuming and technically intensive task.\nMost researchers follow similar workflows - writing ad hoc scripts to extract,\nfilter, and analyze review data from platforms like GitHub and GitLab. This\npaper introduces RevMine, a conceptual tool that streamlines the entire code\nreview mining pipeline using large language models (LLMs). RevMine guides users\nthrough authentication, endpoint discovery, and natural language-driven data\ncollection, significantly reducing the need for manual scripting. After\nretrieving review data, it supports both quantitative and qualitative analysis\nbased on user-defined filters or LLM-inferred patterns. This poster outlines\nthe tool's architecture, use cases, and research potential. By lowering the\nbarrier to entry, RevMine aims to democratize code review mining and enable a\nbroader range of empirical software engineering studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empirical research on code review processes is increasingly central to\nunderstanding software quality and collaboration. However, collecting and\nanalyzing review data remains a time-consuming and technically intensive task.\nMost researchers follow similar workflows - writing ad hoc scripts to extract,\nfilter, and analyze review data from platforms like GitHub and GitLab. This\npaper introduces RevMine, a conceptual tool that streamlines the entire code\nreview mining pipeline using large language models (LLMs). RevMine guides users\nthrough authentication, endpoint discovery, and natural language-driven data\ncollection, significantly reducing the need for manual scripting. After\nretrieving review data, it supports both quantitative and qualitative analysis\nbased on user-defined filters or LLM-inferred patterns. This poster outlines\nthe tool's architecture, use cases, and research potential. By lowering the\nbarrier to entry, RevMine aims to democratize code review mining and enable a\nbroader range of empirical software engineering studies."
                },
                "authors": [
                    {
                        "name": "Samah Kansab"
                    },
                    {
                        "name": "Francis Bordeleau"
                    },
                    {
                        "name": "Ali Tizghadam"
                    }
                ],
                "author_detail": {
                    "name": "Ali Tizghadam"
                },
                "author": "Ali Tizghadam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04791v1",
                "updated": "2025-10-06T13:15:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    13,
                    15,
                    24,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T13:15:24Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    13,
                    15,
                    24,
                    0,
                    279,
                    0
                ],
                "title": "GUISpector: An MLLM Agent Framework for Automated Verification of\n  Natural Language Requirements in GUI Prototypes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUISpector: An MLLM Agent Framework for Automated Verification of\n  Natural Language Requirements in GUI Prototypes"
                },
                "summary": "GUIs are foundational to interactive systems and play a pivotal role in early\nrequirements elicitation through prototyping. Ensuring that GUI implementations\nfulfill NL requirements is essential for robust software engineering,\nespecially as LLM-driven programming agents become increasingly integrated into\ndevelopment workflows. Existing GUI testing approaches, whether traditional or\nLLM-driven, often fall short in handling the complexity of modern interfaces,\nand typically lack actionable feedback and effective integration with automated\ndevelopment agents. In this paper, we introduce GUISpector, a novel framework\nthat leverages a multi-modal (M)LLM-based agent for the automated verification\nof NL requirements in GUI prototypes. First, GUISpector adapts a MLLM agent to\ninterpret and operationalize NL requirements, enabling to autonomously plan and\nexecute verification trajectories across GUI applications. Second, GUISpector\nsystematically extracts detailed NL feedback from the agent's verification\nprocess, providing developers with actionable insights that can be used to\niteratively refine the GUI artifact or directly inform LLM-based code\ngeneration in a closed feedback loop. Third, we present an integrated tool that\nunifies these capabilities, offering practitioners an accessible interface for\nsupervising verification runs, inspecting agent rationales and managing the\nend-to-end requirements verification process. We evaluated GUISpector on a\ncomprehensive set of 150 requirements based on 900 acceptance criteria\nannotations across diverse GUI applications, demonstrating effective detection\nof requirement satisfaction and violations and highlighting its potential for\nseamless integration of actionable feedback into automated LLM-driven\ndevelopment workflows. The video presentation of GUISpector is available at:\nhttps://youtu.be/JByYF6BNQeE, showcasing its main capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUIs are foundational to interactive systems and play a pivotal role in early\nrequirements elicitation through prototyping. Ensuring that GUI implementations\nfulfill NL requirements is essential for robust software engineering,\nespecially as LLM-driven programming agents become increasingly integrated into\ndevelopment workflows. Existing GUI testing approaches, whether traditional or\nLLM-driven, often fall short in handling the complexity of modern interfaces,\nand typically lack actionable feedback and effective integration with automated\ndevelopment agents. In this paper, we introduce GUISpector, a novel framework\nthat leverages a multi-modal (M)LLM-based agent for the automated verification\nof NL requirements in GUI prototypes. First, GUISpector adapts a MLLM agent to\ninterpret and operationalize NL requirements, enabling to autonomously plan and\nexecute verification trajectories across GUI applications. Second, GUISpector\nsystematically extracts detailed NL feedback from the agent's verification\nprocess, providing developers with actionable insights that can be used to\niteratively refine the GUI artifact or directly inform LLM-based code\ngeneration in a closed feedback loop. Third, we present an integrated tool that\nunifies these capabilities, offering practitioners an accessible interface for\nsupervising verification runs, inspecting agent rationales and managing the\nend-to-end requirements verification process. We evaluated GUISpector on a\ncomprehensive set of 150 requirements based on 900 acceptance criteria\nannotations across diverse GUI applications, demonstrating effective detection\nof requirement satisfaction and violations and highlighting its potential for\nseamless integration of actionable feedback into automated LLM-driven\ndevelopment workflows. The video presentation of GUISpector is available at:\nhttps://youtu.be/JByYF6BNQeE, showcasing its main capabilities."
                },
                "authors": [
                    {
                        "name": "Kristian Kolthoff"
                    },
                    {
                        "name": "Felix Kretzer"
                    },
                    {
                        "name": "Simone Paolo Ponzetto"
                    },
                    {
                        "name": "Alexander Maedche"
                    },
                    {
                        "name": "Christian Bartelt"
                    }
                ],
                "author_detail": {
                    "name": "Christian Bartelt"
                },
                "author": "Christian Bartelt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03120v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03120v2",
                "updated": "2025-10-06T13:13:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    13,
                    13,
                    37,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-03T15:49:09Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    49,
                    9,
                    4,
                    276,
                    0
                ],
                "title": "SurveyBench: Can LLM(-Agents) Write Academic Surveys that Align with\n  Reader Needs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SurveyBench: Can LLM(-Agents) Write Academic Surveys that Align with\n  Reader Needs?"
                },
                "summary": "Academic survey writing, which distills vast literature into a coherent and\ninsightful narrative, remains a labor-intensive and intellectually demanding\ntask. While recent approaches, such as general DeepResearch agents and\nsurvey-specialized methods, can generate surveys automatically (a.k.a.\nLLM4Survey), their outputs often fall short of human standards and there lacks\na rigorous, reader-aligned benchmark for thoroughly revealing their\ndeficiencies. To fill the gap, we propose a fine-grained, quiz-driven\nevaluation framework SurveyBench, featuring (1) typical survey topics source\nfrom recent 11,343 arXiv papers and corresponding 4,947 high-quality surveys;\n(2) a multifaceted metric hierarchy that assesses the outline quality (e.g.,\ncoverage breadth, logical coherence), content quality (e.g., synthesis\ngranularity, clarity of insights), and non-textual richness; and (3) a\ndual-mode evaluation protocol that includes content-based and quiz-based\nanswerability tests, explicitly aligned with readers' informational needs.\nResults show SurveyBench effectively challenges existing LLM4Survey approaches\n(e.g., on average 21% lower than human in content-based evaluation).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Academic survey writing, which distills vast literature into a coherent and\ninsightful narrative, remains a labor-intensive and intellectually demanding\ntask. While recent approaches, such as general DeepResearch agents and\nsurvey-specialized methods, can generate surveys automatically (a.k.a.\nLLM4Survey), their outputs often fall short of human standards and there lacks\na rigorous, reader-aligned benchmark for thoroughly revealing their\ndeficiencies. To fill the gap, we propose a fine-grained, quiz-driven\nevaluation framework SurveyBench, featuring (1) typical survey topics source\nfrom recent 11,343 arXiv papers and corresponding 4,947 high-quality surveys;\n(2) a multifaceted metric hierarchy that assesses the outline quality (e.g.,\ncoverage breadth, logical coherence), content quality (e.g., synthesis\ngranularity, clarity of insights), and non-textual richness; and (3) a\ndual-mode evaluation protocol that includes content-based and quiz-based\nanswerability tests, explicitly aligned with readers' informational needs.\nResults show SurveyBench effectively challenges existing LLM4Survey approaches\n(e.g., on average 21% lower than human in content-based evaluation)."
                },
                "authors": [
                    {
                        "name": "Zhaojun Sun"
                    },
                    {
                        "name": "Xuzhou Zhu"
                    },
                    {
                        "name": "Xuanhe Zhou"
                    },
                    {
                        "name": "Xin Tong"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jie Fu"
                    },
                    {
                        "name": "Guoliang Li"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Fan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fan Wu"
                },
                "author": "Fan Wu",
                "arxiv_comment": "Visit our code repository at: https://github.com/weAIDB/SurveyBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03120v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03120v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04787v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04787v1",
                "updated": "2025-10-06T13:08:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    13,
                    8,
                    55,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T13:08:55Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    13,
                    8,
                    55,
                    0,
                    279,
                    0
                ],
                "title": "Trade in Minutes! Rationality-Driven Agentic System for Quantitative\n  Financial Trading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trade in Minutes! Rationality-Driven Agentic System for Quantitative\n  Financial Trading"
                },
                "summary": "Recent advancements in large language models (LLMs) and agentic systems have\nshown exceptional decision-making capabilities, revealing significant potential\nfor autonomic finance. Current financial trading agents predominantly simulate\nanthropomorphic roles that inadvertently introduce emotional biases and rely on\nperipheral information, while being constrained by the necessity for continuous\ninference during deployment. In this paper, we pioneer the harmonization of\nstrategic depth in agents with the mechanical rationality essential for\nquantitative trading. Consequently, we present TiMi (Trade in Minutes), a\nrationality-driven multi-agent system that architecturally decouples strategy\ndevelopment from minute-level deployment. TiMi leverages specialized LLM\ncapabilities of semantic analysis, code programming, and mathematical reasoning\nwithin a comprehensive policy-optimization-deployment chain. Specifically, we\npropose a two-tier analytical paradigm from macro patterns to micro\ncustomization, layered programming design for trading bot implementation, and\nclosed-loop optimization driven by mathematical reflection. Extensive\nevaluations across 200+ trading pairs in stock and cryptocurrency markets\nempirically validate the efficacy of TiMi in stable profitability, action\nefficiency, and risk control under volatile market dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) and agentic systems have\nshown exceptional decision-making capabilities, revealing significant potential\nfor autonomic finance. Current financial trading agents predominantly simulate\nanthropomorphic roles that inadvertently introduce emotional biases and rely on\nperipheral information, while being constrained by the necessity for continuous\ninference during deployment. In this paper, we pioneer the harmonization of\nstrategic depth in agents with the mechanical rationality essential for\nquantitative trading. Consequently, we present TiMi (Trade in Minutes), a\nrationality-driven multi-agent system that architecturally decouples strategy\ndevelopment from minute-level deployment. TiMi leverages specialized LLM\ncapabilities of semantic analysis, code programming, and mathematical reasoning\nwithin a comprehensive policy-optimization-deployment chain. Specifically, we\npropose a two-tier analytical paradigm from macro patterns to micro\ncustomization, layered programming design for trading bot implementation, and\nclosed-loop optimization driven by mathematical reflection. Extensive\nevaluations across 200+ trading pairs in stock and cryptocurrency markets\nempirically validate the efficacy of TiMi in stable profitability, action\nefficiency, and risk control under volatile market dynamics."
                },
                "authors": [
                    {
                        "name": "Zifan Song"
                    },
                    {
                        "name": "Kaitao Song"
                    },
                    {
                        "name": "Guosheng Hu"
                    },
                    {
                        "name": "Ding Qi"
                    },
                    {
                        "name": "Junyao Gao"
                    },
                    {
                        "name": "Xiaohua Wang"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Cairong Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Cairong Zhao"
                },
                "author": "Cairong Zhao",
                "arxiv_comment": "16 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04787v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04787v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22075v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22075v2",
                "updated": "2025-10-06T12:56:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    12,
                    56,
                    1,
                    0,
                    279,
                    0
                ],
                "published": "2025-09-26T08:55:09Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    8,
                    55,
                    9,
                    4,
                    269,
                    0
                ],
                "title": "COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary\n  Learning"
                },
                "summary": "Post-training compression of large language models (LLMs) largely relies on\nlow-rank weight approximation, which represents each column of a weight matrix\nin a shared low-dimensional subspace. While this is a computationally efficient\nstrategy, the imposed structural constraint is rigid and can lead to a\nnoticeable model accuracy drop. In this work, we propose CoSpaDi (Compression\nvia Sparse Dictionary Learning), a novel training-free compression framework\nthat replaces low-rank decomposition with a more flexible structured sparse\nfactorization in which each weight matrix is represented with a dense\ndictionary and a column-sparse coefficient matrix. This formulation enables a\nunion-of-subspaces representation: different columns of the original weight\nmatrix are approximated in distinct subspaces spanned by adaptively selected\ndictionary atoms, offering greater expressiveness than a single invariant\nbasis. Crucially, CoSpaDi leverages a small calibration dataset to optimize the\nfactorization such that the output activations of compressed projection layers\nclosely match those of the original ones, thereby minimizing functional\nreconstruction error rather than mere weight approximation. This data-aware\nstrategy preserves better model fidelity without any fine-tuning under\nreasonable compression ratios. Moreover, the resulting structured sparsity\nallows efficient sparse-dense matrix multiplication and is compatible with\npost-training quantization for further memory and latency gains. We evaluate\nCoSpaDi across multiple Llama and Qwen models under per-layer and per-group\nsettings at 20-50\\% compression ratios, demonstrating consistent superiority\nover state-of-the-art data-aware low-rank methods both in accuracy and\nperplexity. Our results establish structured sparse dictionary learning as a\npowerful alternative to conventional low-rank approaches for efficient LLM\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training compression of large language models (LLMs) largely relies on\nlow-rank weight approximation, which represents each column of a weight matrix\nin a shared low-dimensional subspace. While this is a computationally efficient\nstrategy, the imposed structural constraint is rigid and can lead to a\nnoticeable model accuracy drop. In this work, we propose CoSpaDi (Compression\nvia Sparse Dictionary Learning), a novel training-free compression framework\nthat replaces low-rank decomposition with a more flexible structured sparse\nfactorization in which each weight matrix is represented with a dense\ndictionary and a column-sparse coefficient matrix. This formulation enables a\nunion-of-subspaces representation: different columns of the original weight\nmatrix are approximated in distinct subspaces spanned by adaptively selected\ndictionary atoms, offering greater expressiveness than a single invariant\nbasis. Crucially, CoSpaDi leverages a small calibration dataset to optimize the\nfactorization such that the output activations of compressed projection layers\nclosely match those of the original ones, thereby minimizing functional\nreconstruction error rather than mere weight approximation. This data-aware\nstrategy preserves better model fidelity without any fine-tuning under\nreasonable compression ratios. Moreover, the resulting structured sparsity\nallows efficient sparse-dense matrix multiplication and is compatible with\npost-training quantization for further memory and latency gains. We evaluate\nCoSpaDi across multiple Llama and Qwen models under per-layer and per-group\nsettings at 20-50\\% compression ratios, demonstrating consistent superiority\nover state-of-the-art data-aware low-rank methods both in accuracy and\nperplexity. Our results establish structured sparse dictionary learning as a\npowerful alternative to conventional low-rank approaches for efficient LLM\ndeployment."
                },
                "authors": [
                    {
                        "name": "Dmitriy Shopkhoev"
                    },
                    {
                        "name": "Denis Makhov"
                    },
                    {
                        "name": "Magauiya Zhussip"
                    },
                    {
                        "name": "Ammar Ali"
                    },
                    {
                        "name": "Stamatios Lefkimmiatis"
                    }
                ],
                "author_detail": {
                    "name": "Stamatios Lefkimmiatis"
                },
                "author": "Stamatios Lefkimmiatis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22075v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22075v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04774v1",
                "updated": "2025-10-06T12:49:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    12,
                    49,
                    36,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T12:49:36Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    12,
                    49,
                    36,
                    0,
                    279,
                    0
                ],
                "title": "Online automatic code generation for robot swarms: LLMs and\n  self-organizing hierarchy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online automatic code generation for robot swarms: LLMs and\n  self-organizing hierarchy"
                },
                "summary": "Our recently introduced self-organizing nervous system (SoNS) provides robot\nswarms with 1) ease of behavior design and 2) global estimation of the swarm\nconfiguration and its collective environment, facilitating the implementation\nof online automatic code generation for robot swarms. In a demonstration with 6\nreal robots and simulation trials with >30 robots, we show that when a\nSoNS-enhanced robot swarm gets stuck, it can automatically solicit and run code\ngenerated by an external LLM on the fly, completing its mission with an 85%\nsuccess rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our recently introduced self-organizing nervous system (SoNS) provides robot\nswarms with 1) ease of behavior design and 2) global estimation of the swarm\nconfiguration and its collective environment, facilitating the implementation\nof online automatic code generation for robot swarms. In a demonstration with 6\nreal robots and simulation trials with >30 robots, we show that when a\nSoNS-enhanced robot swarm gets stuck, it can automatically solicit and run code\ngenerated by an external LLM on the fly, completing its mission with an 85%\nsuccess rate."
                },
                "authors": [
                    {
                        "name": "Weixu Zhu"
                    },
                    {
                        "name": "Marco Dorigo"
                    },
                    {
                        "name": "Mary Katherine Heinrich"
                    }
                ],
                "author_detail": {
                    "name": "Mary Katherine Heinrich"
                },
                "author": "Mary Katherine Heinrich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04773v1",
                "updated": "2025-10-06T12:49:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    12,
                    49,
                    0,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T12:49:00Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    12,
                    49,
                    0,
                    0,
                    279,
                    0
                ],
                "title": "Distribution Preference Optimization: A Fine-grained Perspective for LLM\n  Unlearning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distribution Preference Optimization: A Fine-grained Perspective for LLM\n  Unlearning"
                },
                "summary": "As Large Language Models (LLMs) demonstrate remarkable capabilities learned\nfrom vast corpora, concerns regarding data privacy and safety are receiving\nincreasing attention. LLM unlearning, which aims to remove the influence of\nspecific data while preserving overall model utility, is becoming an important\nresearch area. One of the mainstream unlearning classes is optimization-based\nmethods, which achieve forgetting directly through fine-tuning, exemplified by\nNegative Preference Optimization (NPO). However, NPO's effectiveness is limited\nby its inherent lack of explicit positive preference signals. Attempts to\nintroduce such signals by constructing preferred responses often necessitate\ndomain-specific knowledge or well-designed prompts, fundamentally restricting\ntheir generalizability. In this paper, we shift the focus to the\ndistribution-level, directly targeting the next-token probability distribution\ninstead of entire responses, and derive a novel unlearning algorithm termed\n\\textbf{Di}stribution \\textbf{P}reference \\textbf{O}ptimization (DiPO). We show\nthat the requisite preference distribution pairs for DiPO, which are\ndistributions over the model's output tokens, can be constructed by selectively\namplifying or suppressing the model's high-confidence output logits, thereby\neffectively overcoming NPO's limitations. We theoretically prove the\nconsistency of DiPO's loss function with the desired unlearning direction.\nExtensive experiments demonstrate that DiPO achieves a strong trade-off between\nmodel utility and forget quality. Notably, DiPO attains the highest forget\nquality on the TOFU benchmark, and maintains leading scalability and\nsustainability in utility preservation on the MUSE benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) demonstrate remarkable capabilities learned\nfrom vast corpora, concerns regarding data privacy and safety are receiving\nincreasing attention. LLM unlearning, which aims to remove the influence of\nspecific data while preserving overall model utility, is becoming an important\nresearch area. One of the mainstream unlearning classes is optimization-based\nmethods, which achieve forgetting directly through fine-tuning, exemplified by\nNegative Preference Optimization (NPO). However, NPO's effectiveness is limited\nby its inherent lack of explicit positive preference signals. Attempts to\nintroduce such signals by constructing preferred responses often necessitate\ndomain-specific knowledge or well-designed prompts, fundamentally restricting\ntheir generalizability. In this paper, we shift the focus to the\ndistribution-level, directly targeting the next-token probability distribution\ninstead of entire responses, and derive a novel unlearning algorithm termed\n\\textbf{Di}stribution \\textbf{P}reference \\textbf{O}ptimization (DiPO). We show\nthat the requisite preference distribution pairs for DiPO, which are\ndistributions over the model's output tokens, can be constructed by selectively\namplifying or suppressing the model's high-confidence output logits, thereby\neffectively overcoming NPO's limitations. We theoretically prove the\nconsistency of DiPO's loss function with the desired unlearning direction.\nExtensive experiments demonstrate that DiPO achieves a strong trade-off between\nmodel utility and forget quality. Notably, DiPO attains the highest forget\nquality on the TOFU benchmark, and maintains leading scalability and\nsustainability in utility preservation on the MUSE benchmark."
                },
                "authors": [
                    {
                        "name": "Kai Qin"
                    },
                    {
                        "name": "Jiaqi Wu"
                    },
                    {
                        "name": "Jianxiang He"
                    },
                    {
                        "name": "Haoyuan Sun"
                    },
                    {
                        "name": "Yifei Zhao"
                    },
                    {
                        "name": "Bin Liang"
                    },
                    {
                        "name": "Yongzhe Chang"
                    },
                    {
                        "name": "Tiantian Zhang"
                    },
                    {
                        "name": "Houde Liu"
                    }
                ],
                "author_detail": {
                    "name": "Houde Liu"
                },
                "author": "Houde Liu",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01238v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01238v2",
                "updated": "2025-10-06T12:48:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    12,
                    48,
                    5,
                    0,
                    279,
                    0
                ],
                "published": "2025-09-23T22:57:44Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    22,
                    57,
                    44,
                    1,
                    266,
                    0
                ],
                "title": "Silent Tokens, Loud Effects: Padding in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Silent Tokens, Loud Effects: Padding in LLMs"
                },
                "summary": "Padding tokens are widely used in large language models (LLMs) to equalize\nsequence lengths during batched inference. While they should be fully masked,\nimplementation errors can cause them to influence computation, and the extent\nof this influence is not well understood. We systematically study this effect\nacross three open-source model families (Llama, Gemma, Qwen), inserting\ncontrolled amounts of padding and evaluating outcomes along four axes:\nactivations, generation quality, bias, and safety. Even small amounts of\npadding shift hidden representations, degrade quality in smaller models, alter\nbias in unpredictable ways, and weaken safety guardrails. These findings\ndemonstrate that padding is not a harmless detail but a robustness risk that\nmust be carefully handled in deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Padding tokens are widely used in large language models (LLMs) to equalize\nsequence lengths during batched inference. While they should be fully masked,\nimplementation errors can cause them to influence computation, and the extent\nof this influence is not well understood. We systematically study this effect\nacross three open-source model families (Llama, Gemma, Qwen), inserting\ncontrolled amounts of padding and evaluating outcomes along four axes:\nactivations, generation quality, bias, and safety. Even small amounts of\npadding shift hidden representations, degrade quality in smaller models, alter\nbias in unpredictable ways, and weaken safety guardrails. These findings\ndemonstrate that padding is not a harmless detail but a robustness risk that\nmust be carefully handled in deployment."
                },
                "authors": [
                    {
                        "name": "Rom Himelstein"
                    },
                    {
                        "name": "Amit LeVi"
                    },
                    {
                        "name": "Yonatan Belinkov"
                    },
                    {
                        "name": "Avi Mendelson"
                    }
                ],
                "author_detail": {
                    "name": "Avi Mendelson"
                },
                "author": "Avi Mendelson",
                "arxiv_comment": "Accepted to NeurIPS 2025 Workshop on Evaluating the Evolving LLM\n  Lifecycle: Benchmarks, Emergent Abilities, and Scaling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01238v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01238v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04767v1",
                "updated": "2025-10-06T12:41:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    12,
                    41,
                    31,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T12:41:31Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    12,
                    41,
                    31,
                    0,
                    279,
                    0
                ],
                "title": "ParallelBench: Understanding the Trade-offs of Parallel Decoding in\n  Diffusion LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParallelBench: Understanding the Trade-offs of Parallel Decoding in\n  Diffusion LLMs"
                },
                "summary": "While most autoregressive LLMs are constrained to one-by-one decoding,\ndiffusion LLMs (dLLMs) have attracted growing interest for their potential to\ndramatically accelerate inference through parallel decoding. Despite this\npromise, the conditional independence assumption in dLLMs causes parallel\ndecoding to ignore token dependencies, inevitably degrading generation quality\nwhen these dependencies are strong. However, existing works largely overlook\nthese inherent challenges, and evaluations on standard benchmarks (e.g., math\nand coding) are not sufficient to capture the quality degradation caused by\nparallel decoding. To address this gap, we first provide an\ninformation-theoretic analysis of parallel decoding. We then conduct case\nstudies on analytically tractable synthetic list operations from both data\ndistribution and decoding strategy perspectives, offering quantitative insights\nthat highlight the fundamental limitations of parallel decoding. Building on\nthese insights, we propose ParallelBench, the first benchmark specifically\ndesigned for dLLMs, featuring realistic tasks that are trivial for humans and\nautoregressive LLMs yet exceptionally challenging for dLLMs under parallel\ndecoding. Using ParallelBench, we systematically analyze both dLLMs and\nautoregressive LLMs, revealing that: (i) dLLMs under parallel decoding can\nsuffer dramatic quality degradation in real-world scenarios, and (ii) current\nparallel decoding strategies struggle to adapt their degree of parallelism\nbased on task difficulty, thus failing to achieve meaningful speedup without\ncompromising quality. Our findings underscore the pressing need for innovative\ndecoding methods that can overcome the current speed-quality trade-off. We\nrelease our benchmark to help accelerate the development of truly efficient\ndLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While most autoregressive LLMs are constrained to one-by-one decoding,\ndiffusion LLMs (dLLMs) have attracted growing interest for their potential to\ndramatically accelerate inference through parallel decoding. Despite this\npromise, the conditional independence assumption in dLLMs causes parallel\ndecoding to ignore token dependencies, inevitably degrading generation quality\nwhen these dependencies are strong. However, existing works largely overlook\nthese inherent challenges, and evaluations on standard benchmarks (e.g., math\nand coding) are not sufficient to capture the quality degradation caused by\nparallel decoding. To address this gap, we first provide an\ninformation-theoretic analysis of parallel decoding. We then conduct case\nstudies on analytically tractable synthetic list operations from both data\ndistribution and decoding strategy perspectives, offering quantitative insights\nthat highlight the fundamental limitations of parallel decoding. Building on\nthese insights, we propose ParallelBench, the first benchmark specifically\ndesigned for dLLMs, featuring realistic tasks that are trivial for humans and\nautoregressive LLMs yet exceptionally challenging for dLLMs under parallel\ndecoding. Using ParallelBench, we systematically analyze both dLLMs and\nautoregressive LLMs, revealing that: (i) dLLMs under parallel decoding can\nsuffer dramatic quality degradation in real-world scenarios, and (ii) current\nparallel decoding strategies struggle to adapt their degree of parallelism\nbased on task difficulty, thus failing to achieve meaningful speedup without\ncompromising quality. Our findings underscore the pressing need for innovative\ndecoding methods that can overcome the current speed-quality trade-off. We\nrelease our benchmark to help accelerate the development of truly efficient\ndLLMs."
                },
                "authors": [
                    {
                        "name": "Wonjun Kang"
                    },
                    {
                        "name": "Kevin Galim"
                    },
                    {
                        "name": "Seunghyuk Oh"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Yuchen Zeng"
                    },
                    {
                        "name": "Shuibai Zhang"
                    },
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Yuezhou Hu"
                    },
                    {
                        "name": "Hyung Il Koo"
                    },
                    {
                        "name": "Nam Ik Cho"
                    },
                    {
                        "name": "Kangwook Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kangwook Lee"
                },
                "author": "Kangwook Lee",
                "arxiv_comment": "Project Page: https://parallelbench.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04764v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04764v1",
                "updated": "2025-10-06T12:38:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    12,
                    38,
                    41,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T12:38:41Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    12,
                    38,
                    41,
                    0,
                    279,
                    0
                ],
                "title": "Are BabyLMs Deaf to Gricean Maxims? A Pragmatic Evaluation of\n  Sample-efficient Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are BabyLMs Deaf to Gricean Maxims? A Pragmatic Evaluation of\n  Sample-efficient Language Models"
                },
                "summary": "Implicit meanings are integral to human communication, making it essential\nfor language models to be capable of identifying and interpreting them. Grice\n(1975) proposed a set of conversational maxims that guide cooperative dialogue,\nnoting that speakers may deliberately violate these principles to express\nmeanings beyond literal words, and that listeners, in turn, recognize such\nviolations to draw pragmatic inferences.\n  Building on Surian et al. (1996)'s study of children's sensitivity to\nviolations of Gricean maxims, we introduce a novel benchmark to test whether\nlanguage models pretrained on less than 10M and less than 100M tokens can\ndistinguish maxim-adhering from maxim-violating utterances. We compare these\nBabyLMs across five maxims and situate their performance relative to children\nand a Large Language Model (LLM) pretrained on 3T tokens.\n  We find that overall, models trained on less than 100M tokens outperform\nthose trained on less than 10M, yet fall short of child-level and LLM\ncompetence. Our results suggest that modest data increases improve some aspects\nof pragmatic behavior, leading to finer-grained differentiation between\npragmatic dimensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit meanings are integral to human communication, making it essential\nfor language models to be capable of identifying and interpreting them. Grice\n(1975) proposed a set of conversational maxims that guide cooperative dialogue,\nnoting that speakers may deliberately violate these principles to express\nmeanings beyond literal words, and that listeners, in turn, recognize such\nviolations to draw pragmatic inferences.\n  Building on Surian et al. (1996)'s study of children's sensitivity to\nviolations of Gricean maxims, we introduce a novel benchmark to test whether\nlanguage models pretrained on less than 10M and less than 100M tokens can\ndistinguish maxim-adhering from maxim-violating utterances. We compare these\nBabyLMs across five maxims and situate their performance relative to children\nand a Large Language Model (LLM) pretrained on 3T tokens.\n  We find that overall, models trained on less than 100M tokens outperform\nthose trained on less than 10M, yet fall short of child-level and LLM\ncompetence. Our results suggest that modest data increases improve some aspects\nof pragmatic behavior, leading to finer-grained differentiation between\npragmatic dimensions."
                },
                "authors": [
                    {
                        "name": "Raha Askari"
                    },
                    {
                        "name": "Sina ZarrieÃ"
                    },
                    {
                        "name": "Ãzge Alacam"
                    },
                    {
                        "name": "Judith Sieker"
                    }
                ],
                "author_detail": {
                    "name": "Judith Sieker"
                },
                "author": "Judith Sieker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04764v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04764v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01667v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01667v3",
                "updated": "2025-10-06T12:36:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    12,
                    36,
                    33,
                    0,
                    279,
                    0
                ],
                "published": "2025-04-02T12:16:14Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    12,
                    16,
                    14,
                    2,
                    92,
                    0
                ],
                "title": "Testing Low-Resource Language Support in LLMs Using Language Proficiency\n  Exams: the Case of Luxembourgish",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing Low-Resource Language Support in LLMs Using Language Proficiency\n  Exams: the Case of Luxembourgish"
                },
                "summary": "Large Language Models (LLMs) have become an increasingly important tool in\nresearch and society at large. While LLMs are regularly used all over the world\nby experts and lay-people alike, they are predominantly developed with\nEnglish-speaking users in mind, performing well in English and other\nwide-spread languages while less-resourced languages such as Luxembourgish are\nseen as a lower priority. This lack of attention is also reflected in the\nsparsity of available evaluation tools and datasets. In this study, we\ninvestigate the viability of language proficiency exams as such evaluation\ntools for the Luxembourgish language. We find that large models such as Claude\nand DeepSeek-R1 typically achieve high scores, while smaller models show weak\nperformances. We also find that the performances in such language exams can be\nused to predict performances in other NLP tasks in Luxembourgish.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become an increasingly important tool in\nresearch and society at large. While LLMs are regularly used all over the world\nby experts and lay-people alike, they are predominantly developed with\nEnglish-speaking users in mind, performing well in English and other\nwide-spread languages while less-resourced languages such as Luxembourgish are\nseen as a lower priority. This lack of attention is also reflected in the\nsparsity of available evaluation tools and datasets. In this study, we\ninvestigate the viability of language proficiency exams as such evaluation\ntools for the Luxembourgish language. We find that large models such as Claude\nand DeepSeek-R1 typically achieve high scores, while smaller models show weak\nperformances. We also find that the performances in such language exams can be\nused to predict performances in other NLP tasks in Luxembourgish."
                },
                "authors": [
                    {
                        "name": "Cedric Lothritz"
                    },
                    {
                        "name": "Jordi Cabot"
                    },
                    {
                        "name": "Laura Bernardy"
                    }
                ],
                "author_detail": {
                    "name": "Laura Bernardy"
                },
                "author": "Laura Bernardy",
                "arxiv_comment": "23pages, 4 figures, 14 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01667v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01667v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04757v1",
                "updated": "2025-10-06T12:34:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    12,
                    34,
                    55,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T12:34:55Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    12,
                    34,
                    55,
                    0,
                    279,
                    0
                ],
                "title": "ModernBERT + ColBERT: Enhancing biomedical RAG through an advanced\n  re-ranking retriever",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ModernBERT + ColBERT: Enhancing biomedical RAG through an advanced\n  re-ranking retriever"
                },
                "summary": "Retrieval-Augmented Generation (RAG) is a powerful technique for enriching\nLarge Language Models (LLMs) with external knowledge, allowing for factually\ngrounded responses, a critical requirement in high-stakes domains such as\nhealthcare. However, the efficacy of RAG systems is fundamentally restricted by\nthe performance of their retrieval module, since irrelevant or semantically\nmisaligned documents directly compromise the accuracy of the final generated\nresponse. General-purpose dense retrievers can struggle with the nuanced\nlanguage of specialised domains, while the high accuracy of in-domain models is\noften achieved at prohibitive computational costs. In this work, we aim to\naddress this trade-off by developing and evaluating a two-stage retrieval\narchitecture that combines a lightweight ModernBERT bidirectional encoder for\nefficient initial candidate retrieval with a ColBERTv2 late-interaction model\nfor fine-grained re-ranking. We conduct comprehensive evaluations of our\nretriever module performance and RAG system performance in the biomedical\ncontext, fine-tuning the IR module using 10k question-passage pairs from\nPubMedQA. Our analysis of the retriever module confirmed the positive impact of\nthe ColBERT re-ranker, which improved Recall@3 by up to 4.2 percentage points\ncompared to its retrieve-only counterpart. When integrated into the biomedical\nRAG, our IR module leads to a state-of-the-art average accuracy of 0.4448 on\nthe five tasks of the MIRAGE question-answering benchmark, outperforming strong\nbaselines such as MedCPT (0.4436). Our ablation studies reveal that this\nperformance is critically dependent on a joint fine-tuning process that aligns\nthe retriever and re-ranker; otherwise, the re-ranker might degrade the\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) is a powerful technique for enriching\nLarge Language Models (LLMs) with external knowledge, allowing for factually\ngrounded responses, a critical requirement in high-stakes domains such as\nhealthcare. However, the efficacy of RAG systems is fundamentally restricted by\nthe performance of their retrieval module, since irrelevant or semantically\nmisaligned documents directly compromise the accuracy of the final generated\nresponse. General-purpose dense retrievers can struggle with the nuanced\nlanguage of specialised domains, while the high accuracy of in-domain models is\noften achieved at prohibitive computational costs. In this work, we aim to\naddress this trade-off by developing and evaluating a two-stage retrieval\narchitecture that combines a lightweight ModernBERT bidirectional encoder for\nefficient initial candidate retrieval with a ColBERTv2 late-interaction model\nfor fine-grained re-ranking. We conduct comprehensive evaluations of our\nretriever module performance and RAG system performance in the biomedical\ncontext, fine-tuning the IR module using 10k question-passage pairs from\nPubMedQA. Our analysis of the retriever module confirmed the positive impact of\nthe ColBERT re-ranker, which improved Recall@3 by up to 4.2 percentage points\ncompared to its retrieve-only counterpart. When integrated into the biomedical\nRAG, our IR module leads to a state-of-the-art average accuracy of 0.4448 on\nthe five tasks of the MIRAGE question-answering benchmark, outperforming strong\nbaselines such as MedCPT (0.4436). Our ablation studies reveal that this\nperformance is critically dependent on a joint fine-tuning process that aligns\nthe retriever and re-ranker; otherwise, the re-ranker might degrade the\nperformance."
                },
                "authors": [
                    {
                        "name": "Eduardo MartÃ­nez Rivera"
                    },
                    {
                        "name": "Filippo Menolascina"
                    }
                ],
                "author_detail": {
                    "name": "Filippo Menolascina"
                },
                "author": "Filippo Menolascina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04749v1",
                "updated": "2025-10-06T12:27:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    12,
                    27,
                    45,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T12:27:45Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    12,
                    27,
                    45,
                    0,
                    279,
                    0
                ],
                "title": "LLM-Based Information Extraction to Support Scientific Literature\n  Research and Publication Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Information Extraction to Support Scientific Literature\n  Research and Publication Workflows"
                },
                "summary": "The increasing volume of scholarly publications requires advanced tools for\nefficient knowledge discovery and management. This paper introduces ongoing\nwork on a system using Large Language Models (LLMs) for the semantic extraction\nof key concepts from scientific documents. Our research, conducted within the\nGerman National Research Data Infrastructure for and with Computer Science\n(NFDIxCS) project, seeks to support FAIR (Findable, Accessible, Interoperable,\nand Reusable) principles in scientific publishing. We outline our explorative\nwork, which uses in-context learning with various LLMs to extract concepts from\npapers, initially focusing on the Business Process Management (BPM) domain. A\nkey advantage of this approach is its potential for rapid domain adaptation,\noften requiring few or even zero examples to define extraction targets for new\nscientific fields. We conducted technical evaluations to compare the\nperformance of commercial and open-source LLMs and created an online demo\napplication to collect feedback from an initial user-study. Additionally, we\ngathered insights from the computer science research community through user\nstories collected during a dedicated workshop, actively guiding the ongoing\ndevelopment of our future services. These services aim to support structured\nliterature reviews, concept-based information retrieval, and integration of\nextracted knowledge into existing knowledge graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing volume of scholarly publications requires advanced tools for\nefficient knowledge discovery and management. This paper introduces ongoing\nwork on a system using Large Language Models (LLMs) for the semantic extraction\nof key concepts from scientific documents. Our research, conducted within the\nGerman National Research Data Infrastructure for and with Computer Science\n(NFDIxCS) project, seeks to support FAIR (Findable, Accessible, Interoperable,\nand Reusable) principles in scientific publishing. We outline our explorative\nwork, which uses in-context learning with various LLMs to extract concepts from\npapers, initially focusing on the Business Process Management (BPM) domain. A\nkey advantage of this approach is its potential for rapid domain adaptation,\noften requiring few or even zero examples to define extraction targets for new\nscientific fields. We conducted technical evaluations to compare the\nperformance of commercial and open-source LLMs and created an online demo\napplication to collect feedback from an initial user-study. Additionally, we\ngathered insights from the computer science research community through user\nstories collected during a dedicated workshop, actively guiding the ongoing\ndevelopment of our future services. These services aim to support structured\nliterature reviews, concept-based information retrieval, and integration of\nextracted knowledge into existing knowledge graphs."
                },
                "authors": [
                    {
                        "name": "Samy Ateia"
                    },
                    {
                        "name": "Udo Kruschwitz"
                    },
                    {
                        "name": "Melanie Scholz"
                    },
                    {
                        "name": "Agnes Koschmider"
                    },
                    {
                        "name": "Moayad Almohaishi"
                    }
                ],
                "author_detail": {
                    "name": "Moayad Almohaishi"
                },
                "author": "Moayad Almohaishi",
                "arxiv_doi": "10.1007/978-3-032-06136-2_9",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-032-06136-2_9",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.04749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This PDF is the author-prepared camera-ready version corresponding to\n  the accepted manuscript and supersedes the submitted version that was\n  inadvertently published as the version of record",
                "arxiv_journal_ref": "New Trends in Theory and Practice of Digital Libraries. TPDL 2025.\n  Communications in Computer and Information Science, vol 2694. pp 90-99",
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04741v1",
                "updated": "2025-10-06T12:13:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    12,
                    13,
                    56,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T12:13:56Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    12,
                    13,
                    56,
                    0,
                    279,
                    0
                ],
                "title": "Anomaly-Aware YOLO: A Frugal yet Robust Approach to Infrared Small\n  Target Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anomaly-Aware YOLO: A Frugal yet Robust Approach to Infrared Small\n  Target Detection"
                },
                "summary": "Infrared Small Target Detection (IRSTD) is a challenging task in defense\napplications, where complex backgrounds and tiny target sizes often result in\nnumerous false alarms using conventional object detectors. To overcome this\nlimitation, we propose Anomaly-Aware YOLO (AA-YOLO), which integrates a\nstatistical anomaly detection test into its detection head. By treating small\ntargets as unexpected patterns against the background, AA-YOLO effectively\ncontrols the false alarm rate. Our approach not only achieves competitive\nperformance on several IRSTD benchmarks, but also demonstrates remarkable\nrobustness in scenarios with limited training data, noise, and domain shifts.\nFurthermore, since only the detection head is modified, our design is highly\ngeneric and has been successfully applied across various YOLO backbones,\nincluding lightweight models. It also provides promising results when\nintegrated into an instance segmentation YOLO. This versatility makes AA-YOLO\nan attractive solution for real-world deployments where resources are\nconstrained. The code will be publicly released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Infrared Small Target Detection (IRSTD) is a challenging task in defense\napplications, where complex backgrounds and tiny target sizes often result in\nnumerous false alarms using conventional object detectors. To overcome this\nlimitation, we propose Anomaly-Aware YOLO (AA-YOLO), which integrates a\nstatistical anomaly detection test into its detection head. By treating small\ntargets as unexpected patterns against the background, AA-YOLO effectively\ncontrols the false alarm rate. Our approach not only achieves competitive\nperformance on several IRSTD benchmarks, but also demonstrates remarkable\nrobustness in scenarios with limited training data, noise, and domain shifts.\nFurthermore, since only the detection head is modified, our design is highly\ngeneric and has been successfully applied across various YOLO backbones,\nincluding lightweight models. It also provides promising results when\nintegrated into an instance segmentation YOLO. This versatility makes AA-YOLO\nan attractive solution for real-world deployments where resources are\nconstrained. The code will be publicly released."
                },
                "authors": [
                    {
                        "name": "Alina Ciocarlan"
                    },
                    {
                        "name": "Sylvie Le HÃ©garat-Mascle"
                    },
                    {
                        "name": "Sidonie Lefebvre"
                    }
                ],
                "author_detail": {
                    "name": "Sidonie Lefebvre"
                },
                "author": "Sidonie Lefebvre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07644v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07644v2",
                "updated": "2025-10-06T12:00:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    12,
                    0,
                    21,
                    0,
                    279,
                    0
                ],
                "published": "2025-07-10T11:16:48Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    11,
                    16,
                    48,
                    3,
                    191,
                    0
                ],
                "title": "FloorplanQA: A Benchmark for Spatial Reasoning in LLMs using Structured\n  Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FloorplanQA: A Benchmark for Spatial Reasoning in LLMs using Structured\n  Representations"
                },
                "summary": "We introduce FloorplanQA, a diagnostic benchmark for evaluating spatial\nreasoning in large-language models (LLMs). FloorplanQA is grounded in\nstructured representations of indoor scenes, such as (e.g., kitchens, living\nrooms, bedrooms, bathrooms, and others), encoded symbolically in JSON or XML\nlayouts. The benchmark covers core spatial tasks, including distance\nmeasurement, visibility, path finding, and object placement within constrained\nspaces. Our results across a variety of frontier open-source and commercial\nLLMs reveal that while models may succeed in shallow queries, they often fail\nto respect physical constraints, preserve spatial coherence, though they remain\nmostly robust to small spatial perturbations. FloorplanQA uncovers a blind spot\nin today's LLMs: inconsistent reasoning about indoor layouts. We hope this\nbenchmark inspires new work on language models that can accurately infer and\nmanipulate spatial and geometric properties in practical settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce FloorplanQA, a diagnostic benchmark for evaluating spatial\nreasoning in large-language models (LLMs). FloorplanQA is grounded in\nstructured representations of indoor scenes, such as (e.g., kitchens, living\nrooms, bedrooms, bathrooms, and others), encoded symbolically in JSON or XML\nlayouts. The benchmark covers core spatial tasks, including distance\nmeasurement, visibility, path finding, and object placement within constrained\nspaces. Our results across a variety of frontier open-source and commercial\nLLMs reveal that while models may succeed in shallow queries, they often fail\nto respect physical constraints, preserve spatial coherence, though they remain\nmostly robust to small spatial perturbations. FloorplanQA uncovers a blind spot\nin today's LLMs: inconsistent reasoning about indoor layouts. We hope this\nbenchmark inspires new work on language models that can accurately infer and\nmanipulate spatial and geometric properties in practical settings."
                },
                "authors": [
                    {
                        "name": "Fedor Rodionov"
                    },
                    {
                        "name": "Abdelrahman Eldesokey"
                    },
                    {
                        "name": "Michael Birsak"
                    },
                    {
                        "name": "John Femiani"
                    },
                    {
                        "name": "Bernard Ghanem"
                    },
                    {
                        "name": "Peter Wonka"
                    }
                ],
                "author_detail": {
                    "name": "Peter Wonka"
                },
                "author": "Peter Wonka",
                "arxiv_comment": "v2, Project page: https://OldDelorean.github.io/FloorplanQA/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07644v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07644v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04721v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04721v1",
                "updated": "2025-10-06T11:41:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    11,
                    41,
                    46,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T11:41:46Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    11,
                    41,
                    46,
                    0,
                    279,
                    0
                ],
                "title": "BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs"
                },
                "summary": "Large language models (LLMs) have recently shown strong performance on\nmathematical benchmarks. At the same time, they are prone to hallucination and\nsycophancy, often providing convincing but flawed proofs for incorrect\nmathematical statements provided by users. This significantly limits the\napplicability of LLMs in theorem proving, as verification of these flawed\nproofs must be done manually by expert mathematicians. However, existing\nbenchmarks that measure sycophancy in mathematics are limited: they focus\nsolely on final-answer problems, rely on very simple and often contaminated\ndatasets, and construct benchmark samples using synthetic modifications that\ncreate ill-posed questions rather than well-posed questions that are\ndemonstrably false. To address these issues, we introduce BrokenMath, the first\nbenchmark for evaluating sycophantic behavior in LLMs within the context of\nnatural language theorem proving. BrokenMath is built from advanced 2025\ncompetition problems, which are perturbed with an LLM to produce false\nstatements and subsequently refined through expert review. Using an\nLLM-as-a-judge framework, we evaluate state-of-the-art LLMs and agentic systems\nand find that sycophancy is widespread, with the best model, GPT-5, producing\nsycophantic answers 29% of the time. We further investigate several mitigation\nstrategies, including test-time interventions and supervised fine-tuning on\ncurated sycophantic examples. These approaches substantially reduce, but do not\neliminate, sycophantic behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently shown strong performance on\nmathematical benchmarks. At the same time, they are prone to hallucination and\nsycophancy, often providing convincing but flawed proofs for incorrect\nmathematical statements provided by users. This significantly limits the\napplicability of LLMs in theorem proving, as verification of these flawed\nproofs must be done manually by expert mathematicians. However, existing\nbenchmarks that measure sycophancy in mathematics are limited: they focus\nsolely on final-answer problems, rely on very simple and often contaminated\ndatasets, and construct benchmark samples using synthetic modifications that\ncreate ill-posed questions rather than well-posed questions that are\ndemonstrably false. To address these issues, we introduce BrokenMath, the first\nbenchmark for evaluating sycophantic behavior in LLMs within the context of\nnatural language theorem proving. BrokenMath is built from advanced 2025\ncompetition problems, which are perturbed with an LLM to produce false\nstatements and subsequently refined through expert review. Using an\nLLM-as-a-judge framework, we evaluate state-of-the-art LLMs and agentic systems\nand find that sycophancy is widespread, with the best model, GPT-5, producing\nsycophantic answers 29% of the time. We further investigate several mitigation\nstrategies, including test-time interventions and supervised fine-tuning on\ncurated sycophantic examples. These approaches substantially reduce, but do not\neliminate, sycophantic behavior."
                },
                "authors": [
                    {
                        "name": "Ivo Petrov"
                    },
                    {
                        "name": "Jasper Dekoninck"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04721v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04721v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00264v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00264v2",
                "updated": "2025-10-06T11:39:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    11,
                    39,
                    10,
                    0,
                    279,
                    0
                ],
                "published": "2025-09-30T20:36:58Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    20,
                    36,
                    58,
                    1,
                    273,
                    0
                ],
                "title": "Low Resource Audio Codec Challenge Baseline Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low Resource Audio Codec Challenge Baseline Systems"
                },
                "summary": "The Low-Resource Audio Codec (LRAC) Challenge aims to advance neural audio\ncoding for deployment in resource-constrained environments. The first edition\nfocuses on low-resource neural speech codecs that must operate reliably under\neveryday noise and reverberation, while satisfying strict constraints on\ncomputational complexity, latency, and bitrate. Track 1 targets transparency\ncodecs, which aim to preserve the perceptual transparency of input speech under\nmild noise and reverberation. Track 2 addresses enhancement codecs, which\ncombine coding and compression with denoising and dereverberation. This paper\npresents the official baseline systems for both tracks in the 2025 LRAC\nChallenge. The baselines are convolutional neural codec models with Residual\nVector Quantization, trained end-to-end using a combination of adversarial and\nreconstruction objectives. We detail the data filtering and augmentation\nstrategies, model architectures, optimization procedures, and checkpoint\nselection criteria.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Low-Resource Audio Codec (LRAC) Challenge aims to advance neural audio\ncoding for deployment in resource-constrained environments. The first edition\nfocuses on low-resource neural speech codecs that must operate reliably under\neveryday noise and reverberation, while satisfying strict constraints on\ncomputational complexity, latency, and bitrate. Track 1 targets transparency\ncodecs, which aim to preserve the perceptual transparency of input speech under\nmild noise and reverberation. Track 2 addresses enhancement codecs, which\ncombine coding and compression with denoising and dereverberation. This paper\npresents the official baseline systems for both tracks in the 2025 LRAC\nChallenge. The baselines are convolutional neural codec models with Residual\nVector Quantization, trained end-to-end using a combination of adversarial and\nreconstruction objectives. We detail the data filtering and augmentation\nstrategies, model architectures, optimization procedures, and checkpoint\nselection criteria."
                },
                "authors": [
                    {
                        "name": "Yusuf Ziya Isik"
                    },
                    {
                        "name": "RafaÅ Åaganowski"
                    }
                ],
                "author_detail": {
                    "name": "RafaÅ Åaganowski"
                },
                "author": "RafaÅ Åaganowski",
                "arxiv_comment": "Low-Resource Audio Codec Challenge 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00264v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00264v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06256v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06256v3",
                "updated": "2025-10-06T11:37:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    11,
                    37,
                    13,
                    0,
                    279,
                    0
                ],
                "published": "2025-01-09T09:45:05Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    9,
                    45,
                    5,
                    3,
                    9,
                    0
                ],
                "title": "Unlocking In-Context Learning for Natural Datasets Beyond Language\n  Modelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking In-Context Learning for Natural Datasets Beyond Language\n  Modelling"
                },
                "summary": "Large Language Models (LLMs) exhibit In-Context Learning (ICL), which enables\nthe model to perform new tasks conditioning only on the examples provided in\nthe context without updating the model's weights. While ICL offers fast\nadaptation across natural language tasks and domains, its emergence is less\nstraightforward for modalities beyond text. In this work, we systematically\nuncover properties present in LLMs that support the emergence of ICL for\nautoregressive models and various modalities by promoting the learning of the\nneeded mechanisms for ICL. We identify exact token repetitions in the training\ndata sequences as an important factor for ICL. Such repetitions further improve\nstability and reduce transiency in ICL performance. Moreover, we emphasise the\nsignificance of training task difficulty for the emergence of ICL. Finally, by\napplying our novel insights on ICL emergence, we unlock ICL capabilities for\nvarious visual datasets and a more challenging EEG classification task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit In-Context Learning (ICL), which enables\nthe model to perform new tasks conditioning only on the examples provided in\nthe context without updating the model's weights. While ICL offers fast\nadaptation across natural language tasks and domains, its emergence is less\nstraightforward for modalities beyond text. In this work, we systematically\nuncover properties present in LLMs that support the emergence of ICL for\nautoregressive models and various modalities by promoting the learning of the\nneeded mechanisms for ICL. We identify exact token repetitions in the training\ndata sequences as an important factor for ICL. Such repetitions further improve\nstability and reduce transiency in ICL performance. Moreover, we emphasise the\nsignificance of training task difficulty for the emergence of ICL. Finally, by\napplying our novel insights on ICL emergence, we unlock ICL capabilities for\nvarious visual datasets and a more challenging EEG classification task."
                },
                "authors": [
                    {
                        "name": "Jelena BratuliÄ"
                    },
                    {
                        "name": "Sudhanshu Mittal"
                    },
                    {
                        "name": "David T. Hoffmann"
                    },
                    {
                        "name": "Samuel BÃ¶hm"
                    },
                    {
                        "name": "Robin Tibor Schirrmeister"
                    },
                    {
                        "name": "Tonio Ball"
                    },
                    {
                        "name": "Christian Rupprecht"
                    },
                    {
                        "name": "Thomas Brox"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Brox"
                },
                "author": "Thomas Brox",
                "arxiv_comment": "Best Paper Honorable Mention at GCPR 2025 (German Conference on\n  Pattern Recognition). This is the updated version submitted to the\n  conference, not the official conference proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06256v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06256v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04717v1",
                "updated": "2025-10-06T11:36:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    11,
                    36,
                    46,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T11:36:46Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    11,
                    36,
                    46,
                    0,
                    279,
                    0
                ],
                "title": "JSON Whisperer: Efficient JSON Editing with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JSON Whisperer: Efficient JSON Editing with LLMs"
                },
                "summary": "Large language models (LLMs) can modify JSON documents through natural\nlanguage commands, but current approaches regenerate entire structures for each\nedit, resulting in computational inefficiency. We present JSON Whisperer, a\nframework that enables LLMs to generate RFC 6902 diff patches-expressing only\nthe necessary modifications-rather than complete documents. We identify two key\nchallenges in patch-based editing: (1) LLMs often miss related updates when\ngenerating isolated patches, and (2) array manipulations require tracking index\nshifts across operations, which LLMs handle poorly. To address these issues, we\nintroduce EASE (Explicitly Addressed Sequence Encoding), which transforms\narrays into dictionaries with stable keys, eliminating index arithmetic\ncomplexities. Our evaluation shows that patch generation with EASE reduces\ntoken usage by 31% while maintaining edit quality within 5% of full\nregeneration with particular gains for complex instructions and list\nmanipulations. The dataset is available at:\nhttps://github.com/emnlp2025/JSON-Whisperer/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can modify JSON documents through natural\nlanguage commands, but current approaches regenerate entire structures for each\nedit, resulting in computational inefficiency. We present JSON Whisperer, a\nframework that enables LLMs to generate RFC 6902 diff patches-expressing only\nthe necessary modifications-rather than complete documents. We identify two key\nchallenges in patch-based editing: (1) LLMs often miss related updates when\ngenerating isolated patches, and (2) array manipulations require tracking index\nshifts across operations, which LLMs handle poorly. To address these issues, we\nintroduce EASE (Explicitly Addressed Sequence Encoding), which transforms\narrays into dictionaries with stable keys, eliminating index arithmetic\ncomplexities. Our evaluation shows that patch generation with EASE reduces\ntoken usage by 31% while maintaining edit quality within 5% of full\nregeneration with particular gains for complex instructions and list\nmanipulations. The dataset is available at:\nhttps://github.com/emnlp2025/JSON-Whisperer/"
                },
                "authors": [
                    {
                        "name": "Sarel Duanis"
                    },
                    {
                        "name": "Asnat Greenstein-Messica"
                    },
                    {
                        "name": "Eliya Habba"
                    }
                ],
                "author_detail": {
                    "name": "Eliya Habba"
                },
                "author": "Eliya Habba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02348v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02348v3",
                "updated": "2025-10-06T11:35:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    11,
                    35,
                    2,
                    0,
                    279,
                    0
                ],
                "published": "2024-11-04T18:18:38Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    18,
                    38,
                    0,
                    309,
                    0
                ],
                "title": "Can Large Language Models generalize analogy solving like children can?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models generalize analogy solving like children can?"
                },
                "summary": "In people, the ability to solve analogies such as \"body : feet :: table : ?\"\nemerges in childhood, and appears to transfer easily to other domains, such as\nthe visual domain \"( : ) :: < : ?\". Recent research shows that large language\nmodels (LLMs) can solve various forms of analogies. However, can LLMs\ngeneralize analogy solving to new domains like people can? To investigate this,\nwe had children, adults, and LLMs solve a series of letter-string analogies\n(e.g., a b : a c :: j k : ?) in the Latin alphabet, in a near transfer domain\n(Greek alphabet), and a far transfer domain (list of symbols). Children and\nadults easily generalized their knowledge to unfamiliar domains, whereas LLMs\ndid not. This key difference between human and AI performance is evidence that\nthese LLMs still struggle with robust human-like analogical transfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In people, the ability to solve analogies such as \"body : feet :: table : ?\"\nemerges in childhood, and appears to transfer easily to other domains, such as\nthe visual domain \"( : ) :: < : ?\". Recent research shows that large language\nmodels (LLMs) can solve various forms of analogies. However, can LLMs\ngeneralize analogy solving to new domains like people can? To investigate this,\nwe had children, adults, and LLMs solve a series of letter-string analogies\n(e.g., a b : a c :: j k : ?) in the Latin alphabet, in a near transfer domain\n(Greek alphabet), and a far transfer domain (list of symbols). Children and\nadults easily generalized their knowledge to unfamiliar domains, whereas LLMs\ndid not. This key difference between human and AI performance is evidence that\nthese LLMs still struggle with robust human-like analogical transfer."
                },
                "authors": [
                    {
                        "name": "Claire E. Stevenson"
                    },
                    {
                        "name": "Alexandra Pafford"
                    },
                    {
                        "name": "Han L. J. van der Maas"
                    },
                    {
                        "name": "Melanie Mitchell"
                    }
                ],
                "author_detail": {
                    "name": "Melanie Mitchell"
                },
                "author": "Melanie Mitchell",
                "arxiv_comment": "Accepted to Transactions of the Association for Computational\n  Linguistics (TACL)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02348v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02348v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04710v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04710v1",
                "updated": "2025-10-06T11:24:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    11,
                    24,
                    53,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T11:24:53Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    11,
                    24,
                    53,
                    0,
                    279,
                    0
                ],
                "title": "ViTs: Teaching Machines to See Time Series Anomalies Like Human Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ViTs: Teaching Machines to See Time Series Anomalies Like Human Experts"
                },
                "summary": "Web service administrators must ensure the stability of multiple systems by\npromptly detecting anomalies in Key Performance Indicators (KPIs). Achieving\nthe goal of \"train once, infer across scenarios\" remains a fundamental\nchallenge for time series anomaly detection models. Beyond improving zero-shot\ngeneralization, such models must also flexibly handle sequences of varying\nlengths during inference, ranging from one hour to one week, without\nretraining. Conventional approaches rely on sliding-window encoding and\nself-supervised learning, which restrict inference to fixed-length inputs.\nLarge Language Models (LLMs) have demonstrated remarkable zero-shot\ncapabilities across general domains. However, when applied to time series data,\nthey face inherent limitations due to context length. To address this issue, we\npropose ViTs, a Vision-Language Model (VLM)-based framework that converts time\nseries curves into visual representations. By rescaling time series images,\ntemporal dependencies are preserved while maintaining a consistent input size,\nthereby enabling efficient processing of arbitrarily long sequences without\ncontext constraints. Training VLMs for this purpose introduces unique\nchallenges, primarily due to the scarcity of aligned time series image-text\ndata. To overcome this, we employ an evolutionary algorithm to automatically\ngenerate thousands of high-quality image-text pairs and design a three-stage\ntraining pipeline consisting of: (1) time series knowledge injection, (2)\nanomaly detection enhancement, and (3) anomaly reasoning refinement. Extensive\nexperiments demonstrate that ViTs substantially enhance the ability of VLMs to\nunderstand and detect anomalies in time series data. All datasets and code will\nbe publicly released at: https://anonymous.4open.science/r/ViTs-C484/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web service administrators must ensure the stability of multiple systems by\npromptly detecting anomalies in Key Performance Indicators (KPIs). Achieving\nthe goal of \"train once, infer across scenarios\" remains a fundamental\nchallenge for time series anomaly detection models. Beyond improving zero-shot\ngeneralization, such models must also flexibly handle sequences of varying\nlengths during inference, ranging from one hour to one week, without\nretraining. Conventional approaches rely on sliding-window encoding and\nself-supervised learning, which restrict inference to fixed-length inputs.\nLarge Language Models (LLMs) have demonstrated remarkable zero-shot\ncapabilities across general domains. However, when applied to time series data,\nthey face inherent limitations due to context length. To address this issue, we\npropose ViTs, a Vision-Language Model (VLM)-based framework that converts time\nseries curves into visual representations. By rescaling time series images,\ntemporal dependencies are preserved while maintaining a consistent input size,\nthereby enabling efficient processing of arbitrarily long sequences without\ncontext constraints. Training VLMs for this purpose introduces unique\nchallenges, primarily due to the scarcity of aligned time series image-text\ndata. To overcome this, we employ an evolutionary algorithm to automatically\ngenerate thousands of high-quality image-text pairs and design a three-stage\ntraining pipeline consisting of: (1) time series knowledge injection, (2)\nanomaly detection enhancement, and (3) anomaly reasoning refinement. Extensive\nexperiments demonstrate that ViTs substantially enhance the ability of VLMs to\nunderstand and detect anomalies in time series data. All datasets and code will\nbe publicly released at: https://anonymous.4open.science/r/ViTs-C484/."
                },
                "authors": [
                    {
                        "name": "Zexin Wang"
                    },
                    {
                        "name": "Changhua Pei"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Hengyue Jiang"
                    },
                    {
                        "name": "Quan Zhou"
                    },
                    {
                        "name": "Haotian Si"
                    },
                    {
                        "name": "Hang Cui"
                    },
                    {
                        "name": "Jianhui Li"
                    },
                    {
                        "name": "Gaogang Xie"
                    },
                    {
                        "name": "Jingjing Li"
                    },
                    {
                        "name": "Dan Pei"
                    }
                ],
                "author_detail": {
                    "name": "Dan Pei"
                },
                "author": "Dan Pei",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04710v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04710v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04704v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04704v2",
                "updated": "2025-10-07T04:08:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    4,
                    8,
                    44,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-06T11:17:56Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    11,
                    17,
                    56,
                    0,
                    279,
                    0
                ],
                "title": "AtomWorld: A Benchmark for Evaluating Spatial Reasoning in Large\n  Language Models on Crystalline Materials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AtomWorld: A Benchmark for Evaluating Spatial Reasoning in Large\n  Language Models on Crystalline Materials"
                },
                "summary": "Large Language Models (LLMs) excel at textual reasoning and are beginning to\ndevelop spatial understanding, prompting the question of whether these\nabilities can be combined for complex, domain-specific tasks. This question is\nessential in fields like materials science, where deep understanding of 3D\natomic structures is fundamental. While initial studies have successfully\napplied LLMs to tasks involving pure crystal generation or coordinate\nunderstandings, a standardized benchmark to systematically evaluate their core\nreasoning abilities across diverse atomic structures has been notably absent.\nTo address this gap, we introduce the AtomWorld benchmark to evaluate LLMs on\ntasks based in Crystallographic Information Files (CIFs), a standard structure\nrepresentation format. These tasks, including structural editing, CIF\nperception, and property-guided modeling, reveal a critical limitation: current\nmodels, despite establishing promising baselines, consistently fail in\nstructural understanding and spatial reasoning. Our experiments show that these\nmodels make frequent errors on structure modification tasks, and even in the\nbasic CIF format understandings, potentially leading to cumulative errors in\nsubsequent analysis and materials insights. By defining these standardized\ntasks, AtomWorld lays the ground for advancing LLMs toward robust atomic-scale\nmodeling, crucial for accelerating materials research and automating scientific\nworkflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at textual reasoning and are beginning to\ndevelop spatial understanding, prompting the question of whether these\nabilities can be combined for complex, domain-specific tasks. This question is\nessential in fields like materials science, where deep understanding of 3D\natomic structures is fundamental. While initial studies have successfully\napplied LLMs to tasks involving pure crystal generation or coordinate\nunderstandings, a standardized benchmark to systematically evaluate their core\nreasoning abilities across diverse atomic structures has been notably absent.\nTo address this gap, we introduce the AtomWorld benchmark to evaluate LLMs on\ntasks based in Crystallographic Information Files (CIFs), a standard structure\nrepresentation format. These tasks, including structural editing, CIF\nperception, and property-guided modeling, reveal a critical limitation: current\nmodels, despite establishing promising baselines, consistently fail in\nstructural understanding and spatial reasoning. Our experiments show that these\nmodels make frequent errors on structure modification tasks, and even in the\nbasic CIF format understandings, potentially leading to cumulative errors in\nsubsequent analysis and materials insights. By defining these standardized\ntasks, AtomWorld lays the ground for advancing LLMs toward robust atomic-scale\nmodeling, crucial for accelerating materials research and automating scientific\nworkflows."
                },
                "authors": [
                    {
                        "name": "Taoyuze Lv"
                    },
                    {
                        "name": "Alexander Chen"
                    },
                    {
                        "name": "Fengyu Xie"
                    },
                    {
                        "name": "Chu Wu"
                    },
                    {
                        "name": "Jeffrey Meng"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Bram Hoex"
                    },
                    {
                        "name": "Zhicheng Zhong"
                    },
                    {
                        "name": "Tong Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tong Xie"
                },
                "author": "Tong Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04704v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04704v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13755v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13755v4",
                "updated": "2025-10-06T11:12:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    11,
                    12,
                    22,
                    0,
                    279,
                    0
                ],
                "published": "2025-08-19T11:51:40Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    51,
                    40,
                    1,
                    231,
                    0
                ],
                "title": "Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with\n  Adaptive Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with\n  Adaptive Exploration"
                },
                "summary": "Reinforcement Learning with Verifiable Reward (RLVR) has emerged as a\npowerful paradigm for unlocking reasoning capabilities in large language\nmodels, yet its full potential is hindered by two under-explored dimensions:\nDepth-the hardest problem a model can sample; Breadth-the number of instances\nconsumed in a single iteration. We dissect the popular GRPO algorithm and\nreveal a systematic bias: the cumulative-advantage disproportionately weights\nsamples with medium accuracy, while down-weighting the low-accuracy instances\nthat are crucial for pushing reasoning boundaries. To rectify the depth\nneglect, we introduce Difficulty Adaptive Rollout Sampling (DARS), which\nre-weights hard problems through targeted multi-stage rollouts, thereby\nincreasing the number of positive rollouts for hard problems. Empirically,\nnaively enlarging rollout size only accelerates convergence and even hurts\nPass@K. Our DARS, in contrast, delivers consistent Pass@K gains without extra\ninference cost at convergence. Just as we adaptively expanded the depth of\nexploration, we now ask whether aggressively scaling the breadth of training\ndata can further amplify reasoning gains. To this end, we intensely scale batch\nsize and replace PPO's mini-batch iterations with full-batch updates over\nmultiple epochs. Increasing breadth significantly enhances Pass@1 performance.\nLarge-breadth training sustains high token-level entropy, indicating continued\nexploration and reduced gradient noise. We further present DARS-B, which\naugments DARS with large breadth, and demonstrate simultaneous gains in Pass@K\nand Pass@1. The results confirm that breadth and adaptive exploration across\ndepth operate as orthogonal dimensions in RLVR, which are key to unleashing the\nreasoning power of RLVR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Reward (RLVR) has emerged as a\npowerful paradigm for unlocking reasoning capabilities in large language\nmodels, yet its full potential is hindered by two under-explored dimensions:\nDepth-the hardest problem a model can sample; Breadth-the number of instances\nconsumed in a single iteration. We dissect the popular GRPO algorithm and\nreveal a systematic bias: the cumulative-advantage disproportionately weights\nsamples with medium accuracy, while down-weighting the low-accuracy instances\nthat are crucial for pushing reasoning boundaries. To rectify the depth\nneglect, we introduce Difficulty Adaptive Rollout Sampling (DARS), which\nre-weights hard problems through targeted multi-stage rollouts, thereby\nincreasing the number of positive rollouts for hard problems. Empirically,\nnaively enlarging rollout size only accelerates convergence and even hurts\nPass@K. Our DARS, in contrast, delivers consistent Pass@K gains without extra\ninference cost at convergence. Just as we adaptively expanded the depth of\nexploration, we now ask whether aggressively scaling the breadth of training\ndata can further amplify reasoning gains. To this end, we intensely scale batch\nsize and replace PPO's mini-batch iterations with full-batch updates over\nmultiple epochs. Increasing breadth significantly enhances Pass@1 performance.\nLarge-breadth training sustains high token-level entropy, indicating continued\nexploration and reduced gradient noise. We further present DARS-B, which\naugments DARS with large breadth, and demonstrate simultaneous gains in Pass@K\nand Pass@1. The results confirm that breadth and adaptive exploration across\ndepth operate as orthogonal dimensions in RLVR, which are key to unleashing the\nreasoning power of RLVR."
                },
                "authors": [
                    {
                        "name": "Zhicheng Yang"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Yinya Huang"
                    },
                    {
                        "name": "Yongxin Wang"
                    },
                    {
                        "name": "Dongchun Xie"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Xiaodan Liang"
                    },
                    {
                        "name": "Jing Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Tang"
                },
                "author": "Jing Tang",
                "arxiv_comment": "18 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13755v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13755v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04695v1",
                "updated": "2025-10-06T11:09:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    11,
                    9,
                    45,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T11:09:45Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    11,
                    9,
                    45,
                    0,
                    279,
                    0
                ],
                "title": "Beyond Outcome Reward: Decoupling Search and Answering Improves LLM\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Outcome Reward: Decoupling Search and Answering Improves LLM\n  Agents"
                },
                "summary": "Enabling large language models (LLMs) to utilize search tools offers a\npromising path to overcoming fundamental limitations such as knowledge cutoffs\nand hallucinations. Recent work has explored reinforcement learning (RL) for\ntraining search-augmented agents that interleave reasoning and retrieval before\nanswering. These approaches usually rely on outcome-based rewards (e.g., exact\nmatch), implicitly assuming that optimizing for final answers will also yield\neffective intermediate search behaviors. Our analysis challenges this\nassumption: we uncover multiple systematic deficiencies in search that arise\nunder outcome-only training and ultimately degrade final answer quality,\nincluding failure to invoke tools, invalid queries, and redundant searches. To\naddress these shortcomings, we introduce DeSA (Decoupling\nSearch-and-Answering), a simple two-stage training framework that explicitly\nseparates search optimization from answer generation. In Stage 1, agents are\ntrained to improve search effectiveness with retrieval recall-based rewards. In\nStage 2, outcome rewards are employed to optimize final answer generation.\nAcross seven QA benchmarks, DeSA-trained agents consistently improve search\nbehaviors, delivering substantially higher search recall and answer accuracy\nthan outcome-only baselines. Notably, DeSA outperforms single-stage training\napproaches that simultaneously optimize recall and outcome rewards,\nunderscoring the necessity of explicitly decoupling the two objectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling large language models (LLMs) to utilize search tools offers a\npromising path to overcoming fundamental limitations such as knowledge cutoffs\nand hallucinations. Recent work has explored reinforcement learning (RL) for\ntraining search-augmented agents that interleave reasoning and retrieval before\nanswering. These approaches usually rely on outcome-based rewards (e.g., exact\nmatch), implicitly assuming that optimizing for final answers will also yield\neffective intermediate search behaviors. Our analysis challenges this\nassumption: we uncover multiple systematic deficiencies in search that arise\nunder outcome-only training and ultimately degrade final answer quality,\nincluding failure to invoke tools, invalid queries, and redundant searches. To\naddress these shortcomings, we introduce DeSA (Decoupling\nSearch-and-Answering), a simple two-stage training framework that explicitly\nseparates search optimization from answer generation. In Stage 1, agents are\ntrained to improve search effectiveness with retrieval recall-based rewards. In\nStage 2, outcome rewards are employed to optimize final answer generation.\nAcross seven QA benchmarks, DeSA-trained agents consistently improve search\nbehaviors, delivering substantially higher search recall and answer accuracy\nthan outcome-only baselines. Notably, DeSA outperforms single-stage training\napproaches that simultaneously optimize recall and outcome rewards,\nunderscoring the necessity of explicitly decoupling the two objectives."
                },
                "authors": [
                    {
                        "name": "Yiding Wang"
                    },
                    {
                        "name": "Zhepei Wei"
                    },
                    {
                        "name": "Xinyu Zhu"
                    },
                    {
                        "name": "Yu Meng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Meng"
                },
                "author": "Yu Meng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04694v1",
                "updated": "2025-10-06T11:09:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    11,
                    9,
                    20,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T11:09:20Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    11,
                    9,
                    20,
                    0,
                    279,
                    0
                ],
                "title": "Multilingual Routing in Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Routing in Mixture-of-Experts"
                },
                "summary": "Mixture-of-Experts (MoE) architectures have become the key to scaling modern\nLLMs, yet little is understood about how their sparse routing dynamics respond\nto multilingual data. In this work, we analyze expert routing patterns using\nparallel multilingual datasets and present highly interpretable layer-wise\nphenomena. We find that MoE models route tokens in language-specific ways in\nthe early and late decoder layers but exhibit significant cross-lingual routing\nalignment in middle layers, mirroring parameter-sharing trends observed in\ndense LLMs. In particular, we reveal a clear, strong correlation between a\nmodel's performance in a given language and how similarly its tokens are routed\nto English in these layers. Extending beyond correlation, we explore\ninference-time interventions that induce higher cross-lingual routing\nalignment. We introduce a method that steers the router by promoting\nmiddle-layer task experts frequently activated in English, and it successfully\nincreases multilingual performance. These 1-2% gains are remarkably consistent\nacross two evaluation tasks, three models, and 15+ languages, especially given\nthat these simple interventions override routers of extensively trained,\nstate-of-the-art LLMs. In comparison, interventions outside of the middle\nlayers or targeting multilingual-specialized experts only yield performance\ndegradation. Altogether, we present numerous findings that explain how MoEs\nprocess non-English text and demonstrate that generalization is limited by the\nmodel's ability to leverage language-universal experts in all languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) architectures have become the key to scaling modern\nLLMs, yet little is understood about how their sparse routing dynamics respond\nto multilingual data. In this work, we analyze expert routing patterns using\nparallel multilingual datasets and present highly interpretable layer-wise\nphenomena. We find that MoE models route tokens in language-specific ways in\nthe early and late decoder layers but exhibit significant cross-lingual routing\nalignment in middle layers, mirroring parameter-sharing trends observed in\ndense LLMs. In particular, we reveal a clear, strong correlation between a\nmodel's performance in a given language and how similarly its tokens are routed\nto English in these layers. Extending beyond correlation, we explore\ninference-time interventions that induce higher cross-lingual routing\nalignment. We introduce a method that steers the router by promoting\nmiddle-layer task experts frequently activated in English, and it successfully\nincreases multilingual performance. These 1-2% gains are remarkably consistent\nacross two evaluation tasks, three models, and 15+ languages, especially given\nthat these simple interventions override routers of extensively trained,\nstate-of-the-art LLMs. In comparison, interventions outside of the middle\nlayers or targeting multilingual-specialized experts only yield performance\ndegradation. Altogether, we present numerous findings that explain how MoEs\nprocess non-English text and demonstrate that generalization is limited by the\nmodel's ability to leverage language-universal experts in all languages."
                },
                "authors": [
                    {
                        "name": "Lucas Bandarkar"
                    },
                    {
                        "name": "Chenyuan Yang"
                    },
                    {
                        "name": "Mohsen Fayyaz"
                    },
                    {
                        "name": "Junlin Hu"
                    },
                    {
                        "name": "Nanyun Peng"
                    }
                ],
                "author_detail": {
                    "name": "Nanyun Peng"
                },
                "author": "Nanyun Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04692v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04692v1",
                "updated": "2025-10-06T11:05:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    11,
                    5,
                    46,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T11:05:46Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    11,
                    5,
                    46,
                    0,
                    279,
                    0
                ],
                "title": "Bio-Inspired Robotic Houbara: From Development to Field Deployment for\n  Behavioral Studies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bio-Inspired Robotic Houbara: From Development to Field Deployment for\n  Behavioral Studies"
                },
                "summary": "Biomimetic intelligence and robotics are transforming field ecology by\nenabling lifelike robotic surrogates that interact naturally with animals under\nreal world conditions. Studying avian behavior in the wild remains challenging\ndue to the need for highly realistic morphology, durable outdoor operation, and\nintelligent perception that can adapt to uncontrolled environments. We present\na next generation bio inspired robotic platform that replicates the morphology\nand visual appearance of the female Houbara bustard to support controlled\nethological studies and conservation oriented field research. The system\nintroduces a fully digitally replicable fabrication workflow that combines high\nresolution structured light 3D scanning, parametric CAD modelling, articulated\n3D printing, and photorealistic UV textured vinyl finishing to achieve\nanatomically accurate and durable robotic surrogates. A six wheeled rocker\nbogie chassis ensures stable mobility on sand and irregular terrain, while an\nembedded NVIDIA Jetson module enables real time RGB and thermal perception,\nlightweight YOLO based detection, and an autonomous visual servoing loop that\naligns the robot's head toward detected targets without human intervention. A\nlightweight thermal visible fusion module enhances perception in low light\nconditions. Field trials in desert aviaries demonstrated reliable real time\noperation at 15 to 22 FPS with latency under 100 ms and confirmed that the\nplatform elicits natural recognition and interactive responses from live\nHoubara bustards under harsh outdoor conditions. This integrated framework\nadvances biomimetic field robotics by uniting reproducible digital fabrication,\nembodied visual intelligence, and ecological validation, providing a\ntransferable blueprint for animal robot interaction research, conservation\nrobotics, and public engagement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biomimetic intelligence and robotics are transforming field ecology by\nenabling lifelike robotic surrogates that interact naturally with animals under\nreal world conditions. Studying avian behavior in the wild remains challenging\ndue to the need for highly realistic morphology, durable outdoor operation, and\nintelligent perception that can adapt to uncontrolled environments. We present\na next generation bio inspired robotic platform that replicates the morphology\nand visual appearance of the female Houbara bustard to support controlled\nethological studies and conservation oriented field research. The system\nintroduces a fully digitally replicable fabrication workflow that combines high\nresolution structured light 3D scanning, parametric CAD modelling, articulated\n3D printing, and photorealistic UV textured vinyl finishing to achieve\nanatomically accurate and durable robotic surrogates. A six wheeled rocker\nbogie chassis ensures stable mobility on sand and irregular terrain, while an\nembedded NVIDIA Jetson module enables real time RGB and thermal perception,\nlightweight YOLO based detection, and an autonomous visual servoing loop that\naligns the robot's head toward detected targets without human intervention. A\nlightweight thermal visible fusion module enhances perception in low light\nconditions. Field trials in desert aviaries demonstrated reliable real time\noperation at 15 to 22 FPS with latency under 100 ms and confirmed that the\nplatform elicits natural recognition and interactive responses from live\nHoubara bustards under harsh outdoor conditions. This integrated framework\nadvances biomimetic field robotics by uniting reproducible digital fabrication,\nembodied visual intelligence, and ecological validation, providing a\ntransferable blueprint for animal robot interaction research, conservation\nrobotics, and public engagement."
                },
                "authors": [
                    {
                        "name": "Lyes Saad Saoud"
                    },
                    {
                        "name": "Irfan Hussain"
                    }
                ],
                "author_detail": {
                    "name": "Irfan Hussain"
                },
                "author": "Irfan Hussain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04692v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19645v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19645v3",
                "updated": "2025-10-06T10:53:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    10,
                    53,
                    42,
                    0,
                    279,
                    0
                ],
                "published": "2025-05-26T08:01:45Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    8,
                    1,
                    45,
                    0,
                    146,
                    0
                ],
                "title": "MoESD: Unveil Speculative Decoding's Potential for Accelerating Sparse\n  MoE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoESD: Unveil Speculative Decoding's Potential for Accelerating Sparse\n  MoE"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across many\napplications, with Mixture of Experts (MoE) models demonstrating great\npotential. Compared to traditional dense models, MoEs achieve better\nperformance with less computation. Speculative decoding (SD) is a widely used\ntechnique to accelerate LLM inference without accuracy loss, but it has been\nconsidered efficient only for dense models. In this work, we first demonstrate\nthat, under medium batch sizes, MoE surprisingly benefits more from SD than\ndense models. Furthermore, as MoE becomes sparser -- the prevailing trend in\nMoE designs -- the batch size range where SD acceleration is expected to be\neffective becomes broader. To quantitatively understand tradeoffs involved in\nSD, we develop a reliable modeling based on theoretical analyses. While current\nSD research primarily focuses on improving acceptance rates of algorithms,\nchanges in workload and model architecture can still lead to degraded SD\nacceleration even with high acceptance rates. To address this limitation, we\nintroduce a new metric 'target efficiency' that characterizes these effects,\nthus helping researchers identify system bottlenecks and understand SD\nacceleration more comprehensively. For scenarios like private serving, this\nwork unveils a new perspective to speed up MoE inference, where existing\nsolutions struggle. Experiments on different GPUs show up to 2.29x speedup for\nQwen2-57B-A14B at medium batch sizes and validate our theoretical predictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across many\napplications, with Mixture of Experts (MoE) models demonstrating great\npotential. Compared to traditional dense models, MoEs achieve better\nperformance with less computation. Speculative decoding (SD) is a widely used\ntechnique to accelerate LLM inference without accuracy loss, but it has been\nconsidered efficient only for dense models. In this work, we first demonstrate\nthat, under medium batch sizes, MoE surprisingly benefits more from SD than\ndense models. Furthermore, as MoE becomes sparser -- the prevailing trend in\nMoE designs -- the batch size range where SD acceleration is expected to be\neffective becomes broader. To quantitatively understand tradeoffs involved in\nSD, we develop a reliable modeling based on theoretical analyses. While current\nSD research primarily focuses on improving acceptance rates of algorithms,\nchanges in workload and model architecture can still lead to degraded SD\nacceleration even with high acceptance rates. To address this limitation, we\nintroduce a new metric 'target efficiency' that characterizes these effects,\nthus helping researchers identify system bottlenecks and understand SD\nacceleration more comprehensively. For scenarios like private serving, this\nwork unveils a new perspective to speed up MoE inference, where existing\nsolutions struggle. Experiments on different GPUs show up to 2.29x speedup for\nQwen2-57B-A14B at medium batch sizes and validate our theoretical predictions."
                },
                "authors": [
                    {
                        "name": "Zongle Huang"
                    },
                    {
                        "name": "Lei Zhu"
                    },
                    {
                        "name": "Zongyuan Zhan"
                    },
                    {
                        "name": "Ting Hu"
                    },
                    {
                        "name": "Weikai Mao"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Yongpan Liu"
                    },
                    {
                        "name": "Tianyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tianyu Zhang"
                },
                "author": "Tianyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19645v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19645v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04682v1",
                "updated": "2025-10-06T10:47:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    10,
                    47,
                    22,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T10:47:22Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    10,
                    47,
                    22,
                    0,
                    279,
                    0
                ],
                "title": "TiTok: Transfer Token-level Knowledge via Contrastive Excess to\n  Transplant LoRA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TiTok: Transfer Token-level Knowledge via Contrastive Excess to\n  Transplant LoRA"
                },
                "summary": "Large Language Models (LLMs) are widely applied in real world scenarios, but\nfine-tuning them comes with significant computational and storage costs.\nParameter-Efficient Fine-Tuning (PEFT) methods such as LoRA mitigate these\ncosts, but the adapted parameters are dependent on the base model and cannot be\ntransferred across different backbones. One way to address this issue is\nthrough knowledge distillation, but its effectiveness inherently depends on\ntraining data. Recent work such as TransLoRA avoids this by generating\nsynthetic data, but this adds complexity because it requires training an\nadditional discriminator model. In this paper, we propose TiTok, a new\nframework that enables effective LoRA Transplantation through Token-level\nknowledge transfer. Specifically, TiTok captures task-relevant information\nthrough a contrastive excess between a source model with and without LoRA. This\nexcess highlights informative tokens and enables selective filtering of\nsynthetic data, all without additional models or overhead. Through experiments\non three benchmarks across multiple transfer settings, our experiments show\nthat the proposed method is consistently effective, achieving average\nperformance gains of +4~8% compared to baselines overall.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely applied in real world scenarios, but\nfine-tuning them comes with significant computational and storage costs.\nParameter-Efficient Fine-Tuning (PEFT) methods such as LoRA mitigate these\ncosts, but the adapted parameters are dependent on the base model and cannot be\ntransferred across different backbones. One way to address this issue is\nthrough knowledge distillation, but its effectiveness inherently depends on\ntraining data. Recent work such as TransLoRA avoids this by generating\nsynthetic data, but this adds complexity because it requires training an\nadditional discriminator model. In this paper, we propose TiTok, a new\nframework that enables effective LoRA Transplantation through Token-level\nknowledge transfer. Specifically, TiTok captures task-relevant information\nthrough a contrastive excess between a source model with and without LoRA. This\nexcess highlights informative tokens and enables selective filtering of\nsynthetic data, all without additional models or overhead. Through experiments\non three benchmarks across multiple transfer settings, our experiments show\nthat the proposed method is consistently effective, achieving average\nperformance gains of +4~8% compared to baselines overall."
                },
                "authors": [
                    {
                        "name": "Chanjoo Jung"
                    },
                    {
                        "name": "Jaehyung Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jaehyung Kim"
                },
                "author": "Jaehyung Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04678v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04678v1",
                "updated": "2025-10-06T10:44:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    10,
                    44,
                    4,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T10:44:04Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    10,
                    44,
                    4,
                    0,
                    279,
                    0
                ],
                "title": "Multi-Agent Tool-Integrated Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Tool-Integrated Policy Optimization"
                },
                "summary": "Large language models (LLMs) increasingly rely on multi-turn tool-integrated\nplanning for knowledge-intensive and complex reasoning tasks. Existing\nimplementations typically rely on a single agent, but they suffer from limited\ncontext length and noisy tool responses. A natural solution is to adopt a\nmulti-agent framework with planner- and worker-agents to manage context.\nHowever, no existing methods support effective reinforcement learning\npost-training of tool-integrated multi-agent frameworks. To address this gap,\nwe propose Multi-Agent Tool-Integrated Policy Optimization (MATPO), which\nenables distinct roles (planner and worker) to be trained within a single LLM\ninstance using role-specific prompts via reinforcement learning. MATPO is\nderived from a principled credit assignment mechanism across planner and worker\nrollouts. This design eliminates the need to deploy multiple LLMs, which would\nbe memory-intensive, while preserving the benefits of specialization.\nExperiments on GAIA-text, WebWalkerQA, and FRAMES show that MATPO consistently\noutperforms single-agent baselines by an average of 18.38% relative improvement\nin performance and exhibits greater robustness to noisy tool outputs. Our\nfindings highlight the effectiveness of unifying multiple agent roles within a\nsingle LLM and provide practical insights for stable and efficient multi-agent\nRL training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) increasingly rely on multi-turn tool-integrated\nplanning for knowledge-intensive and complex reasoning tasks. Existing\nimplementations typically rely on a single agent, but they suffer from limited\ncontext length and noisy tool responses. A natural solution is to adopt a\nmulti-agent framework with planner- and worker-agents to manage context.\nHowever, no existing methods support effective reinforcement learning\npost-training of tool-integrated multi-agent frameworks. To address this gap,\nwe propose Multi-Agent Tool-Integrated Policy Optimization (MATPO), which\nenables distinct roles (planner and worker) to be trained within a single LLM\ninstance using role-specific prompts via reinforcement learning. MATPO is\nderived from a principled credit assignment mechanism across planner and worker\nrollouts. This design eliminates the need to deploy multiple LLMs, which would\nbe memory-intensive, while preserving the benefits of specialization.\nExperiments on GAIA-text, WebWalkerQA, and FRAMES show that MATPO consistently\noutperforms single-agent baselines by an average of 18.38% relative improvement\nin performance and exhibits greater robustness to noisy tool outputs. Our\nfindings highlight the effectiveness of unifying multiple agent roles within a\nsingle LLM and provide practical insights for stable and efficient multi-agent\nRL training."
                },
                "authors": [
                    {
                        "name": "Zhanfeng Mo"
                    },
                    {
                        "name": "Xingxuan Li"
                    },
                    {
                        "name": "Yuntao Chen"
                    },
                    {
                        "name": "Lidong Bing"
                    }
                ],
                "author_detail": {
                    "name": "Lidong Bing"
                },
                "author": "Lidong Bing",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04678v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02089v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02089v4",
                "updated": "2025-10-06T10:38:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    10,
                    38,
                    59,
                    0,
                    279,
                    0
                ],
                "published": "2025-06-02T13:59:08Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    13,
                    59,
                    8,
                    0,
                    153,
                    0
                ],
                "title": "SALAD: Systematic Assessment of Machine Unlearning on LLM-Aided Hardware\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALAD: Systematic Assessment of Machine Unlearning on LLM-Aided Hardware\n  Design"
                },
                "summary": "Large Language Models (LLMs) offer transformative capabilities for hardware\ndesign automation, particularly in Verilog code generation. However, they also\npose significant data security challenges, including Verilog evaluation data\ncontamination, intellectual property (IP) design leakage, and the risk of\nmalicious Verilog generation. We introduce SALAD, a comprehensive assessment\nthat leverages machine unlearning to mitigate these threats. Our approach\nenables the selective removal of contaminated benchmarks, sensitive IP and\ndesign artifacts, or malicious code patterns from pre-trained LLMs, all without\nrequiring full retraining. Through detailed case studies, we demonstrate how\nmachine unlearning techniques effectively reduce data security risks in\nLLM-aided hardware design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) offer transformative capabilities for hardware\ndesign automation, particularly in Verilog code generation. However, they also\npose significant data security challenges, including Verilog evaluation data\ncontamination, intellectual property (IP) design leakage, and the risk of\nmalicious Verilog generation. We introduce SALAD, a comprehensive assessment\nthat leverages machine unlearning to mitigate these threats. Our approach\nenables the selective removal of contaminated benchmarks, sensitive IP and\ndesign artifacts, or malicious code patterns from pre-trained LLMs, all without\nrequiring full retraining. Through detailed case studies, we demonstrate how\nmachine unlearning techniques effectively reduce data security risks in\nLLM-aided hardware design."
                },
                "authors": [
                    {
                        "name": "Zeng Wang"
                    },
                    {
                        "name": "Minghao Shao"
                    },
                    {
                        "name": "Rupesh Karn"
                    },
                    {
                        "name": "Likhitha Mankali"
                    },
                    {
                        "name": "Jitendra Bhandari"
                    },
                    {
                        "name": "Ramesh Karri"
                    },
                    {
                        "name": "Ozgur Sinanoglu"
                    },
                    {
                        "name": "Muhammad Shafique"
                    },
                    {
                        "name": "Johann Knechtel"
                    }
                ],
                "author_detail": {
                    "name": "Johann Knechtel"
                },
                "author": "Johann Knechtel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02089v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02089v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04674v1",
                "updated": "2025-10-06T10:29:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    10,
                    29,
                    7,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T10:29:07Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    10,
                    29,
                    7,
                    0,
                    279,
                    0
                ],
                "title": "Semantic Channel Equalization Strategies for Deep Joint Source-Channel\n  Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Channel Equalization Strategies for Deep Joint Source-Channel\n  Coding"
                },
                "summary": "Deep joint source-channel coding (DeepJSCC) has emerged as a powerful\nparadigm for end-to-end semantic communications, jointly learning to compress\nand protect task-relevant features over noisy channels. However, existing\nDeepJSCC schemes assume a shared latent space at transmitter (TX) and receiver\n(RX) - an assumption that fails in multi-vendor deployments where encoders and\ndecoders cannot be co-trained. This mismatch introduces \"semantic noise\",\ndegrading reconstruction quality and downstream task performance. In this\npaper, we systematize and evaluate methods for semantic channel equalization\nfor DeepJSCC, introducing an additional processing stage that aligns\nheterogeneous latent spaces under both physical and semantic impairments. We\ninvestigate three classes of aligners: (i) linear maps, which admit closed-form\nsolutions; (ii) lightweight neural networks, offering greater expressiveness;\nand (iii) a Parseval-frame equalizer, which operates in zero-shot mode without\nthe need for training. Through extensive experiments on image reconstruction\nover AWGN and fading channels, we quantify trade-offs among complexity, data\nefficiency, and fidelity, providing guidelines for deploying DeepJSCC in\nheterogeneous AI-native wireless networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep joint source-channel coding (DeepJSCC) has emerged as a powerful\nparadigm for end-to-end semantic communications, jointly learning to compress\nand protect task-relevant features over noisy channels. However, existing\nDeepJSCC schemes assume a shared latent space at transmitter (TX) and receiver\n(RX) - an assumption that fails in multi-vendor deployments where encoders and\ndecoders cannot be co-trained. This mismatch introduces \"semantic noise\",\ndegrading reconstruction quality and downstream task performance. In this\npaper, we systematize and evaluate methods for semantic channel equalization\nfor DeepJSCC, introducing an additional processing stage that aligns\nheterogeneous latent spaces under both physical and semantic impairments. We\ninvestigate three classes of aligners: (i) linear maps, which admit closed-form\nsolutions; (ii) lightweight neural networks, offering greater expressiveness;\nand (iii) a Parseval-frame equalizer, which operates in zero-shot mode without\nthe need for training. Through extensive experiments on image reconstruction\nover AWGN and fading channels, we quantify trade-offs among complexity, data\nefficiency, and fidelity, providing guidelines for deploying DeepJSCC in\nheterogeneous AI-native wireless networks."
                },
                "authors": [
                    {
                        "name": "Lorenzo Pannacci"
                    },
                    {
                        "name": "Simone Fiorellino"
                    },
                    {
                        "name": "Mario Edoardo Pandolfo"
                    },
                    {
                        "name": "Emilio Calvanese Strinati"
                    },
                    {
                        "name": "Paolo Di Lorenzo"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Di Lorenzo"
                },
                "author": "Paolo Di Lorenzo",
                "arxiv_comment": "Proceedings of IEEE Globecom 2025 Workshops",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]