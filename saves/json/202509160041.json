[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.10372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10372v1",
                "updated": "2025-09-12T16:05:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    5,
                    27,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T16:05:27Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    5,
                    27,
                    4,
                    255,
                    0
                ],
                "title": "MCBP: A Memory-Compute Efficient LLM Inference Accelerator Leveraging\n  Bit-Slice-enabled Sparsity and Repetitiveness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCBP: A Memory-Compute Efficient LLM Inference Accelerator Leveraging\n  Bit-Slice-enabled Sparsity and Repetitiveness"
                },
                "summary": "Large language models (LLMs) face significant inference latency due to\ninefficiencies in GEMM operations, weight access, and KV cache access,\nespecially in real-time scenarios. This highlights the need for a versatile\ncompute-memory efficient accelerator. Unfortunately, existing Transformer\naccelerators struggle to address both aspects simultaneously, as they focus on\nvalue-level processing, missing fine-grained opportunities to optimize\ncomputation and memory collaboratively. This paper introduces MCBP, a\nbit-grained compute-memory efficient algorithm-hardware co-design that\nleverages bit-slice (BS) enabled repetitiveness and sparsity to accelerate LLM\ninference. MCBP features three key innovations: 1) BS-repetitiveness-enabled\ncomputation reduction (BRCR), which eliminates redundant GEMM computations via\nleveraging redundancy hidden among BS vectors; 2) BS-sparsity-enabled two-state\ncoding (BSTC), which reduces weight access via exploiting significant sparsity\nin high-order bit-slice weight; 3) Bit-grained progressive prediction (BGPP),\nwhich reduces KV cache access by leveraging early-termination-based bit-grained\nprediction. These techniques, supported by custom accelerator designs,\neffectively alleviate the burden in GEMM, weight access, and KV cache access.\nExtensive experiments on 26 benchmarks show that MCBP achieves 9.43x speed up\nand 31.1x higher energy efficiency than Nvidia A100 GPU. Compared to SOTA\nTransformer accelerators, MCBP achieves 35x, 5.2x and 3.2x energy saving than\nSpatten, FACT and SOFA, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face significant inference latency due to\ninefficiencies in GEMM operations, weight access, and KV cache access,\nespecially in real-time scenarios. This highlights the need for a versatile\ncompute-memory efficient accelerator. Unfortunately, existing Transformer\naccelerators struggle to address both aspects simultaneously, as they focus on\nvalue-level processing, missing fine-grained opportunities to optimize\ncomputation and memory collaboratively. This paper introduces MCBP, a\nbit-grained compute-memory efficient algorithm-hardware co-design that\nleverages bit-slice (BS) enabled repetitiveness and sparsity to accelerate LLM\ninference. MCBP features three key innovations: 1) BS-repetitiveness-enabled\ncomputation reduction (BRCR), which eliminates redundant GEMM computations via\nleveraging redundancy hidden among BS vectors; 2) BS-sparsity-enabled two-state\ncoding (BSTC), which reduces weight access via exploiting significant sparsity\nin high-order bit-slice weight; 3) Bit-grained progressive prediction (BGPP),\nwhich reduces KV cache access by leveraging early-termination-based bit-grained\nprediction. These techniques, supported by custom accelerator designs,\neffectively alleviate the burden in GEMM, weight access, and KV cache access.\nExtensive experiments on 26 benchmarks show that MCBP achieves 9.43x speed up\nand 31.1x higher energy efficiency than Nvidia A100 GPU. Compared to SOTA\nTransformer accelerators, MCBP achieves 35x, 5.2x and 3.2x energy saving than\nSpatten, FACT and SOFA, respectively."
                },
                "authors": [
                    {
                        "name": "Huizheng Wang"
                    },
                    {
                        "name": "Zichuan Wang"
                    },
                    {
                        "name": "Zhiheng Yue"
                    },
                    {
                        "name": "Yousheng Long"
                    },
                    {
                        "name": "Taiquan Wei"
                    },
                    {
                        "name": "Jianxun Yang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Shaojun Wei"
                    },
                    {
                        "name": "Yang Hu"
                    },
                    {
                        "name": "Shouyi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Shouyi Yin"
                },
                "author": "Shouyi Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10312v1",
                "updated": "2025-09-12T14:53:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    53,
                    45,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T14:53:45Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    53,
                    45,
                    4,
                    255,
                    0
                ],
                "title": "Compute Only 16 Tokens in One Timestep: Accelerating Diffusion\n  Transformers with Cluster-Driven Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Only 16 Tokens in One Timestep: Accelerating Diffusion\n  Transformers with Cluster-Driven Feature Caching"
                },
                "summary": "Diffusion transformers have gained significant attention in recent years for\ntheir ability to generate high-quality images and videos, yet still suffer from\na huge computational cost due to their iterative denoising process. Recently,\nfeature caching has been introduced to accelerate diffusion transformers by\ncaching the feature computation in previous timesteps and reusing it in the\nfollowing timesteps, which leverage the temporal similarity of diffusion models\nwhile ignoring the similarity in the spatial dimension. In this paper, we\nintroduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and\ncomplementary perspective for previous feature caching. Specifically, ClusCa\nperforms spatial clustering on tokens in each timestep, computes only one token\nin each cluster and propagates their information to all the other tokens, which\nis able to reduce the number of tokens by over 90%. Extensive experiments on\nDiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image\nand text-to-video generation. Besides, it can be directly applied to any\ndiffusion transformer without requirements for training. For instance, ClusCa\nachieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing\nthe original model by 0.51%. The code is available at\nhttps://github.com/Shenyi-Z/Cache4Diffusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have gained significant attention in recent years for\ntheir ability to generate high-quality images and videos, yet still suffer from\na huge computational cost due to their iterative denoising process. Recently,\nfeature caching has been introduced to accelerate diffusion transformers by\ncaching the feature computation in previous timesteps and reusing it in the\nfollowing timesteps, which leverage the temporal similarity of diffusion models\nwhile ignoring the similarity in the spatial dimension. In this paper, we\nintroduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and\ncomplementary perspective for previous feature caching. Specifically, ClusCa\nperforms spatial clustering on tokens in each timestep, computes only one token\nin each cluster and propagates their information to all the other tokens, which\nis able to reduce the number of tokens by over 90%. Extensive experiments on\nDiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image\nand text-to-video generation. Besides, it can be directly applied to any\ndiffusion transformer without requirements for training. For instance, ClusCa\nachieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing\nthe original model by 0.51%. The code is available at\nhttps://github.com/Shenyi-Z/Cache4Diffusion."
                },
                "authors": [
                    {
                        "name": "Zhixin Zheng"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Shaobo Wang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "11 pages, 11 figures; Accepted by ACM MM2025; Mainly focus on feature\n  caching for diffusion transformers acceleration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10251v1",
                "updated": "2025-09-12T13:49:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    49,
                    27,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T13:49:27Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    49,
                    27,
                    4,
                    255,
                    0
                ],
                "title": "XBOF: A Cost-Efficient CXL JBOF with Inter-SSD Compute Resource Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XBOF: A Cost-Efficient CXL JBOF with Inter-SSD Compute Resource Sharing"
                },
                "summary": "Enterprise SSDs integrate numerous computing resources (e.g., ARM processor\nand onboard DRAM) to satisfy the ever-increasing performance requirements of\nI/O bursts. While these resources substantially elevate the monetary costs of\nSSDs, the sporadic nature of I/O bursts causes severe SSD resource\nunderutilization in just a bunch of flash (JBOF) level. Tackling this\nchallenge, we propose XBOF, a cost-efficient JBOF design, which only reserves\nmoderate computing resources in SSDs at low monetary cost, while achieving\ndemanded I/O performance through efficient inter-SSD resource sharing.\nSpecifically, XBOF first disaggregates SSD architecture into multiple disjoint\nparts based on their functionality, enabling fine-grained SSD internal resource\nmanagement. XBOF then employs a decentralized scheme to manage these\ndisaggregated resources and harvests the computing resources of idle SSDs to\nassist busy SSDs in handling I/O bursts. This idea is facilitated by the\ncache-coherent capability of Compute eXpress Link (CXL), with which the busy\nSSDs can directly utilize the harvested computing resources to accelerate\nmetadata processing. The evaluation results show that XBOF improves SSD\nresource utilization by 50.4% and saves 19.0% monetary costs with a negligible\nperformance loss, compared to existing JBOF designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enterprise SSDs integrate numerous computing resources (e.g., ARM processor\nand onboard DRAM) to satisfy the ever-increasing performance requirements of\nI/O bursts. While these resources substantially elevate the monetary costs of\nSSDs, the sporadic nature of I/O bursts causes severe SSD resource\nunderutilization in just a bunch of flash (JBOF) level. Tackling this\nchallenge, we propose XBOF, a cost-efficient JBOF design, which only reserves\nmoderate computing resources in SSDs at low monetary cost, while achieving\ndemanded I/O performance through efficient inter-SSD resource sharing.\nSpecifically, XBOF first disaggregates SSD architecture into multiple disjoint\nparts based on their functionality, enabling fine-grained SSD internal resource\nmanagement. XBOF then employs a decentralized scheme to manage these\ndisaggregated resources and harvests the computing resources of idle SSDs to\nassist busy SSDs in handling I/O bursts. This idea is facilitated by the\ncache-coherent capability of Compute eXpress Link (CXL), with which the busy\nSSDs can directly utilize the harvested computing resources to accelerate\nmetadata processing. The evaluation results show that XBOF improves SSD\nresource utilization by 50.4% and saves 19.0% monetary costs with a negligible\nperformance loss, compared to existing JBOF designs."
                },
                "authors": [
                    {
                        "name": "Shushu Yi"
                    },
                    {
                        "name": "Yuda An"
                    },
                    {
                        "name": "Li Peng"
                    },
                    {
                        "name": "Xiurui Pan"
                    },
                    {
                        "name": "Qiao Li"
                    },
                    {
                        "name": "Jieming Yin"
                    },
                    {
                        "name": "Guangyan Zhang"
                    },
                    {
                        "name": "Wenfei Wu"
                    },
                    {
                        "name": "Diyu Zhou"
                    },
                    {
                        "name": "Zhenlin Wang"
                    },
                    {
                        "name": "Xiaolin Wang"
                    },
                    {
                        "name": "Yingwei Luo"
                    },
                    {
                        "name": "Ke Zhou"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10016v1",
                "updated": "2025-09-12T07:20:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    20,
                    53,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T07:20:53Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    20,
                    53,
                    4,
                    255,
                    0
                ],
                "title": "SvalMIZ-25 Svalbard Marginal Ice Zone Campaign 2025 -- Cruise Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SvalMIZ-25 Svalbard Marginal Ice Zone Campaign 2025 -- Cruise Report"
                },
                "summary": "The coupling of weather, sea-ice, ocean, and wave forecasting systems has\nbeen a long-standing research focus to improve Arctic forecasting systems and\ntheir realism and is also a priority of international initiatives such as the\nWMO research project PCAPS. The goal of the Svalbard Marginal Ice Zone 2025\nCampaign (SvalMIZ-25) was to observe and better understand the complex\ninterplay between atmosphere, waves, and sea-ice in the winter Marginal Ice\nZone (MIZ) in order to advance predictive skill of coupled Arctic forecasting\nsystems. The main objective has been to set up a network of observations with a\nspatial distribution that allows for a representative comparison between in\nsitu observations and gridded model data. The observed variables include air\nand surface temperature, sea-ice drift, and wave energy spectra. With the\nsupport of the Norwegian Coast Guard, we participated in the research cruise\nwith KV Svalbard from 22.April - 11.May 2025. In total 21 buoys were deployed\nin the Marginal Ice Zone north of the Svalbard Archipelago.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The coupling of weather, sea-ice, ocean, and wave forecasting systems has\nbeen a long-standing research focus to improve Arctic forecasting systems and\ntheir realism and is also a priority of international initiatives such as the\nWMO research project PCAPS. The goal of the Svalbard Marginal Ice Zone 2025\nCampaign (SvalMIZ-25) was to observe and better understand the complex\ninterplay between atmosphere, waves, and sea-ice in the winter Marginal Ice\nZone (MIZ) in order to advance predictive skill of coupled Arctic forecasting\nsystems. The main objective has been to set up a network of observations with a\nspatial distribution that allows for a representative comparison between in\nsitu observations and gridded model data. The observed variables include air\nand surface temperature, sea-ice drift, and wave energy spectra. With the\nsupport of the Norwegian Coast Guard, we participated in the research cruise\nwith KV Svalbard from 22.April - 11.May 2025. In total 21 buoys were deployed\nin the Marginal Ice Zone north of the Svalbard Archipelago."
                },
                "authors": [
                    {
                        "name": "M. Müller"
                    },
                    {
                        "name": "J. Rabault"
                    },
                    {
                        "name": "C. Palerme"
                    },
                    {
                        "name": "J. Tjernström"
                    }
                ],
                "author_detail": {
                    "name": "J. Tjernström"
                },
                "author": "J. Tjernström",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09754v1",
                "updated": "2025-09-11T16:48:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    48,
                    24,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T16:48:24Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    48,
                    24,
                    3,
                    254,
                    0
                ],
                "title": "LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation"
                },
                "summary": "KV Cache is commonly used to accelerate LLM inference with long contexts, yet\nits high memory demand drives the need for cache compression. Existing\ncompression methods, however, are largely heuristic and lack dynamic budget\nallocation. To address this limitation, we introduce a unified framework for\ncache compression by minimizing information loss in Transformer residual\nstreams. Building on it, we analyze the layer attention output loss and derive\na new metric to compare cache entries across heads, enabling layer-wise\ncompression with dynamic head budgets. Additionally, by contrasting cross-layer\ninformation, we also achieve dynamic layer budgets. LAVa is the first unified\nstrategy for cache eviction and dynamic budget allocation that, unlike prior\nmethods, does not rely on training or the combination of multiple strategies.\nExperiments with benchmarks (LongBench, Needle-In-A-Haystack, Ruler, and\nInfiniteBench) demonstrate its superiority. Moreover, our experiments reveal a\nnew insight: dynamic layer budgets are crucial for generation tasks (e.g., code\ncompletion), while dynamic head budgets play a key role in extraction tasks\n(e.g., extractive QA). As a fully dynamic compression method, LAVa consistently\nmaintains top performance across task types. Our code is available at\nhttps://github.com/MGDDestiny/Lava.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache is commonly used to accelerate LLM inference with long contexts, yet\nits high memory demand drives the need for cache compression. Existing\ncompression methods, however, are largely heuristic and lack dynamic budget\nallocation. To address this limitation, we introduce a unified framework for\ncache compression by minimizing information loss in Transformer residual\nstreams. Building on it, we analyze the layer attention output loss and derive\na new metric to compare cache entries across heads, enabling layer-wise\ncompression with dynamic head budgets. Additionally, by contrasting cross-layer\ninformation, we also achieve dynamic layer budgets. LAVa is the first unified\nstrategy for cache eviction and dynamic budget allocation that, unlike prior\nmethods, does not rely on training or the combination of multiple strategies.\nExperiments with benchmarks (LongBench, Needle-In-A-Haystack, Ruler, and\nInfiniteBench) demonstrate its superiority. Moreover, our experiments reveal a\nnew insight: dynamic layer budgets are crucial for generation tasks (e.g., code\ncompletion), while dynamic head budgets play a key role in extraction tasks\n(e.g., extractive QA). As a fully dynamic compression method, LAVa consistently\nmaintains top performance across task types. Our code is available at\nhttps://github.com/MGDDestiny/Lava."
                },
                "authors": [
                    {
                        "name": "Yiqun Shen"
                    },
                    {
                        "name": "Song Yuan"
                    },
                    {
                        "name": "Zhengze Zhang"
                    },
                    {
                        "name": "Xiaoliang Wang"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Nguyen Cam-Tu"
                    }
                ],
                "author_detail": {
                    "name": "Nguyen Cam-Tu"
                },
                "author": "Nguyen Cam-Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09525v1",
                "updated": "2025-09-11T15:06:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    6,
                    3,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T15:06:03Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    6,
                    3,
                    3,
                    254,
                    0
                ],
                "title": "TrEnv: Transparently Share Serverless Execution Environments Across\n  Different Functions and Nodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrEnv: Transparently Share Serverless Execution Environments Across\n  Different Functions and Nodes"
                },
                "summary": "Serverless computing provides dynamic scalability, but its infrastructure\noverhead becomes a bottleneck for emerging workloads such as LLM agents, which\nexhibit unpredictable invocation patterns and variable resource demands. Our\nanalysis shows that for these agents, the cost of running on serverless\nplatforms can reach up to 70% of the cost of LLM API calls. This finding\nmotivates the need for a more efficient, high-density serverless platform. We\npresent TrEnv, a co-designed serverless platform that supports both container-\nand VM-based environments, optimized for the unique demands of LLM agents.\nTrEnv reduces startup latency and memory usage through repurposable sandboxes\nand memory templates, which enable fast reuse and restoration of execution\nenvironments. To further reduce overhead in VM-based agent workloads, TrEnv\nleverages browser sharing and a page cache bypassing mechanism. Evaluations\nshow that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in\ncontainer-based settings, and achieves up to 58% lower P99 latency and 61%\nmemory savings for VM-based agents compared to state-of-the-art systems like\nE2B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing provides dynamic scalability, but its infrastructure\noverhead becomes a bottleneck for emerging workloads such as LLM agents, which\nexhibit unpredictable invocation patterns and variable resource demands. Our\nanalysis shows that for these agents, the cost of running on serverless\nplatforms can reach up to 70% of the cost of LLM API calls. This finding\nmotivates the need for a more efficient, high-density serverless platform. We\npresent TrEnv, a co-designed serverless platform that supports both container-\nand VM-based environments, optimized for the unique demands of LLM agents.\nTrEnv reduces startup latency and memory usage through repurposable sandboxes\nand memory templates, which enable fast reuse and restoration of execution\nenvironments. To further reduce overhead in VM-based agent workloads, TrEnv\nleverages browser sharing and a page cache bypassing mechanism. Evaluations\nshow that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in\ncontainer-based settings, and achieves up to 58% lower P99 latency and 61%\nmemory savings for VM-based agents compared to state-of-the-art systems like\nE2B."
                },
                "authors": [
                    {
                        "name": "Jialiang Huang"
                    },
                    {
                        "name": "Teng Ma"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Sixing Lin"
                    },
                    {
                        "name": "Kang Chen"
                    },
                    {
                        "name": "Jinlei Jiang"
                    },
                    {
                        "name": "Xia Liao"
                    },
                    {
                        "name": "Yingdi Shan"
                    },
                    {
                        "name": "Yongwei Wu"
                    },
                    {
                        "name": "Ning Zhang"
                    },
                    {
                        "name": "Mengting Lu"
                    },
                    {
                        "name": "Tao Ma"
                    },
                    {
                        "name": "Haifeng Gong"
                    },
                    {
                        "name": "Mingxing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mingxing Zhang"
                },
                "author": "Mingxing Zhang",
                "arxiv_comment": "38 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09494v1",
                "updated": "2025-09-11T14:34:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    34,
                    1,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T14:34:01Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    34,
                    1,
                    3,
                    254,
                    0
                ],
                "title": "In-Loop Filtering Using Learned Look-Up Tables for Video Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Loop Filtering Using Learned Look-Up Tables for Video Coding"
                },
                "summary": "In-loop filtering (ILF) is a key technology in video coding standards to\nreduce artifacts and enhance visual quality. Recently, neural network-based ILF\nschemes have achieved remarkable coding gains, emerging as a powerful candidate\nfor next-generation video coding standards. However, the use of deep neural\nnetworks (DNN) brings significant computational and time complexity or high\ndemands for dedicated hardware, making it challenging for general use. To\naddress this limitation, we study a practical ILF solution by adopting look-up\ntables (LUTs). After training a DNN with a restricted reference range for ILF,\nall possible inputs are traversed, and the output values of the DNN are cached\ninto LUTs. During the coding process, the filtering process is performed by\nsimply retrieving the filtered pixel through locating the input pixels and\ninterpolating between the cached values, instead of relying on heavy inference\ncomputations. In this paper, we propose a universal LUT-based ILF framework,\ntermed LUT-ILF++. First, we introduce the cooperation of multiple kinds of\nfiltering LUTs and propose a series of customized indexing mechanisms to enable\nbetter filtering reference perception with limited storage consumption. Second,\nwe propose the cross-component indexing mechanism to enable the filtering of\ndifferent color components jointly. Third, in order to make our solution\npractical for coding uses, we propose the LUT compaction scheme to enable the\nLUT pruning, achieving a lower storage cost of the entire solution. The\nproposed framework is implemented in the VVC reference software. Experimental\nresults show that the proposed framework achieves on average 0.82%/2.97%/1.63%\nand 0.85%/4.11%/2.06% bitrate reduction for common test sequences, under the AI\nand RA configurations, respectively. Compared to DNN-based solutions, our\nproposed solution has much lower time complexity and storage cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-loop filtering (ILF) is a key technology in video coding standards to\nreduce artifacts and enhance visual quality. Recently, neural network-based ILF\nschemes have achieved remarkable coding gains, emerging as a powerful candidate\nfor next-generation video coding standards. However, the use of deep neural\nnetworks (DNN) brings significant computational and time complexity or high\ndemands for dedicated hardware, making it challenging for general use. To\naddress this limitation, we study a practical ILF solution by adopting look-up\ntables (LUTs). After training a DNN with a restricted reference range for ILF,\nall possible inputs are traversed, and the output values of the DNN are cached\ninto LUTs. During the coding process, the filtering process is performed by\nsimply retrieving the filtered pixel through locating the input pixels and\ninterpolating between the cached values, instead of relying on heavy inference\ncomputations. In this paper, we propose a universal LUT-based ILF framework,\ntermed LUT-ILF++. First, we introduce the cooperation of multiple kinds of\nfiltering LUTs and propose a series of customized indexing mechanisms to enable\nbetter filtering reference perception with limited storage consumption. Second,\nwe propose the cross-component indexing mechanism to enable the filtering of\ndifferent color components jointly. Third, in order to make our solution\npractical for coding uses, we propose the LUT compaction scheme to enable the\nLUT pruning, achieving a lower storage cost of the entire solution. The\nproposed framework is implemented in the VVC reference software. Experimental\nresults show that the proposed framework achieves on average 0.82%/2.97%/1.63%\nand 0.85%/4.11%/2.06% bitrate reduction for common test sequences, under the AI\nand RA configurations, respectively. Compared to DNN-based solutions, our\nproposed solution has much lower time complexity and storage cost."
                },
                "authors": [
                    {
                        "name": "Zhuoyuan Li"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Yao Li"
                    },
                    {
                        "name": "Jialin Li"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Feng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Wu"
                },
                "author": "Feng Wu",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05211v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05211v2",
                "updated": "2025-09-11T12:06:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    6,
                    49,
                    3,
                    254,
                    0
                ],
                "published": "2025-08-07T09:47:21Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    9,
                    47,
                    21,
                    3,
                    219,
                    0
                ],
                "title": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization"
                },
                "summary": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference."
                },
                "authors": [
                    {
                        "name": "Sihan Yang"
                    },
                    {
                        "name": "Runsen Xu"
                    },
                    {
                        "name": "Chenhang Cui"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05211v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05211v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19880v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19880v2",
                "updated": "2025-09-11T10:20:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    10,
                    20,
                    20,
                    3,
                    254,
                    0
                ],
                "published": "2025-05-26T12:06:12Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    12,
                    6,
                    12,
                    0,
                    146,
                    0
                ],
                "title": "Universal Workers: A Vision for Eliminating Cold Starts in Serverless\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal Workers: A Vision for Eliminating Cold Starts in Serverless\n  Computing"
                },
                "summary": "Serverless computing enables developers to deploy code without managing\ninfrastructure, but suffers from cold start overhead when initializing new\nfunction instances. Existing solutions such as \"keep-alive\" or \"pre-warming\"\nare costly and unreliable under bursty workloads. We propose universal workers,\nwhich are computational units capable of executing any function with minimal\ninitialization overhead. Based on an analysis of production workload traces,\nour key insight is that requests in Function-as-a-Service (FaaS) platforms show\na highly skewed distribution, with most requests invoking a small subset of\nfunctions. We exploit this observation to approximate universal workers through\nlocality groups and three-tier caching (handler, install, import). With this\nwork, we aim to enable more efficient and scalable FaaS platforms capable of\nhandling diverse workloads with minimal initialization overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing enables developers to deploy code without managing\ninfrastructure, but suffers from cold start overhead when initializing new\nfunction instances. Existing solutions such as \"keep-alive\" or \"pre-warming\"\nare costly and unreliable under bursty workloads. We propose universal workers,\nwhich are computational units capable of executing any function with minimal\ninitialization overhead. Based on an analysis of production workload traces,\nour key insight is that requests in Function-as-a-Service (FaaS) platforms show\na highly skewed distribution, with most requests invoking a small subset of\nfunctions. We exploit this observation to approximate universal workers through\nlocality groups and three-tier caching (handler, install, import). With this\nwork, we aim to enable more efficient and scalable FaaS platforms capable of\nhandling diverse workloads with minimal initialization overhead."
                },
                "authors": [
                    {
                        "name": "Saman Akbari"
                    },
                    {
                        "name": "Manfred Hauswirth"
                    }
                ],
                "author_detail": {
                    "name": "Manfred Hauswirth"
                },
                "author": "Manfred Hauswirth",
                "arxiv_doi": "10.1109/CLOUD67622.2025.00051",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/CLOUD67622.2025.00051",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.19880v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19880v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in the 2025 IEEE 18th International Conference on Cloud\n  Computing (CLOUD)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19740v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19740v3",
                "updated": "2025-09-11T06:45:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    6,
                    45,
                    58,
                    3,
                    254,
                    0
                ],
                "published": "2025-08-27T10:11:27Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    11,
                    27,
                    2,
                    239,
                    0
                ],
                "title": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval"
                },
                "summary": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding."
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Haiyuan Wan"
                    },
                    {
                        "name": "Ziyang Gong"
                    },
                    {
                        "name": "Fei Chao"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19740v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19740v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01085v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01085v3",
                "updated": "2025-09-11T06:16:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    6,
                    16,
                    31,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-01T03:16:52Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    16,
                    52,
                    0,
                    244,
                    0
                ],
                "title": "Bidirectional Sparse Attention for Faster Video Diffusion Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bidirectional Sparse Attention for Faster Video Diffusion Training"
                },
                "summary": "Video diffusion Transformer (DiT) models excel in generative quality but hit\nmajor computational bottlenecks when producing high-resolution, long-duration\nvideos. The quadratic complexity of full attention leads to prohibitively high\ntraining and inference costs. Full attention inefficiency stems from two key\nchallenges: excessive computation due to the inherent sparsity of Queries and\nKey-Value pairs, and redundant computation as fixed sparse patterns fail to\nleverage DiT's dynamic attention. To overcome this limitation, we propose a\nBidirectional Sparse Attention (BSA) framework for faster video DiT training,\nthe first to dynamically sparsify both Queries and Key-Value pairs within 3D\nfull attention, thereby substantially improving training and inference\nefficiency. BSA addresses these issues through two key components. Query\nsparsity is optimized by selecting the most informative query tokens via\nsemantic similarity and with a dynamic spatial-time training strategy, while KV\nsparsity is achieved by computing a statistical dynamic threshold to retain\nonly the most salient KV blocks for computation. Extensive experiments\ndemonstrate that BSA significantly accelerates DiT training across long\nsequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention\ntraining, while preserving or even surpassing the generative quality of full\nattention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video diffusion Transformer (DiT) models excel in generative quality but hit\nmajor computational bottlenecks when producing high-resolution, long-duration\nvideos. The quadratic complexity of full attention leads to prohibitively high\ntraining and inference costs. Full attention inefficiency stems from two key\nchallenges: excessive computation due to the inherent sparsity of Queries and\nKey-Value pairs, and redundant computation as fixed sparse patterns fail to\nleverage DiT's dynamic attention. To overcome this limitation, we propose a\nBidirectional Sparse Attention (BSA) framework for faster video DiT training,\nthe first to dynamically sparsify both Queries and Key-Value pairs within 3D\nfull attention, thereby substantially improving training and inference\nefficiency. BSA addresses these issues through two key components. Query\nsparsity is optimized by selecting the most informative query tokens via\nsemantic similarity and with a dynamic spatial-time training strategy, while KV\nsparsity is achieved by computing a statistical dynamic threshold to retain\nonly the most salient KV blocks for computation. Extensive experiments\ndemonstrate that BSA significantly accelerates DiT training across long\nsequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention\ntraining, while preserving or even surpassing the generative quality of full\nattention."
                },
                "authors": [
                    {
                        "name": "Chenlu Zhan"
                    },
                    {
                        "name": "Wen Li"
                    },
                    {
                        "name": "Chuyu Shen"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Suhui Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01085v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01085v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09094v1",
                "updated": "2025-09-11T02:00:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    2,
                    0,
                    27,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T02:00:27Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    2,
                    0,
                    27,
                    3,
                    254,
                    0
                ],
                "title": "Coherence-Aware Task Graph Modeling for Realistic Application",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherence-Aware Task Graph Modeling for Realistic Application"
                },
                "summary": "As multicore systems continue to scale, cache coherence has emerged as a\ncritical determinant of system performance, with coherence behavior and task\nexecution closely intertwined, reshaping inter-task dependencies. Task graph\nmodeling provides a structured way to capture such dependencies and serves as\nthe foundation for many system-level design strategies. However, these\nstrategies typically rely on predefined task graphs, while many real-world\napplications lack explicit graphs and exhibit dynamic, data-dependent behavior,\nlimiting the effectiveness of static approaches. To address this, several task\ngraph modeling methods for realistic workloads have been developed. Yet, they\neither rely on implicit techniques that use application-specific features\nwithout producing explicit graphs, or they generate graphs tailored to fixed\nscheduling models, which limits generality. More importantly, they often\noverlook coherence interactions, creating a gap between design assumptions and\nactual runtime behavior. To overcome these limitations, we propose CoTAM, a\nCoherence-Aware Task Graph Modeling framework for realistic workloads that\nconstructs a unified task graph reflecting runtime behavior. CoTAM analyzes the\nimpact of coherence by decoupling its effects from overall execution,\nquantifies its influence through a learned weighting scheme, and infers\ninter-task dependencies for coherence-aware graph generation. Extensive\nexperiments show that CoTAM outperforms implicit methods, bridging the gap\nbetween dynamic workload behavior and existing designs while demonstrating the\nimportance of incorporating cache coherence into task graph modeling for\naccurate and generalizable system-level analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As multicore systems continue to scale, cache coherence has emerged as a\ncritical determinant of system performance, with coherence behavior and task\nexecution closely intertwined, reshaping inter-task dependencies. Task graph\nmodeling provides a structured way to capture such dependencies and serves as\nthe foundation for many system-level design strategies. However, these\nstrategies typically rely on predefined task graphs, while many real-world\napplications lack explicit graphs and exhibit dynamic, data-dependent behavior,\nlimiting the effectiveness of static approaches. To address this, several task\ngraph modeling methods for realistic workloads have been developed. Yet, they\neither rely on implicit techniques that use application-specific features\nwithout producing explicit graphs, or they generate graphs tailored to fixed\nscheduling models, which limits generality. More importantly, they often\noverlook coherence interactions, creating a gap between design assumptions and\nactual runtime behavior. To overcome these limitations, we propose CoTAM, a\nCoherence-Aware Task Graph Modeling framework for realistic workloads that\nconstructs a unified task graph reflecting runtime behavior. CoTAM analyzes the\nimpact of coherence by decoupling its effects from overall execution,\nquantifies its influence through a learned weighting scheme, and infers\ninter-task dependencies for coherence-aware graph generation. Extensive\nexperiments show that CoTAM outperforms implicit methods, bridging the gap\nbetween dynamic workload behavior and existing designs while demonstrating the\nimportance of incorporating cache coherence into task graph modeling for\naccurate and generalizable system-level analysis."
                },
                "authors": [
                    {
                        "name": "Guochu Xiong"
                    },
                    {
                        "name": "Xiangzhong Luo"
                    },
                    {
                        "name": "Weichen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weichen Liu"
                },
                "author": "Weichen Liu",
                "arxiv_doi": "10.1145/3742875.3754678",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3742875.3754678",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.09094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by MEMOCODE'25, 10 pages",
                "arxiv_journal_ref": "International Symposium on Formal Methods and Models for System\n  Design (MEMOCODE '25), September 28-October 3, 2025, Taipei, Taiwan",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23674v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23674v2",
                "updated": "2025-09-10T17:59:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    59,
                    8,
                    2,
                    253,
                    0
                ],
                "published": "2025-07-31T15:50:57Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    50,
                    57,
                    3,
                    212,
                    0
                ],
                "title": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses"
                },
                "summary": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience."
                },
                "authors": [
                    {
                        "name": "Muhammad Taha Cheema"
                    },
                    {
                        "name": "Abeer Aamir"
                    },
                    {
                        "name": "Khawaja Gul Muhammad"
                    },
                    {
                        "name": "Naveed Anwar Bhatti"
                    },
                    {
                        "name": "Ihsan Ayyub Qazi"
                    },
                    {
                        "name": "Zafar Ayyub Qazi"
                    }
                ],
                "author_detail": {
                    "name": "Zafar Ayyub Qazi"
                },
                "author": "Zafar Ayyub Qazi",
                "arxiv_comment": "13 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23674v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23674v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08696v1",
                "updated": "2025-09-10T15:41:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    41,
                    15,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T15:41:15Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    41,
                    15,
                    2,
                    253,
                    0
                ],
                "title": "Accelerating Diffusion Transformer-Based Text-to-Speech with Transformer\n  Layer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer-Based Text-to-Speech with Transformer\n  Layer Caching"
                },
                "summary": "This paper presents a method to accelerate the inference process of diffusion\ntransformer (DiT)-based text-to-speech (TTS) models by applying a selective\ncaching mechanism to transformer layers. Specifically, I integrate SmoothCache\ninto the F5-TTS architecture, focusing on caching outputs of self-attention and\nfeed-forward network layers to reduce redundant computations during the\ndenoising process. A calibration phase is introduced to analyze L1 relative\nerrors between timesteps, guiding the selection of cache schedules that\nminimize quality degradation. To address the problem of inter-layer dependency,\na unified caching schedule is adopted, applying the cache pattern derived from\nself-attention layers to both layer types. Experiments on LibriSpeech-PC and\nSeed-TTS datasets evaluate various cache thresholds and denoising step\nconfigurations. Results show that caching at higher denoising steps reduces\ninference time without compromising output quality, whereas caching at lower\nsteps can negatively impact synthesis quality similarly to reducing the total\nnumber of denoising steps. Objective and subjective metrics confirm the\neffectiveness of SmoothCache in maintaining performance while improving\ncomputational efficiency. Comparisons between cached inference and reduced-step\ninference further highlight the benefits of selective caching, especially under\nhigh-step configurations. This work demonstrates that transformer layer caching\nis a practical solution for optimizing diffusion transformer-based TTS models\nwithout requiring architectural changes or retraining. Example inference\nresults can be heard at https://siratish.github.io/F5-TTS_SmoothCache/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a method to accelerate the inference process of diffusion\ntransformer (DiT)-based text-to-speech (TTS) models by applying a selective\ncaching mechanism to transformer layers. Specifically, I integrate SmoothCache\ninto the F5-TTS architecture, focusing on caching outputs of self-attention and\nfeed-forward network layers to reduce redundant computations during the\ndenoising process. A calibration phase is introduced to analyze L1 relative\nerrors between timesteps, guiding the selection of cache schedules that\nminimize quality degradation. To address the problem of inter-layer dependency,\na unified caching schedule is adopted, applying the cache pattern derived from\nself-attention layers to both layer types. Experiments on LibriSpeech-PC and\nSeed-TTS datasets evaluate various cache thresholds and denoising step\nconfigurations. Results show that caching at higher denoising steps reduces\ninference time without compromising output quality, whereas caching at lower\nsteps can negatively impact synthesis quality similarly to reducing the total\nnumber of denoising steps. Objective and subjective metrics confirm the\neffectiveness of SmoothCache in maintaining performance while improving\ncomputational efficiency. Comparisons between cached inference and reduced-step\ninference further highlight the benefits of selective caching, especially under\nhigh-step configurations. This work demonstrates that transformer layer caching\nis a practical solution for optimizing diffusion transformer-based TTS models\nwithout requiring architectural changes or retraining. Example inference\nresults can be heard at https://siratish.github.io/F5-TTS_SmoothCache/ ."
                },
                "authors": [
                    {
                        "name": "Siratish Sakpiboonchit"
                    }
                ],
                "author_detail": {
                    "name": "Siratish Sakpiboonchit"
                },
                "author": "Siratish Sakpiboonchit",
                "arxiv_comment": "9 pages, 2 tables, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08542v1",
                "updated": "2025-09-10T12:46:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    46,
                    29,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T12:46:29Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    46,
                    29,
                    2,
                    253,
                    0
                ],
                "title": "BitROM: Weight Reload-Free CiROM Architecture Towards Billion-Parameter\n  1.58-bit LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitROM: Weight Reload-Free CiROM Architecture Towards Billion-Parameter\n  1.58-bit LLM Inference"
                },
                "summary": "Compute-in-Read-Only-Memory (CiROM) accelerators offer outstanding energy\nefficiency for CNNs by eliminating runtime weight updates. However, their\nscalability to Large Language Models (LLMs) is fundamentally constrained by\ntheir vast parameter sizes. Notably, LLaMA-7B - the smallest model in LLaMA\nseries - demands more than 1,000 cm2 of silicon area even in advanced CMOS\nnodes. This paper presents BitROM, the first CiROM-based accelerator that\novercomes this limitation through co-design with BitNet's 1.58-bit quantization\nmodel, enabling practical and efficient LLM inference at the edge. BitROM\nintroduces three key innovations: 1) a novel Bidirectional ROM Array that\nstores two ternary weights per transistor; 2) a Tri-Mode Local Accumulator\noptimized for ternary-weight computations; and 3) an integrated Decode-Refresh\n(DR) eDRAM that supports on-die KV-cache management, significantly reducing\nexternal memory access during decoding. In addition, BitROM integrates\nLoRA-based adapters to enable efficient transfer learning across various\ndownstream tasks. Evaluated in 65nm CMOS, BitROM achieves 20.8 TOPS/W and a bit\ndensity of 4,967 kB/mm2 - offering a 10x improvement in area efficiency over\nprior digital CiROM designs. Moreover, the DR eDRAM contributes to a 43.6%\nreduction in external DRAM access, further enhancing deployment efficiency for\nLLMs in edge applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute-in-Read-Only-Memory (CiROM) accelerators offer outstanding energy\nefficiency for CNNs by eliminating runtime weight updates. However, their\nscalability to Large Language Models (LLMs) is fundamentally constrained by\ntheir vast parameter sizes. Notably, LLaMA-7B - the smallest model in LLaMA\nseries - demands more than 1,000 cm2 of silicon area even in advanced CMOS\nnodes. This paper presents BitROM, the first CiROM-based accelerator that\novercomes this limitation through co-design with BitNet's 1.58-bit quantization\nmodel, enabling practical and efficient LLM inference at the edge. BitROM\nintroduces three key innovations: 1) a novel Bidirectional ROM Array that\nstores two ternary weights per transistor; 2) a Tri-Mode Local Accumulator\noptimized for ternary-weight computations; and 3) an integrated Decode-Refresh\n(DR) eDRAM that supports on-die KV-cache management, significantly reducing\nexternal memory access during decoding. In addition, BitROM integrates\nLoRA-based adapters to enable efficient transfer learning across various\ndownstream tasks. Evaluated in 65nm CMOS, BitROM achieves 20.8 TOPS/W and a bit\ndensity of 4,967 kB/mm2 - offering a 10x improvement in area efficiency over\nprior digital CiROM designs. Moreover, the DR eDRAM contributes to a 43.6%\nreduction in external DRAM access, further enhancing deployment efficiency for\nLLMs in edge applications."
                },
                "authors": [
                    {
                        "name": "Wenlun Zhang"
                    },
                    {
                        "name": "Xinyu Li"
                    },
                    {
                        "name": "Shimpei Ando"
                    },
                    {
                        "name": "Kentaro Yoshioka"
                    }
                ],
                "author_detail": {
                    "name": "Kentaro Yoshioka"
                },
                "author": "Kentaro Yoshioka",
                "arxiv_comment": "Accepted to ASP-DAC 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08342v1",
                "updated": "2025-09-10T07:28:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    7,
                    28,
                    24,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T07:28:24Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    7,
                    28,
                    24,
                    2,
                    253,
                    0
                ],
                "title": "Accelerating Mixture-of-Expert Inference with Adaptive Expert Split\n  Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Mixture-of-Expert Inference with Adaptive Expert Split\n  Mechanism"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a promising architecture for modern\nlarge language models (LLMs). However, massive parameters impose heavy GPU\nmemory (i.e., VRAM) demands, hindering the widespread adoption of MoE LLMs.\nOffloading the expert parameters to CPU RAM offers an effective way to\nalleviate the VRAM requirements for MoE inference. Existing approaches\ntypically cache a small subset of experts in VRAM and dynamically prefetch\nexperts from RAM during inference, leading to significant degradation in\ninference speed due to the poor cache hit rate and substantial expert loading\nlatency. In this work, we propose MoEpic, an efficient MoE inference system\nwith a novel expert split mechanism. Specifically, each expert is vertically\ndivided into two segments: top and bottom. MoEpic caches the top segment of hot\nexperts, so that more experts will be stored under the limited VRAM budget,\nthereby improving the cache hit rate. During each layer's inference, MoEpic\npredicts and prefetches the activated experts for the next layer. Since the top\nsegments of cached experts are exempt from fetching, the loading time is\nreduced, which allows efficient transfer-computation overlap. Nevertheless, the\nperformance of MoEpic critically depends on the cache configuration (i.e., each\nlayer's VRAM budget and expert split ratio). To this end, we propose a\ndivide-and-conquer algorithm based on fixed-point iteration for adaptive cache\nconfiguration. Extensive experiments on popular MoE LLMs demonstrate that\nMoEpic can save about half of the GPU cost, while lowering the inference\nlatency by about 37.51%-65.73% compared to the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a promising architecture for modern\nlarge language models (LLMs). However, massive parameters impose heavy GPU\nmemory (i.e., VRAM) demands, hindering the widespread adoption of MoE LLMs.\nOffloading the expert parameters to CPU RAM offers an effective way to\nalleviate the VRAM requirements for MoE inference. Existing approaches\ntypically cache a small subset of experts in VRAM and dynamically prefetch\nexperts from RAM during inference, leading to significant degradation in\ninference speed due to the poor cache hit rate and substantial expert loading\nlatency. In this work, we propose MoEpic, an efficient MoE inference system\nwith a novel expert split mechanism. Specifically, each expert is vertically\ndivided into two segments: top and bottom. MoEpic caches the top segment of hot\nexperts, so that more experts will be stored under the limited VRAM budget,\nthereby improving the cache hit rate. During each layer's inference, MoEpic\npredicts and prefetches the activated experts for the next layer. Since the top\nsegments of cached experts are exempt from fetching, the loading time is\nreduced, which allows efficient transfer-computation overlap. Nevertheless, the\nperformance of MoEpic critically depends on the cache configuration (i.e., each\nlayer's VRAM budget and expert split ratio). To this end, we propose a\ndivide-and-conquer algorithm based on fixed-point iteration for adaptive cache\nconfiguration. Extensive experiments on popular MoE LLMs demonstrate that\nMoEpic can save about half of the GPU cost, while lowering the inference\nlatency by about 37.51%-65.73% compared to the baselines."
                },
                "authors": [
                    {
                        "name": "Jiaming Yan"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Liusheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Liusheng Huang"
                },
                "author": "Liusheng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08315v1",
                "updated": "2025-09-10T06:32:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    6,
                    32,
                    49,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T06:32:49Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    6,
                    32,
                    49,
                    2,
                    253,
                    0
                ],
                "title": "EvolKV: Evolutionary KV Cache Compression for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvolKV: Evolutionary KV Cache Compression for LLM Inference"
                },
                "summary": "Existing key-value (KV) cache compression methods typically rely on\nheuristics, such as uniform cache allocation across layers or static eviction\npolicies, however, they ignore the critical interplays among layer-specific\nfeature patterns and task performance, which can lead to degraded\ngeneralization. In this paper, we propose EvolKV, an adaptive framework for\nlayer-wise, task-driven KV cache compression that jointly optimizes the memory\nefficiency and task performance. By reformulating cache allocation as a\nmulti-objective optimization problem, EvolKV leverages evolutionary search to\ndynamically configure layer budgets while directly maximizing downstream\nperformance. Extensive experiments on 11 tasks demonstrate that our approach\noutperforms all baseline methods across a wide range of KV cache budgets on\nlong-context tasks and surpasses heuristic baselines by up to 7 percentage\npoints on GSM8K. Notably, EvolKV achieves superior performance over the full KV\ncache setting on code completion while utilizing only 1.5% of the original\nbudget, suggesting the untapped potential in learned compression strategies for\nKV cache budget allocation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing key-value (KV) cache compression methods typically rely on\nheuristics, such as uniform cache allocation across layers or static eviction\npolicies, however, they ignore the critical interplays among layer-specific\nfeature patterns and task performance, which can lead to degraded\ngeneralization. In this paper, we propose EvolKV, an adaptive framework for\nlayer-wise, task-driven KV cache compression that jointly optimizes the memory\nefficiency and task performance. By reformulating cache allocation as a\nmulti-objective optimization problem, EvolKV leverages evolutionary search to\ndynamically configure layer budgets while directly maximizing downstream\nperformance. Extensive experiments on 11 tasks demonstrate that our approach\noutperforms all baseline methods across a wide range of KV cache budgets on\nlong-context tasks and surpasses heuristic baselines by up to 7 percentage\npoints on GSM8K. Notably, EvolKV achieves superior performance over the full KV\ncache setting on code completion while utilizing only 1.5% of the original\nbudget, suggesting the untapped potential in learned compression strategies for\nKV cache budget allocation."
                },
                "authors": [
                    {
                        "name": "Bohan Yu"
                    },
                    {
                        "name": "Yekun Chai"
                    }
                ],
                "author_detail": {
                    "name": "Yekun Chai"
                },
                "author": "Yekun Chai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v3",
                "updated": "2025-09-09T13:30:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    30,
                    17,
                    1,
                    252,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Tianfu Wang"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "arxiv_comment": "Accepted by EMNLP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07379v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07379v1",
                "updated": "2025-09-09T04:00:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    4,
                    0,
                    43,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T04:00:43Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    4,
                    0,
                    43,
                    1,
                    252,
                    0
                ],
                "title": "DuoServe-MoE: Dual-Phase Expert Prefetch and Cache Scheduling for\n  Efficient MoE LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DuoServe-MoE: Dual-Phase Expert Prefetch and Cache Scheduling for\n  Efficient MoE LLM Inference"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of deep learning tasks. Mixture of Experts (MoE) further enhances\ntheir capabilities by increasing model width through sparsely activated expert\nbranches, which keeps inference computation efficient. However, the large\nnumber of expert weights introduces significant GPU memory pressure, especially\nin resource-constrained environments such as single-GPU servers. More\nimportantly, MoE inference consists of two fundamentally different stages: a\nprefill stage where most experts are activated densely, and a decode stage\nwhere only a few experts are triggered sparsely. Treating these stages with a\nuniform scheduling strategy often leads to suboptimal latency and memory usage.\nTo address this, we propose DuoServe-MoE, an inference serving system that\nexplicitly separates prefill and decode stages and applies tailored expert\nscheduling strategies to each. In the prefill stage, DuoServe-MoE uses a\ntwo-stream CUDA pipeline that overlaps expert weight prefetching with the\ncomputation of non-MoE layers, limiting expert residency in GPU memory. In the\ndecode stage, a lightweight layer-level predictor trained offline from\nactivation traces is used to prefetch only the most likely activated experts,\nwithout requiring any changes to the model. Experiments on 4-bit Mixtral-8x7B\nand 8x22B models show that DuoServe-MoE improves end-to-end latency by 1.42 to\n7.54 times while keeping peak memory usage at only 15 percent of the full model\nsize.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of deep learning tasks. Mixture of Experts (MoE) further enhances\ntheir capabilities by increasing model width through sparsely activated expert\nbranches, which keeps inference computation efficient. However, the large\nnumber of expert weights introduces significant GPU memory pressure, especially\nin resource-constrained environments such as single-GPU servers. More\nimportantly, MoE inference consists of two fundamentally different stages: a\nprefill stage where most experts are activated densely, and a decode stage\nwhere only a few experts are triggered sparsely. Treating these stages with a\nuniform scheduling strategy often leads to suboptimal latency and memory usage.\nTo address this, we propose DuoServe-MoE, an inference serving system that\nexplicitly separates prefill and decode stages and applies tailored expert\nscheduling strategies to each. In the prefill stage, DuoServe-MoE uses a\ntwo-stream CUDA pipeline that overlaps expert weight prefetching with the\ncomputation of non-MoE layers, limiting expert residency in GPU memory. In the\ndecode stage, a lightweight layer-level predictor trained offline from\nactivation traces is used to prefetch only the most likely activated experts,\nwithout requiring any changes to the model. Experiments on 4-bit Mixtral-8x7B\nand 8x22B models show that DuoServe-MoE improves end-to-end latency by 1.42 to\n7.54 times while keeping peak memory usage at only 15 percent of the full model\nsize."
                },
                "authors": [
                    {
                        "name": "Yuning Zhang"
                    },
                    {
                        "name": "Grant Pinkert"
                    },
                    {
                        "name": "Nan Yang"
                    },
                    {
                        "name": "Yanli Li"
                    },
                    {
                        "name": "Dong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yuan"
                },
                "author": "Dong Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07379v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01742v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01742v2",
                "updated": "2025-09-09T00:15:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    0,
                    15,
                    5,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-01T19:49:21Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    19,
                    49,
                    21,
                    0,
                    244,
                    0
                ],
                "title": "BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure\n  HBM Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure\n  HBM Accelerators"
                },
                "summary": "While Trusted Execution Environments provide a strong foundation for secure\ncloud computing, they remain vulnerable to access pattern leakages. Oblivious\nMaps (OMAPs) mitigate this by fully hiding access patterns but suffer from high\noverhead due to randomized remapping and worst-case padding. We argue these\ncosts are not fundamental. Modern accelerators featuring High-Bandwidth Memory\n(HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that\neavesdropping on HBM is difficult -- even for physical attackers -- as its\nmemory channels are sealed together with processor cores inside the same\nphysical package. Later, Hunt et al. [NSDI'20] show that, with proper\nisolation, HBM can be turned into an unobservable region where both data and\nmemory traces are hidden. This motivates a rethink of OMAP design with\nHBM-backed solutions to finally overcome their traditional performance limits.\nBuilding on these insights, we present BOLT, a Bandwidth Optimized,\nLightning-fast OMAP accelerator that, for the first time, achieves O(1) +\nO(log_2(log_2 (N))) bandwidth overhead. BOLT introduces three key innovations:\n(i) a new OMAP algorithm that leverages isolated HBM as an unobservable cache\nto accelerate oblivious access to large host memory; (ii) a self-hosted\narchitecture that offloads execution and memory control from the host to\nmitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs\nthat maximize resource efficiency. We implement a prototype BOLT on a Xilinx\nU55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in\ninitialization and query time, respectively, over state-of-the-art OMAPs,\nincluding an industry implementation from Facebook.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Trusted Execution Environments provide a strong foundation for secure\ncloud computing, they remain vulnerable to access pattern leakages. Oblivious\nMaps (OMAPs) mitigate this by fully hiding access patterns but suffer from high\noverhead due to randomized remapping and worst-case padding. We argue these\ncosts are not fundamental. Modern accelerators featuring High-Bandwidth Memory\n(HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that\neavesdropping on HBM is difficult -- even for physical attackers -- as its\nmemory channels are sealed together with processor cores inside the same\nphysical package. Later, Hunt et al. [NSDI'20] show that, with proper\nisolation, HBM can be turned into an unobservable region where both data and\nmemory traces are hidden. This motivates a rethink of OMAP design with\nHBM-backed solutions to finally overcome their traditional performance limits.\nBuilding on these insights, we present BOLT, a Bandwidth Optimized,\nLightning-fast OMAP accelerator that, for the first time, achieves O(1) +\nO(log_2(log_2 (N))) bandwidth overhead. BOLT introduces three key innovations:\n(i) a new OMAP algorithm that leverages isolated HBM as an unobservable cache\nto accelerate oblivious access to large host memory; (ii) a self-hosted\narchitecture that offloads execution and memory control from the host to\nmitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs\nthat maximize resource efficiency. We implement a prototype BOLT on a Xilinx\nU55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in\ninitialization and query time, respectively, over state-of-the-art OMAPs,\nincluding an industry implementation from Facebook."
                },
                "authors": [
                    {
                        "name": "Yitong Guo"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Haobin Hiroki Chen"
                    },
                    {
                        "name": "Yukui Luo"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Chenghong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chenghong Wang"
                },
                "author": "Chenghong Wang",
                "arxiv_comment": "Accepted by CCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01742v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01742v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06949v1",
                "updated": "2025-09-08T17:58:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    58,
                    6,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:58:06Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    58,
                    6,
                    0,
                    251,
                    0
                ],
                "title": "Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models"
                },
                "summary": "We propose TraceRL, a trajectory-aware reinforcement learning framework for\ndiffusion language models (DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with a diffusion-based value model that enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improves sampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-art diffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Through curriculum learning, we also derive the first\nlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates accelerated KV-cache techniques and inference engines for\nboth inference and reinforcement learning, and includes implementations of\nvarious supervised fine-tuning and RL methods for mathematics, coding, and\ngeneral tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose TraceRL, a trajectory-aware reinforcement learning framework for\ndiffusion language models (DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with a diffusion-based value model that enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improves sampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-art diffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Through curriculum learning, we also derive the first\nlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates accelerated KV-cache techniques and inference engines for\nboth inference and reinforcement learning, and includes implementations of\nvarious supervised fine-tuning and RL methods for mathematics, coding, and\ngeneral tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL"
                },
                "authors": [
                    {
                        "name": "Yinjie Wang"
                    },
                    {
                        "name": "Ling Yang"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Ke Shen"
                    },
                    {
                        "name": "Mengdi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mengdi Wang"
                },
                "author": "Mengdi Wang",
                "arxiv_comment": "Code and Models: https://github.com/Gen-Verse/dLLM-RL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03377v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03377v2",
                "updated": "2025-09-08T17:22:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    22,
                    17,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-03T14:53:45Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    53,
                    45,
                    2,
                    246,
                    0
                ],
                "title": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via\n  Transparent Near-Data Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via\n  Transparent Near-Data Processing"
                },
                "summary": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03377v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03377v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v4",
                "updated": "2025-09-08T13:34:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    34,
                    54,
                    0,
                    251,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AGI/AMD-Hybrid-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AGI/AMD-Hybrid-Models."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06579v1",
                "updated": "2025-09-08T11:49:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    49,
                    51,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T11:49:51Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    49,
                    51,
                    0,
                    251,
                    0
                ],
                "title": "CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View\n  Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View\n  Synthesis"
                },
                "summary": "Multi-view diffusion models have shown promise in 3D novel view synthesis,\nbut most existing methods adopt a non-autoregressive formulation. This limits\ntheir applicability in world modeling, as they only support a fixed number of\nviews and suffer from slow inference due to denoising all frames\nsimultaneously. To address these limitations, we propose CausNVS, a multi-view\ndiffusion model in an autoregressive setting, which supports arbitrary\ninput-output view configurations and generates views sequentially. We train\nCausNVS with causal masking and per-frame noise, using pairwise-relative camera\npose encodings (CaPE) for precise camera control. At inference time, we combine\na spatially-aware sliding-window with key-value caching and noise conditioning\naugmentation to mitigate drift. Our experiments demonstrate that CausNVS\nsupports a broad range of camera trajectories, enables flexible autoregressive\nnovel view synthesis, and achieves consistently strong visual quality across\ndiverse settings. Project page: https://kxhit.github.io/CausNVS.html.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-view diffusion models have shown promise in 3D novel view synthesis,\nbut most existing methods adopt a non-autoregressive formulation. This limits\ntheir applicability in world modeling, as they only support a fixed number of\nviews and suffer from slow inference due to denoising all frames\nsimultaneously. To address these limitations, we propose CausNVS, a multi-view\ndiffusion model in an autoregressive setting, which supports arbitrary\ninput-output view configurations and generates views sequentially. We train\nCausNVS with causal masking and per-frame noise, using pairwise-relative camera\npose encodings (CaPE) for precise camera control. At inference time, we combine\na spatially-aware sliding-window with key-value caching and noise conditioning\naugmentation to mitigate drift. Our experiments demonstrate that CausNVS\nsupports a broad range of camera trajectories, enables flexible autoregressive\nnovel view synthesis, and achieves consistently strong visual quality across\ndiverse settings. Project page: https://kxhit.github.io/CausNVS.html."
                },
                "authors": [
                    {
                        "name": "Xin Kong"
                    },
                    {
                        "name": "Daniel Watson"
                    },
                    {
                        "name": "Yannick Strümpler"
                    },
                    {
                        "name": "Michael Niemeyer"
                    },
                    {
                        "name": "Federico Tombari"
                    }
                ],
                "author_detail": {
                    "name": "Federico Tombari"
                },
                "author": "Federico Tombari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06493v1",
                "updated": "2025-09-08T09:54:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    54,
                    18,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T09:54:18Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    54,
                    18,
                    0,
                    251,
                    0
                ],
                "title": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers"
                },
                "summary": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search."
                },
                "authors": [
                    {
                        "name": "Ran Xin"
                    },
                    {
                        "name": "Zeyu Zheng"
                    },
                    {
                        "name": "Yanchen Nie"
                    },
                    {
                        "name": "Kun Yuan"
                    },
                    {
                        "name": "Xia Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Xia Xiao"
                },
                "author": "Xia Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09822v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09822v4",
                "updated": "2025-09-08T09:09:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    9,
                    36,
                    0,
                    251,
                    0
                ],
                "published": "2025-08-13T13:54:51Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    54,
                    51,
                    2,
                    225,
                    0
                ],
                "title": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining"
                },
                "summary": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining. The project page is here:\nhttps://hcplab-sysu.github.io/PhysicalAutoregressiveModel/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining. The project page is here:\nhttps://hcplab-sysu.github.io/PhysicalAutoregressiveModel/"
                },
                "authors": [
                    {
                        "name": "Zijian Song"
                    },
                    {
                        "name": "Sihan Qin"
                    },
                    {
                        "name": "Tianshui Chen"
                    },
                    {
                        "name": "Liang Lin"
                    },
                    {
                        "name": "Guangrun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guangrun Wang"
                },
                "author": "Guangrun Wang",
                "arxiv_comment": "16 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09822v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09822v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06444v1",
                "updated": "2025-09-08T08:44:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    44,
                    24,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T08:44:24Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    44,
                    24,
                    0,
                    251,
                    0
                ],
                "title": "HyFedRAG: A Federated Retrieval-Augmented Generation Framework for\n  Heterogeneous and Privacy-Sensitive Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyFedRAG: A Federated Retrieval-Augmented Generation Framework for\n  Heterogeneous and Privacy-Sensitive Data"
                },
                "summary": "Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive\ndata, especially in distributed healthcare settings where patient data spans\nSQL, knowledge graphs, and clinical notes. Clinicians face difficulties\nretrieving rare disease cases due to privacy constraints and the limitations of\ntraditional cloud-based RAG systems in handling diverse formats and edge\ndevices. To address this, we introduce HyFedRAG, a unified and efficient\nFederated RAG framework tailored for Hybrid data modalities. By leveraging an\nedge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across\ndiverse data sources while preserving data privacy. Our key contributions are:\n(1) We design an edge-cloud collaborative RAG framework built on Flower, which\nsupports querying structured SQL data, semi-structured knowledge graphs, and\nunstructured documents. The edge-side LLMs convert diverse data into\nstandardized privacy-preserving representations, and the server-side LLMs\nintegrates them for global reasoning and generation. (2) We integrate\nlightweight local retrievers with privacy-aware LLMs and provide three\nanonymization tools that enable each client to produce semantically rich,\nde-identified summaries for global inference across devices. (3) To optimize\nresponse latency and reduce redundant computation, we design a three-tier\ncaching strategy consisting of local cache, intermediate representation cache,\nand cloud inference cache. Experimental results on PMC-Patients demonstrate\nthat HyFedRAG outperforms existing baselines in terms of retrieval quality,\ngeneration consistency, and system efficiency. Our framework offers a scalable\nand privacy-compliant solution for RAG over structural-heterogeneous data,\nunlocking the potential of LLMs in sensitive and diverse data environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive\ndata, especially in distributed healthcare settings where patient data spans\nSQL, knowledge graphs, and clinical notes. Clinicians face difficulties\nretrieving rare disease cases due to privacy constraints and the limitations of\ntraditional cloud-based RAG systems in handling diverse formats and edge\ndevices. To address this, we introduce HyFedRAG, a unified and efficient\nFederated RAG framework tailored for Hybrid data modalities. By leveraging an\nedge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across\ndiverse data sources while preserving data privacy. Our key contributions are:\n(1) We design an edge-cloud collaborative RAG framework built on Flower, which\nsupports querying structured SQL data, semi-structured knowledge graphs, and\nunstructured documents. The edge-side LLMs convert diverse data into\nstandardized privacy-preserving representations, and the server-side LLMs\nintegrates them for global reasoning and generation. (2) We integrate\nlightweight local retrievers with privacy-aware LLMs and provide three\nanonymization tools that enable each client to produce semantically rich,\nde-identified summaries for global inference across devices. (3) To optimize\nresponse latency and reduce redundant computation, we design a three-tier\ncaching strategy consisting of local cache, intermediate representation cache,\nand cloud inference cache. Experimental results on PMC-Patients demonstrate\nthat HyFedRAG outperforms existing baselines in terms of retrieval quality,\ngeneration consistency, and system efficiency. Our framework offers a scalable\nand privacy-compliant solution for RAG over structural-heterogeneous data,\nunlocking the potential of LLMs in sensitive and diverse data environments."
                },
                "authors": [
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Hainan Zhang"
                    },
                    {
                        "name": "Yongxin Tong"
                    },
                    {
                        "name": "Hong-Wei Zheng"
                    },
                    {
                        "name": "Zhiming Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Zheng"
                },
                "author": "Zhiming Zheng",
                "arxiv_comment": "9 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06436v1",
                "updated": "2025-09-08T08:34:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    34,
                    2,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T08:34:02Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    34,
                    2,
                    0,
                    251,
                    0
                ],
                "title": "Tree of Agents: Improving Long-Context Capabilities of Large Language\n  Models through Multi-Perspective Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Agents: Improving Long-Context Capabilities of Large Language\n  Models through Multi-Perspective Reasoning"
                },
                "summary": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents."
                },
                "authors": [
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Xiaofei Xu"
                    },
                    {
                        "name": "Ke Deng"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Lin Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lin Tian"
                },
                "author": "Lin Tian",
                "arxiv_comment": "19 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06261v1",
                "updated": "2025-09-08T00:57:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    0,
                    57,
                    50,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T00:57:50Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    0,
                    57,
                    50,
                    0,
                    251,
                    0
                ],
                "title": "FineServe: Precision-Aware KV Slab and Two-Level Scheduling for\n  Heterogeneous Precision LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FineServe: Precision-Aware KV Slab and Two-Level Scheduling for\n  Heterogeneous Precision LLM Serving"
                },
                "summary": "Recent advances in Post-Training Quantization (PTQ) techniques have\nsignificantly increased demand for serving quantized large language models\n(LLMs), enabling higher throughput and substantially reduced memory usage with\nminimal accuracy loss. Quantized models address memory constraints in LLMs and\nenhance GPU resource utilization through efficient GPU sharing. However,\nquantized models have smaller KV block sizes than non-quantized models, causing\nlimited memory efficiency due to memory fragmentation. Also, distinct resource\nusage patterns between quantized and non-quantized models require efficient\nscheduling to maximize throughput. To address these challenges, we propose\nFineServe, an inference serving framework for mixed-precision LLMs. FineServe's\nkey contributions include: (1) KV Slab, a precision-aware adaptive memory\nmanagement technique dynamically allocating KV cache based on model\nquantization characteristics, significantly reducing GPU memory fragmentation,\nand (2) a two-level scheduling framework comprising a global scheduler that\nplaces models to GPUs based on request rates, latency SLOs, and memory\nconstraints and efficiency, and a local scheduler that adaptively adjusts batch\nsizes according to real-time request fluctuations. Experimental results\ndemonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x\nhigher token generation throughput compared to the state-of-the-art GPU sharing\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Post-Training Quantization (PTQ) techniques have\nsignificantly increased demand for serving quantized large language models\n(LLMs), enabling higher throughput and substantially reduced memory usage with\nminimal accuracy loss. Quantized models address memory constraints in LLMs and\nenhance GPU resource utilization through efficient GPU sharing. However,\nquantized models have smaller KV block sizes than non-quantized models, causing\nlimited memory efficiency due to memory fragmentation. Also, distinct resource\nusage patterns between quantized and non-quantized models require efficient\nscheduling to maximize throughput. To address these challenges, we propose\nFineServe, an inference serving framework for mixed-precision LLMs. FineServe's\nkey contributions include: (1) KV Slab, a precision-aware adaptive memory\nmanagement technique dynamically allocating KV cache based on model\nquantization characteristics, significantly reducing GPU memory fragmentation,\nand (2) a two-level scheduling framework comprising a global scheduler that\nplaces models to GPUs based on request rates, latency SLOs, and memory\nconstraints and efficiency, and a local scheduler that adaptively adjusts batch\nsizes according to real-time request fluctuations. Experimental results\ndemonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x\nhigher token generation throughput compared to the state-of-the-art GPU sharing\nsystems."
                },
                "authors": [
                    {
                        "name": "Kyungmin Bin"
                    },
                    {
                        "name": "Seungbeom Choi"
                    },
                    {
                        "name": "Jimyoung Son"
                    },
                    {
                        "name": "Jieun Choi"
                    },
                    {
                        "name": "Daseul Bae"
                    },
                    {
                        "name": "Daehyeon Baek"
                    },
                    {
                        "name": "Kihyo Moon"
                    },
                    {
                        "name": "Minsung Jang"
                    },
                    {
                        "name": "Hyojung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hyojung Lee"
                },
                "author": "Hyojung Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06047v1",
                "updated": "2025-09-07T13:15:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    7,
                    13,
                    15,
                    17,
                    6,
                    250,
                    0
                ],
                "published": "2025-09-07T13:15:17Z",
                "published_parsed": [
                    2025,
                    9,
                    7,
                    13,
                    15,
                    17,
                    6,
                    250,
                    0
                ],
                "title": "A facile vector substrate platform via BaTiO3 membrane transfer enables\n  high quality solution processed epitaxial PZT on silicon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A facile vector substrate platform via BaTiO3 membrane transfer enables\n  high quality solution processed epitaxial PZT on silicon"
                },
                "summary": "The direct integration of high-performance ferroelectric oxides with silicon\nremains challenging due to lattice mismatch, thermal incompatibility, and the\nneed for high-temperature epitaxial growth. Here, a hybrid integration approach\nis demonstrated in which crystalline BaTiO3 (BTO) membranes are first\ntransferred onto Pt coated Si substrates and subsequently used as vector\nsubstrates (VS) for the growth of epitaxial (001) Pb(Zr0.52Ti0.48)O3 (PZT) thin\nfilms via chemical solution deposition (CSD). A KI and HCl based etchant\nenables rapid and complete dissolution of the SrVO3 sacrificial layer in about\n30 minutes, reducing the release time from days to minutes compared with\nconventional water based approaches to dissolve AVO3 and AMoO3 (A is Ca, Sr,\nBa). The BTO VS imposes dominant (00l) out of plane orientation and in plane\ncube on cube epitaxy in the overlying PZT. Devices exhibit remnant polarization\n10 to 12 micro coulomb/cm2 and coercive field of 100 kV/cm, with stable\nswitching to 10^8 cycles on the VS. From piezoelectric butterfly loops, we\nextract effective d33 of 70 pm/V for PZT on VS, and 54 pm/V for PZT grown on\nconventional Pt Si substrates. This approach demonstrates a scalable and cost\neffective route for integrating functional ferroelectric materials onto silicon\nand offers a promising platform for future CMOS compatible oxide electronics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The direct integration of high-performance ferroelectric oxides with silicon\nremains challenging due to lattice mismatch, thermal incompatibility, and the\nneed for high-temperature epitaxial growth. Here, a hybrid integration approach\nis demonstrated in which crystalline BaTiO3 (BTO) membranes are first\ntransferred onto Pt coated Si substrates and subsequently used as vector\nsubstrates (VS) for the growth of epitaxial (001) Pb(Zr0.52Ti0.48)O3 (PZT) thin\nfilms via chemical solution deposition (CSD). A KI and HCl based etchant\nenables rapid and complete dissolution of the SrVO3 sacrificial layer in about\n30 minutes, reducing the release time from days to minutes compared with\nconventional water based approaches to dissolve AVO3 and AMoO3 (A is Ca, Sr,\nBa). The BTO VS imposes dominant (00l) out of plane orientation and in plane\ncube on cube epitaxy in the overlying PZT. Devices exhibit remnant polarization\n10 to 12 micro coulomb/cm2 and coercive field of 100 kV/cm, with stable\nswitching to 10^8 cycles on the VS. From piezoelectric butterfly loops, we\nextract effective d33 of 70 pm/V for PZT on VS, and 54 pm/V for PZT grown on\nconventional Pt Si substrates. This approach demonstrates a scalable and cost\neffective route for integrating functional ferroelectric materials onto silicon\nand offers a promising platform for future CMOS compatible oxide electronics."
                },
                "authors": [
                    {
                        "name": "Asraful Haque"
                    },
                    {
                        "name": "Antony Jeyaseelan"
                    },
                    {
                        "name": "Shubham Kumar Parate"
                    },
                    {
                        "name": "Srinivasan Raghavan"
                    },
                    {
                        "name": "Pavan Nukala"
                    }
                ],
                "author_detail": {
                    "name": "Pavan Nukala"
                },
                "arxiv_affiliation": "Centre for Nanoscience and Engineering, Indian Institute of Science, Bengaluru, India",
                "author": "Pavan Nukala",
                "arxiv_comment": "17 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13863v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13863v2",
                "updated": "2025-09-06T05:58:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    6,
                    5,
                    58,
                    51,
                    5,
                    249,
                    0
                ],
                "published": "2025-08-19T14:30:41Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    30,
                    41,
                    1,
                    231,
                    0
                ],
                "title": "Tight Cache Contention Analysis for WCET Estimation on Multicore Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tight Cache Contention Analysis for WCET Estimation on Multicore Systems"
                },
                "summary": "WCET (Worst-Case Execution Time) estimation on multicore architecture is\nparticularly challenging mainly due to the complex accesses over cache shared\nby multiple cores. Existing analysis identifies possible contentions between\nparallel tasks by leveraging the partial order of the tasks or their program\nregions. Unfortunately, they overestimate the number of cache misses caused by\na remote block access without considering the actual cache state and the number\nof accesses. This paper reports a new analysis for inter-core cache contention.\nBased on the order of program regions in a task, we first identify memory\nreferences that could be affected if a remote access occurs in a region.\nAfterwards, a fine-grained contention analysis is constructed that computes the\nnumber of cache misses based on the access quantity of local and remote blocks.\nWe demonstrate that the overall inter-core cache interference of a task can be\nobtained via dynamic programming. Experiments show that compared to existing\nmethods, the proposed analysis reduces inter-core cache interference and WCET\nestimations by 52.31% and 8.94% on average, without significantly increasing\ncomputation overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WCET (Worst-Case Execution Time) estimation on multicore architecture is\nparticularly challenging mainly due to the complex accesses over cache shared\nby multiple cores. Existing analysis identifies possible contentions between\nparallel tasks by leveraging the partial order of the tasks or their program\nregions. Unfortunately, they overestimate the number of cache misses caused by\na remote block access without considering the actual cache state and the number\nof accesses. This paper reports a new analysis for inter-core cache contention.\nBased on the order of program regions in a task, we first identify memory\nreferences that could be affected if a remote access occurs in a region.\nAfterwards, a fine-grained contention analysis is constructed that computes the\nnumber of cache misses based on the access quantity of local and remote blocks.\nWe demonstrate that the overall inter-core cache interference of a task can be\nobtained via dynamic programming. Experiments show that compared to existing\nmethods, the proposed analysis reduces inter-core cache interference and WCET\nestimations by 52.31% and 8.94% on average, without significantly increasing\ncomputation overhead."
                },
                "authors": [
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Jieyu Jiang"
                    },
                    {
                        "name": "Shenlin Cai"
                    },
                    {
                        "name": "Yaowei Liang"
                    },
                    {
                        "name": "Chen Jie"
                    },
                    {
                        "name": "Yinjie Fang"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Guoquan Zhang"
                    },
                    {
                        "name": "Yaoyao Gu"
                    },
                    {
                        "name": "Xiang Xiao"
                    },
                    {
                        "name": "Wei Qin"
                    },
                    {
                        "name": "Xiangzhen Ouyang"
                    },
                    {
                        "name": "Wanli Chang"
                    }
                ],
                "author_detail": {
                    "name": "Wanli Chang"
                },
                "author": "Wanli Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13863v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13863v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05207v1",
                "updated": "2025-09-05T16:10:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    10,
                    20,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T16:10:20Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    10,
                    20,
                    4,
                    248,
                    0
                ],
                "title": "RapidGNN: Energy and Communication-Efficient Distributed Training on\n  Large-Scale Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RapidGNN: Energy and Communication-Efficient Distributed Training on\n  Large-Scale Graph Neural Networks"
                },
                "summary": "Graph Neural Networks (GNNs) have become popular across a diverse set of\ntasks in exploring structural relationships between entities. However, due to\nthe highly connected structure of the datasets, distributed training of GNNs on\nlarge-scale graphs poses significant challenges. Traditional sampling-based\napproaches mitigate the computational loads, yet the communication overhead\nremains a challenge. This paper presents RapidGNN, a distributed GNN training\nframework with deterministic sampling-based scheduling to enable efficient\ncache construction and prefetching of remote features. Evaluation on benchmark\ngraph datasets demonstrates RapidGNN's effectiveness across different scales\nand topologies. RapidGNN improves end-to-end training throughput by 2.46x to\n3.00x on average over baseline methods across the benchmark datasets, while\ncutting remote feature fetches by over 9.70x to 15.39x. RapidGNN further\ndemonstrates near-linear scalability with an increasing number of computing\nunits efficiently. Furthermore, it achieves increased energy efficiency over\nthe baseline methods for both CPU and GPU by 44% and 32%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have become popular across a diverse set of\ntasks in exploring structural relationships between entities. However, due to\nthe highly connected structure of the datasets, distributed training of GNNs on\nlarge-scale graphs poses significant challenges. Traditional sampling-based\napproaches mitigate the computational loads, yet the communication overhead\nremains a challenge. This paper presents RapidGNN, a distributed GNN training\nframework with deterministic sampling-based scheduling to enable efficient\ncache construction and prefetching of remote features. Evaluation on benchmark\ngraph datasets demonstrates RapidGNN's effectiveness across different scales\nand topologies. RapidGNN improves end-to-end training throughput by 2.46x to\n3.00x on average over baseline methods across the benchmark datasets, while\ncutting remote feature fetches by over 9.70x to 15.39x. RapidGNN further\ndemonstrates near-linear scalability with an increasing number of computing\nunits efficiently. Furthermore, it achieves increased energy efficiency over\nthe baseline methods for both CPU and GPU by 44% and 32%, respectively."
                },
                "authors": [
                    {
                        "name": "Arefin Niam"
                    },
                    {
                        "name": "Tevfik Kosar"
                    },
                    {
                        "name": "M S Q Zulkar Nine"
                    }
                ],
                "author_detail": {
                    "name": "M S Q Zulkar Nine"
                },
                "author": "M S Q Zulkar Nine",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2505.10806",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05165v1",
                "updated": "2025-09-05T14:58:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    58,
                    24,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T14:58:24Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    58,
                    24,
                    4,
                    248,
                    0
                ],
                "title": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens"
                },
                "summary": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment."
                },
                "authors": [
                    {
                        "name": "Dmitry Akulov"
                    },
                    {
                        "name": "Mohamed Sana"
                    },
                    {
                        "name": "Antonio De Domenico"
                    },
                    {
                        "name": "Tareq Si Salem"
                    },
                    {
                        "name": "Nicola Piovesan"
                    },
                    {
                        "name": "Fadhel Ayed"
                    }
                ],
                "author_detail": {
                    "name": "Fadhel Ayed"
                },
                "author": "Fadhel Ayed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09758v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09758v2",
                "updated": "2025-09-05T10:39:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    10,
                    39,
                    3,
                    4,
                    248,
                    0
                ],
                "published": "2025-06-11T14:03:13Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    3,
                    13,
                    2,
                    162,
                    0
                ],
                "title": "Mainframe-Style Channel Controllers for Modern Disaggregated Memory\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mainframe-Style Channel Controllers for Modern Disaggregated Memory\n  Systems"
                },
                "summary": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs."
                },
                "authors": [
                    {
                        "name": "Zikai Liu"
                    },
                    {
                        "name": "Jasmin Schult"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "arxiv_doi": "10.1145/3725783.3764403",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3725783.3764403",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.09758v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09758v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Camera-ready authors' version for APSys'25",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04377v1",
                "updated": "2025-09-04T16:40:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    40,
                    1,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T16:40:01Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    40,
                    1,
                    3,
                    247,
                    0
                ],
                "title": "PagedEviction: Structured Block-wise KV Cache Pruning for Efficient\n  Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PagedEviction: Structured Block-wise KV Cache Pruning for Efficient\n  Large Language Model Inference"
                },
                "summary": "KV caching significantly improves the efficiency of Large Language Model\n(LLM) inference by storing attention states from previously processed tokens,\nenabling faster generation of subsequent tokens. However, as sequence length\nincreases, the KV cache quickly becomes a major memory bottleneck. To address\nthis, we propose PagedEviction, a novel fine-grained, structured KV cache\npruning strategy that enhances the memory efficiency of vLLM's PagedAttention.\nUnlike existing approaches that rely on attention-based token importance or\nevict tokens across different vLLM pages, PagedEviction introduces an efficient\nblock-wise eviction algorithm tailored for paged memory layouts. Our method\nintegrates seamlessly with PagedAttention without requiring any modifications\nto its CUDA attention kernels. We evaluate PagedEviction across\nLlama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models\non the LongBench benchmark suite, demonstrating improved memory usage with\nbetter accuracy than baselines on long context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV caching significantly improves the efficiency of Large Language Model\n(LLM) inference by storing attention states from previously processed tokens,\nenabling faster generation of subsequent tokens. However, as sequence length\nincreases, the KV cache quickly becomes a major memory bottleneck. To address\nthis, we propose PagedEviction, a novel fine-grained, structured KV cache\npruning strategy that enhances the memory efficiency of vLLM's PagedAttention.\nUnlike existing approaches that rely on attention-based token importance or\nevict tokens across different vLLM pages, PagedEviction introduces an efficient\nblock-wise eviction algorithm tailored for paged memory layouts. Our method\nintegrates seamlessly with PagedAttention without requiring any modifications\nto its CUDA attention kernels. We evaluate PagedEviction across\nLlama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models\non the LongBench benchmark suite, demonstrating improved memory usage with\nbetter accuracy than baselines on long context tasks."
                },
                "authors": [
                    {
                        "name": "Krishna Teja Chitty-Venkata"
                    },
                    {
                        "name": "Jie Ye"
                    },
                    {
                        "name": "Xian-He Sun"
                    },
                    {
                        "name": "Anthony Kougkas"
                    },
                    {
                        "name": "Murali Emani"
                    },
                    {
                        "name": "Venkatram Vishwanath"
                    },
                    {
                        "name": "Bogdan Nicolae"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Nicolae"
                },
                "author": "Bogdan Nicolae",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12084v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12084v2",
                "updated": "2025-09-04T15:21:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    21,
                    11,
                    3,
                    247,
                    0
                ],
                "published": "2025-01-21T12:19:02Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    19,
                    2,
                    1,
                    21,
                    0
                ],
                "title": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis"
                },
                "summary": "This study presents a comprehensive multi-level analysis of the NVIDIA Hopper\nGPU architecture, focusing on its performance characteristics and novel\nfeatures. We benchmark Hopper's memory subsystem, highlighting improvements in\nthe L2 partitioned cache and global memory access compared to Ampere and Ada\nLovelace. The evaluation of Hopper's fourth-generation tensor cores reveals the\nbenefits of FP8 precision and asynchronous wgmma instructions for matrix\noperations. Additionally, we investigate the performance of DPX instructions\nfor dynamic programming, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. Through multi-level evaluation, we discover that the Hopper\narchitecture demonstrates significant acceleration potential in real-world\napplications. For instance, the asynchronous programming model supported by TMA\nachieves a 1.5x speedup in matrix multiplication, FP8 delivers nearly double\nthe performance of FP16, and DPX instructions accelerate a computational\nbiology algorithm by at least 4.75x. Our findings provide actionable insights\nfor optimizing compute-intensive workloads, from AI training to bioinformatics,\non Hopper GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents a comprehensive multi-level analysis of the NVIDIA Hopper\nGPU architecture, focusing on its performance characteristics and novel\nfeatures. We benchmark Hopper's memory subsystem, highlighting improvements in\nthe L2 partitioned cache and global memory access compared to Ampere and Ada\nLovelace. The evaluation of Hopper's fourth-generation tensor cores reveals the\nbenefits of FP8 precision and asynchronous wgmma instructions for matrix\noperations. Additionally, we investigate the performance of DPX instructions\nfor dynamic programming, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. Through multi-level evaluation, we discover that the Hopper\narchitecture demonstrates significant acceleration potential in real-world\napplications. For instance, the asynchronous programming model supported by TMA\nachieves a 1.5x speedup in matrix multiplication, FP8 delivers nearly double\nthe performance of FP16, and DPX instructions accelerate a computational\nbiology algorithm by at least 4.75x. Our findings provide actionable insights\nfor optimizing compute-intensive workloads, from AI training to bioinformatics,\non Hopper GPUs."
                },
                "authors": [
                    {
                        "name": "Weile Luo"
                    },
                    {
                        "name": "Ruibo Fan"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Hongyuan Liu"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2402.13499",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12084v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12084v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08523v2",
                "updated": "2025-09-04T13:14:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    14,
                    33,
                    3,
                    247,
                    0
                ],
                "published": "2025-07-11T12:21:29Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "title": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching"
                },
                "summary": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy."
                },
                "authors": [
                    {
                        "name": "Yilun Wang"
                    },
                    {
                        "name": "Pengfei Chen"
                    },
                    {
                        "name": "Haiyu Huang"
                    },
                    {
                        "name": "Zilong He"
                    },
                    {
                        "name": "Gou Tan"
                    },
                    {
                        "name": "Chuanfu Zhang"
                    },
                    {
                        "name": "Jingkai He"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04185v1",
                "updated": "2025-09-04T13:02:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    2,
                    39,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T13:02:39Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    2,
                    39,
                    3,
                    247,
                    0
                ],
                "title": "Set Block Decoding is a Language Model Inference Accelerator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Set Block Decoding is a Language Model Inference Accelerator"
                },
                "summary": "Autoregressive next token prediction language models offer powerful\ncapabilities but face significant challenges in practical deployment due to the\nhigh computational and memory costs of inference, particularly during the\ndecoding stage. We introduce Set Block Decoding (SBD), a simple and flexible\nparadigm that accelerates generation by integrating standard next token\nprediction (NTP) and masked token prediction (MATP) within a single\narchitecture. SBD allows the model to sample multiple, not necessarily\nconsecutive, future tokens in parallel, a key distinction from previous\nacceleration methods. This flexibility allows the use of advanced solvers from\nthe discrete diffusion literature, offering significant speedups without\nsacrificing accuracy. SBD requires no architectural changes or extra training\nhyperparameters, maintains compatibility with exact KV-caching, and can be\nimplemented by fine-tuning existing next token prediction models. By\nfine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x\nreduction in the number of forward passes required for generation while\nachieving same performance as equivalent NTP training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive next token prediction language models offer powerful\ncapabilities but face significant challenges in practical deployment due to the\nhigh computational and memory costs of inference, particularly during the\ndecoding stage. We introduce Set Block Decoding (SBD), a simple and flexible\nparadigm that accelerates generation by integrating standard next token\nprediction (NTP) and masked token prediction (MATP) within a single\narchitecture. SBD allows the model to sample multiple, not necessarily\nconsecutive, future tokens in parallel, a key distinction from previous\nacceleration methods. This flexibility allows the use of advanced solvers from\nthe discrete diffusion literature, offering significant speedups without\nsacrificing accuracy. SBD requires no architectural changes or extra training\nhyperparameters, maintains compatibility with exact KV-caching, and can be\nimplemented by fine-tuning existing next token prediction models. By\nfine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x\nreduction in the number of forward passes required for generation while\nachieving same performance as equivalent NTP training."
                },
                "authors": [
                    {
                        "name": "Itai Gat"
                    },
                    {
                        "name": "Heli Ben-Hamu"
                    },
                    {
                        "name": "Marton Havasi"
                    },
                    {
                        "name": "Daniel Haziza"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Gabriel Synnaeve"
                    },
                    {
                        "name": "David Lopez-Paz"
                    },
                    {
                        "name": "Brian Karrer"
                    },
                    {
                        "name": "Yaron Lipman"
                    }
                ],
                "author_detail": {
                    "name": "Yaron Lipman"
                },
                "author": "Yaron Lipman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04180v1",
                "updated": "2025-09-04T12:54:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    54,
                    32,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T12:54:32Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    54,
                    32,
                    3,
                    247,
                    0
                ],
                "title": "VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer\n  Vision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer\n  Vision"
                },
                "summary": "AI models rely on annotated data to learn pattern and perform prediction.\nAnnotation is usually a labor-intensive step that require associating labels\nranging from a simple classification label to more complex tasks such as object\ndetection, oriented bounding box estimation, and instance segmentation.\nTraditional tools often require extensive manual input, limiting scalability\nfor large datasets. To address this, we introduce VisioFirm, an open-source web\napplication designed to streamline image labeling through AI-assisted\nautomation. VisioFirm integrates state-of-the-art foundation models into an\ninterface with a filtering pipeline to reduce human-in-the-loop efforts. This\nhybrid approach employs CLIP combined with pre-trained detectors like\nUltralytics models for common classes and zero-shot models such as Grounding\nDINO for custom labels, generating initial annotations with low-confidence\nthresholding to maximize recall. Through this framework, when tested on\nCOCO-type of classes, initial prediction have been proven to be mostly correct\nthough the users can refine these via interactive tools supporting bounding\nboxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has\non-the-fly segmentation powered by Segment Anything accelerated through WebGPU\nfor browser-side efficiency. The tool supports multiple export formats (YOLO,\nCOCO, Pascal VOC, CSV) and operates offline after model caching, enhancing\naccessibility. VisioFirm demonstrates up to 90\\% reduction in manual effort\nthrough benchmarks on diverse datasets, while maintaining high annotation\naccuracy via clustering of connected CLIP-based disambiguate components and\nIoU-graph for redundant detection suppression. VisioFirm can be accessed from\n\\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI models rely on annotated data to learn pattern and perform prediction.\nAnnotation is usually a labor-intensive step that require associating labels\nranging from a simple classification label to more complex tasks such as object\ndetection, oriented bounding box estimation, and instance segmentation.\nTraditional tools often require extensive manual input, limiting scalability\nfor large datasets. To address this, we introduce VisioFirm, an open-source web\napplication designed to streamline image labeling through AI-assisted\nautomation. VisioFirm integrates state-of-the-art foundation models into an\ninterface with a filtering pipeline to reduce human-in-the-loop efforts. This\nhybrid approach employs CLIP combined with pre-trained detectors like\nUltralytics models for common classes and zero-shot models such as Grounding\nDINO for custom labels, generating initial annotations with low-confidence\nthresholding to maximize recall. Through this framework, when tested on\nCOCO-type of classes, initial prediction have been proven to be mostly correct\nthough the users can refine these via interactive tools supporting bounding\nboxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has\non-the-fly segmentation powered by Segment Anything accelerated through WebGPU\nfor browser-side efficiency. The tool supports multiple export formats (YOLO,\nCOCO, Pascal VOC, CSV) and operates offline after model caching, enhancing\naccessibility. VisioFirm demonstrates up to 90\\% reduction in manual effort\nthrough benchmarks on diverse datasets, while maintaining high annotation\naccuracy via clustering of connected CLIP-based disambiguate components and\nIoU-graph for redundant detection suppression. VisioFirm can be accessed from\n\\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}."
                },
                "authors": [
                    {
                        "name": "Safouane El Ghazouali"
                    },
                    {
                        "name": "Umberto Michelucci"
                    }
                ],
                "author_detail": {
                    "name": "Umberto Michelucci"
                },
                "author": "Umberto Michelucci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04010v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04010v1",
                "updated": "2025-09-04T08:41:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    41,
                    6,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T08:41:06Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    41,
                    6,
                    3,
                    247,
                    0
                ],
                "title": "Systematic Timing Leakage Analysis of NIST PQDSS Candidates: Tooling and\n  Lessons Learned",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Timing Leakage Analysis of NIST PQDSS Candidates: Tooling and\n  Lessons Learned"
                },
                "summary": "The PQDSS standardization process requires cryptographic primitives to be\nfree from vulnerabilities, including timing and cache side-channels. Resistance\nto timing leakage is therefore an essential property, and achieving this\ntypically relies on software implementations that follow constant-time\nprinciples. Moreover, ensuring that all implementations are constant-time is\ncrucial for fair performance comparisons, as secure implementations often incur\nadditional overhead. Such analysis also helps identify scheme proposals that\nare inherently difficult to implement in constant time. Because constant-time\nproperties can be broken during compilation, it is often necessary to analyze\nthe compiled binary directly. Since manual binary analysis is extremely\nchallenging, automated analysis becomes highly important. Although several\ntools exist to assist with such analysis, they often have usability limitations\nand are difficult to set up correctly. To support the developers besides the\nNIST committee in verifying candidates, we developed a toolchain that automates\nconfiguration, execution, and result analysis for several widely used\nconstant-time analysis tools. We selected TIMECOP and Binsec/Rel2 to verify\nconstant-time policy compliance at the binary level, and dudect and RTLF to\ndetect side-channel vulnerabilities through statistical analysis of execution\ntime behavior. We demonstrate its effectiveness and practicability by\nevaluating the NIST PQDSS round 1 and round 2 implementations. We reported 26\nissues in total to the respective developers, and 5 of them have already been\nfixed. We also discuss our different findings, as well as the benefits of\nshortcomings of the different tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The PQDSS standardization process requires cryptographic primitives to be\nfree from vulnerabilities, including timing and cache side-channels. Resistance\nto timing leakage is therefore an essential property, and achieving this\ntypically relies on software implementations that follow constant-time\nprinciples. Moreover, ensuring that all implementations are constant-time is\ncrucial for fair performance comparisons, as secure implementations often incur\nadditional overhead. Such analysis also helps identify scheme proposals that\nare inherently difficult to implement in constant time. Because constant-time\nproperties can be broken during compilation, it is often necessary to analyze\nthe compiled binary directly. Since manual binary analysis is extremely\nchallenging, automated analysis becomes highly important. Although several\ntools exist to assist with such analysis, they often have usability limitations\nand are difficult to set up correctly. To support the developers besides the\nNIST committee in verifying candidates, we developed a toolchain that automates\nconfiguration, execution, and result analysis for several widely used\nconstant-time analysis tools. We selected TIMECOP and Binsec/Rel2 to verify\nconstant-time policy compliance at the binary level, and dudect and RTLF to\ndetect side-channel vulnerabilities through statistical analysis of execution\ntime behavior. We demonstrate its effectiveness and practicability by\nevaluating the NIST PQDSS round 1 and round 2 implementations. We reported 26\nissues in total to the respective developers, and 5 of them have already been\nfixed. We also discuss our different findings, as well as the benefits of\nshortcomings of the different tools."
                },
                "authors": [
                    {
                        "name": "Olivier Adjonyo"
                    },
                    {
                        "name": "Sebastien Bardin"
                    },
                    {
                        "name": "Emanuele Bellini"
                    },
                    {
                        "name": "Gilbert Ndollane Dione"
                    },
                    {
                        "name": "Mahmudul Faisal Al Ameen"
                    },
                    {
                        "name": "Robert Merget"
                    },
                    {
                        "name": "Frederic Recoules"
                    },
                    {
                        "name": "Yanis Sellami"
                    }
                ],
                "author_detail": {
                    "name": "Yanis Sellami"
                },
                "author": "Yanis Sellami",
                "arxiv_comment": "20 pages, 1 figure, to be published and presented at Sixth PQC\n  Standardization Conference by NIST, partially supported by the \"France 2030\"\n  government investment plan managed by the French National Research Agency,\n  under the reference ANR-22-PECY-0005",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04010v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12689v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12689v3",
                "updated": "2025-09-04T06:20:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    6,
                    20,
                    55,
                    3,
                    247,
                    0
                ],
                "published": "2025-01-22T07:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    7,
                    52,
                    38,
                    2,
                    22,
                    0
                ],
                "title": "IC-Cache: Efficient Large Language Model Serving via In-context Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IC-Cache: Efficient Large Language Model Serving via In-context Caching"
                },
                "summary": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 70% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge transfer among requests. However, naively caching and reusing past\nresponses leads to a big quality drop. In this paper, we introduce IC-Cache, a\ncaching system that enables live LLM capability augmentation to improve serving\nefficiency: by leveraging historical request-response pairs from larger models\nas in-context examples, IC-Cache empowers small LLMs to imitate and even exceed\nthe compositional abilities (e.g., reasoning) of their larger counterparts,\nenabling selective offloading of requests to reduce cost and latency. Achieving\nthis live augmentation at scale introduces intricate trade-offs between\nresponse quality, latency, and system throughput. For a new request, IC-Cache\nefficiently selects similar, high-utility examples to prepend them to the new\nrequest's input. At scale, it adaptively routes requests across LLMs of varying\ncapabilities, accounting for response quality and serving loads. IC-Cache\nemploys a cost-aware cache replay mechanism that refines example quality\noffline to maximize online cache utility and efficiency. Evaluations on\nmillions of realistic requests demonstrate that IC-Cache improves LLM serving\nthroughput by 1.4-5.9x and reduces latency by 28-71% without hurting response\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 70% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge transfer among requests. However, naively caching and reusing past\nresponses leads to a big quality drop. In this paper, we introduce IC-Cache, a\ncaching system that enables live LLM capability augmentation to improve serving\nefficiency: by leveraging historical request-response pairs from larger models\nas in-context examples, IC-Cache empowers small LLMs to imitate and even exceed\nthe compositional abilities (e.g., reasoning) of their larger counterparts,\nenabling selective offloading of requests to reduce cost and latency. Achieving\nthis live augmentation at scale introduces intricate trade-offs between\nresponse quality, latency, and system throughput. For a new request, IC-Cache\nefficiently selects similar, high-utility examples to prepend them to the new\nrequest's input. At scale, it adaptively routes requests across LLMs of varying\ncapabilities, accounting for response quality and serving loads. IC-Cache\nemploys a cost-aware cache replay mechanism that refines example quality\noffline to maximize online cache utility and efficiency. Evaluations on\nmillions of realistic requests demonstrate that IC-Cache improves LLM serving\nthroughput by 1.4-5.9x and reduces latency by 28-71% without hurting response\nquality."
                },
                "authors": [
                    {
                        "name": "Yifan Yu"
                    },
                    {
                        "name": "Yu Gan"
                    },
                    {
                        "name": "Nikhil Sarda"
                    },
                    {
                        "name": "Lillian Tsai"
                    },
                    {
                        "name": "Jiaming Shen"
                    },
                    {
                        "name": "Yanqi Zhou"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Henry M. Levy"
                    },
                    {
                        "name": "David Culler"
                    }
                ],
                "author_detail": {
                    "name": "David Culler"
                },
                "author": "David Culler",
                "arxiv_doi": "10.1145/3731569.3764829",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3731569.3764829",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.12689v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12689v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01228v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01228v2",
                "updated": "2025-09-03T20:54:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    20,
                    54,
                    57,
                    2,
                    246,
                    0
                ],
                "published": "2024-10-02T04:12:13Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    4,
                    12,
                    13,
                    2,
                    276,
                    0
                ],
                "title": "ConServe: Fine-Grained GPU Harvesting for LLM Online and Offline\n  Co-Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConServe: Fine-Grained GPU Harvesting for LLM Online and Offline\n  Co-Serving"
                },
                "summary": "Large language model (LLM) serving demands low latency and high throughput,\nbut high load variability makes it challenging to achieve high GPU utilization.\nIn this paper, we identify a synergetic but overlooked opportunity to co-serve\nlatency-critical online requests alongside latency-tolerant offline tasks such\nas model benchmarking. While promising, existing serving systems fail to\nco-serve them efficiently, as their coarse-grained resource management at the\nrequest or iteration level cannot harvest millisecond-level GPU idle cycles\nwithout introducing interference that violates online latency objectives.\nConServe is a new LLM co-serving system that achieves high throughput and\nstrong online latency guarantees by managing resources at finer granularities.\nConServe introduces three techniques: (1) a latency-aware token-level scheduler\nthat precisely sizes offline batches and tokens to fit within online latency\nobjectives; (2) sub-iteration, layer-wise preemption that allows offline tasks\nto yield to online load spikes; and (3) incremental KV cache management that\nenables preempting and resuming offline requests at near-zero cost. Evaluations\nwith Llama-3.1 and Qwen-2.5 models on real-world workloads show that ConServe\ndelivers an average of 2.2$\\times$ higher throughput and reduces online serving\ntail latency by 2.9$\\times$ on average compared to state-of-the-art systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) serving demands low latency and high throughput,\nbut high load variability makes it challenging to achieve high GPU utilization.\nIn this paper, we identify a synergetic but overlooked opportunity to co-serve\nlatency-critical online requests alongside latency-tolerant offline tasks such\nas model benchmarking. While promising, existing serving systems fail to\nco-serve them efficiently, as their coarse-grained resource management at the\nrequest or iteration level cannot harvest millisecond-level GPU idle cycles\nwithout introducing interference that violates online latency objectives.\nConServe is a new LLM co-serving system that achieves high throughput and\nstrong online latency guarantees by managing resources at finer granularities.\nConServe introduces three techniques: (1) a latency-aware token-level scheduler\nthat precisely sizes offline batches and tokens to fit within online latency\nobjectives; (2) sub-iteration, layer-wise preemption that allows offline tasks\nto yield to online load spikes; and (3) incremental KV cache management that\nenables preempting and resuming offline requests at near-zero cost. Evaluations\nwith Llama-3.1 and Qwen-2.5 models on real-world workloads show that ConServe\ndelivers an average of 2.2$\\times$ higher throughput and reduces online serving\ntail latency by 2.9$\\times$ on average compared to state-of-the-art systems."
                },
                "authors": [
                    {
                        "name": "Yifan Qiao"
                    },
                    {
                        "name": "Shu Anzai"
                    },
                    {
                        "name": "Shan Yu"
                    },
                    {
                        "name": "Haoran Ma"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Miryung Kim"
                    },
                    {
                        "name": "Yongji Wu"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jiarong Xing"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Harry Xu"
                    }
                ],
                "author_detail": {
                    "name": "Harry Xu"
                },
                "author": "Harry Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01228v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01228v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03394v1",
                "updated": "2025-09-03T15:15:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    15,
                    44,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T15:15:44Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    15,
                    44,
                    2,
                    246,
                    0
                ],
                "title": "CloudFormer: An Attention-based Performance Prediction for Public Clouds\n  with Unknown Workload",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CloudFormer: An Attention-based Performance Prediction for Public Clouds\n  with Unknown Workload"
                },
                "summary": "Cloud platforms are increasingly relied upon to host diverse,\nresource-intensive workloads due to their scalability, flexibility, and\ncost-efficiency. In multi-tenant cloud environments, virtual machines are\nconsolidated on shared physical servers to improve resource utilization. While\nvirtualization guarantees resource partitioning for CPU, memory, and storage,\nit cannot ensure performance isolation. Competition for shared resources such\nas last-level cache, memory bandwidth, and network interfaces often leads to\nsevere performance degradation. Existing management techniques, including VM\nscheduling and resource provisioning, require accurate performance prediction\nto mitigate interference. However, this remains challenging in public clouds\ndue to the black-box nature of VMs and the highly dynamic nature of workloads.\nTo address these limitations, we propose CloudFormer, a dual-branch\nTransformer-based model designed to predict VM performance degradation in\nblack-box environments. CloudFormer jointly models temporal dynamics and\nsystem-level interactions, leveraging 206 system metrics at one-second\nresolution across both static and dynamic scenarios. This design enables the\nmodel to capture transient interference effects and adapt to varying workload\nconditions without scenario-specific tuning. Complementing the methodology, we\nprovide a fine-grained dataset that significantly expands the temporal\nresolution and metric diversity compared to existing benchmarks. Experimental\nresults demonstrate that CloudFormer consistently outperforms state-of-the-art\nbaselines across multiple evaluation metrics, achieving robust generalization\nacross diverse and previously unseen workloads. Notably, CloudFormer attains a\nmean absolute error (MAE) of just 7.8%, representing a substantial improvement\nin predictive accuracy and outperforming existing methods at least by 28%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud platforms are increasingly relied upon to host diverse,\nresource-intensive workloads due to their scalability, flexibility, and\ncost-efficiency. In multi-tenant cloud environments, virtual machines are\nconsolidated on shared physical servers to improve resource utilization. While\nvirtualization guarantees resource partitioning for CPU, memory, and storage,\nit cannot ensure performance isolation. Competition for shared resources such\nas last-level cache, memory bandwidth, and network interfaces often leads to\nsevere performance degradation. Existing management techniques, including VM\nscheduling and resource provisioning, require accurate performance prediction\nto mitigate interference. However, this remains challenging in public clouds\ndue to the black-box nature of VMs and the highly dynamic nature of workloads.\nTo address these limitations, we propose CloudFormer, a dual-branch\nTransformer-based model designed to predict VM performance degradation in\nblack-box environments. CloudFormer jointly models temporal dynamics and\nsystem-level interactions, leveraging 206 system metrics at one-second\nresolution across both static and dynamic scenarios. This design enables the\nmodel to capture transient interference effects and adapt to varying workload\nconditions without scenario-specific tuning. Complementing the methodology, we\nprovide a fine-grained dataset that significantly expands the temporal\nresolution and metric diversity compared to existing benchmarks. Experimental\nresults demonstrate that CloudFormer consistently outperforms state-of-the-art\nbaselines across multiple evaluation metrics, achieving robust generalization\nacross diverse and previously unseen workloads. Notably, CloudFormer attains a\nmean absolute error (MAE) of just 7.8%, representing a substantial improvement\nin predictive accuracy and outperforming existing methods at least by 28%."
                },
                "authors": [
                    {
                        "name": "Amirhossein Shahbazinia"
                    },
                    {
                        "name": "Darong Huang"
                    },
                    {
                        "name": "Luis Costero"
                    },
                    {
                        "name": "David Atienza"
                    }
                ],
                "author_detail": {
                    "name": "David Atienza"
                },
                "author": "David Atienza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00079v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00079v4",
                "updated": "2025-09-03T14:56:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    56,
                    29,
                    2,
                    246,
                    0
                ],
                "published": "2024-06-24T02:05:32Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    2,
                    5,
                    32,
                    0,
                    176,
                    0
                ],
                "title": "Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving"
                },
                "summary": "Mooncake is the serving platform for Kimi, a leading LLM service provided by\nMoonshot AI. It features a KVCache-centric disaggregated architecture that\nseparates the prefill and decoding clusters. It also leverages the\nunderutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a\ndisaggregated cache of KVCache. The core of Mooncake is its KVCache-centric\nscheduler, which balances maximizing overall effective throughput while meeting\nlatency-related Service Level Objectives (SLOs). Unlike traditional studies\nthat assume all requests will be processed, Mooncake faces challenges due to\nhighly overloaded scenarios. To mitigate these, we developed a prediction-based\nearly rejection policy. Experiments show that Mooncake excels in long-context\nscenarios. Compared to the baseline method, Mooncake can achieve up to a 525%\nincrease in throughput in certain simulated scenarios while adhering to SLOs.\nUnder real workloads, Mooncake's innovative architecture enables Kimi to handle\n75% more requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mooncake is the serving platform for Kimi, a leading LLM service provided by\nMoonshot AI. It features a KVCache-centric disaggregated architecture that\nseparates the prefill and decoding clusters. It also leverages the\nunderutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a\ndisaggregated cache of KVCache. The core of Mooncake is its KVCache-centric\nscheduler, which balances maximizing overall effective throughput while meeting\nlatency-related Service Level Objectives (SLOs). Unlike traditional studies\nthat assume all requests will be processed, Mooncake faces challenges due to\nhighly overloaded scenarios. To mitigate these, we developed a prediction-based\nearly rejection policy. Experiments show that Mooncake excels in long-context\nscenarios. Compared to the baseline method, Mooncake can achieve up to a 525%\nincrease in throughput in certain simulated scenarios while adhering to SLOs.\nUnder real workloads, Mooncake's innovative architecture enables Kimi to handle\n75% more requests."
                },
                "authors": [
                    {
                        "name": "Ruoyu Qin"
                    },
                    {
                        "name": "Zheming Li"
                    },
                    {
                        "name": "Weiran He"
                    },
                    {
                        "name": "Mingxing Zhang"
                    },
                    {
                        "name": "Yongwei Wu"
                    },
                    {
                        "name": "Weimin Zheng"
                    },
                    {
                        "name": "Xinran Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xinran Xu"
                },
                "author": "Xinran Xu",
                "arxiv_comment": "23 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00079v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00079v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04416v2",
                "updated": "2025-09-03T14:28:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    28,
                    23,
                    2,
                    246,
                    0
                ],
                "published": "2025-07-06T15:08:49Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    15,
                    8,
                    49,
                    6,
                    187,
                    0
                ],
                "title": "RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based\n  Sequence Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based\n  Sequence Modeling"
                },
                "summary": "Transformers have become the cornerstone of modern large-scale language\nmodels, but their reliance on softmax attention poses a computational\nbottleneck at both training and inference. Recurrent models offer high\nefficiency, but compressing the full sequence into a fixed-size and holistic\nrepresentation suffers from memory degradation in long contexts and limits\nfine-grained retrieval. To address this, we propose RAT, an intermediate design\nthat bridges the efficiency of RNNs and capacity of attention. RAT partitions\nthe input into chunks, applies recurrence within each chunk for local\ndependencies, and softmax-based attention across chunks for long-range\ninteractions. This design mitigates memory degradation and enables direct\naccess to distant tokens, while retaining computational efficiency.\nEmpirically, with a chunk size of 16, the RAT block achieves a 7x improvement\nin training speed with 100K token sequences and 9x in generation at the 4K\nposition, while maintaining similar performance compared to standard attention.\nWe demonstrate this by training 1.3B parameter models from scratch and\nperforming large-scale evaluations, including short- and long-context\nbenchmarks, as well as supervised fine-tuning~(SFT). We further propose a\nhybrid architecture that interleaves RAT with local attention. By combining\nefficient long-range modeling with strong local interactions, this hybrid\ndesign not only improves inference speed and reduces cache memory usage, but\nalso consistently enhances performance and shows the overall best results. Code\nis available at https://github.com/CLAIRE-Labo/RAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have become the cornerstone of modern large-scale language\nmodels, but their reliance on softmax attention poses a computational\nbottleneck at both training and inference. Recurrent models offer high\nefficiency, but compressing the full sequence into a fixed-size and holistic\nrepresentation suffers from memory degradation in long contexts and limits\nfine-grained retrieval. To address this, we propose RAT, an intermediate design\nthat bridges the efficiency of RNNs and capacity of attention. RAT partitions\nthe input into chunks, applies recurrence within each chunk for local\ndependencies, and softmax-based attention across chunks for long-range\ninteractions. This design mitigates memory degradation and enables direct\naccess to distant tokens, while retaining computational efficiency.\nEmpirically, with a chunk size of 16, the RAT block achieves a 7x improvement\nin training speed with 100K token sequences and 9x in generation at the 4K\nposition, while maintaining similar performance compared to standard attention.\nWe demonstrate this by training 1.3B parameter models from scratch and\nperforming large-scale evaluations, including short- and long-context\nbenchmarks, as well as supervised fine-tuning~(SFT). We further propose a\nhybrid architecture that interleaves RAT with local attention. By combining\nefficient long-range modeling with strong local interactions, this hybrid\ndesign not only improves inference speed and reduces cache memory usage, but\nalso consistently enhances performance and shows the overall best results. Code\nis available at https://github.com/CLAIRE-Labo/RAT."
                },
                "authors": [
                    {
                        "name": "Xiuying Wei"
                    },
                    {
                        "name": "Anunay Yadav"
                    },
                    {
                        "name": "Razvan Pascanu"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03560v1",
                "updated": "2025-09-03T11:23:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    11,
                    23,
                    35,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T11:23:35Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    11,
                    23,
                    35,
                    2,
                    246,
                    0
                ],
                "title": "A Cegar-centric Bounded Reachability Analysis for Compositional Affine\n  Hybrid Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Cegar-centric Bounded Reachability Analysis for Compositional Affine\n  Hybrid Systems"
                },
                "summary": "Reachability analysis of compositional hybrid systems, where individual\ncomponents are modeled as hybrid automata, poses unique challenges. In addition\nto preserving the compositional semantics while computing system behaviors,\nalgorithms have to cater to the explosion in the number of locations in the\nparallel product automaton. In this paper, we propose a bounded reachability\nanalysis algorithm for compositional hybrid systems with piecewise affine\ndynamics, based on the principle of counterexample guided abstraction\nrefinement (CEGAR). In particular, the algorithm searches for a counterexample\nin the discrete abstraction of the composition model, without explicitly\ncomputing a product automaton. When a counterexample is discovered in the\nabstraction, its validity is verified by a refinement of the state-space guided\nby the abstract counterexample. The state-space refinement is through a\nsymbolic reachability analysis, particularly using a state-of-the-art algorithm\nwith support functions as the continuous state representation. In addition, the\nalgorithm mixes different semantics of composition with the objective of\nimproved efficiency. Step compositional semantics is followed while exploring\nthe abstract (discrete) state-space, while shallow compositional semantics is\nfollowed during state-space refinement with symbolic reachability analysis.\nOptimizations such as caching the results of the symbolic reachability\nanalysis, which can be later reused, have been proposed. We implement this\nalgorithm in the tool SAT-Reach and demonstrate the scalability benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reachability analysis of compositional hybrid systems, where individual\ncomponents are modeled as hybrid automata, poses unique challenges. In addition\nto preserving the compositional semantics while computing system behaviors,\nalgorithms have to cater to the explosion in the number of locations in the\nparallel product automaton. In this paper, we propose a bounded reachability\nanalysis algorithm for compositional hybrid systems with piecewise affine\ndynamics, based on the principle of counterexample guided abstraction\nrefinement (CEGAR). In particular, the algorithm searches for a counterexample\nin the discrete abstraction of the composition model, without explicitly\ncomputing a product automaton. When a counterexample is discovered in the\nabstraction, its validity is verified by a refinement of the state-space guided\nby the abstract counterexample. The state-space refinement is through a\nsymbolic reachability analysis, particularly using a state-of-the-art algorithm\nwith support functions as the continuous state representation. In addition, the\nalgorithm mixes different semantics of composition with the objective of\nimproved efficiency. Step compositional semantics is followed while exploring\nthe abstract (discrete) state-space, while shallow compositional semantics is\nfollowed during state-space refinement with symbolic reachability analysis.\nOptimizations such as caching the results of the symbolic reachability\nanalysis, which can be later reused, have been proposed. We implement this\nalgorithm in the tool SAT-Reach and demonstrate the scalability benefits."
                },
                "authors": [
                    {
                        "name": "Atanu Kundu"
                    },
                    {
                        "name": "Pratyay Sarkar"
                    },
                    {
                        "name": "Rajarshi Ray"
                    }
                ],
                "author_detail": {
                    "name": "Rajarshi Ray"
                },
                "author": "Rajarshi Ray",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03136v1",
                "updated": "2025-09-03T08:38:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    38,
                    40,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T08:38:40Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    38,
                    40,
                    2,
                    246,
                    0
                ],
                "title": "Adaptive KV-Cache Compression without Manually Setting Budget",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive KV-Cache Compression without Manually Setting Budget"
                },
                "summary": "Large language models (LLMs) inference relies heavily on KV-caches to\naccelerate autoregressive decoding, but the resulting memory footprint grows\nrapidly with sequence length, posing significant efficiency challenges. Current\nKV-cache compression methods suffer from a Procrustes' bed problem: they force\ndiverse workloads into fixed compression ratios, leading to suboptimal resource\nallocation and inference performance. To this end, we present GVote, an\nadaptive KV-cache compression scheme that eliminates manual budget\nspecification while achieving superior accuracy-efficiency trade-offs. GVote\noperates on the principle that the important keys are the aggregation of keys\nrequired by future queries. The method predicts future query attention demands\nby Monte-Carlo style sampling potential queries and aggregating selected keys\nto determine the optimal cache budget without manual specification.\nExperimental evaluation demonstrates GVote's effectiveness across multiple\nbenchmarks, including GSM8K, RULER and Longbench. Compared to baselines, GVote\nexhibits 2$\\times$ memory reduction while the accuracy maintains higher or\ncomparable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) inference relies heavily on KV-caches to\naccelerate autoregressive decoding, but the resulting memory footprint grows\nrapidly with sequence length, posing significant efficiency challenges. Current\nKV-cache compression methods suffer from a Procrustes' bed problem: they force\ndiverse workloads into fixed compression ratios, leading to suboptimal resource\nallocation and inference performance. To this end, we present GVote, an\nadaptive KV-cache compression scheme that eliminates manual budget\nspecification while achieving superior accuracy-efficiency trade-offs. GVote\noperates on the principle that the important keys are the aggregation of keys\nrequired by future queries. The method predicts future query attention demands\nby Monte-Carlo style sampling potential queries and aggregating selected keys\nto determine the optimal cache budget without manual specification.\nExperimental evaluation demonstrates GVote's effectiveness across multiple\nbenchmarks, including GSM8K, RULER and Longbench. Compared to baselines, GVote\nexhibits 2$\\times$ memory reduction while the accuracy maintains higher or\ncomparable."
                },
                "authors": [
                    {
                        "name": "Chenxia Tang"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Liusheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Liusheng Huang"
                },
                "author": "Liusheng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20353v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20353v2",
                "updated": "2025-09-03T06:56:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    6,
                    56,
                    21,
                    2,
                    246,
                    0
                ],
                "published": "2025-05-26T05:58:49Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    58,
                    49,
                    0,
                    146,
                    0
                ],
                "title": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation"
                },
                "summary": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Yanxuan Yu"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Yifan Li"
                    },
                    {
                        "name": "Ben Lengerich"
                    },
                    {
                        "name": "Ying Nian Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ying Nian Wu"
                },
                "author": "Ying Nian Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20353v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20353v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18002v2",
                "updated": "2025-09-02T18:10:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    18,
                    10,
                    0,
                    1,
                    245,
                    0
                ],
                "published": "2024-10-23T16:25:22Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "title": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges"
                },
                "summary": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks."
                },
                "authors": [
                    {
                        "name": "Zifan Zhang"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Hanzhi Yu"
                    },
                    {
                        "name": "Mingzhe Chen"
                    },
                    {
                        "name": "Yuchen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuchen Liu"
                },
                "author": "Yuchen Liu",
                "arxiv_comment": "Under Major Revision in IEEE Network",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02532v1",
                "updated": "2025-09-02T17:35:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    17,
                    35,
                    42,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T17:35:42Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    17,
                    35,
                    42,
                    1,
                    245,
                    0
                ],
                "title": "A Novel Coded Caching Scheme for Partially Cooperative Device-to-Device\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Coded Caching Scheme for Partially Cooperative Device-to-Device\n  Networks"
                },
                "summary": "Device-to-device (D2D) communication is one of the most promising techniques\nfor future wireless cellular communication systems. This paper considers coded\ncaching in a partially cooperative wireless D2D network, where only a subset of\nusers transmit during delivery, while all users request files. The\nnon-transmitting users are referred to as selfish users. All existing schemes\nthat do not require knowledge of the identity of selfish users before content\nplacement are limited to the high-memory regime, particularly when the number\nof selfish users is large. We propose a novel coded caching scheme for a\npartially cooperative D2D network that operates in all feasible memory regimes,\nregardless of the number of selfish users. We also derive a lower bound on the\ntransmission load of a partially cooperative D2D coded caching scheme. Using\nthis bound, the proposed scheme is shown to be optimal in the high-memory\nregime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Device-to-device (D2D) communication is one of the most promising techniques\nfor future wireless cellular communication systems. This paper considers coded\ncaching in a partially cooperative wireless D2D network, where only a subset of\nusers transmit during delivery, while all users request files. The\nnon-transmitting users are referred to as selfish users. All existing schemes\nthat do not require knowledge of the identity of selfish users before content\nplacement are limited to the high-memory regime, particularly when the number\nof selfish users is large. We propose a novel coded caching scheme for a\npartially cooperative D2D network that operates in all feasible memory regimes,\nregardless of the number of selfish users. We also derive a lower bound on the\ntransmission load of a partially cooperative D2D coded caching scheme. Using\nthis bound, the proposed scheme is shown to be optimal in the high-memory\nregime."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "K. K. Krishnan Namboodiri"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "7 pages and 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21625v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21625v5",
                "updated": "2025-09-02T16:39:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    16,
                    39,
                    56,
                    1,
                    245,
                    0
                ],
                "published": "2024-07-31T14:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    17,
                    49,
                    2,
                    213,
                    0
                ],
                "title": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation"
                },
                "summary": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. However, existing Ethernet-based solutions, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilization due to both increasing traffic demands and\nthe expanding scale of datacenter topologies, which also exacerbate network\nfailures. To address these limitations, we propose REPS, a lightweight\ndecentralized per-packet adaptive load balancing algorithm designed to optimize\nnetwork utilization while ensuring rapid recovery from link failures. REPS\nadapts to network conditions by caching good-performing paths. In case of a\nnetwork failure, REPS re-routes traffic away from it in less than 100\nmicroseconds. REPS is designed to be deployed with next-generation out-of-order\ntransports, such as Ultra Ethernet, and uses less than 25 bytes of\nper-connection state regardless of the topology size. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. However, existing Ethernet-based solutions, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilization due to both increasing traffic demands and\nthe expanding scale of datacenter topologies, which also exacerbate network\nfailures. To address these limitations, we propose REPS, a lightweight\ndecentralized per-packet adaptive load balancing algorithm designed to optimize\nnetwork utilization while ensuring rapid recovery from link failures. REPS\nadapts to network conditions by caching good-performing paths. In case of a\nnetwork failure, REPS re-routes traffic away from it in less than 100\nmicroseconds. REPS is designed to be deployed with next-generation out-of-order\ntransports, such as Ultra Ethernet, and uses less than 25 bytes of\nper-connection state regardless of the topology size. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Michael Papamichael"
                    },
                    {
                        "name": "Mohammad Dohadwala"
                    },
                    {
                        "name": "Lukas Gianinazzi"
                    },
                    {
                        "name": "Mikhail Khalilov"
                    },
                    {
                        "name": "Elias Achermann"
                    },
                    {
                        "name": "Daniele De Sensi"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21625v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21625v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02480v1",
                "updated": "2025-09-02T16:30:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    16,
                    30,
                    49,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T16:30:49Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    16,
                    30,
                    49,
                    1,
                    245,
                    0
                ],
                "title": "MLP-Offload: Multi-Level, Multi-Path Offloading for LLM Pre-training to\n  Break the GPU Memory Wall",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLP-Offload: Multi-Level, Multi-Path Offloading for LLM Pre-training to\n  Break the GPU Memory Wall"
                },
                "summary": "Training LLMs larger than the aggregated memory of multiple GPUs is\nincreasingly necessary due to the faster growth of LLM sizes compared to GPU\nmemory. To this end, multi-tier host memory or disk offloading techniques are\nproposed by state of art. Despite advanced asynchronous multi-tier read/write\nstrategies, such offloading strategies result in significant I/O overheads in\nthe critical path of training, resulting in slower iterations. To this end, we\npropose MLP-Offload, a novel multi-level, multi-path offloading engine\nspecifically designed for optimizing LLM training on resource-constrained\nsetups by mitigating I/O bottlenecks. We make several key observations that\ndrive the design of MLP-Offload, such as I/O overheads during the update\ndominate the iteration time; I/O bandwidth of the third-level remote storage\ntier remains unutilized; and, contention due to concurrent offloading amplifies\nI/O bottlenecks. Driven by these insights, we design and implement MLP-Offload\nto offload the optimizer states across multiple tiers in a cache-efficient and\nconcurrency-controlled fashion to mitigate I/O bottlenecks during the backward\nand update phases. Evaluations on models up to 280B parameters shows that\nMLP-Offload achieves 2.5$\\times$ faster iterations compared to the\nstate-of-the-art LLM training runtimes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training LLMs larger than the aggregated memory of multiple GPUs is\nincreasingly necessary due to the faster growth of LLM sizes compared to GPU\nmemory. To this end, multi-tier host memory or disk offloading techniques are\nproposed by state of art. Despite advanced asynchronous multi-tier read/write\nstrategies, such offloading strategies result in significant I/O overheads in\nthe critical path of training, resulting in slower iterations. To this end, we\npropose MLP-Offload, a novel multi-level, multi-path offloading engine\nspecifically designed for optimizing LLM training on resource-constrained\nsetups by mitigating I/O bottlenecks. We make several key observations that\ndrive the design of MLP-Offload, such as I/O overheads during the update\ndominate the iteration time; I/O bandwidth of the third-level remote storage\ntier remains unutilized; and, contention due to concurrent offloading amplifies\nI/O bottlenecks. Driven by these insights, we design and implement MLP-Offload\nto offload the optimizer states across multiple tiers in a cache-efficient and\nconcurrency-controlled fashion to mitigate I/O bottlenecks during the backward\nand update phases. Evaluations on models up to 280B parameters shows that\nMLP-Offload achieves 2.5$\\times$ faster iterations compared to the\nstate-of-the-art LLM training runtimes."
                },
                "authors": [
                    {
                        "name": "Avinash Maurya"
                    },
                    {
                        "name": "M. Mustafa Rafique"
                    },
                    {
                        "name": "Franck Cappello"
                    },
                    {
                        "name": "Bogdan Nicolae"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Nicolae"
                },
                "author": "Bogdan Nicolae",
                "arxiv_doi": "10.1145/3712285.3759864",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3712285.3759864",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.02480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "SC'25: The International Conference for High Performance Computing,\n  Networking, Storage and Analysis",
                "arxiv_journal_ref": "SC'25: The International Conference for High Performance\n  Computing, Networking, Storage and Analysis, 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.0; E.2; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02408v1",
                "updated": "2025-09-02T15:19:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    15,
                    19,
                    6,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T15:19:06Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    15,
                    19,
                    6,
                    1,
                    245,
                    0
                ],
                "title": "Cache Management for Mixture-of-Experts LLMs -- extended version",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Management for Mixture-of-Experts LLMs -- extended version"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na variety of tasks. One of the main challenges towards the successful\ndeployment of LLMs is memory management, since they typically involve billions\nof parameters. To this end, architectures based on Mixture-of-Experts have been\nproposed, which aim to reduce the size of the parameters that are activated\nwhen producing a token. This raises the equally critical issue of efficiently\nmanaging the limited cache of the system, in that frequently used experts\nshould be stored in the fast cache rather than in the slower secondary memory.\n  In this work, we introduce and study a new paging problem that models expert\nmanagement optimization. Our formulation captures both the layered architecture\nof LLMs and the requirement that experts are cached efficiently. We first\npresent lower bounds on the competitive ratio of both deterministic and\nrandomized algorithms, which show that under mild assumptions, LRU-like\npolicies have good theoretical competitive performance. We then propose a\nlayer-based extension of LRU that is tailored to the problem at hand.\n  Extensive simulations on both synthetic datasets and actual traces of MoE\nusage show that our algorithm outperforms policies for the classic paging\nproblem, such as the standard LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities across\na variety of tasks. One of the main challenges towards the successful\ndeployment of LLMs is memory management, since they typically involve billions\nof parameters. To this end, architectures based on Mixture-of-Experts have been\nproposed, which aim to reduce the size of the parameters that are activated\nwhen producing a token. This raises the equally critical issue of efficiently\nmanaging the limited cache of the system, in that frequently used experts\nshould be stored in the fast cache rather than in the slower secondary memory.\n  In this work, we introduce and study a new paging problem that models expert\nmanagement optimization. Our formulation captures both the layered architecture\nof LLMs and the requirement that experts are cached efficiently. We first\npresent lower bounds on the competitive ratio of both deterministic and\nrandomized algorithms, which show that under mild assumptions, LRU-like\npolicies have good theoretical competitive performance. We then propose a\nlayer-based extension of LRU that is tailored to the problem at hand.\n  Extensive simulations on both synthetic datasets and actual traces of MoE\nusage show that our algorithm outperforms policies for the classic paging\nproblem, such as the standard LRU."
                },
                "authors": [
                    {
                        "name": "Spyros Angelopoulos"
                    },
                    {
                        "name": "Loris Marchal"
                    },
                    {
                        "name": "Adrien Obrecht"
                    },
                    {
                        "name": "Bertrand Simon"
                    }
                ],
                "author_detail": {
                    "name": "Bertrand Simon"
                },
                "author": "Bertrand Simon",
                "arxiv_doi": "10.1007/978-3-031-99872-0_2",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-99872-0_2",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.02408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05530v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05530v2",
                "updated": "2025-09-02T13:09:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    13,
                    9,
                    37,
                    1,
                    245,
                    0
                ],
                "published": "2025-03-07T15:54:04Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "title": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing reliance on expensive vector\ndatabase lookups. To scale efficiently, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically skewed MedRAG workload reduces database calls by 78.9% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our work highlights that approximate caching\nis a viable and effective strategy for optimizing RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing reliance on expensive vector\ndatabase lookups. To scale efficiently, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically skewed MedRAG workload reduces database calls by 78.9% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our work highlights that approximate caching\nis a viable and effective strategy for optimizing RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Shai Bergman"
                    },
                    {
                        "name": "Zhang Ji"
                    },
                    {
                        "name": "Anne-Marie Kermarrec"
                    },
                    {
                        "name": "Diana Petrescu"
                    },
                    {
                        "name": "Rafael Pires"
                    },
                    {
                        "name": "Mathis Randl"
                    },
                    {
                        "name": "Martijn de Vos"
                    }
                ],
                "author_detail": {
                    "name": "Martijn de Vos"
                },
                "author": "Martijn de Vos",
                "arxiv_doi": "10.1145/3721146.3721941",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721146.3721941",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.05530v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05530v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02232v1",
                "updated": "2025-09-02T11:58:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    58,
                    6,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T11:58:06Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    58,
                    6,
                    1,
                    245,
                    0
                ],
                "title": "Efficient Geometry Compression and Communication for 3D Gaussian\n  Splatting Point Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Geometry Compression and Communication for 3D Gaussian\n  Splatting Point Clouds"
                },
                "summary": "Storage and transmission challenges in dynamic 3D scene representation based\non the i3DV platform, With increasing scene complexity, the explosive growth of\n3D Gaussian data volume causes excessive storage space occupancy. To address\nthis issue, we propose adopting the AVS PCRM reference software for efficient\ncompression of Gaussian point cloud geometry data. The strategy deeply\nintegrates the advanced encoding capabilities of AVS PCRM into the i3DV\nplatform, forming technical complementarity with the original rate-distortion\noptimization mechanism based on binary hash tables. On one hand, the hash table\nefficiently caches inter-frame Gaussian point transformation relationships,\nwhich allows for high-fidelity transmission within a 40 Mbps bandwidth\nconstraint. On the other hand, AVS PCRM performs precise compression on\ngeometry data. Experimental results demonstrate that the joint framework\nmaintains the advantages of fast rendering and high-quality synthesis in 3D\nGaussian technology while achieving significant 10\\%-25\\% bitrate savings on\nuniversal test sets. It provides a superior rate-distortion tradeoff solution\nfor the storage, transmission, and interaction of 3D volumetric video.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Storage and transmission challenges in dynamic 3D scene representation based\non the i3DV platform, With increasing scene complexity, the explosive growth of\n3D Gaussian data volume causes excessive storage space occupancy. To address\nthis issue, we propose adopting the AVS PCRM reference software for efficient\ncompression of Gaussian point cloud geometry data. The strategy deeply\nintegrates the advanced encoding capabilities of AVS PCRM into the i3DV\nplatform, forming technical complementarity with the original rate-distortion\noptimization mechanism based on binary hash tables. On one hand, the hash table\nefficiently caches inter-frame Gaussian point transformation relationships,\nwhich allows for high-fidelity transmission within a 40 Mbps bandwidth\nconstraint. On the other hand, AVS PCRM performs precise compression on\ngeometry data. Experimental results demonstrate that the joint framework\nmaintains the advantages of fast rendering and high-quality synthesis in 3D\nGaussian technology while achieving significant 10\\%-25\\% bitrate savings on\nuniversal test sets. It provides a superior rate-distortion tradeoff solution\nfor the storage, transmission, and interaction of 3D volumetric video."
                },
                "authors": [
                    {
                        "name": "Liang Xie"
                    },
                    {
                        "name": "Yanting Li"
                    },
                    {
                        "name": "Luyang Tang"
                    },
                    {
                        "name": "Wei Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wei Gao"
                },
                "author": "Wei Gao",
                "arxiv_doi": "10.1145/3680207.3765659",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3680207.3765659",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.02232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages,5 figures",
                "arxiv_journal_ref": "ACM MOBICOM 2025",
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15212v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15212v2",
                "updated": "2025-09-02T11:29:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    29,
                    34,
                    1,
                    245,
                    0
                ],
                "published": "2025-08-21T03:48:28Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    3,
                    48,
                    28,
                    3,
                    233,
                    0
                ],
                "title": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning"
                },
                "summary": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK."
                },
                "authors": [
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Guanchen Li"
                    },
                    {
                        "name": "Xuanwu Yin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Emad Barsoum"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15212v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15212v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02121v1",
                "updated": "2025-09-02T09:17:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    9,
                    17,
                    40,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T09:17:40Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    9,
                    17,
                    40,
                    1,
                    245,
                    0
                ],
                "title": "Batch Query Processing and Optimization for Agentic Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch Query Processing and Optimization for Agentic Workflows"
                },
                "summary": "Large Language Models (LLMs) in agentic workflows combine multi-step\nreasoning, tool use, and collaboration across multiple specialized agents.\nExisting LLM serving engines optimize individual calls in isolation, while\nmulti-agent frameworks focus on orchestration without system-level performance\nplanning. As a result, repeated prompts, overlapping contexts, and concurrent\nexecutions create substantial redundancy and poor GPU utilization, especially\nin batch analytics scenarios. We introduce Halo, a system that brings batch\nquery processing and optimization into agentic LLM workflows. Halo represents\neach workflow as a structured query plan DAG and constructs a consolidated\ngraph for batched queries that exposes shared computation. Guided by a cost\nmodel that jointly considers prefill and decode costs, cache reuse, and GPU\nplacement, Halo performs plan-level optimization to minimize redundant\nexecution. Its runtime integrates adaptive batching, KV-cache sharing and\nmigration, along with compute-communication overlap to maximize hardware\nefficiency. Evaluation across six benchmarks shows that Halo achieves up to\n18.6x speedup for batch inference and 4.7x throughput improvement under online\nserving, scaling to workloads of tens of thousands of queries and complex\ngraphs. These gains are achieved without compromising output quality. By\nunifying query optimization with LLM serving, Halo enables efficient agentic\nworkflows in data analytics and decision-making applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) in agentic workflows combine multi-step\nreasoning, tool use, and collaboration across multiple specialized agents.\nExisting LLM serving engines optimize individual calls in isolation, while\nmulti-agent frameworks focus on orchestration without system-level performance\nplanning. As a result, repeated prompts, overlapping contexts, and concurrent\nexecutions create substantial redundancy and poor GPU utilization, especially\nin batch analytics scenarios. We introduce Halo, a system that brings batch\nquery processing and optimization into agentic LLM workflows. Halo represents\neach workflow as a structured query plan DAG and constructs a consolidated\ngraph for batched queries that exposes shared computation. Guided by a cost\nmodel that jointly considers prefill and decode costs, cache reuse, and GPU\nplacement, Halo performs plan-level optimization to minimize redundant\nexecution. Its runtime integrates adaptive batching, KV-cache sharing and\nmigration, along with compute-communication overlap to maximize hardware\nefficiency. Evaluation across six benchmarks shows that Halo achieves up to\n18.6x speedup for batch inference and 4.7x throughput improvement under online\nserving, scaling to workloads of tens of thousands of queries and complex\ngraphs. These gains are achieved without compromising output quality. By\nunifying query optimization with LLM serving, Halo enables efficient agentic\nworkflows in data analytics and decision-making applications."
                },
                "authors": [
                    {
                        "name": "Junyi Shen"
                    },
                    {
                        "name": "Noppanat Wadlom"
                    },
                    {
                        "name": "Yao Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yao Lu"
                },
                "author": "Yao Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02004v1",
                "updated": "2025-09-02T06:40:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    6,
                    40,
                    45,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T06:40:45Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    6,
                    40,
                    45,
                    1,
                    245,
                    0
                ],
                "title": "Augmented Shuffle Differential Privacy Protocols for Large-Domain\n  Categorical and Key-Value Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmented Shuffle Differential Privacy Protocols for Large-Domain\n  Categorical and Key-Value Data"
                },
                "summary": "Shuffle DP (Differential Privacy) protocols provide high accuracy and privacy\nby introducing a shuffler who randomly shuffles data in a distributed system.\nHowever, most shuffle DP protocols are vulnerable to two attacks: collusion\nattacks by the data collector and users and data poisoning attacks. A recent\nstudy addresses this issue by introducing an augmented shuffle DP protocol,\nwhere users do not add noise and the shuffler performs random sampling and\ndummy data addition. However, it focuses on frequency estimation over\ncategorical data with a small domain and cannot be applied to a large domain\ndue to prohibitively high communication and computational costs.\n  In this paper, we fill this gap by introducing a novel augmented shuffle DP\nprotocol called the FME (Filtering-with-Multiple-Encryption) protocol. Our FME\nprotocol uses a hash function to filter out unpopular items and then accurately\ncalculates frequencies for popular items. To perform this within one round of\ninteraction between users and the shuffler, our protocol carefully communicates\nwithin a system using multiple encryption. We also apply our FME protocol to\nmore advanced KV (Key-Value) statistics estimation with an additional technique\nto reduce bias. For both categorical and KV data, we prove that our protocol\nprovides computational DP, high robustness to the above two attacks, accuracy,\nand efficiency. We show the effectiveness of our proposals through comparisons\nwith twelve existing protocols.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shuffle DP (Differential Privacy) protocols provide high accuracy and privacy\nby introducing a shuffler who randomly shuffles data in a distributed system.\nHowever, most shuffle DP protocols are vulnerable to two attacks: collusion\nattacks by the data collector and users and data poisoning attacks. A recent\nstudy addresses this issue by introducing an augmented shuffle DP protocol,\nwhere users do not add noise and the shuffler performs random sampling and\ndummy data addition. However, it focuses on frequency estimation over\ncategorical data with a small domain and cannot be applied to a large domain\ndue to prohibitively high communication and computational costs.\n  In this paper, we fill this gap by introducing a novel augmented shuffle DP\nprotocol called the FME (Filtering-with-Multiple-Encryption) protocol. Our FME\nprotocol uses a hash function to filter out unpopular items and then accurately\ncalculates frequencies for popular items. To perform this within one round of\ninteraction between users and the shuffler, our protocol carefully communicates\nwithin a system using multiple encryption. We also apply our FME protocol to\nmore advanced KV (Key-Value) statistics estimation with an additional technique\nto reduce bias. For both categorical and KV data, we prove that our protocol\nprovides computational DP, high robustness to the above two attacks, accuracy,\nand efficiency. We show the effectiveness of our proposals through comparisons\nwith twelve existing protocols."
                },
                "authors": [
                    {
                        "name": "Takao Murakami"
                    },
                    {
                        "name": "Yuichi Sei"
                    },
                    {
                        "name": "Reo Eriguchi"
                    }
                ],
                "author_detail": {
                    "name": "Reo Eriguchi"
                },
                "author": "Reo Eriguchi",
                "arxiv_comment": "Full version of the paper accepted at NDSS 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01395v1",
                "updated": "2025-09-01T11:41:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    11,
                    41,
                    10,
                    0,
                    244,
                    0
                ],
                "published": "2025-09-01T11:41:10Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    11,
                    41,
                    10,
                    0,
                    244,
                    0
                ],
                "title": "LLMs cannot spot math errors, even when allowed to peek into the\n  solution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs cannot spot math errors, even when allowed to peek into the\n  solution"
                },
                "summary": "Large language models (LLMs) demonstrate remarkable performance on math word\nproblems, yet they have been shown to struggle with meta-reasoning tasks such\nas identifying errors in student solutions. In this work, we investigate the\nchallenge of locating the first error step in stepwise solutions using two\nerror reasoning datasets: VtG and PRM800K. Our experiments show that\nstate-of-the-art LLMs struggle to locate the first error step in student\nsolutions even when given access to the reference solution. To that end, we\npropose an approach that generates an intermediate corrected student solution,\naligning more closely with the original student's solution, which helps improve\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable performance on math word\nproblems, yet they have been shown to struggle with meta-reasoning tasks such\nas identifying errors in student solutions. In this work, we investigate the\nchallenge of locating the first error step in stepwise solutions using two\nerror reasoning datasets: VtG and PRM800K. Our experiments show that\nstate-of-the-art LLMs struggle to locate the first error step in student\nsolutions even when given access to the reference solution. To that end, we\npropose an approach that generates an intermediate corrected student solution,\naligning more closely with the original student's solution, which helps improve\nperformance."
                },
                "authors": [
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "Accepted to EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15779v2",
                "updated": "2025-09-01T07:26:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    7,
                    26,
                    57,
                    0,
                    244,
                    0
                ],
                "published": "2025-02-17T08:12:34Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    12,
                    34,
                    0,
                    48,
                    0
                ],
                "title": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer"
                },
                "summary": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code is available at https://github.com/ songsm921/RCP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code is available at https://github.com/ songsm921/RCP."
                },
                "authors": [
                    {
                        "name": "Euntae Choi"
                    },
                    {
                        "name": "Sumin Song"
                    },
                    {
                        "name": "Woosang Lim"
                    },
                    {
                        "name": "Sungjoo Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Sungjoo Yoo"
                },
                "author": "Sungjoo Yoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v3",
                "updated": "2025-09-01T03:51:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    51,
                    9,
                    0,
                    244,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01092v1",
                "updated": "2025-09-01T03:31:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    31,
                    44,
                    0,
                    244,
                    0
                ],
                "published": "2025-09-01T03:31:44Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    31,
                    44,
                    0,
                    244,
                    0
                ],
                "title": "REFRAG: Rethinking RAG based Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REFRAG: Rethinking RAG based Decoding"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes."
                },
                "authors": [
                    {
                        "name": "Xiaoqiang Lin"
                    },
                    {
                        "name": "Aritra Ghosh"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    },
                    {
                        "name": "Anshumali Shrivastava"
                    },
                    {
                        "name": "Vijai Mohan"
                    }
                ],
                "author_detail": {
                    "name": "Vijai Mohan"
                },
                "author": "Vijai Mohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06133v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06133v2",
                "updated": "2025-08-31T15:09:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    15,
                    9,
                    36,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-08T08:54:21Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    54,
                    21,
                    4,
                    220,
                    0
                ],
                "title": "LLM Serving Optimization with Variable Prefill and Decode Lengths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Serving Optimization with Variable Prefill and Decode Lengths"
                },
                "summary": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency."
                },
                "authors": [
                    {
                        "name": "Meixuan Wang"
                    },
                    {
                        "name": "Yinyu Ye"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06133v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06133v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00883v1",
                "updated": "2025-08-31T14:51:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    14,
                    51,
                    19,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-31T14:51:19Z",
                "published_parsed": [
                    2025,
                    8,
                    31,
                    14,
                    51,
                    19,
                    6,
                    243,
                    0
                ],
                "title": "Accelerating Latency-Critical Applications with AI-Powered\n  Semi-Automatic Fine-Grained Parallelization on SMT Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Latency-Critical Applications with AI-Powered\n  Semi-Automatic Fine-Grained Parallelization on SMT Processors"
                },
                "summary": "Latency-critical applications tend to show low utilization of functional\nunits due to frequent cache misses and mispredictions during speculative\nexecution in high-performance superscalar processors. However, due to\nsignificant impact on single-thread performance, Simultaneous Multithreading\n(SMT) technology is rarely used with heavy threads of latency-critical\napplications. In this paper, we explore utilization of SMT technology to\nsupport fine-grained parallelization of latency-critical applications.\nFollowing the advancements in the development of Large Language Models (LLMs),\nwe introduce Aira, an AI-powered Parallelization Adviser. To implement Aira, we\nextend AI Coding Agent in Cursor IDE with additional tools connected through\nModel Context Protocol, enabling end-to-end AI Agent for parallelization.\nAdditional connected tools enable LLM-guided hotspot detection, collection of\ndynamic dependencies with Dynamic Binary Instrumentation, SMT-aware performance\nsimulation to estimate performance gains. We apply Aira with Relic parallel\nframework for fine-grained task parallelism on SMT cores to parallelize\nlatency-critical benchmarks representing real-world applications used in\nindustry. We show 17% geomean performance gain from parallelization of\nlatency-critical benchmarks using Aira with Relic framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency-critical applications tend to show low utilization of functional\nunits due to frequent cache misses and mispredictions during speculative\nexecution in high-performance superscalar processors. However, due to\nsignificant impact on single-thread performance, Simultaneous Multithreading\n(SMT) technology is rarely used with heavy threads of latency-critical\napplications. In this paper, we explore utilization of SMT technology to\nsupport fine-grained parallelization of latency-critical applications.\nFollowing the advancements in the development of Large Language Models (LLMs),\nwe introduce Aira, an AI-powered Parallelization Adviser. To implement Aira, we\nextend AI Coding Agent in Cursor IDE with additional tools connected through\nModel Context Protocol, enabling end-to-end AI Agent for parallelization.\nAdditional connected tools enable LLM-guided hotspot detection, collection of\ndynamic dependencies with Dynamic Binary Instrumentation, SMT-aware performance\nsimulation to estimate performance gains. We apply Aira with Relic parallel\nframework for fine-grained task parallelism on SMT cores to parallelize\nlatency-critical benchmarks representing real-world applications used in\nindustry. We show 17% geomean performance gain from parallelization of\nlatency-critical benchmarks using Aira with Relic framework."
                },
                "authors": [
                    {
                        "name": "Denis Los"
                    },
                    {
                        "name": "Igor Petushkov"
                    }
                ],
                "author_detail": {
                    "name": "Igor Petushkov"
                },
                "author": "Igor Petushkov",
                "arxiv_journal_ref": "International Journal of Open Information Technologies, vol. 13,\n  no. 9, pp. 129-134, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10431v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10431v3",
                "updated": "2025-08-31T05:43:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    5,
                    43,
                    55,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-14T08:04:15Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    8,
                    4,
                    15,
                    3,
                    226,
                    0
                ],
                "title": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches"
                },
                "summary": "Recent work presented at USENIX Security 2025 (SEC'25) claims that\noccupancy-based attacks can recover AES keys from the MIRAGE randomized cache.\nIn this paper, we examine these claims and find that they arise from a modeling\nflaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of\nMIRAGE uses a constant seed to initialize the random number generator used for\nglobal evictions in MIRAGE, causing every AES encryption they trace to evict\nthe same deterministic sequence of cache lines. This artificially creates a\nhighly repeatable timing pattern that is not representative of a realistic\nimplementation of MIRAGE, where eviction sequences vary randomly between\nencryptions. When we instead randomize the eviction seed for each run,\nreflecting realistic operation, the correlation between AES T-table accesses\nand attacker runtimes disappears, and the attack fails. These findings show\nthat the reported leakage is an artifact of incorrect modeling, and not an\nactual vulnerability in MIRAGE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work presented at USENIX Security 2025 (SEC'25) claims that\noccupancy-based attacks can recover AES keys from the MIRAGE randomized cache.\nIn this paper, we examine these claims and find that they arise from a modeling\nflaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of\nMIRAGE uses a constant seed to initialize the random number generator used for\nglobal evictions in MIRAGE, causing every AES encryption they trace to evict\nthe same deterministic sequence of cache lines. This artificially creates a\nhighly repeatable timing pattern that is not representative of a realistic\nimplementation of MIRAGE, where eviction sequences vary randomly between\nencryptions. When we instead randomize the eviction seed for each run,\nreflecting realistic operation, the correlation between AES T-table accesses\nand attacker runtimes disappears, and the attack fails. These findings show\nthat the reported leakage is an artifact of incorrect modeling, and not an\nactual vulnerability in MIRAGE."
                },
                "authors": [
                    {
                        "name": "Chris Cao"
                    },
                    {
                        "name": "Gururaj Saileshwar"
                    }
                ],
                "author_detail": {
                    "name": "Gururaj Saileshwar"
                },
                "author": "Gururaj Saileshwar",
                "arxiv_comment": "This version includes updated analysis of RCO Bugs (one additional\n  bug identified). Appendix added with code snippets for bug fixes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10431v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10431v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00625v1",
                "updated": "2025-08-30T22:47:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    22,
                    47,
                    15,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T22:47:15Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    22,
                    47,
                    15,
                    5,
                    242,
                    0
                ],
                "title": "NetGent: Agent-Based Automation of Network Application Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NetGent: Agent-Based Automation of Network Application Workflows"
                },
                "summary": "We present NetGent, an AI-agent framework for automating complex application\nworkflows to generate realistic network traffic datasets. Developing\ngeneralizable ML models for networking requires data collection from network\nenvironments with traffic that results from a diverse set of real-world web\napplications. However, using existing browser automation tools that are\ndiverse, repeatable, realistic, and efficient remains fragile and costly.\nNetGent addresses this challenge by allowing users to specify workflows as\nnatural-language rules that define state-dependent actions. These abstract\nspecifications are compiled into nondeterministic finite automata (NFAs), which\na state synthesis component translates into reusable, executable code. This\ndesign enables deterministic replay, reduces redundant LLM calls through state\ncaching, and adapts quickly when application interfaces change. In experiments,\nNetGent automated more than 50+ workflows spanning video-on-demand streaming,\nlive video streaming, video conferencing, social media, and web scraping,\nproducing realistic traffic traces while remaining robust to UI variability. By\ncombining the flexibility of language-based agents with the reliability of\ncompiled execution, NetGent provides a scalable foundation for generating the\ndiverse, repeatable datasets needed to advance ML in networking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present NetGent, an AI-agent framework for automating complex application\nworkflows to generate realistic network traffic datasets. Developing\ngeneralizable ML models for networking requires data collection from network\nenvironments with traffic that results from a diverse set of real-world web\napplications. However, using existing browser automation tools that are\ndiverse, repeatable, realistic, and efficient remains fragile and costly.\nNetGent addresses this challenge by allowing users to specify workflows as\nnatural-language rules that define state-dependent actions. These abstract\nspecifications are compiled into nondeterministic finite automata (NFAs), which\na state synthesis component translates into reusable, executable code. This\ndesign enables deterministic replay, reduces redundant LLM calls through state\ncaching, and adapts quickly when application interfaces change. In experiments,\nNetGent automated more than 50+ workflows spanning video-on-demand streaming,\nlive video streaming, video conferencing, social media, and web scraping,\nproducing realistic traffic traces while remaining robust to UI variability. By\ncombining the flexibility of language-based agents with the reliability of\ncompiled execution, NetGent provides a scalable foundation for generating the\ndiverse, repeatable datasets needed to advance ML in networking."
                },
                "authors": [
                    {
                        "name": "Jaber Daneshamooz"
                    },
                    {
                        "name": "Eugene Vuong"
                    },
                    {
                        "name": "Laasya Koduru"
                    },
                    {
                        "name": "Sanjay Chandrasekaran"
                    },
                    {
                        "name": "Arpit Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Arpit Gupta"
                },
                "author": "Arpit Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00579v1",
                "updated": "2025-08-30T18:25:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    18,
                    25,
                    19,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T18:25:19Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    18,
                    25,
                    19,
                    5,
                    242,
                    0
                ],
                "title": "KVComp: A High-Performance, LLM-Aware, Lossy Compression Framework for\n  KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVComp: A High-Performance, LLM-Aware, Lossy Compression Framework for\n  KV Cache"
                },
                "summary": "Transformer-based large language models (LLMs) demonstrate impressive\npotential in various practical applications. However, long context inference\nposes a significant challenge due to the enormous memory requirements of the\nkey-value (KV) cache, which can scale to multiple gigabytes as sequence length\nand batch size increase. In this paper, we present KVComp, a generic and\nefficient KV cache management framework optimized for long-text generation that\nsynergistically works with both latency-critical and throughput-critical\ninference systems. KVComp employs novel lossy compression techniques\nspecifically designed for KV cache data characteristics, featuring careful\nco-design of compression algorithms and system architecture. Our approach\nmaintains compatibility with the growing nature of KV cache while preserving\nhigh computational efficiency. Experimental results show that KVComp achieves\non average 47\\% and up to 83\\% higher memory reduction rate compared to\nexisting methods with little/no model accuracy degradation. Furthermore, KVComp\nachieves extremely high execution throughput, effectively reducing\ndecompression overhead and, in some cases, even accelerating the matrix-vector\nmultiplication operation and outperform cuBLAS-based attention kernels with\nless data movement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) demonstrate impressive\npotential in various practical applications. However, long context inference\nposes a significant challenge due to the enormous memory requirements of the\nkey-value (KV) cache, which can scale to multiple gigabytes as sequence length\nand batch size increase. In this paper, we present KVComp, a generic and\nefficient KV cache management framework optimized for long-text generation that\nsynergistically works with both latency-critical and throughput-critical\ninference systems. KVComp employs novel lossy compression techniques\nspecifically designed for KV cache data characteristics, featuring careful\nco-design of compression algorithms and system architecture. Our approach\nmaintains compatibility with the growing nature of KV cache while preserving\nhigh computational efficiency. Experimental results show that KVComp achieves\non average 47\\% and up to 83\\% higher memory reduction rate compared to\nexisting methods with little/no model accuracy degradation. Furthermore, KVComp\nachieves extremely high execution throughput, effectively reducing\ndecompression overhead and, in some cases, even accelerating the matrix-vector\nmultiplication operation and outperform cuBLAS-based attention kernels with\nless data movement."
                },
                "authors": [
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Taolue Yang"
                    },
                    {
                        "name": "Youyuan Liu"
                    },
                    {
                        "name": "Chengming Zhang"
                    },
                    {
                        "name": "Xubin He"
                    },
                    {
                        "name": "Sian Jin"
                    }
                ],
                "author_detail": {
                    "name": "Sian Jin"
                },
                "author": "Sian Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.13777v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.13777v2",
                "updated": "2025-08-30T14:49:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    14,
                    49,
                    34,
                    5,
                    242,
                    0
                ],
                "published": "2023-10-20T19:22:58Z",
                "published_parsed": [
                    2023,
                    10,
                    20,
                    19,
                    22,
                    58,
                    4,
                    293,
                    0
                ],
                "title": "Discrete and Continuous Caching Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete and Continuous Caching Games"
                },
                "summary": "We investigate a discrete search game called the Multiple Caching Game where\nthe searcher's aim is to find all of a set of $d$ treasures hidden in $n$\nlocations. Allowed queries are sets of locations of size $k$, and the searcher\nwins if in all $d$ queries, at least one treasure is hidden in one of the $k$\npicked locations. P\\'alv\\\"olgyi showed that the value of the game is at most\n$\\frac{k^d}{\\binom{n+d-1}{d}}$, with equality for large enough $n$. We\nconjecture the exact cases of equality. We also investigate variants of the\ngame and show an example where their values are different, answering a question\nof P\\'alv\\\"olgyi.\n  This game is closely related to a continuous variant, Alpern's Caching Game,\nbased on which we define other continous variants of the multiple caching game\nand examine their values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate a discrete search game called the Multiple Caching Game where\nthe searcher's aim is to find all of a set of $d$ treasures hidden in $n$\nlocations. Allowed queries are sets of locations of size $k$, and the searcher\nwins if in all $d$ queries, at least one treasure is hidden in one of the $k$\npicked locations. P\\'alv\\\"olgyi showed that the value of the game is at most\n$\\frac{k^d}{\\binom{n+d-1}{d}}$, with equality for large enough $n$. We\nconjecture the exact cases of equality. We also investigate variants of the\ngame and show an example where their values are different, answering a question\nof P\\'alv\\\"olgyi.\n  This game is closely related to a continuous variant, Alpern's Caching Game,\nbased on which we define other continous variants of the multiple caching game\nand examine their values."
                },
                "authors": [
                    {
                        "name": "Áron Jánosik"
                    },
                    {
                        "name": "Csenge Miklós"
                    },
                    {
                        "name": "Dániel G. Simon"
                    },
                    {
                        "name": "Kristóf Zólomy"
                    }
                ],
                "author_detail": {
                    "name": "Kristóf Zólomy"
                },
                "author": "Kristóf Zólomy",
                "arxiv_doi": "10.1142/S0219198925500057",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1142/S0219198925500057",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.13777v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.13777v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "International Game Theory Review 27 (3), 2025",
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "91A05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v3",
                "updated": "2025-08-30T09:35:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    9,
                    35,
                    22,
                    5,
                    242,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "DiffKV: Differentiated Memory Management for Large Language Models with\n  Parallel KV Compaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffKV: Differentiated Memory Management for Large Language Models with\n  Parallel KV Compaction"
                },
                "summary": "Large language models (LLMs) demonstrate remarkable capabilities but face\nsubstantial serving costs due to their high memory demands, with the key-value\n(KV) cache being a primary bottleneck. State-of-the-art KV cache compression\ntechniques, such as quantization and pruning, apply uniform treatment to both\nkeys and values, and discard unimportant tokens entirely, overlooking the\nfine-grained distinctions in the significance of individual KV cache\ncomponents. To address such limitations, we introduce \\textit{DiffKV}, a novel\nframework for efficient KV cache compression that exploits three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. These levels of\ndifferentiation introduce irregular memory usage patterns across different\nrequests and attention heads, posing significant scalability challenges for\nmemory management. To address these challenges, DiffKV proposes an on-GPU\nmemory manager that compacts fragmented free memory list into contiguous\nregions in parallel, effectively translating sparsity in the KV cache into\nperformance gains. We evaluate DiffKV on several mainstream LLMs, including the\nemerging thinking models that generate extended chains of thought. DiffKV is\nable to compress the KV cache by $2.7\\times$ to $5.7\\times$ with near-lossless\naccuracy on complex workloads requiring sophisticated reasoning and\nlong-generation capabilities, and enhances throughput by $1.9\\times$ to\n$5.4\\times$. Source codes of DiffKV are available at\nhttps://github.com/zyqCSL/DiffKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable capabilities but face\nsubstantial serving costs due to their high memory demands, with the key-value\n(KV) cache being a primary bottleneck. State-of-the-art KV cache compression\ntechniques, such as quantization and pruning, apply uniform treatment to both\nkeys and values, and discard unimportant tokens entirely, overlooking the\nfine-grained distinctions in the significance of individual KV cache\ncomponents. To address such limitations, we introduce \\textit{DiffKV}, a novel\nframework for efficient KV cache compression that exploits three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. These levels of\ndifferentiation introduce irregular memory usage patterns across different\nrequests and attention heads, posing significant scalability challenges for\nmemory management. To address these challenges, DiffKV proposes an on-GPU\nmemory manager that compacts fragmented free memory list into contiguous\nregions in parallel, effectively translating sparsity in the KV cache into\nperformance gains. We evaluate DiffKV on several mainstream LLMs, including the\nemerging thinking models that generate extended chains of thought. DiffKV is\nable to compress the KV cache by $2.7\\times$ to $5.7\\times$ with near-lossless\naccuracy on complex workloads requiring sophisticated reasoning and\nlong-generation capabilities, and enhances throughput by $1.9\\times$ to\n$5.4\\times$. Source codes of DiffKV are available at\nhttps://github.com/zyqCSL/DiffKV."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "SOSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00419v1",
                "updated": "2025-08-30T08:57:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    8,
                    57,
                    53,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T08:57:53Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    8,
                    57,
                    53,
                    5,
                    242,
                    0
                ],
                "title": "LightVLM: Acceleraing Large Multimodal Models with Pyramid Token Merging\n  and KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightVLM: Acceleraing Large Multimodal Models with Pyramid Token Merging\n  and KV Cache Compression"
                },
                "summary": "In this paper, we introduce LightVLM, a simple but effective method that can\nbe seamlessly deployed upon existing Vision-Language Models (VLMs) to greatly\naccelerate the inference process in a training-free manner. We divide the\ninference procedure of VLMs into two stages, i.e., encoding and decoding, and\npropose to simultaneously accelerate VLMs in both stages to largely improve\nmodel efficiency. During encoding, we propose pyramid token merging to reduce\ntokens of different LLM layers in a hierarchical manner by finally only keeping\na few dominant tokens to achieve high efficiency. During decoding, aimed at\nreducing the high latency of outputting long sequences, we propose KV Cache\ncompression to remove unnecessary caches to increase the network throughput.\nExperimental results show that LightVLM successfully retains 100% performance\nwhen only preserving 35% image tokens, and maintains around 98% performance\nwhen keeping only 3% image tokens. LightVLM could 2.02$\\times$ the network\nthroughput and reduce the prefilling time by 3.65$\\times$. LightVLM also makes\nlarge VLMs faster again by enabling a heavy model (e.g., InternVL2.5 26B) to\ninfer faster than significantly smaller models (e.g., InternVL2.5 8B),\nhopefully facilitating the real-world deployment. When generating long text\nsequences (e.g., 4096 tokens), LightVLM could reduce the inference time by\n3.21$\\times$, largely outperforming existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce LightVLM, a simple but effective method that can\nbe seamlessly deployed upon existing Vision-Language Models (VLMs) to greatly\naccelerate the inference process in a training-free manner. We divide the\ninference procedure of VLMs into two stages, i.e., encoding and decoding, and\npropose to simultaneously accelerate VLMs in both stages to largely improve\nmodel efficiency. During encoding, we propose pyramid token merging to reduce\ntokens of different LLM layers in a hierarchical manner by finally only keeping\na few dominant tokens to achieve high efficiency. During decoding, aimed at\nreducing the high latency of outputting long sequences, we propose KV Cache\ncompression to remove unnecessary caches to increase the network throughput.\nExperimental results show that LightVLM successfully retains 100% performance\nwhen only preserving 35% image tokens, and maintains around 98% performance\nwhen keeping only 3% image tokens. LightVLM could 2.02$\\times$ the network\nthroughput and reduce the prefilling time by 3.65$\\times$. LightVLM also makes\nlarge VLMs faster again by enabling a heavy model (e.g., InternVL2.5 26B) to\ninfer faster than significantly smaller models (e.g., InternVL2.5 8B),\nhopefully facilitating the real-world deployment. When generating long text\nsequences (e.g., 4096 tokens), LightVLM could reduce the inference time by\n3.21$\\times$, largely outperforming existing methods."
                },
                "authors": [
                    {
                        "name": "Lianyu Hu"
                    },
                    {
                        "name": "Fanhua Shang"
                    },
                    {
                        "name": "Wei Feng"
                    },
                    {
                        "name": "Liang Wan"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wan"
                },
                "author": "Liang Wan",
                "arxiv_comment": "EMNLP2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00388v1",
                "updated": "2025-08-30T06:56:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    6,
                    56,
                    28,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T06:56:28Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    6,
                    56,
                    28,
                    5,
                    242,
                    0
                ],
                "title": "GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV\n  Cache Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV\n  Cache Eviction"
                },
                "summary": "Efficient Key-Value (KV) cache management is essential for processing long\ntext sequences in large language models (LLMs), where memory constraints often\nlimit performance. Conventional KV eviction strategies, such as top-k selection\nbased on attention scores, depend on static heuristics that fail to capture the\nevolving implicit dependencies among tokens during inference. To overcome this,\nwe propose GraphKV, a graph-based framework that redefines token selection for\nKV cache compression. In GraphKV, tokens are modeled as nodes with importance\nscores, and edges represent their similarity relationships. Through a\ndecay-signal-propagation mechanism, token importance is dynamically updated by\npropagating information across the graph, enabling adaptive retention of the\nmost contextually significant tokens. GraphKV can be seamlessly utilized in\nexisting KV cache eviction methods such as SnapKV and PyramidKV in a\nplug-and-play manner. Codes will be released on Github.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Key-Value (KV) cache management is essential for processing long\ntext sequences in large language models (LLMs), where memory constraints often\nlimit performance. Conventional KV eviction strategies, such as top-k selection\nbased on attention scores, depend on static heuristics that fail to capture the\nevolving implicit dependencies among tokens during inference. To overcome this,\nwe propose GraphKV, a graph-based framework that redefines token selection for\nKV cache compression. In GraphKV, tokens are modeled as nodes with importance\nscores, and edges represent their similarity relationships. Through a\ndecay-signal-propagation mechanism, token importance is dynamically updated by\npropagating information across the graph, enabling adaptive retention of the\nmost contextually significant tokens. GraphKV can be seamlessly utilized in\nexisting KV cache eviction methods such as SnapKV and PyramidKV in a\nplug-and-play manner. Codes will be released on Github."
                },
                "authors": [
                    {
                        "name": "Xuelin Li"
                    },
                    {
                        "name": "Xiangqi Jin"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11435v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11435v2",
                "updated": "2025-08-29T20:39:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    20,
                    39,
                    21,
                    4,
                    241,
                    0
                ],
                "published": "2025-04-15T17:51:39Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    51,
                    39,
                    1,
                    105,
                    0
                ],
                "title": "Robust Containment Queries over Collections of Trimmed NURBS Surfaces\n  via Generalized Winding Numbers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Containment Queries over Collections of Trimmed NURBS Surfaces\n  via Generalized Winding Numbers"
                },
                "summary": "We propose a containment query that is robust to the watertightness of\nregions bound by trimmed NURBS surfaces, as this property is difficult to\nguarantee for in-the-wild CAD models. Containment is determined through the\ngeneralized winding number (GWN), a mathematical construction that is\nindifferent to the arrangement of surfaces in the shape. Applying contemporary\ntechniques for the 3D GWN to trimmed NURBS surfaces requires some form of\ngeometric discretization, introducing computational inefficiency to the\nalgorithm and even risking containment misclassifications near the surface. In\ncontrast, our proposed method uses a novel reformulation of the relevant\nsurface integral based on Stokes' theorem, which operates on the boundary and\ntrimming curves as provided through rapidly converging adaptive quadrature.\nBatches of queries are further accelerated by memoizing (i.e.\\ caching and\nreusing) quadrature node positions and tangents as they are evaluated. We\ndemonstrate that our GWN method is robust to complex trimming geometry in a CAD\nmodel, and is accurate up to arbitrary precision at arbitrary distances from\nthe surface. The derived containment query is therefore robust to model\nnon-watertightness while respecting all curved features of the input shape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a containment query that is robust to the watertightness of\nregions bound by trimmed NURBS surfaces, as this property is difficult to\nguarantee for in-the-wild CAD models. Containment is determined through the\ngeneralized winding number (GWN), a mathematical construction that is\nindifferent to the arrangement of surfaces in the shape. Applying contemporary\ntechniques for the 3D GWN to trimmed NURBS surfaces requires some form of\ngeometric discretization, introducing computational inefficiency to the\nalgorithm and even risking containment misclassifications near the surface. In\ncontrast, our proposed method uses a novel reformulation of the relevant\nsurface integral based on Stokes' theorem, which operates on the boundary and\ntrimming curves as provided through rapidly converging adaptive quadrature.\nBatches of queries are further accelerated by memoizing (i.e.\\ caching and\nreusing) quadrature node positions and tangents as they are evaluated. We\ndemonstrate that our GWN method is robust to complex trimming geometry in a CAD\nmodel, and is accurate up to arbitrary precision at arbitrary distances from\nthe surface. The derived containment query is therefore robust to model\nnon-watertightness while respecting all curved features of the input shape."
                },
                "authors": [
                    {
                        "name": "Jacob Spainhour"
                    },
                    {
                        "name": "Kenneth Weiss"
                    }
                ],
                "author_detail": {
                    "name": "Kenneth Weiss"
                },
                "author": "Kenneth Weiss",
                "arxiv_comment": "18 Pages, 16 Figures, 1 Table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11435v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11435v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68U05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.3.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00202v1",
                "updated": "2025-08-29T19:23:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    23,
                    35,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T19:23:35Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    23,
                    35,
                    4,
                    241,
                    0
                ],
                "title": "From TLinFormer to TConstFormer: The Leap to Constant-Time Transformer\n  Attention: Achieving O(1) Computation and O(1) KV Cache during Autoregressive\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From TLinFormer to TConstFormer: The Leap to Constant-Time Transformer\n  Attention: Achieving O(1) Computation and O(1) KV Cache during Autoregressive\n  Inference"
                },
                "summary": "Although the Transformer has become the cornerstone of modern AI, its\nautoregressive inference suffers from a linearly growing KV Cache and a\ncomputational complexity of O(N^2 d), severely hindering its ability to process\nultra-long sequences. To overcome this limitation, this paper introduces the\nTConstFormer architecture, building upon our previous work, TLinFormer.\nTConstFormer employs an innovative periodic state update mechanism to achieve a\ntruly constant-size O(1) KV Cache. The computational complexity of this\nmechanism is also O(1) in an amortized sense: it performs purely constant-time\ncomputations for $k-1$ consecutive steps (e.g., $k=256$) and executes a single\nlinear-time global information synchronization only on the $k$-th step.\nTheoretical calculations and experimental results demonstrate that TConstFormer\nexhibits an overwhelming advantage over baseline models in terms of speed,\nmemory efficiency, and overall performance on long-text inference tasks. This\nbreakthrough paves the way for efficient and robust streaming language model\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although the Transformer has become the cornerstone of modern AI, its\nautoregressive inference suffers from a linearly growing KV Cache and a\ncomputational complexity of O(N^2 d), severely hindering its ability to process\nultra-long sequences. To overcome this limitation, this paper introduces the\nTConstFormer architecture, building upon our previous work, TLinFormer.\nTConstFormer employs an innovative periodic state update mechanism to achieve a\ntruly constant-size O(1) KV Cache. The computational complexity of this\nmechanism is also O(1) in an amortized sense: it performs purely constant-time\ncomputations for $k-1$ consecutive steps (e.g., $k=256$) and executes a single\nlinear-time global information synchronization only on the $k$-th step.\nTheoretical calculations and experimental results demonstrate that TConstFormer\nexhibits an overwhelming advantage over baseline models in terms of speed,\nmemory efficiency, and overall performance on long-text inference tasks. This\nbreakthrough paves the way for efficient and robust streaming language model\napplications."
                },
                "authors": [
                    {
                        "name": "Zhongpan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongpan Tang"
                },
                "author": "Zhongpan Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00195v1",
                "updated": "2025-08-29T19:12:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    12,
                    4,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T19:12:04Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    12,
                    4,
                    4,
                    241,
                    0
                ],
                "title": "Democratizing Agentic AI with Fast Test-Time Scaling on the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Democratizing Agentic AI with Fast Test-Time Scaling on the Edge"
                },
                "summary": "Deploying agentic AI on edge devices is crucial for privacy and\nresponsiveness, but memory constraints typically relegate these systems to\nsmaller Large Language Models (LLMs) with inferior reasoning capabilities.\nTest-Time Scaling (TTS) can bridge this reasoning gap by dedicating more\ncompute during inference, but existing methods incur prohibitive overhead on\nedge hardware. To overcome this, we introduce FlashTTS, a serving system that\nmakes TTS practical for memory-constrained LLM reasoning. FlashTTS introduces\nthree synergistic optimizations: (i) Speculative Beam Extension to mitigate\nsystem stragglers from irregular reasoning paths; (ii) Asymmetric Multi-Model\nMemory Allocation to dynamically balance memory between generation and\nverification; and (iii) Dynamic Prefix-Aware Scheduling to maximize KV-cache\nreuse. Built as a plug-and-play library for vLLM, FlashTTS enables edge LLMs on\na single consumer GPU (24 GB) to match the accuracy and latency of large cloud\nmodels. Our evaluation demonstrates that FlashTTS achieves an average 2.2x\nhigher goodput and reduces latency by 38%-68% compared to a vLLM baseline,\npaving the way for democratized, high-performance agentic AI on edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying agentic AI on edge devices is crucial for privacy and\nresponsiveness, but memory constraints typically relegate these systems to\nsmaller Large Language Models (LLMs) with inferior reasoning capabilities.\nTest-Time Scaling (TTS) can bridge this reasoning gap by dedicating more\ncompute during inference, but existing methods incur prohibitive overhead on\nedge hardware. To overcome this, we introduce FlashTTS, a serving system that\nmakes TTS practical for memory-constrained LLM reasoning. FlashTTS introduces\nthree synergistic optimizations: (i) Speculative Beam Extension to mitigate\nsystem stragglers from irregular reasoning paths; (ii) Asymmetric Multi-Model\nMemory Allocation to dynamically balance memory between generation and\nverification; and (iii) Dynamic Prefix-Aware Scheduling to maximize KV-cache\nreuse. Built as a plug-and-play library for vLLM, FlashTTS enables edge LLMs on\na single consumer GPU (24 GB) to match the accuracy and latency of large cloud\nmodels. Our evaluation demonstrates that FlashTTS achieves an average 2.2x\nhigher goodput and reduces latency by 38%-68% compared to a vLLM baseline,\npaving the way for democratized, high-performance agentic AI on edge devices."
                },
                "authors": [
                    {
                        "name": "Hao Mark Chen"
                    },
                    {
                        "name": "Zhiwen Mo"
                    },
                    {
                        "name": "Guanxi Lu"
                    },
                    {
                        "name": "Shuang Liang"
                    },
                    {
                        "name": "Lingxiao Ma"
                    },
                    {
                        "name": "Wayne Luk"
                    },
                    {
                        "name": "Hongxiang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Hongxiang Fan"
                },
                "author": "Hongxiang Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16217v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16217v2",
                "updated": "2025-08-29T18:45:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    18,
                    45,
                    22,
                    4,
                    241,
                    0
                ],
                "published": "2025-07-22T04:21:03Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    4,
                    21,
                    3,
                    1,
                    203,
                    0
                ],
                "title": "Towards Compute-Optimal Many-Shot In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Compute-Optimal Many-Shot In-Context Learning"
                },
                "summary": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL."
                },
                "authors": [
                    {
                        "name": "Shahriar Golchin"
                    },
                    {
                        "name": "Yanfei Chen"
                    },
                    {
                        "name": "Rujun Han"
                    },
                    {
                        "name": "Manan Gandhi"
                    },
                    {
                        "name": "Tianli Yu"
                    },
                    {
                        "name": "Swaroop Mishra"
                    },
                    {
                        "name": "Mihai Surdeanu"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Chen-Yu Lee"
                    },
                    {
                        "name": "Tomas Pfister"
                    }
                ],
                "author_detail": {
                    "name": "Tomas Pfister"
                },
                "author": "Tomas Pfister",
                "arxiv_comment": "Final version; accepted at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16217v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16217v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05930v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05930v2",
                "updated": "2025-08-29T09:58:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    58,
                    17,
                    4,
                    241,
                    0
                ],
                "published": "2025-06-06T09:55:59Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    9,
                    55,
                    59,
                    4,
                    157,
                    0
                ],
                "title": "Neural Visibility Cache for Real-Time Light Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Visibility Cache for Real-Time Light Sampling"
                },
                "summary": "Direct illumination with many lights is an inherent component of\nphysically-based rendering, remaining challenging, especially in real-time\nscenarios. We propose an online-trained neural cache that stores visibility\nbetween lights and 3D positions. We feed light visibility to weighted reservoir\nsampling (WRS) to sample a light source. The cache is implemented as a\nfully-fused multilayer perceptron (MLP) with multi-resolution hash-grid\nencoding, enabling online training and efficient inference on modern GPUs in\nreal-time frame rates. The cache can be seamlessly integrated into existing\nrendering frameworks and can be used in combination with other real-time\ntechniques such as spatiotemporal reservoir sampling (ReSTIR).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct illumination with many lights is an inherent component of\nphysically-based rendering, remaining challenging, especially in real-time\nscenarios. We propose an online-trained neural cache that stores visibility\nbetween lights and 3D positions. We feed light visibility to weighted reservoir\nsampling (WRS) to sample a light source. The cache is implemented as a\nfully-fused multilayer perceptron (MLP) with multi-resolution hash-grid\nencoding, enabling online training and efficient inference on modern GPUs in\nreal-time frame rates. The cache can be seamlessly integrated into existing\nrendering frameworks and can be used in combination with other real-time\ntechniques such as spatiotemporal reservoir sampling (ReSTIR)."
                },
                "authors": [
                    {
                        "name": "Jakub Bokšanský"
                    },
                    {
                        "name": "Daniel Meister"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Meister"
                },
                "author": "Daniel Meister",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05930v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05930v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15683v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15683v2",
                "updated": "2025-08-29T07:40:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    7,
                    40,
                    34,
                    4,
                    241,
                    0
                ],
                "published": "2025-05-21T15:58:08Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    8,
                    2,
                    141,
                    0
                ],
                "title": "FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting\n  Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting\n  Framework for Large Language Models"
                },
                "summary": "Private data holds promise for improving LLMs due to its high quality, but\nits scattered distribution across data silos and the high computational demands\nof LLMs limit their deployment in federated environments. To address this, the\ntransformer-based federated split models are proposed, which offload most model\nparameters to the server (or distributed clients) while retaining only a small\nportion on the client to ensure data privacy. Despite this design, they still\nface three challenges: 1) Peer-to-peer key encryption struggles to secure\ntransmitted vectors effectively; 2) The auto-regressive nature of LLMs means\nthat federated split learning can only train and infer sequentially, causing\nhigh communication overhead; 3) Fixed partition points lack adaptability to\ndownstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure,\nEfficient, and Adaptive Federated splitting framework based on LLaMA2. First,\nwe inject Gaussian noise into forward-pass hidden states to enable secure\nend-to-end vector transmission. Second, we employ attention-mask compression\nand KV cache collaboration to reduce communication costs, accelerating training\nand inference. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements. Experiments on\nnatural language understanding, summarization, and conversational QA tasks show\nthat FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and\nachieves up to 8x speedups in training and inference. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FedSEA-LLaMA in security and adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private data holds promise for improving LLMs due to its high quality, but\nits scattered distribution across data silos and the high computational demands\nof LLMs limit their deployment in federated environments. To address this, the\ntransformer-based federated split models are proposed, which offload most model\nparameters to the server (or distributed clients) while retaining only a small\nportion on the client to ensure data privacy. Despite this design, they still\nface three challenges: 1) Peer-to-peer key encryption struggles to secure\ntransmitted vectors effectively; 2) The auto-regressive nature of LLMs means\nthat federated split learning can only train and infer sequentially, causing\nhigh communication overhead; 3) Fixed partition points lack adaptability to\ndownstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure,\nEfficient, and Adaptive Federated splitting framework based on LLaMA2. First,\nwe inject Gaussian noise into forward-pass hidden states to enable secure\nend-to-end vector transmission. Second, we employ attention-mask compression\nand KV cache collaboration to reduce communication costs, accelerating training\nand inference. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements. Experiments on\nnatural language understanding, summarization, and conversational QA tasks show\nthat FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and\nachieves up to 8x speedups in training and inference. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FedSEA-LLaMA in security and adaptability."
                },
                "authors": [
                    {
                        "name": "Zishuai Zhang"
                    },
                    {
                        "name": "Hainan zhang"
                    },
                    {
                        "name": "Weihua Li"
                    },
                    {
                        "name": "Qinnan zhang"
                    },
                    {
                        "name": "jin Dong"
                    },
                    {
                        "name": "Yongxin Tong"
                    },
                    {
                        "name": "Zhiming Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Zheng"
                },
                "author": "Zhiming Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15683v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15683v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04467v1",
                "updated": "2025-08-29T02:29:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    2,
                    29,
                    52,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T02:29:52Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    2,
                    29,
                    52,
                    4,
                    241,
                    0
                ],
                "title": "Enhancing LLM Efficiency: Targeted Pruning for Prefill-Decode\n  Disaggregation in Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM Efficiency: Targeted Pruning for Prefill-Decode\n  Disaggregation in Inference"
                },
                "summary": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the default settings, our method\nachieves a 20.56% inference speedup and a 4.95 times reduction in data\ntransmission bandwidth consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the default settings, our method\nachieves a 20.56% inference speedup and a 4.95 times reduction in data\ntransmission bandwidth consumption."
                },
                "authors": [
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Mengsi Lyu"
                    },
                    {
                        "name": "Yulong Ao"
                    },
                    {
                        "name": "Yonghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Yonghua Lin"
                },
                "author": "Yonghua Lin",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20865v1",
                "updated": "2025-08-28T14:58:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    58,
                    47,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T14:58:47Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    58,
                    47,
                    3,
                    240,
                    0
                ],
                "title": "Deep Multiple Quantization Network on Long Behavior Sequence for\n  Click-Through Rate Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Multiple Quantization Network on Long Behavior Sequence for\n  Click-Through Rate Prediction"
                },
                "summary": "In Click-Through Rate (CTR) prediction, the long behavior sequence,\ncomprising the user's long period of historical interactions with items has a\nvital influence on assessing the user's interest in the candidate item.\nExisting approaches strike efficiency and effectiveness through a two-stage\nparadigm: first retrieving hundreds of candidate-related items and then\nextracting interest intensity vector through target attention. However, we\nargue that the discrepancy in target attention's relevance distribution between\nthe retrieved items and the full long behavior sequence inevitably leads to a\nperformance decline. To alleviate the discrepancy, we propose the Deep Multiple\nQuantization Network (DMQN) to process long behavior sequence end-to-end\nthrough compressing the long behavior sequence. Firstly, the entire spectrum of\nlong behavior sequence will be quantized into multiple codeword sequences based\non multiple independent codebooks. Hierarchical Sequential Transduction Unit is\nincorporated to facilitate the interaction of reduced codeword sequences. Then,\nattention between the candidate and multiple codeword sequences will output the\ninterest vector. To enable online serving, intermediate representations of the\ncodeword sequences are cached, significantly reducing latency. Our extensive\nexperiments on both industrial and public datasets confirm the effectiveness\nand efficiency of DMQN. The A/B test in our advertising system shows that DMQN\nimproves CTR by 3.5% and RPM by 2.0%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Click-Through Rate (CTR) prediction, the long behavior sequence,\ncomprising the user's long period of historical interactions with items has a\nvital influence on assessing the user's interest in the candidate item.\nExisting approaches strike efficiency and effectiveness through a two-stage\nparadigm: first retrieving hundreds of candidate-related items and then\nextracting interest intensity vector through target attention. However, we\nargue that the discrepancy in target attention's relevance distribution between\nthe retrieved items and the full long behavior sequence inevitably leads to a\nperformance decline. To alleviate the discrepancy, we propose the Deep Multiple\nQuantization Network (DMQN) to process long behavior sequence end-to-end\nthrough compressing the long behavior sequence. Firstly, the entire spectrum of\nlong behavior sequence will be quantized into multiple codeword sequences based\non multiple independent codebooks. Hierarchical Sequential Transduction Unit is\nincorporated to facilitate the interaction of reduced codeword sequences. Then,\nattention between the candidate and multiple codeword sequences will output the\ninterest vector. To enable online serving, intermediate representations of the\ncodeword sequences are cached, significantly reducing latency. Our extensive\nexperiments on both industrial and public datasets confirm the effectiveness\nand efficiency of DMQN. The A/B test in our advertising system shows that DMQN\nimproves CTR by 3.5% and RPM by 2.0%."
                },
                "authors": [
                    {
                        "name": "Zhuoxing Wei"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Qingchen Xie"
                    }
                ],
                "author_detail": {
                    "name": "Qingchen Xie"
                },
                "author": "Qingchen Xie",
                "arxiv_doi": "10.1145/3726302.3730177",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730177",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.20865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 pages, 1 figures, SIGIR 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18250v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18250v2",
                "updated": "2025-08-28T08:49:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    49,
                    24,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-25T17:41:13Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    41,
                    13,
                    0,
                    237,
                    0
                ],
                "title": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study"
                },
                "summary": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM."
                },
                "authors": [
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Fernando García-Redondo"
                    },
                    {
                        "name": "Arvind Sharma"
                    },
                    {
                        "name": "Van Dai Nguyen"
                    },
                    {
                        "name": "Andrea Fantini"
                    },
                    {
                        "name": "Philippe Matagne"
                    },
                    {
                        "name": "Siddharth Rao"
                    },
                    {
                        "name": "Subhali Subhechha"
                    },
                    {
                        "name": "Lynn Verschueren"
                    },
                    {
                        "name": "Mohammed Aftab Baig"
                    },
                    {
                        "name": "Marie Garcia Bardon"
                    },
                    {
                        "name": "Geert Hellings"
                    }
                ],
                "author_detail": {
                    "name": "Geert Hellings"
                },
                "author": "Geert Hellings",
                "arxiv_comment": "Manuscript submitted to IEEE Trans. Elec. Dev. Work enabled in part\n  by NanoIC pilot line; acquisition and operation jointly funded by Chips Joint\n  Undertaking, through EU's Digital Europe (101183266) and Horizon Europe\n  programs (101183277), as well as by the participating states\n  (Belgium-Flanders, France, Germany, Finland, Ireland, Romania)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18250v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18250v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20524v1",
                "updated": "2025-08-28T08:05:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    5,
                    42,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T08:05:42Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    5,
                    42,
                    3,
                    240,
                    0
                ],
                "title": "Matrixed-Spectrum Decomposition Accelerated Linear Boltzmann Transport\n  Equation Solver for Fast Scatter Correction in Multi-Spectral CT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrixed-Spectrum Decomposition Accelerated Linear Boltzmann Transport\n  Equation Solver for Fast Scatter Correction in Multi-Spectral CT"
                },
                "summary": "X-ray scatter has been a serious concern in computed tomography (CT), leading\nto image artifacts and distortion of CT values. The linear Boltzmann transport\nequation (LBTE) is recognized as a fast and accurate approach for scatter\nestimation. However, for multi-spectral CT, it is cumbersome to compute\nmultiple scattering components for different spectra separately when applying\nLBTE-based scatter correction. In this work, we propose a Matrixed-Spectrum\nDecomposition accelerated LBTE solver (MSD-LBTE) that can be used to compute\nX-ray scatter distributions from CT acquisitions at two or more different\nspectra simultaneously, in a unified framework with no sacrifice in accuracy\nand nearly no increase in computation in theory. First, a matrixed-spectrum\nsolver of LBTE is obtained by introducing an additional label dimension to\nexpand the phase space. Then, we propose a ``spectrum basis'' for LBTE and a\nprinciple of selection of basis using the QR decomposition, along with the\nabove solver to construct the MSD-LBTE. Based on MSD-LBTE, a unified scatter\ncorrection method can be established for multi-spectral CT. We validate the\neffectiveness and accuracy of our method by comparing it with the Monte Carlo\nmethod, including the computational time. We also evaluate the scatter\ncorrection performance using two different phantoms for fast-kV switching based\ndual-energy CT, and using an elliptical phantom in a numerical simulation for\nkV-modulation enabled CT scans, validating that our proposed method can\nsignificantly reduce the computational cost at multiple spectra and effectively\nreduce scatter artifact in reconstructed CT images.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-ray scatter has been a serious concern in computed tomography (CT), leading\nto image artifacts and distortion of CT values. The linear Boltzmann transport\nequation (LBTE) is recognized as a fast and accurate approach for scatter\nestimation. However, for multi-spectral CT, it is cumbersome to compute\nmultiple scattering components for different spectra separately when applying\nLBTE-based scatter correction. In this work, we propose a Matrixed-Spectrum\nDecomposition accelerated LBTE solver (MSD-LBTE) that can be used to compute\nX-ray scatter distributions from CT acquisitions at two or more different\nspectra simultaneously, in a unified framework with no sacrifice in accuracy\nand nearly no increase in computation in theory. First, a matrixed-spectrum\nsolver of LBTE is obtained by introducing an additional label dimension to\nexpand the phase space. Then, we propose a ``spectrum basis'' for LBTE and a\nprinciple of selection of basis using the QR decomposition, along with the\nabove solver to construct the MSD-LBTE. Based on MSD-LBTE, a unified scatter\ncorrection method can be established for multi-spectral CT. We validate the\neffectiveness and accuracy of our method by comparing it with the Monte Carlo\nmethod, including the computational time. We also evaluate the scatter\ncorrection performance using two different phantoms for fast-kV switching based\ndual-energy CT, and using an elliptical phantom in a numerical simulation for\nkV-modulation enabled CT scans, validating that our proposed method can\nsignificantly reduce the computational cost at multiple spectra and effectively\nreduce scatter artifact in reconstructed CT images."
                },
                "authors": [
                    {
                        "name": "Guoxi Zhu"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Zhiqiang Chen"
                    },
                    {
                        "name": "Hewei Gao"
                    }
                ],
                "author_detail": {
                    "name": "Hewei Gao"
                },
                "author": "Hewei Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20433v1",
                "updated": "2025-08-28T05:22:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    5,
                    22,
                    25,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T05:22:25Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    5,
                    22,
                    25,
                    3,
                    240,
                    0
                ],
                "title": "MegaCacheX: Towards Cost-Effective Hierarchical Collaborative Content\n  Caching in Emerging Mega-Constellations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MegaCacheX: Towards Cost-Effective Hierarchical Collaborative Content\n  Caching in Emerging Mega-Constellations"
                },
                "summary": "Significant latency in global content delivery primarily arises from\ninsufficient terrestrial infrastructure. Deploying space-based content delivery\nnetworks within emerging mega-constellations provides an effective means to\nbridge the digital divide. However, space-based caching faces constraints from\nphysical-layer dynamics, including dynamic topologies, time-varying\ninter-satellite link conditions, and limited onboard energy. In addition,\nexisting mechanisms often lack fine-grained content categorization and global\noptimization. This paper proposes MegaCacheX, a cost-effective hierarchical\nframework for collaborative content distribution that achieves\n\"Earth-independence\" by providing cloud services directly from space.\nSpecifically, data centers in Sun-synchronous orbit act as primary content\nsources, while caching nodes in mega-constellations and ground stations\ncollaboratively form a distributed edge layer. MegaCacheX optimizes caching\nstrategies by integrating content popularity, regional user distribution, and\nsatellite trajectory predictions. Multi-tier caching nodes serve as service\nanchors, enabling seamless content delivery with low latency. A prototype\nimplemented on a microservices-based, containerized testbed demonstrates that\nMegaCacheX reduces global content access latency by about 36% compared to\nbaseline approaches, while maintaining cost efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant latency in global content delivery primarily arises from\ninsufficient terrestrial infrastructure. Deploying space-based content delivery\nnetworks within emerging mega-constellations provides an effective means to\nbridge the digital divide. However, space-based caching faces constraints from\nphysical-layer dynamics, including dynamic topologies, time-varying\ninter-satellite link conditions, and limited onboard energy. In addition,\nexisting mechanisms often lack fine-grained content categorization and global\noptimization. This paper proposes MegaCacheX, a cost-effective hierarchical\nframework for collaborative content distribution that achieves\n\"Earth-independence\" by providing cloud services directly from space.\nSpecifically, data centers in Sun-synchronous orbit act as primary content\nsources, while caching nodes in mega-constellations and ground stations\ncollaboratively form a distributed edge layer. MegaCacheX optimizes caching\nstrategies by integrating content popularity, regional user distribution, and\nsatellite trajectory predictions. Multi-tier caching nodes serve as service\nanchors, enabling seamless content delivery with low latency. A prototype\nimplemented on a microservices-based, containerized testbed demonstrates that\nMegaCacheX reduces global content access latency by about 36% compared to\nbaseline approaches, while maintaining cost efficiency."
                },
                "authors": [
                    {
                        "name": "Haoyang Shi"
                    },
                    {
                        "name": "Xing Zhang"
                    },
                    {
                        "name": "Sitong Li"
                    },
                    {
                        "name": "Minghang Li"
                    },
                    {
                        "name": "Xinming Lu"
                    },
                    {
                        "name": "Shaoxiang Xu"
                    },
                    {
                        "name": "Guoquan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guoquan Wang"
                },
                "author": "Guoquan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20424v1",
                "updated": "2025-08-28T04:46:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    46,
                    44,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T04:46:44Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    46,
                    44,
                    3,
                    240,
                    0
                ],
                "title": "Breaking Diffusion with Cache: Exploiting Approximate Caches in\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking Diffusion with Cache: Exploiting Approximate Caches in\n  Diffusion Models"
                },
                "summary": "Diffusion models are a powerful class of generative models that produce\ncontent, such as images, from user prompts, but they are computationally\nintensive. To mitigate this cost, recent academic and industry work has adopted\napproximate caching, which reuses intermediate states from similar prompts in a\ncache. While efficient, this optimization introduces new security risks by\nbreaking isolation among users. This work aims to comprehensively assess new\nsecurity vulnerabilities arising from approximate caching. First, we\ndemonstrate a remote covert channel established with the cache, where a sender\ninjects prompts with special keywords into the cache and a receiver can recover\nthat even after days, to exchange information. Second, we introduce a prompt\nstealing attack using the cache, where an attacker can recover existing cached\nprompts based on cache hit prompts. Finally, we introduce a poisoning attack\nthat embeds the attacker's logos into the previously stolen prompt, to render\nthem in future user prompts that hit the cache. These attacks are all performed\nremotely through the serving system, which indicates severe security\nvulnerabilities in approximate caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models are a powerful class of generative models that produce\ncontent, such as images, from user prompts, but they are computationally\nintensive. To mitigate this cost, recent academic and industry work has adopted\napproximate caching, which reuses intermediate states from similar prompts in a\ncache. While efficient, this optimization introduces new security risks by\nbreaking isolation among users. This work aims to comprehensively assess new\nsecurity vulnerabilities arising from approximate caching. First, we\ndemonstrate a remote covert channel established with the cache, where a sender\ninjects prompts with special keywords into the cache and a receiver can recover\nthat even after days, to exchange information. Second, we introduce a prompt\nstealing attack using the cache, where an attacker can recover existing cached\nprompts based on cache hit prompts. Finally, we introduce a poisoning attack\nthat embeds the attacker's logos into the previously stolen prompt, to render\nthem in future user prompts that hit the cache. These attacks are all performed\nremotely through the serving system, which indicates severe security\nvulnerabilities in approximate caching."
                },
                "authors": [
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Shuncheng Jie"
                    },
                    {
                        "name": "Sihang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Sihang Liu"
                },
                "author": "Sihang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20407v1",
                "updated": "2025-08-28T04:10:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    10,
                    19,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T04:10:19Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    10,
                    19,
                    3,
                    240,
                    0
                ],
                "title": "Rethinking Transformer Connectivity: TLinFormer, A Path to Exact, Full\n  Context-Aware Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Transformer Connectivity: TLinFormer, A Path to Exact, Full\n  Context-Aware Linear Attention"
                },
                "summary": "The Transformer architecture has become a cornerstone of modern artificial\nintelligence, but its core self-attention mechanism suffers from a complexity\nbottleneck that scales quadratically with sequence length, severely limiting\nits application in long-sequence tasks. To address this challenge, existing\nlinear attention methods typically sacrifice model performance by relying on\ndata-agnostic kernel approximations or restrictive context selection. This\npaper returns to the first principles of connectionism, starting from the\ntopological structure of information flow, to introduce a novel linear\nattention architecture-\\textbf{TLinFormer}. By reconfiguring neuron connection\npatterns, TLinFormer achieves strict linear complexity while computing exact\nattention scores and ensuring information flow remains aware of the full\nhistorical context. This design aims to bridge the performance gap prevalent\nbetween existing efficient attention methods and standard attention. Through a\nseries of experiments, we systematically evaluate the performance of TLinFormer\nagainst a standard Transformer baseline on long-sequence inference tasks. The\nresults demonstrate that TLinFormer exhibits overwhelming advantages in key\nmetrics such as \\textbf{inference latency}, \\textbf{KV cache efficiency},\n\\textbf{memory footprint}, and \\textbf{overall speedup}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Transformer architecture has become a cornerstone of modern artificial\nintelligence, but its core self-attention mechanism suffers from a complexity\nbottleneck that scales quadratically with sequence length, severely limiting\nits application in long-sequence tasks. To address this challenge, existing\nlinear attention methods typically sacrifice model performance by relying on\ndata-agnostic kernel approximations or restrictive context selection. This\npaper returns to the first principles of connectionism, starting from the\ntopological structure of information flow, to introduce a novel linear\nattention architecture-\\textbf{TLinFormer}. By reconfiguring neuron connection\npatterns, TLinFormer achieves strict linear complexity while computing exact\nattention scores and ensuring information flow remains aware of the full\nhistorical context. This design aims to bridge the performance gap prevalent\nbetween existing efficient attention methods and standard attention. Through a\nseries of experiments, we systematically evaluate the performance of TLinFormer\nagainst a standard Transformer baseline on long-sequence inference tasks. The\nresults demonstrate that TLinFormer exhibits overwhelming advantages in key\nmetrics such as \\textbf{inference latency}, \\textbf{KV cache efficiency},\n\\textbf{memory footprint}, and \\textbf{overall speedup}."
                },
                "authors": [
                    {
                        "name": "Zhongpan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongpan Tang"
                },
                "author": "Zhongpan Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v5",
                "updated": "2025-08-28T03:57:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    3,
                    57,
                    52,
                    3,
                    240,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from (1) the distribution variance in the LLM activations and (2) the\nsensitivity difference among various kinds of layers. To address these issues,\nwe propose a training-free approach called Activation-aware Singular Value\nDecomposition (ASVD). Specifically, ASVD manages activation outliers by\ntransforming the weight matrix based on the activation distribution. This\ntransformation allows the outliers in the activation matrix to be absorbed into\nthe transformed weight matrix, thereby enhancing decomposition accuracy.\nAdditionally, we propose an efficient iterative calibration process to optimize\nlayer-specific decomposition by addressing the varying sensitivity of different\nLLM layers. In this way, ASVD can compress a network by 10%-30%. Based on the\nsuccess of the low-rank decomposition of projection matrices in the\nself-attention module, we further introduce ASVD to compress the KV cache. By\nreducing the channel dimension of KV activations, memory requirements for KV\ncache can be largely reduced. ASVD can further achieve 50% KV cache reductions\nwithout performance drop in a training-free manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from (1) the distribution variance in the LLM activations and (2) the\nsensitivity difference among various kinds of layers. To address these issues,\nwe propose a training-free approach called Activation-aware Singular Value\nDecomposition (ASVD). Specifically, ASVD manages activation outliers by\ntransforming the weight matrix based on the activation distribution. This\ntransformation allows the outliers in the activation matrix to be absorbed into\nthe transformed weight matrix, thereby enhancing decomposition accuracy.\nAdditionally, we propose an efficient iterative calibration process to optimize\nlayer-specific decomposition by addressing the varying sensitivity of different\nLLM layers. In this way, ASVD can compress a network by 10%-30%. Based on the\nsuccess of the low-rank decomposition of projection matrices in the\nself-attention module, we further introduce ASVD to compress the KV cache. By\nreducing the channel dimension of KV activations, memory requirements for KV\ncache can be largely reduced. ASVD can further achieve 50% KV cache reductions\nwithout performance drop in a training-free manner."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09888v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09888v2",
                "updated": "2025-08-28T01:40:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    1,
                    40,
                    30,
                    3,
                    240,
                    0
                ],
                "published": "2025-02-14T03:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    3,
                    25,
                    9,
                    4,
                    45,
                    0
                ],
                "title": "Climber: Toward Efficient Scaling Laws for Large Recommendation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Climber: Toward Efficient Scaling Laws for Large Recommendation Models"
                },
                "summary": "Transformer-based generative models have achieved remarkable success across\ndomains with various scaling law manifestations. However, our extensive\nexperiments reveal persistent challenges when applying Transformer to\nrecommendation systems: (1) Transformer scaling is not ideal with increased\ncomputational resources, due to structural incompatibilities with\nrecommendation-specific features such as multi-source data heterogeneity; (2)\ncritical online inference latency constraints (tens of milliseconds) that\nintensify with longer user behavior sequences and growing computational\ndemands. We propose Climber, an efficient recommendation framework comprising\ntwo synergistic components: the model architecture for efficient scaling and\nthe co-designed acceleration techniques. Our proposed model adopts two core\ninnovations: (1) multi-scale sequence extraction that achieves a time\ncomplexity reduction by a constant factor, enabling more efficient scaling with\nsequence length; (2) dynamic temperature modulation adapting attention\ndistributions to the multi-scenario and multi-behavior patterns. Complemented\nby acceleration techniques, Climber achieves a 5.15$\\times$ throughput gain\nwithout performance degradation by adopting a \"single user, multiple item\"\nbatched processing and memory-efficient Key-Value caching. Comprehensive\noffline experiments on multiple datasets validate that Climber exhibits a more\nideal scaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19\\% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based generative models have achieved remarkable success across\ndomains with various scaling law manifestations. However, our extensive\nexperiments reveal persistent challenges when applying Transformer to\nrecommendation systems: (1) Transformer scaling is not ideal with increased\ncomputational resources, due to structural incompatibilities with\nrecommendation-specific features such as multi-source data heterogeneity; (2)\ncritical online inference latency constraints (tens of milliseconds) that\nintensify with longer user behavior sequences and growing computational\ndemands. We propose Climber, an efficient recommendation framework comprising\ntwo synergistic components: the model architecture for efficient scaling and\nthe co-designed acceleration techniques. Our proposed model adopts two core\ninnovations: (1) multi-scale sequence extraction that achieves a time\ncomplexity reduction by a constant factor, enabling more efficient scaling with\nsequence length; (2) dynamic temperature modulation adapting attention\ndistributions to the multi-scenario and multi-behavior patterns. Complemented\nby acceleration techniques, Climber achieves a 5.15$\\times$ throughput gain\nwithout performance degradation by adopting a \"single user, multiple item\"\nbatched processing and memory-efficient Key-Value caching. Comprehensive\noffline experiments on multiple datasets validate that Climber exhibits a more\nideal scaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19\\% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily."
                },
                "authors": [
                    {
                        "name": "Songpei Xu"
                    },
                    {
                        "name": "Shijia Wang"
                    },
                    {
                        "name": "Da Guo"
                    },
                    {
                        "name": "Xianwen Guo"
                    },
                    {
                        "name": "Qiang Xiao"
                    },
                    {
                        "name": "Bin Huang"
                    },
                    {
                        "name": "Guanlin Wu"
                    },
                    {
                        "name": "Chuanjiang Luo"
                    }
                ],
                "author_detail": {
                    "name": "Chuanjiang Luo"
                },
                "author": "Chuanjiang Luo",
                "arxiv_doi": "10.1145/3746252.3761561",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746252.3761561",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.09888v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09888v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00105v1",
                "updated": "2025-08-28T00:46:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    0,
                    46,
                    51,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T00:46:51Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    0,
                    46,
                    51,
                    3,
                    240,
                    0
                ],
                "title": "AdaptCache: KV Cache Native Storage Hierarchy for Low-Delay and\n  High-Quality Language Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaptCache: KV Cache Native Storage Hierarchy for Low-Delay and\n  High-Quality Language Model Serving"
                },
                "summary": "Large language model (LLM) applications often reuse previously processed\ncontext, such as chat history and documents, which introduces significant\nredundant computation. Existing LLM serving systems address such redundant\ncomputation by storing the KV caches of processed context and loading the\ncorresponding KV cache when a new request reuses the context. Further, as these\nLLM applications scale, the total size of KV caches becomes excessively large\nand requires both DRAM and SSD for full storage.\n  However, prior work that stores KV caches in DRAM and SSD suffers from high\nloading delays, as most KV cache hits come from SSD, which is slow to load. To\nincrease the KV cache hit rate on DRAM, we identify lossy KV cache compression\nas a promising approach. We design a lossy compression system that decides the\ncompression algorithm, compression rate and device placement for each KV cache\nentry to maximise DRAM hits and minimise loading delay without significantly\ndegrading generation quality. Compared to various static compression baselines\nacross three tasks, our system AdaptCache achieves 1.43--2.4 x delay savings at\nthe same quality and 6--55% quality improvements at the same delay.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) applications often reuse previously processed\ncontext, such as chat history and documents, which introduces significant\nredundant computation. Existing LLM serving systems address such redundant\ncomputation by storing the KV caches of processed context and loading the\ncorresponding KV cache when a new request reuses the context. Further, as these\nLLM applications scale, the total size of KV caches becomes excessively large\nand requires both DRAM and SSD for full storage.\n  However, prior work that stores KV caches in DRAM and SSD suffers from high\nloading delays, as most KV cache hits come from SSD, which is slow to load. To\nincrease the KV cache hit rate on DRAM, we identify lossy KV cache compression\nas a promising approach. We design a lossy compression system that decides the\ncompression algorithm, compression rate and device placement for each KV cache\nentry to maximise DRAM hits and minimise loading delay without significantly\ndegrading generation quality. Compared to various static compression baselines\nacross three tasks, our system AdaptCache achieves 1.43--2.4 x delay savings at\nthe same quality and 6--55% quality improvements at the same delay."
                },
                "authors": [
                    {
                        "name": "Shaoting Feng"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Siddhant Ray"
                    },
                    {
                        "name": "Samuel Shen"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Ganesh Ananthanarayanan"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20272v1",
                "updated": "2025-08-27T21:05:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    21,
                    5,
                    5,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T21:05:05Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    21,
                    5,
                    5,
                    2,
                    239,
                    0
                ],
                "title": "DRR-MDPF: A Queue Management Strategy Based on Dynamic Resource\n  Allocation and Markov Decision Process in Named Data Networking (NDN)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRR-MDPF: A Queue Management Strategy Based on Dynamic Resource\n  Allocation and Markov Decision Process in Named Data Networking (NDN)"
                },
                "summary": "Named Data Networking (NDN) represents a transformative shift in network\narchitecture, prioritizing content names over host addresses to enhance data\ndissemination. Efficient queue and resource management are critical to NDN\nperformance, especially under dynamic and high-traffic conditions. This paper\nintroduces DRR-MDPF, a novel hybrid strategy that integrates the Markov\nDecision Process Forwarding (MDPF) model with the Deficit Round Robin (DRR)\nalgorithm. MDPF enables routers to intelligently predict optimal forwarding\ndecisions based on key metrics such as bandwidth, delay, and the number of\nunsatisfied Interests, while DRR ensures fair and adaptive bandwidth allocation\namong competing data flows. The proposed method models each router as a\nlearning agent capable of adjusting its strategies through continuous feedback\nand probabilistic updates. Simulation results using ndnSIM demonstrate that\nDRR-MDPF significantly outperforms state-of-the-art strategies including SAF,\nRFA, SMDPF, and LA-MDPF across various metrics such as throughput, Interest\nSatisfaction Rate (ISR), packet drop rate, content retrieval time, and load\nbalancing. Notably, DRR-MDPF maintains robustness under limited cache sizes and\nheavy traffic, offering enhanced adaptability and lower computational\ncomplexity due to its single-path routing design. Furthermore, its multi-metric\ndecision-making capability enables more accurate interface selection, leading\nto optimized network performance. Overall, DRR-MDPF serves as an intelligent,\nadaptive, and scalable queue management solution for NDN, effectively\naddressing core challenges such as resource allocation, congestion control, and\nroute optimization in dynamic networking environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Data Networking (NDN) represents a transformative shift in network\narchitecture, prioritizing content names over host addresses to enhance data\ndissemination. Efficient queue and resource management are critical to NDN\nperformance, especially under dynamic and high-traffic conditions. This paper\nintroduces DRR-MDPF, a novel hybrid strategy that integrates the Markov\nDecision Process Forwarding (MDPF) model with the Deficit Round Robin (DRR)\nalgorithm. MDPF enables routers to intelligently predict optimal forwarding\ndecisions based on key metrics such as bandwidth, delay, and the number of\nunsatisfied Interests, while DRR ensures fair and adaptive bandwidth allocation\namong competing data flows. The proposed method models each router as a\nlearning agent capable of adjusting its strategies through continuous feedback\nand probabilistic updates. Simulation results using ndnSIM demonstrate that\nDRR-MDPF significantly outperforms state-of-the-art strategies including SAF,\nRFA, SMDPF, and LA-MDPF across various metrics such as throughput, Interest\nSatisfaction Rate (ISR), packet drop rate, content retrieval time, and load\nbalancing. Notably, DRR-MDPF maintains robustness under limited cache sizes and\nheavy traffic, offering enhanced adaptability and lower computational\ncomplexity due to its single-path routing design. Furthermore, its multi-metric\ndecision-making capability enables more accurate interface selection, leading\nto optimized network performance. Overall, DRR-MDPF serves as an intelligent,\nadaptive, and scalable queue management solution for NDN, effectively\naddressing core challenges such as resource allocation, congestion control, and\nroute optimization in dynamic networking environments."
                },
                "authors": [
                    {
                        "name": "Fatemeh Roshanzadeh"
                    },
                    {
                        "name": "Hamid Barati"
                    },
                    {
                        "name": "Ali Barati"
                    }
                ],
                "author_detail": {
                    "name": "Ali Barati"
                },
                "author": "Ali Barati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20253v1",
                "updated": "2025-08-27T20:18:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    20,
                    18,
                    37,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T20:18:37Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    20,
                    18,
                    37,
                    2,
                    239,
                    0
                ],
                "title": "SpeedMalloc: Improving Multi-threaded Applications via a Lightweight\n  Core for Memory Allocation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeedMalloc: Improving Multi-threaded Applications via a Lightweight\n  Core for Memory Allocation"
                },
                "summary": "Memory allocation, though constituting only a small portion of the executed\ncode, can have a \"butterfly effect\" on overall program performance, leading to\nsignificant and far-reaching impacts. Despite accounting for just approximately\n5% of total instructions, memory allocation can result in up to a 2.7x\nperformance variation depending on the allocator used. This effect arises from\nthe complexity of memory allocation in modern multi-threaded multi-core\nsystems, where allocator metadata becomes intertwined with user data, leading\nto cache pollution or increased cross-thread synchronization overhead.\nOffloading memory allocators to accelerators, e.g., Mallacc and Memento, is a\npotential direction to improve the allocator performance and mitigate cache\npollution. However, these accelerators currently have limited support for\nmulti-threaded applications, and synchronization between cores and accelerators\nremains a significant challenge.\n  We present SpeedMalloc, using a lightweight support-core to process memory\nallocation tasks in multi-threaded applications. The support-core is a\nlightweight programmable processor with efficient cross-core data\nsynchronization and houses all allocator metadata in its own caches. This\ndesign minimizes cache conflicts with user data and eliminates the need for\ncross-core metadata synchronization. In addition, using a general-purpose core\ninstead of domain-specific accelerators makes SpeedMalloc capable of adopting\nnew allocator designs. We compare SpeedMalloc with state-of-the-art software\nand hardware allocators, including Jemalloc, TCMalloc, Mimalloc, Mallacc, and\nMemento. SpeedMalloc achieves 1.75x, 1.18x, 1.15x, 1.23x, and 1.18x speedups on\nmultithreaded workloads over these five allocators, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory allocation, though constituting only a small portion of the executed\ncode, can have a \"butterfly effect\" on overall program performance, leading to\nsignificant and far-reaching impacts. Despite accounting for just approximately\n5% of total instructions, memory allocation can result in up to a 2.7x\nperformance variation depending on the allocator used. This effect arises from\nthe complexity of memory allocation in modern multi-threaded multi-core\nsystems, where allocator metadata becomes intertwined with user data, leading\nto cache pollution or increased cross-thread synchronization overhead.\nOffloading memory allocators to accelerators, e.g., Mallacc and Memento, is a\npotential direction to improve the allocator performance and mitigate cache\npollution. However, these accelerators currently have limited support for\nmulti-threaded applications, and synchronization between cores and accelerators\nremains a significant challenge.\n  We present SpeedMalloc, using a lightweight support-core to process memory\nallocation tasks in multi-threaded applications. The support-core is a\nlightweight programmable processor with efficient cross-core data\nsynchronization and houses all allocator metadata in its own caches. This\ndesign minimizes cache conflicts with user data and eliminates the need for\ncross-core metadata synchronization. In addition, using a general-purpose core\ninstead of domain-specific accelerators makes SpeedMalloc capable of adopting\nnew allocator designs. We compare SpeedMalloc with state-of-the-art software\nand hardware allocators, including Jemalloc, TCMalloc, Mimalloc, Mallacc, and\nMemento. SpeedMalloc achieves 1.75x, 1.18x, 1.15x, 1.23x, and 1.18x speedups on\nmultithreaded workloads over these five allocators, respectively."
                },
                "authors": [
                    {
                        "name": "Ruihao Li"
                    },
                    {
                        "name": "Qinzhe Wu"
                    },
                    {
                        "name": "Krishna Kavi"
                    },
                    {
                        "name": "Gayatri Mehta"
                    },
                    {
                        "name": "Jonathan C. Beard"
                    },
                    {
                        "name": "Neeraja J. Yadwadkar"
                    },
                    {
                        "name": "Lizy K. John"
                    }
                ],
                "author_detail": {
                    "name": "Lizy K. John"
                },
                "author": "Lizy K. John",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00100v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00100v1",
                "updated": "2025-08-27T17:45:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    17,
                    45,
                    16,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T17:45:16Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    17,
                    45,
                    16,
                    2,
                    239,
                    0
                ],
                "title": "MODE: Mixture of Document Experts for RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MODE: Mixture of Document Experts for RAG"
                },
                "summary": "Retrieval-Augmented Generation (RAG) often relies on large vector databases\nand cross-encoders tuned for large-scale corpora, which can be excessive for\nsmall, domain-specific collections. We present MODE (Mixture of Document\nExperts), a lightweight alternative that replaces fine-grained nearest-neighbor\nsearch with cluster-and-route retrieval. Documents are embedded, grouped into\nsemantically coherent clusters, and represented by cached centroids. At query\ntime, we route to the top centroid(s) and retrieve context only within those\nclusters, eliminating external vector-database infrastructure and reranking\nwhile keeping latency low. On HotpotQA and SQuAD corpora with 100-500 chunks,\nMODE matches or exceeds a dense-retrieval baseline in answer quality while\nreducing end-to-end retrieval time. Ablations show that cluster granularity and\nmulti-cluster routing control the recall/precision trade-off, and that tighter\nclusters improve downstream accuracy. MODE offers a practical recipe for small\nand medium corpora where simplicity, speed, and topical focus matter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) often relies on large vector databases\nand cross-encoders tuned for large-scale corpora, which can be excessive for\nsmall, domain-specific collections. We present MODE (Mixture of Document\nExperts), a lightweight alternative that replaces fine-grained nearest-neighbor\nsearch with cluster-and-route retrieval. Documents are embedded, grouped into\nsemantically coherent clusters, and represented by cached centroids. At query\ntime, we route to the top centroid(s) and retrieve context only within those\nclusters, eliminating external vector-database infrastructure and reranking\nwhile keeping latency low. On HotpotQA and SQuAD corpora with 100-500 chunks,\nMODE matches or exceeds a dense-retrieval baseline in answer quality while\nreducing end-to-end retrieval time. Ablations show that cluster granularity and\nmulti-cluster routing control the recall/precision trade-off, and that tighter\nclusters improve downstream accuracy. MODE offers a practical recipe for small\nand medium corpora where simplicity, speed, and topical focus matter."
                },
                "authors": [
                    {
                        "name": "Rahul Anand"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Anand"
                },
                "author": "Rahul Anand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00100v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00100v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13575v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13575v3",
                "updated": "2025-08-27T16:34:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    16,
                    34,
                    47,
                    2,
                    239,
                    0
                ],
                "published": "2025-07-17T23:37:19Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    23,
                    37,
                    19,
                    3,
                    198,
                    0
                ],
                "title": "Apple Intelligence Foundation Language Models: Tech Report 2025",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apple Intelligence Foundation Language Models: Tech Report 2025"
                },
                "summary": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute."
                },
                "authors": [
                    {
                        "name": "Ethan Li"
                    },
                    {
                        "name": "Anders Boesen Lindbo Larsen"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Xiyou Zhou"
                    },
                    {
                        "name": "Jun Qin"
                    },
                    {
                        "name": "Dian Ang Yap"
                    },
                    {
                        "name": "Narendran Raghavan"
                    },
                    {
                        "name": "Xuankai Chang"
                    },
                    {
                        "name": "Margit Bowler"
                    },
                    {
                        "name": "Eray Yildiz"
                    },
                    {
                        "name": "John Peebles"
                    },
                    {
                        "name": "Hannah Gillis Coleman"
                    },
                    {
                        "name": "Matteo Ronchi"
                    },
                    {
                        "name": "Peter Gray"
                    },
                    {
                        "name": "Keen You"
                    },
                    {
                        "name": "Anthony Spalvieri-Kruse"
                    },
                    {
                        "name": "Ruoming Pang"
                    },
                    {
                        "name": "Reed Li"
                    },
                    {
                        "name": "Yuli Yang"
                    },
                    {
                        "name": "Emad Soroush"
                    },
                    {
                        "name": "Zhiyun Lu"
                    },
                    {
                        "name": "Crystal Xiao"
                    },
                    {
                        "name": "Rong Situ"
                    },
                    {
                        "name": "Jordan Huffaker"
                    },
                    {
                        "name": "David Griffiths"
                    },
                    {
                        "name": "Zaid Ahmed"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Daniel Parilla"
                    },
                    {
                        "name": "Asaf Liberman"
                    },
                    {
                        "name": "Jennifer Mallalieu"
                    },
                    {
                        "name": "Parsa Mazaheri"
                    },
                    {
                        "name": "Qibin Chen"
                    },
                    {
                        "name": "Manjot Bilkhu"
                    },
                    {
                        "name": "Aonan Zhang"
                    },
                    {
                        "name": "Eric Wang"
                    },
                    {
                        "name": "Dave Nelson"
                    },
                    {
                        "name": "Michael FitzMaurice"
                    },
                    {
                        "name": "Thomas Voice"
                    },
                    {
                        "name": "Jeremy Liu"
                    },
                    {
                        "name": "Josh Shaffer"
                    },
                    {
                        "name": "Shiwen Zhao"
                    },
                    {
                        "name": "Prasanth Yadla"
                    },
                    {
                        "name": "Farzin Rasteh"
                    },
                    {
                        "name": "Pengsheng Guo"
                    },
                    {
                        "name": "Arsalan Farooq"
                    },
                    {
                        "name": "Jeremy Snow"
                    },
                    {
                        "name": "Stephen Murphy"
                    },
                    {
                        "name": "Tao Lei"
                    },
                    {
                        "name": "Minsik Cho"
                    },
                    {
                        "name": "George Horrell"
                    },
                    {
                        "name": "Sam Dodge"
                    },
                    {
                        "name": "Lindsay Hislop"
                    },
                    {
                        "name": "Sumeet Singh"
                    },
                    {
                        "name": "Alex Dombrowski"
                    },
                    {
                        "name": "Aiswarya Raghavan"
                    },
                    {
                        "name": "Sasha Sirovica"
                    },
                    {
                        "name": "Mandana Saebi"
                    },
                    {
                        "name": "Faye Lao"
                    },
                    {
                        "name": "Max Lam"
                    },
                    {
                        "name": "TJ Lu"
                    },
                    {
                        "name": "Zhaoyang Xu"
                    },
                    {
                        "name": "Karanjeet Singh"
                    },
                    {
                        "name": "Marc Kirchner"
                    },
                    {
                        "name": "David Mizrahi"
                    },
                    {
                        "name": "Rajat Arora"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Henry Mason"
                    },
                    {
                        "name": "Lawrence Zhou"
                    },
                    {
                        "name": "Yi Hua"
                    },
                    {
                        "name": "Ankur Jain"
                    },
                    {
                        "name": "Felix Bai"
                    },
                    {
                        "name": "Joseph Astrauskas"
                    },
                    {
                        "name": "Floris Weers"
                    },
                    {
                        "name": "Josh Gardner"
                    },
                    {
                        "name": "Mira Chiang"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "name": "Tony Sun"
                    },
                    {
                        "name": "Quentin Keunebroek"
                    },
                    {
                        "name": "Matthew Hopkins"
                    },
                    {
                        "name": "Bugu Wu"
                    },
                    {
                        "name": "Tao Jia"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Xingyu Zhou"
                    },
                    {
                        "name": "Nanzhu Wang"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Ruixuan Hou"
                    },
                    {
                        "name": "Rene Rauch"
                    },
                    {
                        "name": "Yuan Gao"
                    },
                    {
                        "name": "Afshin Dehghan"
                    },
                    {
                        "name": "Jonathan Janke"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Cha Chen"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Feng Nan"
                    },
                    {
                        "name": "Josh Elman"
                    },
                    {
                        "name": "Dong Yin"
                    },
                    {
                        "name": "Yusuf Goren"
                    },
                    {
                        "name": "Jeff Lai"
                    },
                    {
                        "name": "Yiran Fei"
                    },
                    {
                        "name": "Syd Evans"
                    },
                    {
                        "name": "Muyang Yu"
                    },
                    {
                        "name": "Guoli Yin"
                    },
                    {
                        "name": "Yi Qin"
                    },
                    {
                        "name": "Erin Feldman"
                    },
                    {
                        "name": "Isha Garg"
                    },
                    {
                        "name": "Aparna Rajamani"
                    },
                    {
                        "name": "Karla Vega"
                    },
                    {
                        "name": "Walker Cheng"
                    },
                    {
                        "name": "TJ Collins"
                    },
                    {
                        "name": "Hans Han"
                    },
                    {
                        "name": "Raul Rea Menacho"
                    },
                    {
                        "name": "Simon Yeung"
                    },
                    {
                        "name": "Sophy Lee"
                    },
                    {
                        "name": "Phani Mutyala"
                    },
                    {
                        "name": "Ying-Chang Cheng"
                    },
                    {
                        "name": "Zhe Gan"
                    },
                    {
                        "name": "Sprite Chu"
                    },
                    {
                        "name": "Justin Lazarow"
                    },
                    {
                        "name": "Alessandro Pappalardo"
                    },
                    {
                        "name": "Federico Scozzafava"
                    },
                    {
                        "name": "Jing Lu"
                    },
                    {
                        "name": "Erik Daxberger"
                    },
                    {
                        "name": "Laurent Duchesne"
                    },
                    {
                        "name": "Jen Liu"
                    },
                    {
                        "name": "David Güera"
                    },
                    {
                        "name": "Stefano Ligas"
                    },
                    {
                        "name": "Mary Beth Kery"
                    },
                    {
                        "name": "Brent Ramerth"
                    },
                    {
                        "name": "Ciro Sannino"
                    },
                    {
                        "name": "Marcin Eichner"
                    },
                    {
                        "name": "Haoshuo Huang"
                    },
                    {
                        "name": "Rui Qian"
                    },
                    {
                        "name": "Moritz Schwarzer-Becker"
                    },
                    {
                        "name": "David Riazati"
                    },
                    {
                        "name": "Mingfei Gao"
                    },
                    {
                        "name": "Bailin Wang"
                    },
                    {
                        "name": "Jack Cackler"
                    },
                    {
                        "name": "Yang Lu"
                    },
                    {
                        "name": "Ransen Niu"
                    },
                    {
                        "name": "John Dennison"
                    },
                    {
                        "name": "Guillaume Klein"
                    },
                    {
                        "name": "Jeffrey Bigham"
                    },
                    {
                        "name": "Deepak Gopinath"
                    },
                    {
                        "name": "Navid Shiee"
                    },
                    {
                        "name": "Darren Botten"
                    },
                    {
                        "name": "Guillaume Tartavel"
                    },
                    {
                        "name": "Alex Guillen Garcia"
                    },
                    {
                        "name": "Sam Xu"
                    },
                    {
                        "name": "Victoria MönchJuan Haladjian"
                    },
                    {
                        "name": "Zi-Yi Dou"
                    },
                    {
                        "name": "Matthias Paulik"
                    },
                    {
                        "name": "Adolfo Lopez Mendez"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Hong-You Chen"
                    },
                    {
                        "name": "Chao Jia"
                    },
                    {
                        "name": "Dhaval Doshi"
                    },
                    {
                        "name": "Zhengdong Zhang"
                    },
                    {
                        "name": "Raunak Manjani"
                    },
                    {
                        "name": "Aaron Franklin"
                    },
                    {
                        "name": "Zhile Ren"
                    },
                    {
                        "name": "David Chen"
                    },
                    {
                        "name": "Artsiom Peshko"
                    },
                    {
                        "name": "Nandhitha Raghuram"
                    },
                    {
                        "name": "Hans Hao"
                    },
                    {
                        "name": "Jiulong Shan"
                    },
                    {
                        "name": "Kavya Nerella"
                    },
                    {
                        "name": "Ramsey Tantawi"
                    },
                    {
                        "name": "Vivek Kumar"
                    },
                    {
                        "name": "Saiwen Wang"
                    },
                    {
                        "name": "Brycen Wershing"
                    },
                    {
                        "name": "Bhuwan Dhingra"
                    },
                    {
                        "name": "Dhruti Shah"
                    },
                    {
                        "name": "Ob Adaranijo"
                    },
                    {
                        "name": "Xin Zheng"
                    },
                    {
                        "name": "Tait Madsen"
                    },
                    {
                        "name": "Hadas Kotek"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Yin Xia"
                    },
                    {
                        "name": "Hanli Li"
                    },
                    {
                        "name": "Suma Jayaram"
                    },
                    {
                        "name": "Yanchao Sun"
                    },
                    {
                        "name": "Ahmed Fakhry"
                    },
                    {
                        "name": "Vasileios Saveris"
                    },
                    {
                        "name": "Dustin Withers"
                    },
                    {
                        "name": "Yanghao Li"
                    },
                    {
                        "name": "Alp Aygar"
                    },
                    {
                        "name": "Andres Romero Mier Y Teran"
                    },
                    {
                        "name": "Kaiwei Huang"
                    },
                    {
                        "name": "Mark Lee"
                    },
                    {
                        "name": "Xiujun Li"
                    },
                    {
                        "name": "Yuhong Li"
                    },
                    {
                        "name": "Tyler Johnson"
                    },
                    {
                        "name": "Jay Tang"
                    },
                    {
                        "name": "Joseph Yitan Cheng"
                    },
                    {
                        "name": "Futang Peng"
                    },
                    {
                        "name": "Andrew Walkingshaw"
                    },
                    {
                        "name": "Lucas Guibert"
                    },
                    {
                        "name": "Abhishek Sharma"
                    },
                    {
                        "name": "Cheng Shen"
                    },
                    {
                        "name": "Piotr Maj"
                    },
                    {
                        "name": "Yasutaka Tanaka"
                    },
                    {
                        "name": "You-Cyuan Jhang"
                    },
                    {
                        "name": "Vivian Ma"
                    },
                    {
                        "name": "Tommi Vehvilainen"
                    },
                    {
                        "name": "Kelvin Zou"
                    },
                    {
                        "name": "Jeff Nichols"
                    },
                    {
                        "name": "Matthew Lei"
                    },
                    {
                        "name": "David Qiu"
                    },
                    {
                        "name": "Yihao Qian"
                    },
                    {
                        "name": "Gokul Santhanam"
                    },
                    {
                        "name": "Wentao Wu"
                    },
                    {
                        "name": "Yena Han"
                    },
                    {
                        "name": "Dominik Moritz"
                    },
                    {
                        "name": "Haijing Fu"
                    },
                    {
                        "name": "Mingze Xu"
                    },
                    {
                        "name": "Vivek Rathod"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Louis D'hauwe"
                    },
                    {
                        "name": "Qin Ba"
                    },
                    {
                        "name": "Haitian Sun"
                    },
                    {
                        "name": "Haoran Yan"
                    },
                    {
                        "name": "Philipp Dufter"
                    },
                    {
                        "name": "Anh Nguyen"
                    },
                    {
                        "name": "Yihao Feng"
                    },
                    {
                        "name": "Emma Wang"
                    },
                    {
                        "name": "Keyu He"
                    },
                    {
                        "name": "Rahul Nair"
                    },
                    {
                        "name": "Sanskruti Shah"
                    },
                    {
                        "name": "Jiarui Lu"
                    },
                    {
                        "name": "Patrick Sonnenberg"
                    },
                    {
                        "name": "Jeremy Warner"
                    },
                    {
                        "name": "Yuanzhi Li"
                    },
                    {
                        "name": "Bowen Pan"
                    },
                    {
                        "name": "Ziyi Zhong"
                    },
                    {
                        "name": "Joe Zhou"
                    },
                    {
                        "name": "Sam Davarnia"
                    },
                    {
                        "name": "Olli Saarikivi"
                    },
                    {
                        "name": "Irina Belousova"
                    },
                    {
                        "name": "Rachel Burger"
                    },
                    {
                        "name": "Shang-Chen Wu"
                    },
                    {
                        "name": "Di Feng"
                    },
                    {
                        "name": "Bas Straathof"
                    },
                    {
                        "name": "James Chou"
                    },
                    {
                        "name": "Yuanyang Zhang"
                    },
                    {
                        "name": "Marco Zuliani"
                    },
                    {
                        "name": "Eduardo Jimenez"
                    },
                    {
                        "name": "Abhishek Sundararajan"
                    },
                    {
                        "name": "Xianzhi Du"
                    },
                    {
                        "name": "Chang Lan"
                    },
                    {
                        "name": "Nilesh Shahdadpuri"
                    },
                    {
                        "name": "Peter Grasch"
                    },
                    {
                        "name": "Sergiu Sima"
                    },
                    {
                        "name": "Josh Newnham"
                    },
                    {
                        "name": "Varsha Paidi"
                    },
                    {
                        "name": "Jianyu Wang"
                    },
                    {
                        "name": "Kaelen Haag"
                    },
                    {
                        "name": "Alex Braunstein"
                    },
                    {
                        "name": "Daniele Molinari"
                    },
                    {
                        "name": "Richard Wei"
                    },
                    {
                        "name": "Brenda Yang"
                    },
                    {
                        "name": "Nicholas Lusskin"
                    },
                    {
                        "name": "Joanna Arreaza-Taylor"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Nicholas Seidl"
                    },
                    {
                        "name": "Simon Wang"
                    },
                    {
                        "name": "Jiaming Hu"
                    },
                    {
                        "name": "Yiping Ma"
                    },
                    {
                        "name": "Mengyu Li"
                    },
                    {
                        "name": "Kieran Liu"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Sachin Ravi"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Kevin Smith"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Binazir Karimzadeh"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Jinhao Lei"
                    },
                    {
                        "name": "Wei Fang"
                    },
                    {
                        "name": "Alec Doane"
                    },
                    {
                        "name": "Sam Wiseman"
                    },
                    {
                        "name": "Ismael Fernandez"
                    },
                    {
                        "name": "Jane Li"
                    },
                    {
                        "name": "Andrew Hansen"
                    },
                    {
                        "name": "Javier Movellan"
                    },
                    {
                        "name": "Christopher Neubauer"
                    },
                    {
                        "name": "Hanzhi Zhou"
                    },
                    {
                        "name": "Chris Chaney"
                    },
                    {
                        "name": "Nazir Kamaldin"
                    },
                    {
                        "name": "Valentin Wolf"
                    },
                    {
                        "name": "Fernando Bermúdez-Medina"
                    },
                    {
                        "name": "Joris Pelemans"
                    },
                    {
                        "name": "Peter Fu"
                    },
                    {
                        "name": "Howard Xing"
                    },
                    {
                        "name": "Xiang Kong"
                    },
                    {
                        "name": "Wayne Shan"
                    },
                    {
                        "name": "Gabriel Jacoby-Cooper"
                    },
                    {
                        "name": "Dongcai Shen"
                    },
                    {
                        "name": "Tom Gunter"
                    },
                    {
                        "name": "Guillaume Seguin"
                    },
                    {
                        "name": "Fangping Shi"
                    },
                    {
                        "name": "Shiyu Li"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Areeba Kamal"
                    },
                    {
                        "name": "Dan Masi"
                    },
                    {
                        "name": "Saptarshi Guha"
                    },
                    {
                        "name": "Qi Zhu"
                    },
                    {
                        "name": "Jenna Thibodeau"
                    },
                    {
                        "name": "Changyuan Zhang"
                    },
                    {
                        "name": "Rebecca Callahan"
                    },
                    {
                        "name": "Charles Maalouf"
                    },
                    {
                        "name": "Wilson Tsao"
                    },
                    {
                        "name": "Boyue Li"
                    },
                    {
                        "name": "Qingqing Cao"
                    },
                    {
                        "name": "Naomy Sabo"
                    },
                    {
                        "name": "Cheng Leong"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Anupama Mann Anupama"
                    },
                    {
                        "name": "Colorado Reed"
                    },
                    {
                        "name": "Kenneth Jung"
                    },
                    {
                        "name": "Zhifeng Chen"
                    },
                    {
                        "name": "Mohana Prasad Sathya Moorthy"
                    },
                    {
                        "name": "Yifei He"
                    },
                    {
                        "name": "Erik Hornberger"
                    },
                    {
                        "name": "Devi Krishna"
                    },
                    {
                        "name": "Senyu Tong"
                    },
                    {
                        "name": "Michael"
                    },
                    {
                        "name": "Lee"
                    },
                    {
                        "name": "David Haldimann"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Chang Gao"
                    },
                    {
                        "name": "Chris Bartels"
                    },
                    {
                        "name": "Sushma Rao"
                    },
                    {
                        "name": "Nathalie Tran"
                    },
                    {
                        "name": "Simon Lehnerer"
                    },
                    {
                        "name": "Co Giang"
                    },
                    {
                        "name": "Patrick Dong"
                    },
                    {
                        "name": "Junting Pan"
                    },
                    {
                        "name": "Biyao Wang"
                    },
                    {
                        "name": "Dongxu Li"
                    },
                    {
                        "name": "Mehrdad Farajtabar"
                    },
                    {
                        "name": "Dongseong Hwang"
                    },
                    {
                        "name": "Grace Duanmu"
                    },
                    {
                        "name": "Eshan Verma"
                    },
                    {
                        "name": "Sujeeth Reddy"
                    },
                    {
                        "name": "Qi Shan"
                    },
                    {
                        "name": "Hongbin Gao"
                    },
                    {
                        "name": "Nan Du"
                    },
                    {
                        "name": "Pragnya Sridhar"
                    },
                    {
                        "name": "Forrest Huang"
                    },
                    {
                        "name": "Yingbo Wang"
                    },
                    {
                        "name": "Nikhil Bhendawade"
                    },
                    {
                        "name": "Diane Zhu"
                    },
                    {
                        "name": "Sai Aitharaju"
                    },
                    {
                        "name": "Fred Hohman"
                    },
                    {
                        "name": "Lauren Gardiner"
                    },
                    {
                        "name": "Chung-Cheng Chiu"
                    },
                    {
                        "name": "Yinfei Yang"
                    },
                    {
                        "name": "Alper Kokmen"
                    },
                    {
                        "name": "Frank Chu"
                    },
                    {
                        "name": "Ke Ye"
                    },
                    {
                        "name": "Kaan Elgin"
                    },
                    {
                        "name": "Oron Levy"
                    },
                    {
                        "name": "John Park"
                    },
                    {
                        "name": "Donald Zhang"
                    },
                    {
                        "name": "Eldon Schoop"
                    },
                    {
                        "name": "Nina Wenzel"
                    },
                    {
                        "name": "Michael Booker"
                    },
                    {
                        "name": "Hyunjik Kim"
                    },
                    {
                        "name": "Chinguun Erdenebileg"
                    },
                    {
                        "name": "Nan Dun"
                    },
                    {
                        "name": "Eric Liang Yang"
                    },
                    {
                        "name": "Priyal Chhatrapati"
                    },
                    {
                        "name": "Vishaal Mahtani"
                    },
                    {
                        "name": "Haiming Gang"
                    },
                    {
                        "name": "Kohen Chia"
                    },
                    {
                        "name": "Deepa Seshadri"
                    },
                    {
                        "name": "Donghan Yu"
                    },
                    {
                        "name": "Yan Meng"
                    },
                    {
                        "name": "Kelsey Peterson"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Yongqiang Wang"
                    },
                    {
                        "name": "Carina Peng"
                    },
                    {
                        "name": "Doug Kang"
                    },
                    {
                        "name": "Anuva Agarwal"
                    },
                    {
                        "name": "Albert Antony"
                    },
                    {
                        "name": "Juan Lao Tebar"
                    },
                    {
                        "name": "Albin Madappally Jose"
                    },
                    {
                        "name": "Regan Poston"
                    },
                    {
                        "name": "Andy De Wang"
                    },
                    {
                        "name": "Gerard Casamayor"
                    },
                    {
                        "name": "Elmira Amirloo"
                    },
                    {
                        "name": "Violet Yao"
                    },
                    {
                        "name": "Wojciech Kryscinski"
                    },
                    {
                        "name": "Kun Duan"
                    },
                    {
                        "name": "Lezhi L"
                    }
                ],
                "author_detail": {
                    "name": "Lezhi L"
                },
                "arxiv_affiliation": "Taoyi",
                "author": "Lezhi L",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13575v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13575v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09570v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09570v2",
                "updated": "2025-08-27T12:13:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    12,
                    13,
                    45,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-13T07:40:25Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    7,
                    40,
                    25,
                    2,
                    225,
                    0
                ],
                "title": "Re-thinking Memory-Bound Limitations in CGRAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-thinking Memory-Bound Limitations in CGRAs"
                },
                "summary": "Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators\ncommonly employed to boost performance in workloads with iterative structures.\nExisting research typically focuses on compiler or architecture optimizations\naimed at improving CGRA performance, energy efficiency, flexibility, and area\nutilization, under the idealistic assumption that kernels can access all data\nfrom Scratchpad Memory (SPM). However, certain complex workloads-particularly\nin fields like graph analytics, irregular database operations, and specialized\nforms of high-performance computing (e.g., unstructured mesh\nsimulations)-exhibit irregular memory access patterns that hinder CGRA\nutilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To\naddress this challenge, we conduct a thorough analysis of the underlying causes\nof performance degradation, then propose a redesigned memory subsystem and\nrefine the memory model. With both microarchitectural and theoretical\noptimization, our solution can effectively manage irregular memory accesses\nthrough CGRA-specific runahead execution mechanism and cache reconfiguration\ntechniques. Our results demonstrate that we can achieve performance comparable\nto the original SPM-only system while requiring only 1.27% of the storage size.\nThe runahead execution mechanism achieves an average 3.04x speedup (up to\n6.91x), with cache reconfiguration technique providing an additional 6.02%\nimprovement, significantly enhancing CGRA performance for irregular memory\naccess patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators\ncommonly employed to boost performance in workloads with iterative structures.\nExisting research typically focuses on compiler or architecture optimizations\naimed at improving CGRA performance, energy efficiency, flexibility, and area\nutilization, under the idealistic assumption that kernels can access all data\nfrom Scratchpad Memory (SPM). However, certain complex workloads-particularly\nin fields like graph analytics, irregular database operations, and specialized\nforms of high-performance computing (e.g., unstructured mesh\nsimulations)-exhibit irregular memory access patterns that hinder CGRA\nutilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To\naddress this challenge, we conduct a thorough analysis of the underlying causes\nof performance degradation, then propose a redesigned memory subsystem and\nrefine the memory model. With both microarchitectural and theoretical\noptimization, our solution can effectively manage irregular memory accesses\nthrough CGRA-specific runahead execution mechanism and cache reconfiguration\ntechniques. Our results demonstrate that we can achieve performance comparable\nto the original SPM-only system while requiring only 1.27% of the storage size.\nThe runahead execution mechanism achieves an average 3.04x speedup (up to\n6.91x), with cache reconfiguration technique providing an additional 6.02%\nimprovement, significantly enhancing CGRA performance for irregular memory\naccess patterns."
                },
                "authors": [
                    {
                        "name": "Xiangfeng Liu"
                    },
                    {
                        "name": "Zhe Jiang"
                    },
                    {
                        "name": "Anzhen Zhu"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Mingsong Lyu"
                    },
                    {
                        "name": "Qingxu Deng"
                    },
                    {
                        "name": "Nan Guan"
                    }
                ],
                "author_detail": {
                    "name": "Nan Guan"
                },
                "author": "Nan Guan",
                "arxiv_doi": "10.1145/3760386",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3760386",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.09570v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09570v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "25 pages, 18 figures, CODES+ISSS 2025",
                "arxiv_journal_ref": "ACM Transactions on Embedded Computing Systems 2025",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.3.0; B.6.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21091v1",
                "updated": "2025-08-27T10:37:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    37,
                    24,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T10:37:24Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    37,
                    24,
                    2,
                    239,
                    0
                ],
                "title": "ERTACache: Error Rectification and Timesteps Adjustment for Efficient\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ERTACache: Error Rectification and Timesteps Adjustment for Efficient\n  Diffusion"
                },
                "summary": "Diffusion models suffer from substantial computational overhead due to their\ninherently iterative inference process. While feature caching offers a\npromising acceleration strategy by reusing intermediate outputs across\ntimesteps, naive reuse often incurs noticeable quality degradation. In this\nwork, we formally analyze the cumulative error introduced by caching and\ndecompose it into two principal components: feature shift error, caused by\ninaccuracies in cached outputs, and step amplification error, which arises from\nerror propagation under fixed timestep schedules. To address these issues, we\npropose ERTACache, a principled caching framework that jointly rectifies both\nerror types. Our method employs an offline residual profiling stage to identify\nreusable steps, dynamically adjusts integration intervals via a\ntrajectory-aware correction coefficient, and analytically approximates\ncache-induced errors through a closed-form residual linearization model.\nTogether, these components enable accurate and efficient sampling under\naggressive cache reuse. Extensive experiments across standard image and video\ngeneration benchmarks show that ERTACache achieves up to 2x inference speedup\nwhile consistently preserving or even improving visual quality. Notably, on the\nstate-of-the-art Wan2.1 video diffusion model, ERTACache delivers 2x\nacceleration with minimal VBench degradation, effectively maintaining baseline\nfidelity while significantly improving efficiency. The code is available at\nhttps://github.com/bytedance/ERTACache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models suffer from substantial computational overhead due to their\ninherently iterative inference process. While feature caching offers a\npromising acceleration strategy by reusing intermediate outputs across\ntimesteps, naive reuse often incurs noticeable quality degradation. In this\nwork, we formally analyze the cumulative error introduced by caching and\ndecompose it into two principal components: feature shift error, caused by\ninaccuracies in cached outputs, and step amplification error, which arises from\nerror propagation under fixed timestep schedules. To address these issues, we\npropose ERTACache, a principled caching framework that jointly rectifies both\nerror types. Our method employs an offline residual profiling stage to identify\nreusable steps, dynamically adjusts integration intervals via a\ntrajectory-aware correction coefficient, and analytically approximates\ncache-induced errors through a closed-form residual linearization model.\nTogether, these components enable accurate and efficient sampling under\naggressive cache reuse. Extensive experiments across standard image and video\ngeneration benchmarks show that ERTACache achieves up to 2x inference speedup\nwhile consistently preserving or even improving visual quality. Notably, on the\nstate-of-the-art Wan2.1 video diffusion model, ERTACache delivers 2x\nacceleration with minimal VBench degradation, effectively maintaining baseline\nfidelity while significantly improving efficiency. The code is available at\nhttps://github.com/bytedance/ERTACache."
                },
                "authors": [
                    {
                        "name": "Xurui Peng"
                    },
                    {
                        "name": "Hong Liu"
                    },
                    {
                        "name": "Chenqian Yan"
                    },
                    {
                        "name": "Rui Ma"
                    },
                    {
                        "name": "Fangmin Chen"
                    },
                    {
                        "name": "Xing Wang"
                    },
                    {
                        "name": "Zhihua Wu"
                    },
                    {
                        "name": "Songwei Liu"
                    },
                    {
                        "name": "Mingbao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Mingbao Lin"
                },
                "author": "Mingbao Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19670v1",
                "updated": "2025-08-27T08:30:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    8,
                    30,
                    33,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T08:30:33Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    8,
                    30,
                    33,
                    2,
                    239,
                    0
                ],
                "title": "Beyond the Bermuda Triangle of Contention: IOMMU Interference in Mixed\n  Criticality Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Bermuda Triangle of Contention: IOMMU Interference in Mixed\n  Criticality Systems"
                },
                "summary": "As Mixed Criticality Systems (MCSs) evolve, they increasingly integrate\nheterogeneous computing platforms, combining general-purpose processors with\nspecialized accelerators such as AI engines, GPUs, and high-speed networking\ninterfaces. This heterogeneity introduces challenges, as these accelerators and\nDMA-capable devices act as independent bus masters, directly accessing memory.\nConsequently, ensuring both security and timing predictability in such\nenvironments becomes critical. To address these concerns, the Input-Output\nMemory Management Unit (IOMMU) plays a key role in mediating and regulating\nmemory access, preventing unauthorized transactions while enforcing isolation\nand access control policies. While prior work has explored IOMMU-related\nside-channel vulnerabilities from a security standpoint, its role in\nperformance interference remains largely unexplored. Moreover, many of the same\narchitectural properties that enable side-channel leakage, such as shared TLBs,\ncaching effects, and translation overheads, can also introduce timing\nunpredictability. In this work, we analyze the contention effects within IOMMU\nstructures using the Xilinx UltraScale+ ZCU104 platform, demonstrating how\ntheir shared nature introduce unpredictable delays. Our findings reveal that\nIOMMU-induced interference primarily affects small memory transactions, where\ntranslation overheads significantly impact execution time. Additionally, we\nhypothesize that contention effects arising from IOTLBs exhibit similar\nbehavior across architectures due to shared caching principles, such as\nprefetching and hierarchical TLB structures. Notably, our experiments show that\nIOMMU interference can delay DMA transactions by up to 1.79x for lower-size\ntransfers on the Arm SMMUv2 implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Mixed Criticality Systems (MCSs) evolve, they increasingly integrate\nheterogeneous computing platforms, combining general-purpose processors with\nspecialized accelerators such as AI engines, GPUs, and high-speed networking\ninterfaces. This heterogeneity introduces challenges, as these accelerators and\nDMA-capable devices act as independent bus masters, directly accessing memory.\nConsequently, ensuring both security and timing predictability in such\nenvironments becomes critical. To address these concerns, the Input-Output\nMemory Management Unit (IOMMU) plays a key role in mediating and regulating\nmemory access, preventing unauthorized transactions while enforcing isolation\nand access control policies. While prior work has explored IOMMU-related\nside-channel vulnerabilities from a security standpoint, its role in\nperformance interference remains largely unexplored. Moreover, many of the same\narchitectural properties that enable side-channel leakage, such as shared TLBs,\ncaching effects, and translation overheads, can also introduce timing\nunpredictability. In this work, we analyze the contention effects within IOMMU\nstructures using the Xilinx UltraScale+ ZCU104 platform, demonstrating how\ntheir shared nature introduce unpredictable delays. Our findings reveal that\nIOMMU-induced interference primarily affects small memory transactions, where\ntranslation overheads significantly impact execution time. Additionally, we\nhypothesize that contention effects arising from IOTLBs exhibit similar\nbehavior across architectures due to shared caching principles, such as\nprefetching and hierarchical TLB structures. Notably, our experiments show that\nIOMMU interference can delay DMA transactions by up to 1.79x for lower-size\ntransfers on the Arm SMMUv2 implementation."
                },
                "authors": [
                    {
                        "name": "Diogo Costa"
                    },
                    {
                        "name": "Jose Martins"
                    },
                    {
                        "name": "Sandro Pinto"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Pinto"
                },
                "author": "Sandro Pinto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v4",
                "updated": "2025-08-27T04:58:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    4,
                    58,
                    58,
                    2,
                    239,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "arxiv_comment": "Accepted to EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19247v1",
                "updated": "2025-08-26T17:59:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    59,
                    47,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T17:59:47Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    59,
                    47,
                    1,
                    238,
                    0
                ],
                "title": "VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D\n  Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D\n  Space"
                },
                "summary": "3D local editing of specified regions is crucial for game industry and robot\ninteraction. Recent methods typically edit rendered multi-view images and then\nreconstruct 3D models, but they face challenges in precisely preserving\nunedited regions and overall coherence. Inspired by structured 3D generative\nmodels, we propose VoxHammer, a novel training-free approach that performs\nprecise and coherent editing in 3D latent space. Given a 3D model, VoxHammer\nfirst predicts its inversion trajectory and obtains its inverted latents and\nkey-value tokens at each timestep. Subsequently, in the denoising and editing\nphase, we replace the denoising features of preserved regions with the\ncorresponding inverted latents and cached key-value tokens. By retaining these\ncontextual features, this approach ensures consistent reconstruction of\npreserved areas and coherent integration of edited parts. To evaluate the\nconsistency of preserved regions, we constructed Edit3D-Bench, a\nhuman-annotated dataset comprising hundreds of samples, each with carefully\nlabeled 3D editing regions. Experiments demonstrate that VoxHammer\nsignificantly outperforms existing methods in terms of both 3D consistency of\npreserved regions and overall quality. Our method holds promise for\nsynthesizing high-quality edited paired data, thereby laying the data\nfoundation for in-context 3D generation. See our project page at\nhttps://huanngzh.github.io/VoxHammer-Page/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D local editing of specified regions is crucial for game industry and robot\ninteraction. Recent methods typically edit rendered multi-view images and then\nreconstruct 3D models, but they face challenges in precisely preserving\nunedited regions and overall coherence. Inspired by structured 3D generative\nmodels, we propose VoxHammer, a novel training-free approach that performs\nprecise and coherent editing in 3D latent space. Given a 3D model, VoxHammer\nfirst predicts its inversion trajectory and obtains its inverted latents and\nkey-value tokens at each timestep. Subsequently, in the denoising and editing\nphase, we replace the denoising features of preserved regions with the\ncorresponding inverted latents and cached key-value tokens. By retaining these\ncontextual features, this approach ensures consistent reconstruction of\npreserved areas and coherent integration of edited parts. To evaluate the\nconsistency of preserved regions, we constructed Edit3D-Bench, a\nhuman-annotated dataset comprising hundreds of samples, each with carefully\nlabeled 3D editing regions. Experiments demonstrate that VoxHammer\nsignificantly outperforms existing methods in terms of both 3D consistency of\npreserved regions and overall quality. Our method holds promise for\nsynthesizing high-quality edited paired data, thereby laying the data\nfoundation for in-context 3D generation. See our project page at\nhttps://huanngzh.github.io/VoxHammer-Page/."
                },
                "authors": [
                    {
                        "name": "Lin Li"
                    },
                    {
                        "name": "Zehuan Huang"
                    },
                    {
                        "name": "Haoran Feng"
                    },
                    {
                        "name": "Gengxiong Zhuang"
                    },
                    {
                        "name": "Rui Chen"
                    },
                    {
                        "name": "Chunchao Guo"
                    },
                    {
                        "name": "Lu Sheng"
                    }
                ],
                "author_detail": {
                    "name": "Lu Sheng"
                },
                "author": "Lu Sheng",
                "arxiv_comment": "Project page: https://huanngzh.github.io/VoxHammer-Page/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18983v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18983v1",
                "updated": "2025-08-26T12:32:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    32,
                    9,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T12:32:09Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    32,
                    9,
                    1,
                    238,
                    0
                ],
                "title": "Enabling MoE on the Edge via Importance-Driven Expert Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling MoE on the Edge via Importance-Driven Expert Scheduling"
                },
                "summary": "The Mixture of Experts (MoE) architecture has emerged as a key technique for\nscaling Large Language Models by activating only a subset of experts per query.\nDeploying MoE on consumer-grade edge hardware, however, is constrained by\nlimited device memory, making dynamic expert offloading essential. Unlike prior\nwork that treats offloading purely as a scheduling problem, we leverage expert\nimportance to guide decisions, substituting low-importance activated experts\nwith functionally similar ones already cached in GPU memory, thereby preserving\naccuracy. As a result, this design reduces memory usage and data transfer,\nwhile largely eliminating PCIe overhead. In addition, we introduce a scheduling\npolicy that maximizes the reuse ratio of GPU-cached experts, further boosting\nefficiency. Extensive evaluations show that our approach delivers 48% lower\ndecoding latency with over 60% expert cache hit rate, while maintaining nearly\nlossless accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture of Experts (MoE) architecture has emerged as a key technique for\nscaling Large Language Models by activating only a subset of experts per query.\nDeploying MoE on consumer-grade edge hardware, however, is constrained by\nlimited device memory, making dynamic expert offloading essential. Unlike prior\nwork that treats offloading purely as a scheduling problem, we leverage expert\nimportance to guide decisions, substituting low-importance activated experts\nwith functionally similar ones already cached in GPU memory, thereby preserving\naccuracy. As a result, this design reduces memory usage and data transfer,\nwhile largely eliminating PCIe overhead. In addition, we introduce a scheduling\npolicy that maximizes the reuse ratio of GPU-cached experts, further boosting\nefficiency. Extensive evaluations show that our approach delivers 48% lower\ndecoding latency with over 60% expert cache hit rate, while maintaining nearly\nlossless accuracy."
                },
                "authors": [
                    {
                        "name": "Guoying Zhu"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Haipeng Dai"
                    },
                    {
                        "name": "Xuechen Liu"
                    },
                    {
                        "name": "Weijun Wang"
                    },
                    {
                        "name": "Keran Li"
                    },
                    {
                        "name": "Jun xiao"
                    },
                    {
                        "name": "Ligeng Chen"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18983v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18983v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18736v1",
                "updated": "2025-08-26T07:09:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    7,
                    9,
                    9,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T07:09:09Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    7,
                    9,
                    9,
                    1,
                    238,
                    0
                ],
                "title": "Rethinking Caching for LLM Serving Systems: Beyond Traditional\n  Heuristics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Caching for LLM Serving Systems: Beyond Traditional\n  Heuristics"
                },
                "summary": "Serving Large Language Models (LLMs) at scale requires meeting strict Service\nLevel Objectives (SLOs) under severe computational and memory constraints.\nNevertheless, traditional caching strategies fall short: exact-matching and\nprefix caches neglect query semantics, while state-of-the-art semantic caches\nremain confined to traditional intuitions, offering little conceptual\ndeparture. Building on this, we present SISO, a semantic caching system that\nredefines efficiency for LLM serving. SISO introduces centroid-based caching to\nmaximize coverage with minimal memory, locality-aware replacement to preserve\nhigh-value entries, and dynamic thresholding to balance accuracy and latency\nunder varying workloads. Across diverse datasets, SISO delivers up to\n1.71$\\times$ higher hit ratios and consistently stronger SLO attainment\ncompared to state-of-the-art systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models (LLMs) at scale requires meeting strict Service\nLevel Objectives (SLOs) under severe computational and memory constraints.\nNevertheless, traditional caching strategies fall short: exact-matching and\nprefix caches neglect query semantics, while state-of-the-art semantic caches\nremain confined to traditional intuitions, offering little conceptual\ndeparture. Building on this, we present SISO, a semantic caching system that\nredefines efficiency for LLM serving. SISO introduces centroid-based caching to\nmaximize coverage with minimal memory, locality-aware replacement to preserve\nhigh-value entries, and dynamic thresholding to balance accuracy and latency\nunder varying workloads. Across diverse datasets, SISO delivers up to\n1.71$\\times$ higher hit ratios and consistently stronger SLO attainment\ncompared to state-of-the-art systems."
                },
                "authors": [
                    {
                        "name": "Jungwoo Kim"
                    },
                    {
                        "name": "Minsang Kim"
                    },
                    {
                        "name": "Jaeheon Lee"
                    },
                    {
                        "name": "Chanwoo Moon"
                    },
                    {
                        "name": "Heejin Kim"
                    },
                    {
                        "name": "Taeho Hwang"
                    },
                    {
                        "name": "Woosuk Chung"
                    },
                    {
                        "name": "Yeseong Kim"
                    },
                    {
                        "name": "Sungjin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sungjin Lee"
                },
                "author": "Sungjin Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08045v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08045v2",
                "updated": "2025-08-26T01:55:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    1,
                    55,
                    27,
                    1,
                    238,
                    0
                ],
                "published": "2025-07-10T01:51:17Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    1,
                    51,
                    17,
                    3,
                    191,
                    0
                ],
                "title": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing"
                },
                "summary": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality."
                },
                "authors": [
                    {
                        "name": "Junyi Wen"
                    },
                    {
                        "name": "Junyuan Liang"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Wuhui Chen"
                    },
                    {
                        "name": "Ting Cai"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08045v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08045v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19365v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19365v3",
                "updated": "2025-08-26T01:45:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    1,
                    45,
                    34,
                    1,
                    238,
                    0
                ],
                "published": "2025-04-27T22:05:14Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    22,
                    5,
                    14,
                    6,
                    117,
                    0
                ],
                "title": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration"
                },
                "summary": "GPUs are critical for compute-intensive applications, yet emerging workloads\nsuch as recommender systems, graph analytics, and data analytics often exceed\nGPU memory capacity. Existing solutions allow GPUs to use CPU DRAM or SSDs as\nexternal memory, and the GPU-centric approach enables GPU threads to directly\nissue NVMe requests, further avoiding CPU intervention. However, current\nGPU-centric approaches adopt synchronous I/O, forcing threads to stall during\nlong communication delays.\n  We propose AGILE, a lightweight asynchronous GPU-centric I/O library that\neliminates deadlock risks and integrates a flexible HBM-based software cache.\nAGILE overlaps computation and I/O, improving performance by up to 1.88$\\times$\nacross workloads with diverse computation-to-communication ratios. Compared to\nBaM on DLRM, AGILE achieves up to 1.75$\\times$ speedup through efficient design\nand overlapping; on graph applications, AGILE reduces software cache overhead\nby up to 3.12$\\times$ and NVMe I/O overhead by up to 2.85$\\times$; AGILE also\nlowers per-thread register usage by up to 1.32$\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPUs are critical for compute-intensive applications, yet emerging workloads\nsuch as recommender systems, graph analytics, and data analytics often exceed\nGPU memory capacity. Existing solutions allow GPUs to use CPU DRAM or SSDs as\nexternal memory, and the GPU-centric approach enables GPU threads to directly\nissue NVMe requests, further avoiding CPU intervention. However, current\nGPU-centric approaches adopt synchronous I/O, forcing threads to stall during\nlong communication delays.\n  We propose AGILE, a lightweight asynchronous GPU-centric I/O library that\neliminates deadlock risks and integrates a flexible HBM-based software cache.\nAGILE overlaps computation and I/O, improving performance by up to 1.88$\\times$\nacross workloads with diverse computation-to-communication ratios. Compared to\nBaM on DLRM, AGILE achieves up to 1.75$\\times$ speedup through efficient design\nand overlapping; on graph applications, AGILE reduces software cache overhead\nby up to 3.12$\\times$ and NVMe I/O overhead by up to 2.85$\\times$; AGILE also\nlowers per-thread register usage by up to 1.32$\\times$."
                },
                "authors": [
                    {
                        "name": "Zhuoping Yang"
                    },
                    {
                        "name": "Jinming Zhuang"
                    },
                    {
                        "name": "Xingzhen Chen"
                    },
                    {
                        "name": "Alex K. Jones"
                    },
                    {
                        "name": "Peipei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Peipei Zhou"
                },
                "author": "Peipei Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19365v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19365v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.10452v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10452v1",
                "updated": "2025-09-12T17:59:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    17,
                    59,
                    9,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T17:59:09Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    17,
                    59,
                    9,
                    4,
                    255,
                    0
                ],
                "title": "WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained\n  Speech Recognition Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained\n  Speech Recognition Transformers"
                },
                "summary": "Pretrained automatic speech recognition (ASR) models such as Whisper perform\nwell but still need domain adaptation to handle unseen vocabulary and parlance.\nIn many real-world settings, collecting speech data is impractical,\nnecessitating text-only adaptation. We propose WhisTLE, a deeply supervised,\ntext-only adaptation method for pretrained encoder-decoder ASR models. WhisTLE\ntrains a variational autoencoder (VAE) to model encoder outputs from text and\nfine-tunes the decoder using the learned text-to-latent encoder, optionally\ncombined with text-to-speech (TTS) adaptation. At inference, the original\nencoder is restored, incurring no extra runtime cost. Across four out-of-domain\ndatasets and four ASR models, WhisTLE with TTS reduces word error rate (WER) by\n12.3% relative to TTS-only adaptation and outperforms all non-WhisTLE baselines\nin 27 of 32 scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretrained automatic speech recognition (ASR) models such as Whisper perform\nwell but still need domain adaptation to handle unseen vocabulary and parlance.\nIn many real-world settings, collecting speech data is impractical,\nnecessitating text-only adaptation. We propose WhisTLE, a deeply supervised,\ntext-only adaptation method for pretrained encoder-decoder ASR models. WhisTLE\ntrains a variational autoencoder (VAE) to model encoder outputs from text and\nfine-tunes the decoder using the learned text-to-latent encoder, optionally\ncombined with text-to-speech (TTS) adaptation. At inference, the original\nencoder is restored, incurring no extra runtime cost. Across four out-of-domain\ndatasets and four ASR models, WhisTLE with TTS reduces word error rate (WER) by\n12.3% relative to TTS-only adaptation and outperforms all non-WhisTLE baselines\nin 27 of 32 scenarios."
                },
                "authors": [
                    {
                        "name": "Akshat Pandey"
                    },
                    {
                        "name": "Karun Kumar"
                    },
                    {
                        "name": "Raphael Tang"
                    }
                ],
                "author_detail": {
                    "name": "Raphael Tang"
                },
                "author": "Raphael Tang",
                "arxiv_comment": "5 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10452v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10452v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10446v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10446v1",
                "updated": "2025-09-12T17:52:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    17,
                    52,
                    35,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T17:52:35Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    17,
                    52,
                    35,
                    4,
                    255,
                    0
                ],
                "title": "DeepDive: Advancing Deep Search Agents with Knowledge Graphs and\n  Multi-Turn RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepDive: Advancing Deep Search Agents with Knowledge Graphs and\n  Multi-Turn RL"
                },
                "summary": "Augmenting large language models (LLMs) with browsing tools substantially\nimproves their potential as deep search agents to solve complex, real-world\ntasks. Yet, open LLMs still perform poorly in such settings due to limited\nlong-horizon reasoning capacity with browsing tools and the lack of\nsufficiently difficult supervised data. To address these challenges, we present\nDeepDive to advance deep search agents. First, we propose a strategy to\nautomatically synthesize complex, difficult, and hard-to-find questions from\nopen knowledge graphs. Second, we apply end-to-end multi-turn reinforcement\nlearning (RL) to enhance LLMs' long-horizon reasoning with deep search.\nExperiments show that DeepDive-32B achieves a new open-source competitive\nresult on BrowseComp, outperforming WebSailor, DeepSeek-R1-Browse, and\nSearch-o1. We demonstrate that multi-turn RL training improves deep search\nability and significantly contributes to the performance improvements across\nmultiple benchmarks. We observe that DeepDive enables test-time scaling of tool\ncalls and parallel sampling. All datasets, models, and code are publicly\navailable at https://github.com/THUDM/DeepDive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmenting large language models (LLMs) with browsing tools substantially\nimproves their potential as deep search agents to solve complex, real-world\ntasks. Yet, open LLMs still perform poorly in such settings due to limited\nlong-horizon reasoning capacity with browsing tools and the lack of\nsufficiently difficult supervised data. To address these challenges, we present\nDeepDive to advance deep search agents. First, we propose a strategy to\nautomatically synthesize complex, difficult, and hard-to-find questions from\nopen knowledge graphs. Second, we apply end-to-end multi-turn reinforcement\nlearning (RL) to enhance LLMs' long-horizon reasoning with deep search.\nExperiments show that DeepDive-32B achieves a new open-source competitive\nresult on BrowseComp, outperforming WebSailor, DeepSeek-R1-Browse, and\nSearch-o1. We demonstrate that multi-turn RL training improves deep search\nability and significantly contributes to the performance improvements across\nmultiple benchmarks. We observe that DeepDive enables test-time scaling of tool\ncalls and parallel sampling. All datasets, models, and code are publicly\navailable at https://github.com/THUDM/DeepDive."
                },
                "authors": [
                    {
                        "name": "Rui Lu"
                    },
                    {
                        "name": "Zhenyu Hou"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Hanchen Zhang"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Yujiang Li"
                    },
                    {
                        "name": "Shi Feng"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Yuxiao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiao Dong"
                },
                "author": "Yuxiao Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10446v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10446v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10436v1",
                "updated": "2025-09-12T17:44:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    17,
                    44,
                    22,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T17:44:22Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    17,
                    44,
                    22,
                    4,
                    255,
                    0
                ],
                "title": "RefactorCoderQA: Benchmarking LLMs for Multi-Domain Coding Question\n  Solutions in Cloud and Edge Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RefactorCoderQA: Benchmarking LLMs for Multi-Domain Coding Question\n  Solutions in Cloud and Edge Deployment"
                },
                "summary": "To optimize the reasoning and problem-solving capabilities of Large Language\nModels (LLMs), we propose a novel cloud-edge collaborative architecture that\nenables a structured, multi-agent prompting framework. This framework comprises\nthree specialized components: GuideLLM, a lightweight model deployed at the\nedge to provide methodological guidance; SolverLLM, a more powerful model\nhosted in the cloud responsible for generating code solutions; and JudgeLLM, an\nautomated evaluator for assessing solution correctness and quality. To evaluate\nand demonstrate the effectiveness of this architecture in realistic settings,\nwe introduce RefactorCoderQA, a comprehensive benchmark designed to evaluate\nand enhance the performance of Large Language Models (LLMs) across multi-domain\ncoding tasks. Motivated by the limitations of existing benchmarks,\nRefactorCoderQA systematically covers various technical domains, including\nSoftware Engineering, Data Science, Machine Learning, and Natural Language\nProcessing, using authentic coding challenges from Stack Overflow. Extensive\nexperiments reveal that our fine-tuned model, RefactorCoder-MoE, achieves\nstate-of-the-art performance, significantly outperforming leading open-source\nand commercial baselines with an overall accuracy of 76.84%. Human evaluations\nfurther validate the interpretability, accuracy, and practical relevance of the\ngenerated solutions. In addition, we evaluate system-level metrics, such as\nthroughput and latency, to gain deeper insights into the performance\ncharacteristics and trade-offs of the proposed architecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To optimize the reasoning and problem-solving capabilities of Large Language\nModels (LLMs), we propose a novel cloud-edge collaborative architecture that\nenables a structured, multi-agent prompting framework. This framework comprises\nthree specialized components: GuideLLM, a lightweight model deployed at the\nedge to provide methodological guidance; SolverLLM, a more powerful model\nhosted in the cloud responsible for generating code solutions; and JudgeLLM, an\nautomated evaluator for assessing solution correctness and quality. To evaluate\nand demonstrate the effectiveness of this architecture in realistic settings,\nwe introduce RefactorCoderQA, a comprehensive benchmark designed to evaluate\nand enhance the performance of Large Language Models (LLMs) across multi-domain\ncoding tasks. Motivated by the limitations of existing benchmarks,\nRefactorCoderQA systematically covers various technical domains, including\nSoftware Engineering, Data Science, Machine Learning, and Natural Language\nProcessing, using authentic coding challenges from Stack Overflow. Extensive\nexperiments reveal that our fine-tuned model, RefactorCoder-MoE, achieves\nstate-of-the-art performance, significantly outperforming leading open-source\nand commercial baselines with an overall accuracy of 76.84%. Human evaluations\nfurther validate the interpretability, accuracy, and practical relevance of the\ngenerated solutions. In addition, we evaluate system-level metrics, such as\nthroughput and latency, to gain deeper insights into the performance\ncharacteristics and trade-offs of the proposed architecture."
                },
                "authors": [
                    {
                        "name": "Shadikur Rahman"
                    },
                    {
                        "name": "Aroosa Hameed"
                    },
                    {
                        "name": "Gautam Srivastava"
                    },
                    {
                        "name": "Syed Muhammad Danish"
                    }
                ],
                "author_detail": {
                    "name": "Syed Muhammad Danish"
                },
                "author": "Syed Muhammad Danish",
                "arxiv_comment": "12 pages, 5 figures, submitted to IEEE Transactions on Services\n  Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05672v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05672v2",
                "updated": "2025-09-12T17:32:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    17,
                    32,
                    41,
                    4,
                    255,
                    0
                ],
                "published": "2025-08-04T16:59:43Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    59,
                    43,
                    0,
                    216,
                    0
                ],
                "title": "LMAR: Language Model Augmented Retriever for Domain-specific Knowledge\n  Indexing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LMAR: Language Model Augmented Retriever for Domain-specific Knowledge\n  Indexing"
                },
                "summary": "Retrieval Augmented Generation (RAG) systems often struggle with\ndomain-specific knowledge due to performance deterioration of pre-trained\nembeddings and prohibitive computational costs of large language model\n(LLM)-based retrievers. While fine-tuning data augmentation embedding models\noffers a promising direction, its effectiveness is limited by the need for\nhigh-quality training data and reliable chunking strategies that preserve\ncontextual integrity. We propose LMAR (Language Model Augmented Retriever), a\nmodel-agnostic framework that addresses these challenges by combining\nLLM-guided data synthesis with contrastive embedding adaptation and efficient\ntext clustering. LMAR consists of a two-stage pipeline: (1) Triplet sampling\nand synthetic data augmentation, where LLMs act as both labeler and validator\nto ensure high-fidelity supervision throughout the pipeline. Experimental\nresults across multiple domain-specific benchmark datasets demonstrate that\nLMAR outperforms multiple baseline models, while maintaining moderate hardware\nrequirements and low latency. Its model-agnostic nature further enables\nseamless integration with emerging RAG architectures and text embedding models,\nensuring continual improvements without redesigning the pipeline. These results\nhighlight LMAR as a practical and cost-effective solution for scalable\ndomain-specific adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) systems often struggle with\ndomain-specific knowledge due to performance deterioration of pre-trained\nembeddings and prohibitive computational costs of large language model\n(LLM)-based retrievers. While fine-tuning data augmentation embedding models\noffers a promising direction, its effectiveness is limited by the need for\nhigh-quality training data and reliable chunking strategies that preserve\ncontextual integrity. We propose LMAR (Language Model Augmented Retriever), a\nmodel-agnostic framework that addresses these challenges by combining\nLLM-guided data synthesis with contrastive embedding adaptation and efficient\ntext clustering. LMAR consists of a two-stage pipeline: (1) Triplet sampling\nand synthetic data augmentation, where LLMs act as both labeler and validator\nto ensure high-fidelity supervision throughout the pipeline. Experimental\nresults across multiple domain-specific benchmark datasets demonstrate that\nLMAR outperforms multiple baseline models, while maintaining moderate hardware\nrequirements and low latency. Its model-agnostic nature further enables\nseamless integration with emerging RAG architectures and text embedding models,\nensuring continual improvements without redesigning the pipeline. These results\nhighlight LMAR as a practical and cost-effective solution for scalable\ndomain-specific adaptation."
                },
                "authors": [
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Yantian Ding"
                    },
                    {
                        "name": "Zhiyue Zhang"
                    },
                    {
                        "name": "Dapeng Yao"
                    },
                    {
                        "name": "Yanxun Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yanxun Xu"
                },
                "author": "Yanxun Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05672v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05672v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10427v1",
                "updated": "2025-09-12T17:31:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    17,
                    31,
                    40,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T17:31:40Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    17,
                    31,
                    40,
                    4,
                    255,
                    0
                ],
                "title": "My Favorite Streamer is an LLM: Discovering, Bonding, and Co-Creating in\n  AI VTuber Fandom",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "My Favorite Streamer is an LLM: Discovering, Bonding, and Co-Creating in\n  AI VTuber Fandom"
                },
                "summary": "AI VTubers, where the performer is not human but algorithmically generated,\nintroduce a new context for fandom. While human VTubers have been substantially\nstudied for their cultural appeal, parasocial dynamics, and community\neconomies, little is known about how audiences engage with their AI\ncounterparts. To address this gap, we present a qualitative study of\nNeuro-sama, the most prominent AI VTuber. Our findings show that engagement is\nanchored in active co-creation: audiences are drawn by the AI's unpredictable\nyet entertaining interactions, cement loyalty through collective emotional\nevents that trigger anthropomorphic projection, and sustain attachment via the\nAI's consistent persona. Financial support emerges not as a reward for\nperformance but as a participatory mechanism for shaping livestream content,\nestablishing a resilient fan economy built on ongoing interaction. These\ndynamics reveal how AI Vtuber fandom reshapes fan-creator relationships and\noffer implications for designing transparent and sustainable AI-mediated\ncommunities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI VTubers, where the performer is not human but algorithmically generated,\nintroduce a new context for fandom. While human VTubers have been substantially\nstudied for their cultural appeal, parasocial dynamics, and community\neconomies, little is known about how audiences engage with their AI\ncounterparts. To address this gap, we present a qualitative study of\nNeuro-sama, the most prominent AI VTuber. Our findings show that engagement is\nanchored in active co-creation: audiences are drawn by the AI's unpredictable\nyet entertaining interactions, cement loyalty through collective emotional\nevents that trigger anthropomorphic projection, and sustain attachment via the\nAI's consistent persona. Financial support emerges not as a reward for\nperformance but as a participatory mechanism for shaping livestream content,\nestablishing a resilient fan economy built on ongoing interaction. These\ndynamics reveal how AI Vtuber fandom reshapes fan-creator relationships and\noffer implications for designing transparent and sustainable AI-mediated\ncommunities."
                },
                "authors": [
                    {
                        "name": "Jiayi Ye"
                    },
                    {
                        "name": "Chaoran Chen"
                    },
                    {
                        "name": "Yue Huang"
                    },
                    {
                        "name": "Yanfang Ye"
                    },
                    {
                        "name": "Toby Jia-Jun Li"
                    },
                    {
                        "name": "Xiangliang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangliang Zhang"
                },
                "author": "Xiangliang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14664v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14664v3",
                "updated": "2025-09-12T17:21:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    17,
                    21,
                    39,
                    4,
                    255,
                    0
                ],
                "published": "2024-09-23T02:08:20Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    2,
                    8,
                    20,
                    0,
                    267,
                    0
                ],
                "title": "Direct Judgement Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Judgement Preference Optimization"
                },
                "summary": "Auto-evaluation is crucial for assessing response quality and offering\nfeedback for model development. Recent studies have explored training large\nlanguage models (LLMs) as generative judges to evaluate and critique other\nmodels' outputs. In this work, we investigate the idea of learning from both\npositive and negative data with preference optimization to enhance the\nevaluation capabilities of LLM judges across an array of different use cases.\nWe achieve this by employing three approaches to collect the preference pairs\nfor different use cases, each aimed at improving our generative judge from a\ndifferent perspective. Our comprehensive study over a wide range of benchmarks\ndemonstrates the effectiveness of our method. In particular, our generative\njudge achieves the best performance on 10 out of 13 benchmarks, outperforming\nstrong baselines like GPT-4o and specialized judge models. Further analysis\nshow that our judge model robustly counters inherent biases such as position\nand length bias, flexibly adapts to any evaluation protocol specified by\npractitioners, and provides helpful language feedback for improving downstream\ngenerator models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-evaluation is crucial for assessing response quality and offering\nfeedback for model development. Recent studies have explored training large\nlanguage models (LLMs) as generative judges to evaluate and critique other\nmodels' outputs. In this work, we investigate the idea of learning from both\npositive and negative data with preference optimization to enhance the\nevaluation capabilities of LLM judges across an array of different use cases.\nWe achieve this by employing three approaches to collect the preference pairs\nfor different use cases, each aimed at improving our generative judge from a\ndifferent perspective. Our comprehensive study over a wide range of benchmarks\ndemonstrates the effectiveness of our method. In particular, our generative\njudge achieves the best performance on 10 out of 13 benchmarks, outperforming\nstrong baselines like GPT-4o and specialized judge models. Further analysis\nshow that our judge model robustly counters inherent biases such as position\nand length bias, flexibly adapts to any evaluation protocol specified by\npractitioners, and provides helpful language feedback for improving downstream\ngenerator models."
                },
                "authors": [
                    {
                        "name": "Peifeng Wang"
                    },
                    {
                        "name": "Austin Xu"
                    },
                    {
                        "name": "Yilun Zhou"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Shafiq Joty"
                    }
                ],
                "author_detail": {
                    "name": "Shafiq Joty"
                },
                "author": "Shafiq Joty",
                "arxiv_comment": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14664v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14664v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10420v1",
                "updated": "2025-09-12T17:18:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    17,
                    18,
                    32,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T17:18:32Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    17,
                    18,
                    32,
                    4,
                    255,
                    0
                ],
                "title": "Testing the nature of compact objects in the lower mass gap using\n  gravitational wave observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing the nature of compact objects in the lower mass gap using\n  gravitational wave observations"
                },
                "summary": "As the compact binary catalog continues to grow rapidly, developing and\nrefining tests to probe the nature of compact objects is essential for a\ncomprehensive understanding of both the observed data and the underlying\nastrophysics of the binary population. We investigate the effectiveness of\nspin-induced multipole moments (SIQM) and tidal deformability measurements in\ndistinguishing lower mass-gap black hole (BH) binaries from non-BH binaries\nwith different mass and spin configurations. We perform model-agnostic tests on\nbinary BH (BBH) simulations using full Bayesian inference, evaluating the\nindependent and joint measurability of SIQM and tidal parameters across the\nparameter space. We extend the analysis to simulations of self-interacting\nspinning boson stars, using synthetic signals that exhibit (a) both SIQM and\ntidal effects and (b) each effect individually. For case (a), recovery is\nperformed using (i) a BBH model, (ii) a model incorporating both SIQM and tidal\neffects, and (iii) models including either SIQM or tidal effects. For case (b),\nwe employ (i) a BBH model and (ii) models incorporating either SIQM or tidal\neffects, consistent with the injection. Simulations employ TaylorF2 waveform\nmodel and consider binaries in the low mass gap with varying spin magnitudes.\nWe find that employing an incorrect model to analyze the signal can lead to\nbiases in parameter inference. Notably, when analyzing a simulated binary boson\nstar-like signal with component masses $\\rm{(4, 4) \\, M_{\\odot}}$ using a BBH\nmodel, the system is incorrectly identified as having masses $\\rm{(8, 2) \\,\nM_{\\odot}}$. In contrast, using the correct recovery model that includes both\nSIQM and tidal deformability effects successfully recovers the true masses,\nhighlighting the significance of waveform model accuracy in performing reliable\ndistinguishability tests for compact objects in the low-mass gap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the compact binary catalog continues to grow rapidly, developing and\nrefining tests to probe the nature of compact objects is essential for a\ncomprehensive understanding of both the observed data and the underlying\nastrophysics of the binary population. We investigate the effectiveness of\nspin-induced multipole moments (SIQM) and tidal deformability measurements in\ndistinguishing lower mass-gap black hole (BH) binaries from non-BH binaries\nwith different mass and spin configurations. We perform model-agnostic tests on\nbinary BH (BBH) simulations using full Bayesian inference, evaluating the\nindependent and joint measurability of SIQM and tidal parameters across the\nparameter space. We extend the analysis to simulations of self-interacting\nspinning boson stars, using synthetic signals that exhibit (a) both SIQM and\ntidal effects and (b) each effect individually. For case (a), recovery is\nperformed using (i) a BBH model, (ii) a model incorporating both SIQM and tidal\neffects, and (iii) models including either SIQM or tidal effects. For case (b),\nwe employ (i) a BBH model and (ii) models incorporating either SIQM or tidal\neffects, consistent with the injection. Simulations employ TaylorF2 waveform\nmodel and consider binaries in the low mass gap with varying spin magnitudes.\nWe find that employing an incorrect model to analyze the signal can lead to\nbiases in parameter inference. Notably, when analyzing a simulated binary boson\nstar-like signal with component masses $\\rm{(4, 4) \\, M_{\\odot}}$ using a BBH\nmodel, the system is incorrectly identified as having masses $\\rm{(8, 2) \\,\nM_{\\odot}}$. In contrast, using the correct recovery model that includes both\nSIQM and tidal deformability effects successfully recovers the true masses,\nhighlighting the significance of waveform model accuracy in performing reliable\ndistinguishability tests for compact objects in the low-mass gap."
                },
                "authors": [
                    {
                        "name": "N. V. Krishnendu"
                    },
                    {
                        "name": "Frank Ohme"
                    },
                    {
                        "name": "K. G. Arun"
                    }
                ],
                "author_detail": {
                    "name": "K. G. Arun"
                },
                "author": "K. G. Arun",
                "arxiv_comment": "14 pages, 8 Figures and 2 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18889v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18889v2",
                "updated": "2025-09-12T17:18:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    17,
                    18,
                    27,
                    4,
                    255,
                    0
                ],
                "published": "2024-10-24T16:27:03Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    27,
                    3,
                    3,
                    298,
                    0
                ],
                "title": "Are LLMs Better than Reported? Detecting Label Errors and Mitigating\n  Their Effect on Model Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are LLMs Better than Reported? Detecting Label Errors and Mitigating\n  Their Effect on Model Performance"
                },
                "summary": "NLP benchmarks rely on standardized datasets for training and evaluating\nmodels and are crucial for advancing the field. Traditionally, expert\nannotations ensure high-quality labels; however, the cost of expert annotation\ndoes not scale well with the growing demand for larger datasets required by\nmodern models. While crowd-sourcing provides a more scalable solution, it often\ncomes at the expense of annotation precision and consistency. Recent\nadvancements in large language models (LLMs) offer new opportunities to enhance\nthe annotation process, particularly for detecting label errors in existing\ndatasets. In this work, we consider the recent approach of LLM-as-a-judge,\nleveraging an ensemble of LLMs to flag potentially mislabeled examples. We\nconduct a case study on four factual consistency datasets from the TRUE\nbenchmark, spanning diverse NLP tasks, and on SummEval, which uses Likert-scale\nratings of summary quality across multiple dimensions. We empirically analyze\nthe labeling quality of existing datasets and compare expert, crowd-sourced,\nand LLM-based annotations in terms of the agreement, label quality, and\nefficiency, demonstrating the strengths and limitations of each annotation\nmethod. Our findings reveal a substantial number of label errors, which, when\ncorrected, induce a significant upward shift in reported model performance.\nThis suggests that many of the LLMs' so-called mistakes are due to label errors\nrather than genuine model failures. Additionally, we discuss the implications\nof mislabeled data and propose methods to mitigate them in training to improve\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NLP benchmarks rely on standardized datasets for training and evaluating\nmodels and are crucial for advancing the field. Traditionally, expert\nannotations ensure high-quality labels; however, the cost of expert annotation\ndoes not scale well with the growing demand for larger datasets required by\nmodern models. While crowd-sourcing provides a more scalable solution, it often\ncomes at the expense of annotation precision and consistency. Recent\nadvancements in large language models (LLMs) offer new opportunities to enhance\nthe annotation process, particularly for detecting label errors in existing\ndatasets. In this work, we consider the recent approach of LLM-as-a-judge,\nleveraging an ensemble of LLMs to flag potentially mislabeled examples. We\nconduct a case study on four factual consistency datasets from the TRUE\nbenchmark, spanning diverse NLP tasks, and on SummEval, which uses Likert-scale\nratings of summary quality across multiple dimensions. We empirically analyze\nthe labeling quality of existing datasets and compare expert, crowd-sourced,\nand LLM-based annotations in terms of the agreement, label quality, and\nefficiency, demonstrating the strengths and limitations of each annotation\nmethod. Our findings reveal a substantial number of label errors, which, when\ncorrected, induce a significant upward shift in reported model performance.\nThis suggests that many of the LLMs' so-called mistakes are due to label errors\nrather than genuine model failures. Additionally, we discuss the implications\nof mislabeled data and propose methods to mitigate them in training to improve\nperformance."
                },
                "authors": [
                    {
                        "name": "Omer Nahum"
                    },
                    {
                        "name": "Nitay Calderon"
                    },
                    {
                        "name": "Orgad Keller"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Roi Reichart"
                    }
                ],
                "author_detail": {
                    "name": "Roi Reichart"
                },
                "author": "Roi Reichart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18889v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18889v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07980v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07980v2",
                "updated": "2025-09-12T17:15:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    17,
                    15,
                    56,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-09T17:59:35Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    59,
                    35,
                    1,
                    252,
                    0
                ],
                "title": "Parallel-R1: Towards Parallel Thinking via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel-R1: Towards Parallel Thinking via Reinforcement Learning"
                },
                "summary": "Parallel thinking has emerged as a novel approach for enhancing the reasoning\ncapabilities of large language models (LLMs) by exploring multiple reasoning\npaths concurrently. However, activating such capabilities through training\nremains challenging, as existing methods predominantly rely on supervised\nfine-tuning (SFT) over synthetic data, which encourages teacher-forced\nimitation rather than exploration and generalization. Different from them, we\npropose \\textbf{Parallel-R1}, the first reinforcement learning (RL) framework\nthat enables parallel thinking behaviors for complex real-world reasoning\ntasks. Our framework employs a progressive curriculum that explicitly addresses\nthe cold-start problem in training parallel thinking with RL. We first use SFT\non prompt-generated trajectories from easier tasks to instill the parallel\nthinking ability, then transition to RL to explore and generalize this skill on\nharder problems. Experiments on various math benchmarks, including MATH, AMC23,\nand AIME, show that Parallel-R1 successfully instills parallel thinking,\nleading to 8.4% accuracy improvements over the sequential thinking model\ntrained directly on challenging tasks with RL. Further analysis reveals a clear\nshift in the model's thinking behavior: at an early stage, it uses parallel\nthinking as an exploration strategy, while in a later stage, it uses the same\ncapability for multi-perspective verification. Most significantly, we validate\nparallel thinking as a \\textbf{mid-training exploration scaffold}, where this\ntemporary exploratory phase unlocks a higher performance ceiling after RL,\nyielding a 42.9% improvement over the baseline on AIME25. Our model, data, and\ncode will be open-source at https://github.com/zhengkid/Parallel-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel thinking has emerged as a novel approach for enhancing the reasoning\ncapabilities of large language models (LLMs) by exploring multiple reasoning\npaths concurrently. However, activating such capabilities through training\nremains challenging, as existing methods predominantly rely on supervised\nfine-tuning (SFT) over synthetic data, which encourages teacher-forced\nimitation rather than exploration and generalization. Different from them, we\npropose \\textbf{Parallel-R1}, the first reinforcement learning (RL) framework\nthat enables parallel thinking behaviors for complex real-world reasoning\ntasks. Our framework employs a progressive curriculum that explicitly addresses\nthe cold-start problem in training parallel thinking with RL. We first use SFT\non prompt-generated trajectories from easier tasks to instill the parallel\nthinking ability, then transition to RL to explore and generalize this skill on\nharder problems. Experiments on various math benchmarks, including MATH, AMC23,\nand AIME, show that Parallel-R1 successfully instills parallel thinking,\nleading to 8.4% accuracy improvements over the sequential thinking model\ntrained directly on challenging tasks with RL. Further analysis reveals a clear\nshift in the model's thinking behavior: at an early stage, it uses parallel\nthinking as an exploration strategy, while in a later stage, it uses the same\ncapability for multi-perspective verification. Most significantly, we validate\nparallel thinking as a \\textbf{mid-training exploration scaffold}, where this\ntemporary exploratory phase unlocks a higher performance ceiling after RL,\nyielding a 42.9% improvement over the baseline on AIME25. Our model, data, and\ncode will be open-source at https://github.com/zhengkid/Parallel-R1."
                },
                "authors": [
                    {
                        "name": "Tong Zheng"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Wenhao Yu"
                    },
                    {
                        "name": "Xiaoyang Wang"
                    },
                    {
                        "name": "Runpeng Dai"
                    },
                    {
                        "name": "Rui Liu"
                    },
                    {
                        "name": "Huiwen Bao"
                    },
                    {
                        "name": "Chengsong Huang"
                    },
                    {
                        "name": "Heng Huang"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "arxiv_comment": "Project website: https://zhengkid.github.io/Parallel_R1.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07980v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07980v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10416v1",
                "updated": "2025-09-12T17:13:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    17,
                    13,
                    18,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T17:13:18Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    17,
                    13,
                    18,
                    4,
                    255,
                    0
                ],
                "title": "TASC: Task-Aware Shared Control for Teleoperated Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TASC: Task-Aware Shared Control for Teleoperated Manipulation"
                },
                "summary": "We present TASC, a Task-Aware Shared Control framework for teleoperated\nmanipulation that infers task-level user intent and provides assistance\nthroughout the task. To support everyday tasks without predefined knowledge,\nTASC constructs an open-vocabulary interaction graph from visual input to\nrepresent functional object relationships, and infers user intent accordingly.\nA shared control policy then provides rotation assistance during both grasping\nand object interaction, guided by spatial constraints predicted by a\nvision-language model. Our method addresses two key challenges in\ngeneral-purpose, long-horizon shared control: (1) understanding and inferring\ntask-level user intent, and (2) generalizing assistance across diverse objects\nand tasks. Experiments in both simulation and the real world demonstrate that\nTASC improves task efficiency and reduces user input effort compared to prior\nmethods. To the best of our knowledge, this is the first shared control\nframework that supports everyday manipulation tasks with zero-shot\ngeneralization. The code that supports our experiments is publicly available at\nhttps://github.com/fitz0401/tasc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present TASC, a Task-Aware Shared Control framework for teleoperated\nmanipulation that infers task-level user intent and provides assistance\nthroughout the task. To support everyday tasks without predefined knowledge,\nTASC constructs an open-vocabulary interaction graph from visual input to\nrepresent functional object relationships, and infers user intent accordingly.\nA shared control policy then provides rotation assistance during both grasping\nand object interaction, guided by spatial constraints predicted by a\nvision-language model. Our method addresses two key challenges in\ngeneral-purpose, long-horizon shared control: (1) understanding and inferring\ntask-level user intent, and (2) generalizing assistance across diverse objects\nand tasks. Experiments in both simulation and the real world demonstrate that\nTASC improves task efficiency and reduces user input effort compared to prior\nmethods. To the best of our knowledge, this is the first shared control\nframework that supports everyday manipulation tasks with zero-shot\ngeneralization. The code that supports our experiments is publicly available at\nhttps://github.com/fitz0401/tasc."
                },
                "authors": [
                    {
                        "name": "Ze Fu"
                    },
                    {
                        "name": "Pinhao Song"
                    },
                    {
                        "name": "Yutong Hu"
                    },
                    {
                        "name": "Renaud Detry"
                    }
                ],
                "author_detail": {
                    "name": "Renaud Detry"
                },
                "author": "Renaud Detry",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13798v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13798v4",
                "updated": "2025-09-12T17:03:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    17,
                    3,
                    33,
                    4,
                    255,
                    0
                ],
                "published": "2024-05-22T16:23:40Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    16,
                    23,
                    40,
                    2,
                    143,
                    0
                ],
                "title": "Slaves to the Law of Large Numbers: An Asymptotic Equipartition Property\n  for Perplexity in Generative Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Slaves to the Law of Large Numbers: An Asymptotic Equipartition Property\n  for Perplexity in Generative Language Models"
                },
                "summary": "We prove a new asymptotic un-equipartition property for the perplexity of\nlong texts generated by a language model and present supporting experimental\nevidence from open-source models. Specifically we show that the logarithmic\nperplexity of any large text generated by a language model must asymptotically\nconverge to the average entropy of its token distributions. This defines a\n``typical set'' that all long synthetic texts generated by a language model\nmust belong to. We refine the concept of ''typical set'' to include only\ngrammatically correct texts. We then show that this refined typical set is a\nvanishingly small subset of all possible grammatically correct texts for a very\ngeneral definition of grammar. This means that language models are strongly\nconstrained in the range of their possible behaviors and outputs. We make no\nsimplifying assumptions (such as stationarity) about the statistics of language\nmodel outputs, and therefore our results are directly applicable to practical\nreal-world models without any approximations. We discuss possible applications\nof the typical set concept to problems such as detecting synthetic texts and\nmembership inference in training datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We prove a new asymptotic un-equipartition property for the perplexity of\nlong texts generated by a language model and present supporting experimental\nevidence from open-source models. Specifically we show that the logarithmic\nperplexity of any large text generated by a language model must asymptotically\nconverge to the average entropy of its token distributions. This defines a\n``typical set'' that all long synthetic texts generated by a language model\nmust belong to. We refine the concept of ''typical set'' to include only\ngrammatically correct texts. We then show that this refined typical set is a\nvanishingly small subset of all possible grammatically correct texts for a very\ngeneral definition of grammar. This means that language models are strongly\nconstrained in the range of their possible behaviors and outputs. We make no\nsimplifying assumptions (such as stationarity) about the statistics of language\nmodel outputs, and therefore our results are directly applicable to practical\nreal-world models without any approximations. We discuss possible applications\nof the typical set concept to problems such as detecting synthetic texts and\nmembership inference in training datasets."
                },
                "authors": [
                    {
                        "name": "Tyler Bell"
                    },
                    {
                        "name": "Avinash Mudireddy"
                    },
                    {
                        "name": "Ivan Johnson-Eversoll"
                    },
                    {
                        "name": "Soura Dasgupta"
                    },
                    {
                        "name": "Raghu Mudumbai"
                    }
                ],
                "author_detail": {
                    "name": "Raghu Mudumbai"
                },
                "author": "Raghu Mudumbai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13798v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13798v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10405v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10405v1",
                "updated": "2025-09-12T16:54:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    54,
                    56,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T16:54:56Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    54,
                    56,
                    4,
                    255,
                    0
                ],
                "title": "Self-supervised Learning Of Visual Pose Estimation Without Pose Labels\n  By Classifying LED States",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-supervised Learning Of Visual Pose Estimation Without Pose Labels\n  By Classifying LED States"
                },
                "summary": "We introduce a model for monocular RGB relative pose estimation of a ground\nrobot that trains from scratch without pose labels nor prior knowledge about\nthe robot's shape or appearance. At training time, we assume: (i) a robot\nfitted with multiple LEDs, whose states are independent and known at each\nframe; (ii) knowledge of the approximate viewing direction of each LED; and\n(iii) availability of a calibration image with a known target distance, to\naddress the ambiguity of monocular depth estimation. Training data is collected\nby a pair of robots moving randomly without needing external infrastructure or\nhuman supervision. Our model trains on the task of predicting from an image the\nstate of each LED on the robot. In doing so, it learns to predict the position\nof the robot in the image, its distance, and its relative bearing. At inference\ntime, the state of the LEDs is unknown, can be arbitrary, and does not affect\nthe pose estimation performance. Quantitative experiments indicate that our\napproach: is competitive with SoA approaches that require supervision from pose\nlabels or a CAD model of the robot; generalizes to different domains; and\nhandles multi-robot pose estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a model for monocular RGB relative pose estimation of a ground\nrobot that trains from scratch without pose labels nor prior knowledge about\nthe robot's shape or appearance. At training time, we assume: (i) a robot\nfitted with multiple LEDs, whose states are independent and known at each\nframe; (ii) knowledge of the approximate viewing direction of each LED; and\n(iii) availability of a calibration image with a known target distance, to\naddress the ambiguity of monocular depth estimation. Training data is collected\nby a pair of robots moving randomly without needing external infrastructure or\nhuman supervision. Our model trains on the task of predicting from an image the\nstate of each LED on the robot. In doing so, it learns to predict the position\nof the robot in the image, its distance, and its relative bearing. At inference\ntime, the state of the LEDs is unknown, can be arbitrary, and does not affect\nthe pose estimation performance. Quantitative experiments indicate that our\napproach: is competitive with SoA approaches that require supervision from pose\nlabels or a CAD model of the robot; generalizes to different domains; and\nhandles multi-robot pose estimation."
                },
                "authors": [
                    {
                        "name": "Nicholas Carlotti"
                    },
                    {
                        "name": "Mirko Nava"
                    },
                    {
                        "name": "Alessandro Giusti"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Giusti"
                },
                "author": "Alessandro Giusti",
                "arxiv_comment": "accepted at CoRL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10405v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10405v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10402v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10402v1",
                "updated": "2025-09-12T16:52:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    52,
                    49,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T16:52:49Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    52,
                    49,
                    4,
                    255,
                    0
                ],
                "title": "Developer-LLM Conversations: An Empirical Study of Interactions and\n  Generated Code Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developer-LLM Conversations: An Empirical Study of Interactions and\n  Generated Code Quality"
                },
                "summary": "Large Language Models (LLMs) are becoming integral to modern software\ndevelopment workflows, assisting developers with code generation, API\nexplanation, and iterative problem-solving through natural language\nconversations. Despite widespread adoption, there is limited understanding of\nhow developers interact with LLMs in practice and how these conversational\ndynamics influence task outcomes, code quality, and software engineering\nworkflows. To address this, we leverage CodeChat, a large dataset comprising\n82,845 real-world developer-LLM conversations, containing 368,506 code snippets\ngenerated across over 20 programming languages, derived from the WildChat\ndataset. We find that LLM responses are substantially longer than developer\nprompts, with a median token-length ratio of 14:1. Multi-turn conversations\naccount for 68% of the dataset and often evolve due to shifting requirements,\nincomplete prompts, or clarification requests. Topic analysis identifies web\ndesign (9.6% of conversations) and neural network training (8.7% of\nconversations) as the most frequent LLM-assisted tasks. Evaluation across five\nlanguages (i.e., Python, JavaScript, C++, Java, and C#) reveals prevalent and\nlanguage-specific issues in LLM-generated code: generated Python and JavaScript\ncode often include undefined variables (83.4% and 75.3% of code snippets,\nrespectively); Java code lacks required comments (75.9%); C++ code frequently\nomits headers (41.1%) and C# code shows unresolved namespaces (49.2%). During a\nconversation, syntax and import errors persist across turns; however,\ndocumentation quality in Java improves by up to 14.7%, and import handling in\nPython improves by 3.7% over 5 turns. Prompts that point out mistakes in code\ngenerated in prior turns and explicitly request a fix are most effective for\nresolving errors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are becoming integral to modern software\ndevelopment workflows, assisting developers with code generation, API\nexplanation, and iterative problem-solving through natural language\nconversations. Despite widespread adoption, there is limited understanding of\nhow developers interact with LLMs in practice and how these conversational\ndynamics influence task outcomes, code quality, and software engineering\nworkflows. To address this, we leverage CodeChat, a large dataset comprising\n82,845 real-world developer-LLM conversations, containing 368,506 code snippets\ngenerated across over 20 programming languages, derived from the WildChat\ndataset. We find that LLM responses are substantially longer than developer\nprompts, with a median token-length ratio of 14:1. Multi-turn conversations\naccount for 68% of the dataset and often evolve due to shifting requirements,\nincomplete prompts, or clarification requests. Topic analysis identifies web\ndesign (9.6% of conversations) and neural network training (8.7% of\nconversations) as the most frequent LLM-assisted tasks. Evaluation across five\nlanguages (i.e., Python, JavaScript, C++, Java, and C#) reveals prevalent and\nlanguage-specific issues in LLM-generated code: generated Python and JavaScript\ncode often include undefined variables (83.4% and 75.3% of code snippets,\nrespectively); Java code lacks required comments (75.9%); C++ code frequently\nomits headers (41.1%) and C# code shows unresolved namespaces (49.2%). During a\nconversation, syntax and import errors persist across turns; however,\ndocumentation quality in Java improves by up to 14.7%, and import handling in\nPython improves by 3.7% over 5 turns. Prompts that point out mistakes in code\ngenerated in prior turns and explicitly request a fix are most effective for\nresolving errors."
                },
                "authors": [
                    {
                        "name": "Suzhen Zhong"
                    },
                    {
                        "name": "Ying Zou"
                    },
                    {
                        "name": "Bram Adams"
                    }
                ],
                "author_detail": {
                    "name": "Bram Adams"
                },
                "author": "Bram Adams",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10402v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10402v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.0; D.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10401v1",
                "updated": "2025-09-12T16:51:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    51,
                    15,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T16:51:15Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    51,
                    15,
                    4,
                    255,
                    0
                ],
                "title": "Abduct, Act, Predict: Scaffolding Causal Inference for Automated Failure\n  Attribution in Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abduct, Act, Predict: Scaffolding Causal Inference for Automated Failure\n  Attribution in Multi-Agent Systems"
                },
                "summary": "Failure attribution in multi-agent systems -- pinpointing the exact step\nwhere a decisive error occurs -- is a critical yet unsolved challenge. Current\nmethods treat this as a pattern recognition task over long conversation logs,\nleading to critically low step-level accuracy (below 17\\%), which renders them\nimpractical for debugging complex systems. Their core weakness is a fundamental\ninability to perform robust counterfactual reasoning: to determine if\ncorrecting a single action would have actually averted the task failure. To\nbridge this counterfactual inference gap, we introduce Abduct-Act-Predict (A2P)\nScaffolding, a novel agent framework that transforms failure attribution from\npattern recognition into a structured causal inference task. A2P explicitly\nguides a large language model through a formal three-step reasoning process\nwithin a single inference pass: (1) Abduction, to infer the hidden root causes\nbehind an agent's actions; (2) Action, to define a minimal corrective\nintervention; and (3) Prediction, to simulate the subsequent trajectory and\nverify if the intervention resolves the failure. This structured approach\nleverages the holistic context of the entire conversation while imposing a\nrigorous causal logic on the model's analysis. Our extensive experiments on the\nWho\\&When benchmark demonstrate its efficacy. On the Algorithm-Generated\ndataset, A2P achieves 47.46\\% step-level accuracy, a 2.85$\\times$ improvement\nover the 16.67\\% of the baseline. On the more complex Hand-Crafted dataset, it\nachieves 29.31\\% step accuracy, a 2.43$\\times$ improvement over the baseline's\n12.07\\%. By reframing the problem through a causal lens, A2P Scaffolding\nprovides a robust, verifiable, and significantly more accurate solution for\nautomated failure attribution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Failure attribution in multi-agent systems -- pinpointing the exact step\nwhere a decisive error occurs -- is a critical yet unsolved challenge. Current\nmethods treat this as a pattern recognition task over long conversation logs,\nleading to critically low step-level accuracy (below 17\\%), which renders them\nimpractical for debugging complex systems. Their core weakness is a fundamental\ninability to perform robust counterfactual reasoning: to determine if\ncorrecting a single action would have actually averted the task failure. To\nbridge this counterfactual inference gap, we introduce Abduct-Act-Predict (A2P)\nScaffolding, a novel agent framework that transforms failure attribution from\npattern recognition into a structured causal inference task. A2P explicitly\nguides a large language model through a formal three-step reasoning process\nwithin a single inference pass: (1) Abduction, to infer the hidden root causes\nbehind an agent's actions; (2) Action, to define a minimal corrective\nintervention; and (3) Prediction, to simulate the subsequent trajectory and\nverify if the intervention resolves the failure. This structured approach\nleverages the holistic context of the entire conversation while imposing a\nrigorous causal logic on the model's analysis. Our extensive experiments on the\nWho\\&When benchmark demonstrate its efficacy. On the Algorithm-Generated\ndataset, A2P achieves 47.46\\% step-level accuracy, a 2.85$\\times$ improvement\nover the 16.67\\% of the baseline. On the more complex Hand-Crafted dataset, it\nachieves 29.31\\% step accuracy, a 2.43$\\times$ improvement over the baseline's\n12.07\\%. By reframing the problem through a causal lens, A2P Scaffolding\nprovides a robust, verifiable, and significantly more accurate solution for\nautomated failure attribution."
                },
                "authors": [
                    {
                        "name": "Alva West"
                    },
                    {
                        "name": "Yixuan Weng"
                    },
                    {
                        "name": "Minjun Zhu"
                    },
                    {
                        "name": "Zhen Lin"
                    },
                    {
                        "name": "Yue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhang"
                },
                "author": "Yue Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04229v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04229v2",
                "updated": "2025-09-12T16:50:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    50,
                    26,
                    4,
                    255,
                    0
                ],
                "published": "2024-11-06T19:50:09Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    19,
                    50,
                    9,
                    2,
                    311,
                    0
                ],
                "title": "Detecting State Changes in Functional Neuronal Connectivity using\n  Factorial Switching Linear Dynamical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting State Changes in Functional Neuronal Connectivity using\n  Factorial Switching Linear Dynamical Systems"
                },
                "summary": "A key question in brain sciences is how to identify time-evolving functional\nconnectivity, such as that obtained from recordings of neuronal activity over\ntime. We wish to explain the observed phenomena in terms of latent states\nwhich, in the case of neuronal activity, might correspond to subnetworks of\nneurons within a brain or organoid. Many existing approaches assume that only\none latent state can be active at a time, in contrast to our domain knowledge.\nWe propose a switching dynamical system based on the factorial hidden Markov\nmodel. Unlike existing approaches, our model acknowledges that neuronal\nactivity can be caused by multiple subnetworks, which may be activated either\njointly or independently. A change in one part of the network does not mean\nthat the entire connectivity pattern will change. We pair our model with\nscalable variational inference algorithm, using a concrete relaxation of the\nunderlying factorial hidden Markov model, to effectively infer the latent\nstates and model parameters. We show that our algorithm can recover\nground-truth structure and yield insights about the maturation of neuronal\nactivity in microelectrode array recordings from in vitro neuronal cultures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key question in brain sciences is how to identify time-evolving functional\nconnectivity, such as that obtained from recordings of neuronal activity over\ntime. We wish to explain the observed phenomena in terms of latent states\nwhich, in the case of neuronal activity, might correspond to subnetworks of\nneurons within a brain or organoid. Many existing approaches assume that only\none latent state can be active at a time, in contrast to our domain knowledge.\nWe propose a switching dynamical system based on the factorial hidden Markov\nmodel. Unlike existing approaches, our model acknowledges that neuronal\nactivity can be caused by multiple subnetworks, which may be activated either\njointly or independently. A change in one part of the network does not mean\nthat the entire connectivity pattern will change. We pair our model with\nscalable variational inference algorithm, using a concrete relaxation of the\nunderlying factorial hidden Markov model, to effectively infer the latent\nstates and model parameters. We show that our algorithm can recover\nground-truth structure and yield insights about the maturation of neuronal\nactivity in microelectrode array recordings from in vitro neuronal cultures."
                },
                "authors": [
                    {
                        "name": "Yiwei Gong"
                    },
                    {
                        "name": "Susanna B. Mierau"
                    },
                    {
                        "name": "Sinead A. Williamson"
                    }
                ],
                "author_detail": {
                    "name": "Sinead A. Williamson"
                },
                "author": "Sinead A. Williamson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04229v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04229v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "37N25, 62M05, 62P10, 62M10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10397v1",
                "updated": "2025-09-12T16:44:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    44,
                    34,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T16:44:34Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    44,
                    34,
                    4,
                    255,
                    0
                ],
                "title": "RecoWorld: Building Simulated Environments for Agentic Recommender\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RecoWorld: Building Simulated Environments for Agentic Recommender\n  Systems"
                },
                "summary": "We present RecoWorld, a blueprint for building simulated environments\ntailored to agentic recommender systems. Such environments give agents a proper\ntraining space where they can learn from errors without impacting real users.\nRecoWorld distinguishes itself with a dual-view architecture: a simulated user\nand an agentic recommender engage in multi-turn interactions aimed at\nmaximizing user retention. The user simulator reviews recommended items,\nupdates its mindset, and when sensing potential user disengagement, generates\nreflective instructions. The agentic recommender adapts its recommendations by\nincorporating these user instructions and reasoning traces, creating a dynamic\nfeedback loop that actively engages users. This process leverages the\nexceptional reasoning capabilities of modern LLMs. We explore diverse content\nrepresentations within the simulator, including text-based, multimodal, and\nsemantic ID modeling, and discuss how multi-turn RL enables the recommender to\nrefine its strategies through iterative interactions. RecoWorld also supports\nmulti-agent simulations, allowing creators to simulate the responses of\ntargeted user populations. It marks an important first step toward recommender\nsystems where users and agents collaboratively shape personalized information\nstreams. We envision new interaction paradigms where \"user instructs,\nrecommender responds,\" jointly optimizing user retention and engagement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present RecoWorld, a blueprint for building simulated environments\ntailored to agentic recommender systems. Such environments give agents a proper\ntraining space where they can learn from errors without impacting real users.\nRecoWorld distinguishes itself with a dual-view architecture: a simulated user\nand an agentic recommender engage in multi-turn interactions aimed at\nmaximizing user retention. The user simulator reviews recommended items,\nupdates its mindset, and when sensing potential user disengagement, generates\nreflective instructions. The agentic recommender adapts its recommendations by\nincorporating these user instructions and reasoning traces, creating a dynamic\nfeedback loop that actively engages users. This process leverages the\nexceptional reasoning capabilities of modern LLMs. We explore diverse content\nrepresentations within the simulator, including text-based, multimodal, and\nsemantic ID modeling, and discuss how multi-turn RL enables the recommender to\nrefine its strategies through iterative interactions. RecoWorld also supports\nmulti-agent simulations, allowing creators to simulate the responses of\ntargeted user populations. It marks an important first step toward recommender\nsystems where users and agents collaboratively shape personalized information\nstreams. We envision new interaction paradigms where \"user instructs,\nrecommender responds,\" jointly optimizing user retention and engagement."
                },
                "authors": [
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Xinyu Lin"
                    },
                    {
                        "name": "Hanchao Yu"
                    },
                    {
                        "name": "Mingyuan Wu"
                    },
                    {
                        "name": "Jianyu Wang"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Zhuokai Zhao"
                    },
                    {
                        "name": "Yinglong Xia"
                    },
                    {
                        "name": "Yao Zhang"
                    },
                    {
                        "name": "Weiwei Li"
                    },
                    {
                        "name": "Mingze Gao"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Lizhu Zhang"
                    },
                    {
                        "name": "Benyu Zhang"
                    },
                    {
                        "name": "Xiangjun Fan"
                    }
                ],
                "author_detail": {
                    "name": "Xiangjun Fan"
                },
                "author": "Xiangjun Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10396v1",
                "updated": "2025-09-12T16:44:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    44,
                    31,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T16:44:31Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    44,
                    31,
                    4,
                    255,
                    0
                ],
                "title": "Inpainting-Guided Policy Optimization for Diffusion Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inpainting-Guided Policy Optimization for Diffusion Large Language\n  Models"
                },
                "summary": "Masked diffusion large language models (dLLMs) are emerging as promising\nalternatives to autoregressive LLMs, offering competitive performance while\nsupporting unique generation capabilities such as inpainting. We explore how\ninpainting can inform RL algorithm design for dLLMs. Aligning LLMs with\nreinforcement learning faces an exploration challenge: sparse reward signals\nand sample waste when models fail to discover correct solutions. While this\ninefficiency affects LLMs broadly, dLLMs offer a distinctive opportunity--their\ninpainting ability can guide exploration. We introduce IGPO (Inpainting Guided\nPolicy Optimization), an RL framework that strategically inserts partial\nground-truth reasoning traces during online sampling. Unlike providing full\nsolutions, inpainting steers exploration toward promising trajectory spaces\nwhile preserving self-generated reasoning, bridging supervised fine-tuning and\nreinforcement learning. We apply IGPO to group-based optimization methods such\nas GRPO, where exploration failures cause zero advantages and gradients. IGPO\nrestores meaningful gradients while improving sample efficiency. We also\npropose supervised fine-tuning on synthetically rewritten concise traces that\nbetter align with dLLM generation patterns. With additional techniques\nincluding entropy-based filtering, our training recipe yields substantial gains\nacross three mathematical benchmarks--GSM8K, Math500, and AMC--achieving new\nstate-of-the-art results for full-attention masked dLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked diffusion large language models (dLLMs) are emerging as promising\nalternatives to autoregressive LLMs, offering competitive performance while\nsupporting unique generation capabilities such as inpainting. We explore how\ninpainting can inform RL algorithm design for dLLMs. Aligning LLMs with\nreinforcement learning faces an exploration challenge: sparse reward signals\nand sample waste when models fail to discover correct solutions. While this\ninefficiency affects LLMs broadly, dLLMs offer a distinctive opportunity--their\ninpainting ability can guide exploration. We introduce IGPO (Inpainting Guided\nPolicy Optimization), an RL framework that strategically inserts partial\nground-truth reasoning traces during online sampling. Unlike providing full\nsolutions, inpainting steers exploration toward promising trajectory spaces\nwhile preserving self-generated reasoning, bridging supervised fine-tuning and\nreinforcement learning. We apply IGPO to group-based optimization methods such\nas GRPO, where exploration failures cause zero advantages and gradients. IGPO\nrestores meaningful gradients while improving sample efficiency. We also\npropose supervised fine-tuning on synthetically rewritten concise traces that\nbetter align with dLLM generation patterns. With additional techniques\nincluding entropy-based filtering, our training recipe yields substantial gains\nacross three mathematical benchmarks--GSM8K, Math500, and AMC--achieving new\nstate-of-the-art results for full-attention masked dLLMs."
                },
                "authors": [
                    {
                        "name": "Siyan Zhao"
                    },
                    {
                        "name": "Mengchen Liu"
                    },
                    {
                        "name": "Jing Huang"
                    },
                    {
                        "name": "Miao Liu"
                    },
                    {
                        "name": "Chenyu Wang"
                    },
                    {
                        "name": "Bo Liu"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Guan Pang"
                    },
                    {
                        "name": "Sean Bell"
                    },
                    {
                        "name": "Aditya Grover"
                    },
                    {
                        "name": "Feiyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Feiyu Chen"
                },
                "author": "Feiyu Chen",
                "arxiv_comment": "preprint; 21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02896v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02896v2",
                "updated": "2025-09-12T16:30:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    30,
                    38,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-02T23:41:50Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    23,
                    41,
                    50,
                    1,
                    245,
                    0
                ],
                "title": "Cut Costs, Not Accuracy: LLM-Powered Data Processing with Guarantees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cut Costs, Not Accuracy: LLM-Powered Data Processing with Guarantees"
                },
                "summary": "Large Language Models (LLMs) are being increasingly used as a building block\nin data systems to process large text datasets. To do so, LLM model providers\noffer multiple LLMs with different sizes, spanning various cost-quality\ntrade-offs when processing text at scale. Top-of-the-line LLMs (e.g., GPT-4o,\nClaude Sonnet) operate with high accuracy but are prohibitively expensive when\nprocessing many records. To avoid high costs, more affordable but lower quality\nLLMs (e.g., GPT-4o-mini, Claude Haiku) can be used to process records, but we\nneed to ensure that the overall accuracy does not deviate substantially from\nthat of the top-of-the-line LLMs. The model cascade framework provides a\nblueprint to manage this trade-off, by using the confidence of LLMs in their\noutput (e.g., log-probabilities) to decide on which records to use the\naffordable LLM. However, existing solutions following this framework provide\nonly marginal cost savings and weak theoretical guarantees because of poor\nestimation of the quality of the affordable LLM's outputs. We present BARGAIN,\na method that judiciously uses affordable LLMs in data processing to\nsignificantly reduce cost while providing strong theoretical guarantees on the\nsolution quality. BARGAIN employs a novel adaptive sampling strategy and\nstatistical estimation procedure that uses data and task characteristics and\nbuilds on recent statistical tools to make accurate estimations with tight\ntheoretical guarantees. Variants of BARGAIN can support guarantees on accuracy,\nprecision, or recall of the output. Experimental results across 8 real-world\ndatasets show that BARGAIN reduces cost, on average, by up to 86% more than\nstate-of-the-art, while providing stronger theoretical guarantees on accuracy\nof output, with similar gains when guaranteeing a desired level of precision or\nrecall.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are being increasingly used as a building block\nin data systems to process large text datasets. To do so, LLM model providers\noffer multiple LLMs with different sizes, spanning various cost-quality\ntrade-offs when processing text at scale. Top-of-the-line LLMs (e.g., GPT-4o,\nClaude Sonnet) operate with high accuracy but are prohibitively expensive when\nprocessing many records. To avoid high costs, more affordable but lower quality\nLLMs (e.g., GPT-4o-mini, Claude Haiku) can be used to process records, but we\nneed to ensure that the overall accuracy does not deviate substantially from\nthat of the top-of-the-line LLMs. The model cascade framework provides a\nblueprint to manage this trade-off, by using the confidence of LLMs in their\noutput (e.g., log-probabilities) to decide on which records to use the\naffordable LLM. However, existing solutions following this framework provide\nonly marginal cost savings and weak theoretical guarantees because of poor\nestimation of the quality of the affordable LLM's outputs. We present BARGAIN,\na method that judiciously uses affordable LLMs in data processing to\nsignificantly reduce cost while providing strong theoretical guarantees on the\nsolution quality. BARGAIN employs a novel adaptive sampling strategy and\nstatistical estimation procedure that uses data and task characteristics and\nbuilds on recent statistical tools to make accurate estimations with tight\ntheoretical guarantees. Variants of BARGAIN can support guarantees on accuracy,\nprecision, or recall of the output. Experimental results across 8 real-world\ndatasets show that BARGAIN reduces cost, on average, by up to 86% more than\nstate-of-the-art, while providing stronger theoretical guarantees on accuracy\nof output, with similar gains when guaranteeing a desired level of precision or\nrecall."
                },
                "authors": [
                    {
                        "name": "Sepanta Zeighami"
                    },
                    {
                        "name": "Shreya Shankar"
                    },
                    {
                        "name": "Aditya Parameswaran"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Parameswaran"
                },
                "author": "Aditya Parameswaran",
                "arxiv_comment": "To appear in SIGMOD'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02896v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02896v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18486v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18486v2",
                "updated": "2025-09-12T16:15:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    15,
                    25,
                    4,
                    255,
                    0
                ],
                "published": "2024-10-24T07:21:33Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    7,
                    21,
                    33,
                    3,
                    298,
                    0
                ],
                "title": "Evolving Voices Based on Temporal Poisson Factorisation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolving Voices Based on Temporal Poisson Factorisation"
                },
                "summary": "The world is evolving and so is the vocabulary used to discuss topics in\nspeech. Analysing political speech data from more than 30 years requires the\nuse of flexible topic models to uncover the latent topics and their change in\nprevalence over time as well as the change in the vocabulary of the topics. We\npropose the temporal Poisson factorisation (TPF) model as an extension to the\nPoisson factorisation model to model sparse count data matrices obtained based\non the bag-of-words assumption from text documents with time stamps. We discuss\nand empirically compare different model specifications for the time-varying\nlatent variables consisting either of a flexible auto-regressive structure of\norder one or a random walk. Estimation is based on variational inference where\nwe consider a combination of coordinate ascent updates with automatic\ndifferentiation using batching of documents. Suitable variational families are\nproposed to ease inference. We compare results obtained using independent\nunivariate variational distributions for the time-varying latent variables to\nthose obtained with a multivariate variant. We discuss in detail the results of\nthe TPF model when analysing speeches from 18 sessions in the U.S. Senate\n(1981-2016).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The world is evolving and so is the vocabulary used to discuss topics in\nspeech. Analysing political speech data from more than 30 years requires the\nuse of flexible topic models to uncover the latent topics and their change in\nprevalence over time as well as the change in the vocabulary of the topics. We\npropose the temporal Poisson factorisation (TPF) model as an extension to the\nPoisson factorisation model to model sparse count data matrices obtained based\non the bag-of-words assumption from text documents with time stamps. We discuss\nand empirically compare different model specifications for the time-varying\nlatent variables consisting either of a flexible auto-regressive structure of\norder one or a random walk. Estimation is based on variational inference where\nwe consider a combination of coordinate ascent updates with automatic\ndifferentiation using batching of documents. Suitable variational families are\nproposed to ease inference. We compare results obtained using independent\nunivariate variational distributions for the time-varying latent variables to\nthose obtained with a multivariate variant. We discuss in detail the results of\nthe TPF model when analysing speeches from 18 sessions in the U.S. Senate\n(1981-2016)."
                },
                "authors": [
                    {
                        "name": "Jan Vávra"
                    },
                    {
                        "name": "Bettina Grün"
                    },
                    {
                        "name": "Paul Hofmarcher"
                    }
                ],
                "author_detail": {
                    "name": "Paul Hofmarcher"
                },
                "arxiv_affiliation": "Paris-Lodron University of Salzburg",
                "author": "Paul Hofmarcher",
                "arxiv_doi": "10.1177/1471082X251355682",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1177/1471082X251355682",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.18486v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18486v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "main paper: 20 pages (2 single figures, 3 double figures, 3 tables),\n  appendix: 2 pages, supplementary materials: 18 pages (2 plots, 4 quadruple\n  plots, 2 tables), references: 3 pages",
                "arxiv_journal_ref": "Statistical Modelling. 2025;0(0)",
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F15 (Primary) 62H99, 68U15 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.3; I.7.m",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08822v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08822v2",
                "updated": "2025-09-12T16:09:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    9,
                    40,
                    4,
                    255,
                    0
                ],
                "published": "2024-09-13T13:36:30Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    13,
                    36,
                    30,
                    4,
                    257,
                    0
                ],
                "title": "Development of a Compton Imager Setup",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development of a Compton Imager Setup"
                },
                "summary": "Hard X-ray photons with energies in the range of hundreds of keV typically\nundergo Compton scattering when they are incident on a detector. In this\nprocess, an incident photon deposits a fraction of its energy at the point of\nincidence and continues onwards with a change in direction that depends on the\namount of energy deposited. By using a pair of detectors to detect the point of\nincidence and the direction of the scattered photon, we can calculate the\nscattering direction and angle. The position of a source in the sky can be\nreconstructed using many Compton photon pairs from a source. We demonstrate\nthis principle in the laboratory by using a pair of Cadmium Zinc Telluride\n(CZT) detectors sensitive in the energy range of 20-200 keV, similar to those\nused in $\\textit{AstroSat}$/CZT Imager (CZTI). The laboratory setup consists of\nthe two detectors placed perpendicular to each other in a lead-lined box. The\ndetectors are read out by a custom-programmed Xilinx PYNQ-Z2 FPGA board, and\ndata are then transferred to a personal computer (PC)}. There are two key\nupdates from CZTI: the detectors are read concurrently rather than serially,\nand the time resolution has been improved from $20~\\mu$s to $7.5~\\mu$s. We\nirradiated the detectors with a collimated $^{133}\\mathrm{Ba}$ source and\nidentified Compton scattering events for the 356 keV line. We run a Compton\nreconstruction algorithm to correctly infer the location of the source in the\ndetector frame, with a location-dependent angular response measure of\n$16\\deg-30\\deg$. This comprises a successful technology demonstration for a\nCompton imaging camera in the hard X-ray regime. We present the details of our\nsetup, the data acquisition process, and software algorithms, and showcase our\nresults. We also quantify the limitations of this setup and discuss ways of\nimproving the performance in future experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hard X-ray photons with energies in the range of hundreds of keV typically\nundergo Compton scattering when they are incident on a detector. In this\nprocess, an incident photon deposits a fraction of its energy at the point of\nincidence and continues onwards with a change in direction that depends on the\namount of energy deposited. By using a pair of detectors to detect the point of\nincidence and the direction of the scattered photon, we can calculate the\nscattering direction and angle. The position of a source in the sky can be\nreconstructed using many Compton photon pairs from a source. We demonstrate\nthis principle in the laboratory by using a pair of Cadmium Zinc Telluride\n(CZT) detectors sensitive in the energy range of 20-200 keV, similar to those\nused in $\\textit{AstroSat}$/CZT Imager (CZTI). The laboratory setup consists of\nthe two detectors placed perpendicular to each other in a lead-lined box. The\ndetectors are read out by a custom-programmed Xilinx PYNQ-Z2 FPGA board, and\ndata are then transferred to a personal computer (PC)}. There are two key\nupdates from CZTI: the detectors are read concurrently rather than serially,\nand the time resolution has been improved from $20~\\mu$s to $7.5~\\mu$s. We\nirradiated the detectors with a collimated $^{133}\\mathrm{Ba}$ source and\nidentified Compton scattering events for the 356 keV line. We run a Compton\nreconstruction algorithm to correctly infer the location of the source in the\ndetector frame, with a location-dependent angular response measure of\n$16\\deg-30\\deg$. This comprises a successful technology demonstration for a\nCompton imaging camera in the hard X-ray regime. We present the details of our\nsetup, the data acquisition process, and software algorithms, and showcase our\nresults. We also quantify the limitations of this setup and discuss ways of\nimproving the performance in future experiments."
                },
                "authors": [
                    {
                        "name": "Anuraag Arya"
                    },
                    {
                        "name": "Harmanjeet Singh Bilkhu"
                    },
                    {
                        "name": "Sandeep Vishwakarma"
                    },
                    {
                        "name": "Hrishikesh Belatikar"
                    },
                    {
                        "name": "Varun Bhalerao"
                    },
                    {
                        "name": "Abhijeet Ghodgaonkar"
                    },
                    {
                        "name": "Jayprakash G. Koyande"
                    },
                    {
                        "name": "Aditi Marathe"
                    },
                    {
                        "name": "N. P. S. Mithun"
                    },
                    {
                        "name": "Sanjoli Narang"
                    },
                    {
                        "name": "Sudhanshu Nimbalkar"
                    },
                    {
                        "name": "Pranav Page"
                    },
                    {
                        "name": "Sourav Palit"
                    },
                    {
                        "name": "Arpit Patel"
                    },
                    {
                        "name": "Amit Shetye"
                    },
                    {
                        "name": "Siddharth Tallur"
                    },
                    {
                        "name": "Shriharsh Tendulkar"
                    },
                    {
                        "name": "Santosh Vadawale"
                    },
                    {
                        "name": "Gaurav Waratkar"
                    }
                ],
                "author_detail": {
                    "name": "Gaurav Waratkar"
                },
                "author": "Gaurav Waratkar",
                "arxiv_comment": "15 pages, 11 figures, accepted for publication in The Journal of\n  Astrophysics and Astronomy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08822v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08822v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10377v1",
                "updated": "2025-09-12T16:09:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    9,
                    39,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T16:09:39Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    9,
                    39,
                    4,
                    255,
                    0
                ],
                "title": "Dropping Experts, Recombining Neurons: Retraining-Free Pruning for\n  Sparse Mixture-of-Experts LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dropping Experts, Recombining Neurons: Retraining-Free Pruning for\n  Sparse Mixture-of-Experts LLMs"
                },
                "summary": "Sparse Mixture-of-Experts (SMoE) architectures are widely used in large\nlanguage models (LLMs) due to their computational efficiency. However, though\nonly a few experts are activated for each token, SMoE still requires loading\nall expert parameters, leading to high memory usage and challenges in\ndeployment. Previous work has tried to reduce the overhead by pruning and\nmerging experts, but primarily focused on expert-level operations, leaving\nneuron-level structure underexplored. We propose DERN (Dropping Experts,\nRecombining Neurons), a task-agnostic and retraining-free framework for expert\npruning and reconstruction. We observe that experts are often misaligned and\ncontain semantic conflicts at the neuron level, which poses challenges for\ndirect merging. To solve this, DERN works in three steps: it first prunes\nredundant experts using router statistics; then it decomposes them into\nneuron-level expert segments, assigning each segment to its most compatible\nretained expert; and finally, it merges segments within each retained expert to\nbuild a compact representation. Experiments on Mixtral, Qwen, and DeepSeek SMoE\nmodels show that DERN improves performance by more than 5% on commonsense\nreasoning and MMLU benchmarks under 50% expert sparsity, without extra\ntraining. It also greatly reduces the number of experts and memory usage,\nmaking SMoE LLMs easier to deploy in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Mixture-of-Experts (SMoE) architectures are widely used in large\nlanguage models (LLMs) due to their computational efficiency. However, though\nonly a few experts are activated for each token, SMoE still requires loading\nall expert parameters, leading to high memory usage and challenges in\ndeployment. Previous work has tried to reduce the overhead by pruning and\nmerging experts, but primarily focused on expert-level operations, leaving\nneuron-level structure underexplored. We propose DERN (Dropping Experts,\nRecombining Neurons), a task-agnostic and retraining-free framework for expert\npruning and reconstruction. We observe that experts are often misaligned and\ncontain semantic conflicts at the neuron level, which poses challenges for\ndirect merging. To solve this, DERN works in three steps: it first prunes\nredundant experts using router statistics; then it decomposes them into\nneuron-level expert segments, assigning each segment to its most compatible\nretained expert; and finally, it merges segments within each retained expert to\nbuild a compact representation. Experiments on Mixtral, Qwen, and DeepSeek SMoE\nmodels show that DERN improves performance by more than 5% on commonsense\nreasoning and MMLU benchmarks under 50% expert sparsity, without extra\ntraining. It also greatly reduces the number of experts and memory usage,\nmaking SMoE LLMs easier to deploy in practice."
                },
                "authors": [
                    {
                        "name": "Yixiao Zhou"
                    },
                    {
                        "name": "Ziyu Zhao"
                    },
                    {
                        "name": "Dongzhou Cheng"
                    },
                    {
                        "name": "zhiliang wu"
                    },
                    {
                        "name": "Jie Gui"
                    },
                    {
                        "name": "Yi Yang"
                    },
                    {
                        "name": "Fei Wu"
                    },
                    {
                        "name": "Yu Cheng"
                    },
                    {
                        "name": "Hehe Fan"
                    }
                ],
                "author_detail": {
                    "name": "Hehe Fan"
                },
                "author": "Hehe Fan",
                "arxiv_comment": "Accepted to EMNLP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10372v1",
                "updated": "2025-09-12T16:05:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    5,
                    27,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T16:05:27Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    5,
                    27,
                    4,
                    255,
                    0
                ],
                "title": "MCBP: A Memory-Compute Efficient LLM Inference Accelerator Leveraging\n  Bit-Slice-enabled Sparsity and Repetitiveness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCBP: A Memory-Compute Efficient LLM Inference Accelerator Leveraging\n  Bit-Slice-enabled Sparsity and Repetitiveness"
                },
                "summary": "Large language models (LLMs) face significant inference latency due to\ninefficiencies in GEMM operations, weight access, and KV cache access,\nespecially in real-time scenarios. This highlights the need for a versatile\ncompute-memory efficient accelerator. Unfortunately, existing Transformer\naccelerators struggle to address both aspects simultaneously, as they focus on\nvalue-level processing, missing fine-grained opportunities to optimize\ncomputation and memory collaboratively. This paper introduces MCBP, a\nbit-grained compute-memory efficient algorithm-hardware co-design that\nleverages bit-slice (BS) enabled repetitiveness and sparsity to accelerate LLM\ninference. MCBP features three key innovations: 1) BS-repetitiveness-enabled\ncomputation reduction (BRCR), which eliminates redundant GEMM computations via\nleveraging redundancy hidden among BS vectors; 2) BS-sparsity-enabled two-state\ncoding (BSTC), which reduces weight access via exploiting significant sparsity\nin high-order bit-slice weight; 3) Bit-grained progressive prediction (BGPP),\nwhich reduces KV cache access by leveraging early-termination-based bit-grained\nprediction. These techniques, supported by custom accelerator designs,\neffectively alleviate the burden in GEMM, weight access, and KV cache access.\nExtensive experiments on 26 benchmarks show that MCBP achieves 9.43x speed up\nand 31.1x higher energy efficiency than Nvidia A100 GPU. Compared to SOTA\nTransformer accelerators, MCBP achieves 35x, 5.2x and 3.2x energy saving than\nSpatten, FACT and SOFA, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face significant inference latency due to\ninefficiencies in GEMM operations, weight access, and KV cache access,\nespecially in real-time scenarios. This highlights the need for a versatile\ncompute-memory efficient accelerator. Unfortunately, existing Transformer\naccelerators struggle to address both aspects simultaneously, as they focus on\nvalue-level processing, missing fine-grained opportunities to optimize\ncomputation and memory collaboratively. This paper introduces MCBP, a\nbit-grained compute-memory efficient algorithm-hardware co-design that\nleverages bit-slice (BS) enabled repetitiveness and sparsity to accelerate LLM\ninference. MCBP features three key innovations: 1) BS-repetitiveness-enabled\ncomputation reduction (BRCR), which eliminates redundant GEMM computations via\nleveraging redundancy hidden among BS vectors; 2) BS-sparsity-enabled two-state\ncoding (BSTC), which reduces weight access via exploiting significant sparsity\nin high-order bit-slice weight; 3) Bit-grained progressive prediction (BGPP),\nwhich reduces KV cache access by leveraging early-termination-based bit-grained\nprediction. These techniques, supported by custom accelerator designs,\neffectively alleviate the burden in GEMM, weight access, and KV cache access.\nExtensive experiments on 26 benchmarks show that MCBP achieves 9.43x speed up\nand 31.1x higher energy efficiency than Nvidia A100 GPU. Compared to SOTA\nTransformer accelerators, MCBP achieves 35x, 5.2x and 3.2x energy saving than\nSpatten, FACT and SOFA, respectively."
                },
                "authors": [
                    {
                        "name": "Huizheng Wang"
                    },
                    {
                        "name": "Zichuan Wang"
                    },
                    {
                        "name": "Zhiheng Yue"
                    },
                    {
                        "name": "Yousheng Long"
                    },
                    {
                        "name": "Taiquan Wei"
                    },
                    {
                        "name": "Jianxun Yang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Shaojun Wei"
                    },
                    {
                        "name": "Yang Hu"
                    },
                    {
                        "name": "Shouyi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Shouyi Yin"
                },
                "author": "Shouyi Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10371v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10371v1",
                "updated": "2025-09-12T16:05:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    5,
                    7,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T16:05:07Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    5,
                    7,
                    4,
                    255,
                    0
                ],
                "title": "Characterizing the Efficiency of Distributed Training: A Power,\n  Performance, and Thermal Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing the Efficiency of Distributed Training: A Power,\n  Performance, and Thermal Perspective"
                },
                "summary": "The rapid scaling of Large Language Models (LLMs) has pushed training\nworkloads far beyond the limits of single-node analysis, demanding a deeper\nunderstanding of how these models behave across large-scale, multi-GPU systems.\nIn this paper, we present a comprehensive characterization of LLM training\nacross diverse real-world workloads and hardware platforms, including NVIDIA\nH100/H200 and AMD MI250 GPUs. We analyze dense and sparse models under various\nparallelism strategies -- tensor, pipeline, data, and expert -- and evaluate\ntheir effects on hardware utilization, power consumption, and thermal behavior.\nWe further evaluate the effectiveness of optimizations such as activation\nrecomputation and compute-communication overlap. Our findings show that\nperformance is not determined solely by scaling hardware capacity. Scale-up\nsystems with fewer, higher-memory GPUs can outperform scale-out systems in\ncommunication-bound regimes, but only under carefully tuned configurations; in\nother cases, scale-out deployments achieve superior throughput. We also show\nthat certain parallelism combinations, such as tensor with pipeline, lead to\nbandwidth underutilization due to inefficient data chunking, while increasing\nmicrobatch sizes beyond a certain point induces bursty execution and peak power\nexcursions that worsen thermal throttling. These insights reveal how training\nperformance is shaped by complex interactions between hardware, system\ntopology, and model execution. We conclude by offering recommendations for\nsystem and hardware design to improve the scalability and reliability of future\nLLM systems and workloads. The source code of this project is available at\nhttps://github.com/sitar-lab/CharLLM-PPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid scaling of Large Language Models (LLMs) has pushed training\nworkloads far beyond the limits of single-node analysis, demanding a deeper\nunderstanding of how these models behave across large-scale, multi-GPU systems.\nIn this paper, we present a comprehensive characterization of LLM training\nacross diverse real-world workloads and hardware platforms, including NVIDIA\nH100/H200 and AMD MI250 GPUs. We analyze dense and sparse models under various\nparallelism strategies -- tensor, pipeline, data, and expert -- and evaluate\ntheir effects on hardware utilization, power consumption, and thermal behavior.\nWe further evaluate the effectiveness of optimizations such as activation\nrecomputation and compute-communication overlap. Our findings show that\nperformance is not determined solely by scaling hardware capacity. Scale-up\nsystems with fewer, higher-memory GPUs can outperform scale-out systems in\ncommunication-bound regimes, but only under carefully tuned configurations; in\nother cases, scale-out deployments achieve superior throughput. We also show\nthat certain parallelism combinations, such as tensor with pipeline, lead to\nbandwidth underutilization due to inefficient data chunking, while increasing\nmicrobatch sizes beyond a certain point induces bursty execution and peak power\nexcursions that worsen thermal throttling. These insights reveal how training\nperformance is shaped by complex interactions between hardware, system\ntopology, and model execution. We conclude by offering recommendations for\nsystem and hardware design to improve the scalability and reliability of future\nLLM systems and workloads. The source code of this project is available at\nhttps://github.com/sitar-lab/CharLLM-PPT."
                },
                "authors": [
                    {
                        "name": "Seokjin Go"
                    },
                    {
                        "name": "Joongun Park"
                    },
                    {
                        "name": "Spandan More"
                    },
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Irene Wang"
                    },
                    {
                        "name": "Aaron Jezghani"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Divya Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Divya Mahajan"
                },
                "author": "Divya Mahajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10371v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10371v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10370v1",
                "updated": "2025-09-12T16:03:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    3,
                    48,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T16:03:48Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    3,
                    48,
                    4,
                    255,
                    0
                ],
                "title": "The Language of Approval: Identifying the Drivers of Positive Feedback\n  Online",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Language of Approval: Identifying the Drivers of Positive Feedback\n  Online"
                },
                "summary": "Positive feedback via likes and awards is central to online governance, yet\nwhich attributes of users' posts elicit rewards -- and how these vary across\nauthors and communities -- remains unclear. To examine this, we combine\nquasi-experimental causal inference with predictive modeling on 11M posts from\n100 subreddits. We identify linguistic patterns and stylistic attributes\ncausally linked to rewards, controlling for author reputation, timing, and\ncommunity context. For example, overtly complicated language, tentative style,\nand toxicity reduce rewards. We use our set of curated features to train models\nthat can detect highly-upvoted posts with high AUC. Our audit of community\nguidelines highlights a ``policy-practice gap'' -- most rules focus primarily\non civility and formatting requirements, with little emphasis on the attributes\nidentified to drive positive feedback. These results inform the design of\ncommunity guidelines, support interfaces that teach users how to craft\ndesirable contributions, and moderation workflows that emphasize positive\nreinforcement over purely punitive enforcement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Positive feedback via likes and awards is central to online governance, yet\nwhich attributes of users' posts elicit rewards -- and how these vary across\nauthors and communities -- remains unclear. To examine this, we combine\nquasi-experimental causal inference with predictive modeling on 11M posts from\n100 subreddits. We identify linguistic patterns and stylistic attributes\ncausally linked to rewards, controlling for author reputation, timing, and\ncommunity context. For example, overtly complicated language, tentative style,\nand toxicity reduce rewards. We use our set of curated features to train models\nthat can detect highly-upvoted posts with high AUC. Our audit of community\nguidelines highlights a ``policy-practice gap'' -- most rules focus primarily\non civility and formatting requirements, with little emphasis on the attributes\nidentified to drive positive feedback. These results inform the design of\ncommunity guidelines, support interfaces that teach users how to craft\ndesirable contributions, and moderation workflows that emphasize positive\nreinforcement over purely punitive enforcement."
                },
                "authors": [
                    {
                        "name": "Agam Goyal"
                    },
                    {
                        "name": "Charlotte Lambert"
                    },
                    {
                        "name": "Eshwar Chandrasekharan"
                    }
                ],
                "author_detail": {
                    "name": "Eshwar Chandrasekharan"
                },
                "author": "Eshwar Chandrasekharan",
                "arxiv_comment": "Preprint: 21 pages, 7 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10356v1",
                "updated": "2025-09-12T15:45:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    15,
                    45,
                    10,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T15:45:10Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    15,
                    45,
                    10,
                    4,
                    255,
                    0
                ],
                "title": "Constrained Variational Inference via Safe Particle Flow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constrained Variational Inference via Safe Particle Flow"
                },
                "summary": "We propose a control barrier function (CBF) formulation for enforcing\nequality and inequality constraints in variational inference. The key idea is\nto define a barrier functional on the space of probability density functions\nthat encode the desired constraints imposed on the variational density. By\nleveraging the Liouville equation, we establish a connection between the time\nderivative of the variational density and the particle drift, which enables the\nsystematic construction of corresponding CBFs associated to the particle drift.\nEnforcing these CBFs gives rise to the safe particle flow and ensures that the\nvariational density satisfies the original constraints imposed by the barrier\nfunctional. This formulation provides a principled and computationally\ntractable solution to constrained variational inference, with theoretical\nguarantees of constraint satisfaction. The effectiveness of the method is\ndemonstrated through numerical simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a control barrier function (CBF) formulation for enforcing\nequality and inequality constraints in variational inference. The key idea is\nto define a barrier functional on the space of probability density functions\nthat encode the desired constraints imposed on the variational density. By\nleveraging the Liouville equation, we establish a connection between the time\nderivative of the variational density and the particle drift, which enables the\nsystematic construction of corresponding CBFs associated to the particle drift.\nEnforcing these CBFs gives rise to the safe particle flow and ensures that the\nvariational density satisfies the original constraints imposed by the barrier\nfunctional. This formulation provides a principled and computationally\ntractable solution to constrained variational inference, with theoretical\nguarantees of constraint satisfaction. The effectiveness of the method is\ndemonstrated through numerical simulations."
                },
                "authors": [
                    {
                        "name": "Yinzhuang Yi"
                    },
                    {
                        "name": "Jorge Cortés"
                    },
                    {
                        "name": "Nikolay Atanasov"
                    }
                ],
                "author_detail": {
                    "name": "Nikolay Atanasov"
                },
                "author": "Nikolay Atanasov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18173v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18173v3",
                "updated": "2025-09-12T15:39:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    15,
                    39,
                    0,
                    4,
                    255,
                    0
                ],
                "published": "2024-06-26T08:44:36Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    8,
                    44,
                    36,
                    2,
                    178,
                    0
                ],
                "title": "UIO-LLMs: Unbiased Incremental Optimization for Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UIO-LLMs: Unbiased Incremental Optimization for Long-Context LLMs"
                },
                "summary": "Managing long texts is challenging for large language models (LLMs) due to\nlimited context window sizes. This study introduces UIO-LLMs, an unbiased\nincremental optimization approach for memory-enhanced transformers under\nlong-context settings. We initially conceptualize the process as a streamlined\nencoder-decoder framework where the weights-shared encoder and decoder\nrespectively encapsulate a context segment into memories and leverage these\nmemories to predict outputs of the subsequent segment. Subsequently, by\ntreating our memory-enhanced transformers as fully-connected recurrent neural\nnetworks (RNNs), we refine the training process using the Truncated\nBackpropagation Through Time (TBPTT) algorithm, which incorporates innovative\nincremental optimization techniques. These techniques not only diminish time\ncomplexity but also address the bias in gradient computation through an\nunbiased optimization process. UIO-LLMs successfully handle long context, such\nas extending the context window of Llama2-7b-chat from 4K to 100K tokens with\nminimal 2% additional parameters, while keeping the inference cost nearly\nlinear as context length increases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Managing long texts is challenging for large language models (LLMs) due to\nlimited context window sizes. This study introduces UIO-LLMs, an unbiased\nincremental optimization approach for memory-enhanced transformers under\nlong-context settings. We initially conceptualize the process as a streamlined\nencoder-decoder framework where the weights-shared encoder and decoder\nrespectively encapsulate a context segment into memories and leverage these\nmemories to predict outputs of the subsequent segment. Subsequently, by\ntreating our memory-enhanced transformers as fully-connected recurrent neural\nnetworks (RNNs), we refine the training process using the Truncated\nBackpropagation Through Time (TBPTT) algorithm, which incorporates innovative\nincremental optimization techniques. These techniques not only diminish time\ncomplexity but also address the bias in gradient computation through an\nunbiased optimization process. UIO-LLMs successfully handle long context, such\nas extending the context window of Llama2-7b-chat from 4K to 100K tokens with\nminimal 2% additional parameters, while keeping the inference cost nearly\nlinear as context length increases."
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Mingbao Lin"
                    },
                    {
                        "name": "Yunshan Zhong"
                    },
                    {
                        "name": "Shuicheng Yan"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "arxiv_comment": "This article was not accepted, and its quality is not very good.\n  Therefore, we have decided to withdraw the submission and will not resubmit\n  it elsewhere",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18173v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18173v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10341v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10341v1",
                "updated": "2025-09-12T15:24:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    15,
                    24,
                    41,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T15:24:41Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    15,
                    24,
                    41,
                    4,
                    255,
                    0
                ],
                "title": "GARD: Gamma-based Anatomical Restoration and Denoising for Retinal OCT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GARD: Gamma-based Anatomical Restoration and Denoising for Retinal OCT"
                },
                "summary": "Optical Coherence Tomography (OCT) is a vital imaging modality for diagnosing\nand monitoring retinal diseases. However, OCT images are inherently degraded by\nspeckle noise, which obscures fine details and hinders accurate interpretation.\nWhile numerous denoising methods exist, many struggle to balance noise\nreduction with the preservation of crucial anatomical structures. This paper\nintroduces GARD (Gamma-based Anatomical Restoration and Denoising), a novel\ndeep learning approach for OCT image despeckling that leverages the strengths\nof diffusion probabilistic models. Unlike conventional diffusion models that\nassume Gaussian noise, GARD employs a Denoising Diffusion Gamma Model to more\naccurately reflect the statistical properties of speckle. Furthermore, we\nintroduce a Noise-Reduced Fidelity Term that utilizes a pre-processed,\nless-noisy image to guide the denoising process. This crucial addition prevents\nthe reintroduction of high-frequency noise. We accelerate the inference process\nby adapting the Denoising Diffusion Implicit Model framework to our Gamma-based\nmodel. Experiments on a dataset with paired noisy and less-noisy OCT B-scans\ndemonstrate that GARD significantly outperforms traditional denoising methods\nand state-of-the-art deep learning models in terms of PSNR, SSIM, and MSE.\nQualitative results confirm that GARD produces sharper edges and better\npreserves fine anatomical details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optical Coherence Tomography (OCT) is a vital imaging modality for diagnosing\nand monitoring retinal diseases. However, OCT images are inherently degraded by\nspeckle noise, which obscures fine details and hinders accurate interpretation.\nWhile numerous denoising methods exist, many struggle to balance noise\nreduction with the preservation of crucial anatomical structures. This paper\nintroduces GARD (Gamma-based Anatomical Restoration and Denoising), a novel\ndeep learning approach for OCT image despeckling that leverages the strengths\nof diffusion probabilistic models. Unlike conventional diffusion models that\nassume Gaussian noise, GARD employs a Denoising Diffusion Gamma Model to more\naccurately reflect the statistical properties of speckle. Furthermore, we\nintroduce a Noise-Reduced Fidelity Term that utilizes a pre-processed,\nless-noisy image to guide the denoising process. This crucial addition prevents\nthe reintroduction of high-frequency noise. We accelerate the inference process\nby adapting the Denoising Diffusion Implicit Model framework to our Gamma-based\nmodel. Experiments on a dataset with paired noisy and less-noisy OCT B-scans\ndemonstrate that GARD significantly outperforms traditional denoising methods\nand state-of-the-art deep learning models in terms of PSNR, SSIM, and MSE.\nQualitative results confirm that GARD produces sharper edges and better\npreserves fine anatomical details."
                },
                "authors": [
                    {
                        "name": "Botond Fazekas"
                    },
                    {
                        "name": "Thomas Pinetz"
                    },
                    {
                        "name": "Guilherme Aresta"
                    },
                    {
                        "name": "Taha Emre"
                    },
                    {
                        "name": "Hrvoje Bogunovic"
                    }
                ],
                "author_detail": {
                    "name": "Hrvoje Bogunovic"
                },
                "author": "Hrvoje Bogunovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10341v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10341v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10334v1",
                "updated": "2025-09-12T15:14:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    15,
                    14,
                    19,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T15:14:19Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    15,
                    14,
                    19,
                    4,
                    255,
                    0
                ],
                "title": "I-Segmenter: Integer-Only Vision Transformer for Efficient Semantic\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I-Segmenter: Integer-Only Vision Transformer for Efficient Semantic\n  Segmentation"
                },
                "summary": "Vision Transformers (ViTs) have recently achieved strong results in semantic\nsegmentation, yet their deployment on resource-constrained devices remains\nlimited due to their high memory footprint and computational cost. Quantization\noffers an effective strategy to improve efficiency, but ViT-based segmentation\nmodels are notoriously fragile under low precision, as quantization errors\naccumulate across deep encoder-decoder pipelines. We introduce I-Segmenter, the\nfirst fully integer-only ViT segmentation framework. Building on the Segmenter\narchitecture, I-Segmenter systematically replaces floating-point operations\nwith integer-only counterparts. To further stabilize both training and\ninference, we propose $\\lambda$-ShiftGELU, a novel activation function that\nmitigates the limitations of uniform quantization in handling long-tailed\nactivation distributions. In addition, we remove the L2 normalization layer and\nreplace bilinear interpolation in the decoder with nearest neighbor upsampling,\nensuring integer-only execution throughout the computational graph. Extensive\nexperiments show that I-Segmenter achieves accuracy within a reasonable margin\nof its FP32 baseline (5.1 % on average), while reducing model size by up to\n3.8x and enabling up to 1.2x faster inference with optimized runtimes. Notably,\neven in one-shot PTQ with a single calibration image, I-Segmenter delivers\ncompetitive accuracy, underscoring its practicality for real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Transformers (ViTs) have recently achieved strong results in semantic\nsegmentation, yet their deployment on resource-constrained devices remains\nlimited due to their high memory footprint and computational cost. Quantization\noffers an effective strategy to improve efficiency, but ViT-based segmentation\nmodels are notoriously fragile under low precision, as quantization errors\naccumulate across deep encoder-decoder pipelines. We introduce I-Segmenter, the\nfirst fully integer-only ViT segmentation framework. Building on the Segmenter\narchitecture, I-Segmenter systematically replaces floating-point operations\nwith integer-only counterparts. To further stabilize both training and\ninference, we propose $\\lambda$-ShiftGELU, a novel activation function that\nmitigates the limitations of uniform quantization in handling long-tailed\nactivation distributions. In addition, we remove the L2 normalization layer and\nreplace bilinear interpolation in the decoder with nearest neighbor upsampling,\nensuring integer-only execution throughout the computational graph. Extensive\nexperiments show that I-Segmenter achieves accuracy within a reasonable margin\nof its FP32 baseline (5.1 % on average), while reducing model size by up to\n3.8x and enabling up to 1.2x faster inference with optimized runtimes. Notably,\neven in one-shot PTQ with a single calibration image, I-Segmenter delivers\ncompetitive accuracy, underscoring its practicality for real-world deployment."
                },
                "authors": [
                    {
                        "name": "Jordan Sassoon"
                    },
                    {
                        "name": "Michal Szczepanski"
                    },
                    {
                        "name": "Martyna Poreba"
                    }
                ],
                "author_detail": {
                    "name": "Martyna Poreba"
                },
                "author": "Martyna Poreba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10333v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10333v1",
                "updated": "2025-09-12T15:13:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    15,
                    13,
                    12,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T15:13:12Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    15,
                    13,
                    12,
                    4,
                    255,
                    0
                ],
                "title": "Revealing Higher-Order Interactions in Complex Networks: A U.S.\n  Diplomacy Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing Higher-Order Interactions in Complex Networks: A U.S.\n  Diplomacy Case Study"
                },
                "summary": "Although diplomatic communication has long been examined in the social\nsciences, its network structure remains underexplored. Using the U.S.\ndiplomatic cables released by WikiLeaks in 2010 as a case study, we adopt a\nnetwork-science perspective. We represent diplomatic interactions as a\nhypergraph and develop a general, random-walk-based pipeline to evaluate this\nrepresentation against traditional pairwise graphs. We further evaluate the\npipeline on legislative co-sponsorship and organizational email data, finding\nimprovements and empirical evidence that clarifies when hypergraph modeling is\npreferable to pairwise graphs. Overall, hypergraphs paired with appropriately\nspecified random-walk dynamics more faithfully capture higher-order,\ngroup-based interactions, yielding a richer structural account of diplomacy and\nsuperior performance on interaction-prediction tasks that enables inferring new\ndiplomatic relationships from existing patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although diplomatic communication has long been examined in the social\nsciences, its network structure remains underexplored. Using the U.S.\ndiplomatic cables released by WikiLeaks in 2010 as a case study, we adopt a\nnetwork-science perspective. We represent diplomatic interactions as a\nhypergraph and develop a general, random-walk-based pipeline to evaluate this\nrepresentation against traditional pairwise graphs. We further evaluate the\npipeline on legislative co-sponsorship and organizational email data, finding\nimprovements and empirical evidence that clarifies when hypergraph modeling is\npreferable to pairwise graphs. Overall, hypergraphs paired with appropriately\nspecified random-walk dynamics more faithfully capture higher-order,\ngroup-based interactions, yielding a richer structural account of diplomacy and\nsuperior performance on interaction-prediction tasks that enables inferring new\ndiplomatic relationships from existing patterns."
                },
                "authors": [
                    {
                        "name": "Arthur Rondeau"
                    },
                    {
                        "name": "Didier Wernli"
                    },
                    {
                        "name": "Roland Bouffanais"
                    }
                ],
                "author_detail": {
                    "name": "Roland Bouffanais"
                },
                "author": "Roland Bouffanais",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10333v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10333v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05084v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05084v2",
                "updated": "2025-09-12T15:09:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    15,
                    9,
                    41,
                    4,
                    255,
                    0
                ],
                "published": "2025-08-07T07:09:31Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    7,
                    9,
                    31,
                    3,
                    219,
                    0
                ],
                "title": "AdaFusion: Prompt-Guided Inference with Adaptive Fusion of Pathology\n  Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaFusion: Prompt-Guided Inference with Adaptive Fusion of Pathology\n  Foundation Models"
                },
                "summary": "Pathology foundation models (PFMs) have demonstrated strong representational\ncapabilities through self-supervised pre-training on large-scale, unannotated\nhistopathology image datasets. However, their diverse yet opaque pretraining\ncontexts, shaped by both data-related and structural/training factors,\nintroduce latent biases that hinder generalisability and transparency in\ndownstream applications. In this paper, we propose AdaFusion, a novel\nprompt-guided inference framework that, to our knowledge, is among the very\nfirst to dynamically integrate complementary knowledge from multiple PFMs. Our\nmethod compresses and aligns tile-level features from diverse models and\nemploys a lightweight attention mechanism to adaptively fuse them based on\ntissue phenotype context. We evaluate AdaFusion on three real-world benchmarks\nspanning treatment response prediction, tumour grading, and spatial gene\nexpression inference. Our approach consistently surpasses individual PFMs\nacross both classification and regression tasks, while offering interpretable\ninsights into each model's biosemantic specialisation. These results highlight\nAdaFusion's ability to bridge heterogeneous PFMs, achieving both enhanced\nperformance and interpretability of model-specific inductive biases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pathology foundation models (PFMs) have demonstrated strong representational\ncapabilities through self-supervised pre-training on large-scale, unannotated\nhistopathology image datasets. However, their diverse yet opaque pretraining\ncontexts, shaped by both data-related and structural/training factors,\nintroduce latent biases that hinder generalisability and transparency in\ndownstream applications. In this paper, we propose AdaFusion, a novel\nprompt-guided inference framework that, to our knowledge, is among the very\nfirst to dynamically integrate complementary knowledge from multiple PFMs. Our\nmethod compresses and aligns tile-level features from diverse models and\nemploys a lightweight attention mechanism to adaptively fuse them based on\ntissue phenotype context. We evaluate AdaFusion on three real-world benchmarks\nspanning treatment response prediction, tumour grading, and spatial gene\nexpression inference. Our approach consistently surpasses individual PFMs\nacross both classification and regression tasks, while offering interpretable\ninsights into each model's biosemantic specialisation. These results highlight\nAdaFusion's ability to bridge heterogeneous PFMs, achieving both enhanced\nperformance and interpretability of model-specific inductive biases."
                },
                "authors": [
                    {
                        "name": "Yuxiang Xiao"
                    },
                    {
                        "name": "Yang Hu"
                    },
                    {
                        "name": "Bin Li"
                    },
                    {
                        "name": "Tianyang Zhang"
                    },
                    {
                        "name": "Zexi Li"
                    },
                    {
                        "name": "Huazhu Fu"
                    },
                    {
                        "name": "Jens Rittscher"
                    },
                    {
                        "name": "Kaixiang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Kaixiang Yang"
                },
                "author": "Kaixiang Yang",
                "arxiv_comment": "6 Tables, 11 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05084v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05084v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10322v1",
                "updated": "2025-09-12T15:01:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    15,
                    1,
                    31,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T15:01:31Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    15,
                    1,
                    31,
                    4,
                    255,
                    0
                ],
                "title": "Effects of the Strict-Tolerant Approach on Intuitionistic and Minimal\n  Logic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effects of the Strict-Tolerant Approach on Intuitionistic and Minimal\n  Logic"
                },
                "summary": "This paper extends the literature on the strict-tolerant logical approach by\napplying its methods to intuitionistic and minimal logic. In short, the\nstrict-tolerant approach modifies the usual notion of logical consequence by\nstipulating that, in order for an inference to be valid, from the truth of the\npremises must follow the non-falsity of the conclusion. This notion can also be\ngeneralized to define strict-tolerant metainferences, metametainferences and so\non, which may or may not generate logics distinct from those obtained on the\ninferential level. It is already known that strict-tolerant definitions can\nmake the notion of inference for non-classical logics collapse into the\nclassical notion, but the strength of this effect is not yet fully known. This\npaper shows that intuitionistic strict-tolerant inferences also collapse into\nclassical ones, but minimal ones do not. However, minimal strict-tolerant logic\nhas the property that no inferences are valid (which is not carried over to the\nmetainferential level). Additionally, it is shown that the logics obtained from\nintuitionistic, minimal and classical logic at the metainferential level are\ndistinct from each other.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper extends the literature on the strict-tolerant logical approach by\napplying its methods to intuitionistic and minimal logic. In short, the\nstrict-tolerant approach modifies the usual notion of logical consequence by\nstipulating that, in order for an inference to be valid, from the truth of the\npremises must follow the non-falsity of the conclusion. This notion can also be\ngeneralized to define strict-tolerant metainferences, metametainferences and so\non, which may or may not generate logics distinct from those obtained on the\ninferential level. It is already known that strict-tolerant definitions can\nmake the notion of inference for non-classical logics collapse into the\nclassical notion, but the strength of this effect is not yet fully known. This\npaper shows that intuitionistic strict-tolerant inferences also collapse into\nclassical ones, but minimal ones do not. However, minimal strict-tolerant logic\nhas the property that no inferences are valid (which is not carried over to the\nmetainferential level). Additionally, it is shown that the logics obtained from\nintuitionistic, minimal and classical logic at the metainferential level are\ndistinct from each other."
                },
                "authors": [
                    {
                        "name": "Victor Barroso-Nascimento"
                    },
                    {
                        "name": "German Mejia"
                    }
                ],
                "author_detail": {
                    "name": "German Mejia"
                },
                "author": "German Mejia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10317v1",
                "updated": "2025-09-12T14:59:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    59,
                    4,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T14:59:04Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    59,
                    4,
                    4,
                    255,
                    0
                ],
                "title": "Robot guide with multi-agent control and automatic scenario generation\n  with LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robot guide with multi-agent control and automatic scenario generation\n  with LLM"
                },
                "summary": "The work describes the development of a hybrid control architecture for an\nanthropomorphic tour guide robot, combining a multi-agent resource management\nsystem with automatic behavior scenario generation based on large language\nmodels. The proposed approach aims to overcome the limitations of traditional\nsystems, which rely on manual tuning of behavior scenarios. These limitations\ninclude manual configuration, low flexibility, and lack of naturalness in robot\nbehavior. The process of preparing tour scenarios is implemented through a\ntwo-stage generation: first, a stylized narrative is created, then non-verbal\naction tags are integrated into the text. The multi-agent system ensures\ncoordination and conflict resolution during the execution of parallel actions,\nas well as maintaining default behavior after the completion of main\noperations, contributing to more natural robot behavior. The results obtained\nfrom the trial demonstrate the potential of the proposed approach for\nautomating and scaling social robot control systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The work describes the development of a hybrid control architecture for an\nanthropomorphic tour guide robot, combining a multi-agent resource management\nsystem with automatic behavior scenario generation based on large language\nmodels. The proposed approach aims to overcome the limitations of traditional\nsystems, which rely on manual tuning of behavior scenarios. These limitations\ninclude manual configuration, low flexibility, and lack of naturalness in robot\nbehavior. The process of preparing tour scenarios is implemented through a\ntwo-stage generation: first, a stylized narrative is created, then non-verbal\naction tags are integrated into the text. The multi-agent system ensures\ncoordination and conflict resolution during the execution of parallel actions,\nas well as maintaining default behavior after the completion of main\noperations, contributing to more natural robot behavior. The results obtained\nfrom the trial demonstrate the potential of the proposed approach for\nautomating and scaling social robot control systems."
                },
                "authors": [
                    {
                        "name": "Elizaveta D. Moskovskaya"
                    },
                    {
                        "name": "Anton D. Moscowsky"
                    }
                ],
                "author_detail": {
                    "name": "Anton D. Moscowsky"
                },
                "author": "Anton D. Moscowsky",
                "arxiv_comment": "14 pages, 5 figures, 2 tables, 1 demo-video and repository link",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "93C85",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.9; I.2.7; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23592v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23592v2",
                "updated": "2025-09-12T14:58:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    58,
                    31,
                    4,
                    255,
                    0
                ],
                "published": "2025-05-29T16:04:04Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    4,
                    4,
                    3,
                    149,
                    0
                ],
                "title": "A Modern Theory of Cross-Validation through the Lens of Stability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Modern Theory of Cross-Validation through the Lens of Stability"
                },
                "summary": "Modern data analysis and statistical learning are marked by complex data\nstructures and black-box algorithms. Data complexity stems from technologies\nlike imaging, remote sensing, wearables, and genomic sequencing.\nSimultaneously, black-box models -- especially deep neural networks -- have\nachieved impressive results. This combination raises new challenges for\nuncertainty quantification and statistical inference, which we term \"black-box\ninference.\"\n  Black-box inference is difficult due to the lack of traditional modeling\nassumptions and the opaque behavior of modern estimators. These make it hard to\ncharacterize the distribution of estimation errors. A popular solution is\npost-hoc randomization, which, under mild assumptions like exchangeability, can\nyield valid uncertainty quantification. Such methods range from classical\ntechniques like permutation tests, jackknife, and bootstrap, to recent\ninnovations like conformal inference. These approaches typically need little\nknowledge of data distributions or the internal working of estimators. Many\nrely on the idea that estimators behave similarly under small data changes -- a\nconcept formalized as stability. Over time, stability has become a key\nprinciple in data science, influencing generalization error, privacy, and\nadaptive inference.\n  This article investigates cross-validation (CV) -- a widely used resampling\nmethod -- through the lens of stability. We first review recent theoretical\nresults on CV for estimating generalization error and model selection under\nstability. We then examine uncertainty quantification for CV-based risk\nestimates. Together, these insights yield new theory and tools, which we apply\nto topics like model selection, selective inference, and conformal prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern data analysis and statistical learning are marked by complex data\nstructures and black-box algorithms. Data complexity stems from technologies\nlike imaging, remote sensing, wearables, and genomic sequencing.\nSimultaneously, black-box models -- especially deep neural networks -- have\nachieved impressive results. This combination raises new challenges for\nuncertainty quantification and statistical inference, which we term \"black-box\ninference.\"\n  Black-box inference is difficult due to the lack of traditional modeling\nassumptions and the opaque behavior of modern estimators. These make it hard to\ncharacterize the distribution of estimation errors. A popular solution is\npost-hoc randomization, which, under mild assumptions like exchangeability, can\nyield valid uncertainty quantification. Such methods range from classical\ntechniques like permutation tests, jackknife, and bootstrap, to recent\ninnovations like conformal inference. These approaches typically need little\nknowledge of data distributions or the internal working of estimators. Many\nrely on the idea that estimators behave similarly under small data changes -- a\nconcept formalized as stability. Over time, stability has become a key\nprinciple in data science, influencing generalization error, privacy, and\nadaptive inference.\n  This article investigates cross-validation (CV) -- a widely used resampling\nmethod -- through the lens of stability. We first review recent theoretical\nresults on CV for estimating generalization error and model selection under\nstability. We then examine uncertainty quantification for CV-based risk\nestimates. Together, these insights yield new theory and tools, which we apply\nto topics like model selection, selective inference, and conformal prediction."
                },
                "authors": [
                    {
                        "name": "Jing Lei"
                    }
                ],
                "author_detail": {
                    "name": "Jing Lei"
                },
                "author": "Jing Lei",
                "arxiv_comment": "130 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23592v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23592v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10310v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10310v1",
                "updated": "2025-09-12T14:52:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    52,
                    42,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T14:52:42Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    52,
                    42,
                    4,
                    255,
                    0
                ],
                "title": "A Stochastic Birth-and-Death Approach for Street Furniture Geolocation\n  in Urban Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Stochastic Birth-and-Death Approach for Street Furniture Geolocation\n  in Urban Environments"
                },
                "summary": "In this paper we address the problem of precise geolocation of street\nfurniture in complex urban environments, which is a critical task for effective\nmonitoring and maintenance of public infrastructure by local authorities and\nprivate stakeholders. To this end, we propose a probabilistic framework based\non energy maps that encode the spatial likelihood of object locations.\nRepresenting the energy in a map-based geopositioned format allows the\noptimisation process to seamlessly integrate external geospatial information,\nsuch as GIS layers, road maps, or placement constraints, which improves\ncontextual awareness and localisation accuracy. A stochastic birth-and-death\noptimisation algorithm is introduced to infer the most probable configuration\nof assets. We evaluate our approach using a realistic simulation informed by a\ngeolocated dataset of street lighting infrastructure in Dublin city centre,\ndemonstrating its potential for scalable and accurate urban asset mapping. The\nimplementation of the algorithm will be made available in the GitHub repository\nhttps://github.com/EMurphy0108/SBD_Street_Furniture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we address the problem of precise geolocation of street\nfurniture in complex urban environments, which is a critical task for effective\nmonitoring and maintenance of public infrastructure by local authorities and\nprivate stakeholders. To this end, we propose a probabilistic framework based\non energy maps that encode the spatial likelihood of object locations.\nRepresenting the energy in a map-based geopositioned format allows the\noptimisation process to seamlessly integrate external geospatial information,\nsuch as GIS layers, road maps, or placement constraints, which improves\ncontextual awareness and localisation accuracy. A stochastic birth-and-death\noptimisation algorithm is introduced to infer the most probable configuration\nof assets. We evaluate our approach using a realistic simulation informed by a\ngeolocated dataset of street lighting infrastructure in Dublin city centre,\ndemonstrating its potential for scalable and accurate urban asset mapping. The\nimplementation of the algorithm will be made available in the GitHub repository\nhttps://github.com/EMurphy0108/SBD_Street_Furniture."
                },
                "authors": [
                    {
                        "name": "Evan Murphy"
                    },
                    {
                        "name": "Marco Viola"
                    },
                    {
                        "name": "Vladimir A. Krylov"
                    }
                ],
                "author_detail": {
                    "name": "Vladimir A. Krylov"
                },
                "author": "Vladimir A. Krylov",
                "arxiv_comment": "Accepted for publication in the Proceedings of the 27th Irish Machine\n  Vision and Image Processing Conference (IMVIP 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10310v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10310v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10308v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10308v1",
                "updated": "2025-09-12T14:50:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    50,
                    56,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T14:50:56Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    50,
                    56,
                    4,
                    255,
                    0
                ],
                "title": "GraphCSVAE: Graph Categorical Structured Variational Autoencoder for\n  Spatiotemporal Auditing of Physical Vulnerability Towards Sustainable\n  Post-Disaster Risk Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphCSVAE: Graph Categorical Structured Variational Autoencoder for\n  Spatiotemporal Auditing of Physical Vulnerability Towards Sustainable\n  Post-Disaster Risk Reduction"
                },
                "summary": "In the aftermath of disasters, many institutions worldwide face challenges in\ncontinually monitoring changes in disaster risk, limiting the ability of key\ndecision-makers to assess progress towards the UN Sendai Framework for Disaster\nRisk Reduction 2015-2030. While numerous efforts have substantially advanced\nthe large-scale modeling of hazard and exposure through Earth observation and\ndata-driven methods, progress remains limited in modeling another equally\nimportant yet challenging element of the risk equation: physical vulnerability.\nTo address this gap, we introduce Graph Categorical Structured Variational\nAutoencoder (GraphCSVAE), a novel probabilistic data-driven framework for\nmodeling physical vulnerability by integrating deep learning, graph\nrepresentation, and categorical probabilistic inference, using time-series\nsatellite-derived datasets and prior expert belief systems. We introduce a\nweakly supervised first-order transition matrix that reflects the changes in\nthe spatiotemporal distribution of physical vulnerability in two\ndisaster-stricken and socioeconomically disadvantaged areas: (1) the\ncyclone-impacted coastal Khurushkul community in Bangladesh and (2) the\nmudslide-affected city of Freetown in Sierra Leone. Our work reveals\npost-disaster regional dynamics in physical vulnerability, offering valuable\ninsights into localized spatiotemporal auditing and sustainable strategies for\npost-disaster risk reduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the aftermath of disasters, many institutions worldwide face challenges in\ncontinually monitoring changes in disaster risk, limiting the ability of key\ndecision-makers to assess progress towards the UN Sendai Framework for Disaster\nRisk Reduction 2015-2030. While numerous efforts have substantially advanced\nthe large-scale modeling of hazard and exposure through Earth observation and\ndata-driven methods, progress remains limited in modeling another equally\nimportant yet challenging element of the risk equation: physical vulnerability.\nTo address this gap, we introduce Graph Categorical Structured Variational\nAutoencoder (GraphCSVAE), a novel probabilistic data-driven framework for\nmodeling physical vulnerability by integrating deep learning, graph\nrepresentation, and categorical probabilistic inference, using time-series\nsatellite-derived datasets and prior expert belief systems. We introduce a\nweakly supervised first-order transition matrix that reflects the changes in\nthe spatiotemporal distribution of physical vulnerability in two\ndisaster-stricken and socioeconomically disadvantaged areas: (1) the\ncyclone-impacted coastal Khurushkul community in Bangladesh and (2) the\nmudslide-affected city of Freetown in Sierra Leone. Our work reveals\npost-disaster regional dynamics in physical vulnerability, offering valuable\ninsights into localized spatiotemporal auditing and sustainable strategies for\npost-disaster risk reduction."
                },
                "authors": [
                    {
                        "name": "Joshua Dimasaka"
                    },
                    {
                        "name": "Christian Geiß"
                    },
                    {
                        "name": "Robert Muir-Wood"
                    },
                    {
                        "name": "Emily So"
                    }
                ],
                "author_detail": {
                    "name": "Emily So"
                },
                "author": "Emily So",
                "arxiv_comment": "Accepted full paper at the 8th International Disaster and Risk\n  Conference, IDRC 2025 | Keywords: weakly supervised, graph deep learning,\n  categorical distribution, physical vulnerability, remote sensing,\n  spatiotemporal disaster risk, transition matrix | The data and code are\n  respectively available at https://doi.org/10.5281/zenodo.16656471 and\n  https://github.com/riskaudit/GraphCSVAE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10308v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10308v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04435v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04435v2",
                "updated": "2025-09-12T14:42:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    42,
                    50,
                    4,
                    255,
                    0
                ],
                "published": "2025-08-06T13:22:15Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    13,
                    22,
                    15,
                    2,
                    218,
                    0
                ],
                "title": "Cognitive Effort in the Two-Step Task: An Active Inference\n  Drift-Diffusion Model Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Effort in the Two-Step Task: An Active Inference\n  Drift-Diffusion Model Approach"
                },
                "summary": "High-level theories rooted in the Bayesian Brain Hypothesis often frame\ncognitive effort as the cost of resolving the conflict between habits and\noptimal policies. In parallel, evidence accumulator models (EAMs) provide a\nmechanistic account of how effort arises from competition between the\nsubjective values of available options. Although EAMs have been combined with\nframeworks like Reinforcement Learning to bridge the gap between high-level\ntheories and process-level mechanisms, relatively less attention has been paid\nto their implications for a unified notion of cognitive effort. Here, we\ncombine Active Inference (AIF) with the Drift-Diffusion Model (DDM) to\ninvestigate whether the resulting AIF-DDM can simultaneously account for effort\narising from both habit violation and value discriminability. To our knowledge,\nthis is the first time AIF has been combined with an EAM. We tested the AIF-DDM\non a behavioral dataset from the two-step task and compared its predictions to\nan information-theoretic definition of cognitive effort based on AIF. The\nmodel's predictions successfully accounted for second-stage reaction times but\nfailed to capture the dynamics of the first stage. We argue the latter\ndiscrepancy likely stems from the experimental design rather than a fundamental\nflaw in the model's assumptions about cognitive effort. Accordingly, we propose\nseveral modifications of the two-step task to better measure and isolate\ncognitive effort. Finally, we found that integrating the DDM significantly\nimproved parameter recovery, which could help future studies to obtain more\nreliable parameter estimates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-level theories rooted in the Bayesian Brain Hypothesis often frame\ncognitive effort as the cost of resolving the conflict between habits and\noptimal policies. In parallel, evidence accumulator models (EAMs) provide a\nmechanistic account of how effort arises from competition between the\nsubjective values of available options. Although EAMs have been combined with\nframeworks like Reinforcement Learning to bridge the gap between high-level\ntheories and process-level mechanisms, relatively less attention has been paid\nto their implications for a unified notion of cognitive effort. Here, we\ncombine Active Inference (AIF) with the Drift-Diffusion Model (DDM) to\ninvestigate whether the resulting AIF-DDM can simultaneously account for effort\narising from both habit violation and value discriminability. To our knowledge,\nthis is the first time AIF has been combined with an EAM. We tested the AIF-DDM\non a behavioral dataset from the two-step task and compared its predictions to\nan information-theoretic definition of cognitive effort based on AIF. The\nmodel's predictions successfully accounted for second-stage reaction times but\nfailed to capture the dynamics of the first stage. We argue the latter\ndiscrepancy likely stems from the experimental design rather than a fundamental\nflaw in the model's assumptions about cognitive effort. Accordingly, we propose\nseveral modifications of the two-step task to better measure and isolate\ncognitive effort. Finally, we found that integrating the DDM significantly\nimproved parameter recovery, which could help future studies to obtain more\nreliable parameter estimates."
                },
                "authors": [
                    {
                        "name": "Alvaro Garrido Perez"
                    },
                    {
                        "name": "Viktor Lemoine"
                    },
                    {
                        "name": "Amrapali Pednekar"
                    },
                    {
                        "name": "Yara Khaluf"
                    },
                    {
                        "name": "Pieter Simoens"
                    }
                ],
                "author_detail": {
                    "name": "Pieter Simoens"
                },
                "author": "Pieter Simoens",
                "arxiv_comment": "Paper accepted in the International Workshop on Active Inference,\n  2025: https://iwaiworkshop.github.io/#",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04435v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04435v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15690v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15690v2",
                "updated": "2025-09-12T14:40:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    40,
                    8,
                    4,
                    255,
                    0
                ],
                "published": "2025-05-21T16:05:29Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    5,
                    29,
                    2,
                    141,
                    0
                ],
                "title": "Toward Open Earth Science as Fast and Accessible as Natural Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Open Earth Science as Fast and Accessible as Natural Language"
                },
                "summary": "Is natural-language-driven earth observation data analysis now feasible with\nthe assistance of Large Language Models (LLMs)? For open science in service of\npublic interest, feasibility requires reliably high accuracy, interactive\nlatencies, low (sustainable) costs, open LLMs, and openly maintainable software\n-- hence, the challenge. What are the techniques and programming system\nrequirements necessary for satisfying these constraints, and what is the\ncorresponding development and maintenance burden in practice? This study lays\nthe groundwork for exploring these questions, introducing an impactful earth\nscience use-case, and providing a software framework with evaluation data and\nmetrics, along with initial results from employing model scaling,\nprompt-optimization, and inference-time scaling optimization techniques. While\nwe attain high accuracy (near 100%) across 10 of 11 metrics, the analysis\nfurther considers cost (token-spend), latency, and maintainability across this\nspace of techniques. Finally, we enumerate opportunities for further research,\ngeneral programming and evaluation framework development, and ongoing work for\na comprehensive, deployable solution. This is a call for collaboration and\ncontribution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is natural-language-driven earth observation data analysis now feasible with\nthe assistance of Large Language Models (LLMs)? For open science in service of\npublic interest, feasibility requires reliably high accuracy, interactive\nlatencies, low (sustainable) costs, open LLMs, and openly maintainable software\n-- hence, the challenge. What are the techniques and programming system\nrequirements necessary for satisfying these constraints, and what is the\ncorresponding development and maintenance burden in practice? This study lays\nthe groundwork for exploring these questions, introducing an impactful earth\nscience use-case, and providing a software framework with evaluation data and\nmetrics, along with initial results from employing model scaling,\nprompt-optimization, and inference-time scaling optimization techniques. While\nwe attain high accuracy (near 100%) across 10 of 11 metrics, the analysis\nfurther considers cost (token-spend), latency, and maintainability across this\nspace of techniques. Finally, we enumerate opportunities for further research,\ngeneral programming and evaluation framework development, and ongoing work for\na comprehensive, deployable solution. This is a call for collaboration and\ncontribution."
                },
                "authors": [
                    {
                        "name": "Marquita Ellis"
                    },
                    {
                        "name": "Iksha Gurung"
                    },
                    {
                        "name": "Muthukumaran Ramasubramanian"
                    },
                    {
                        "name": "Rahul Ramachandran"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Ramachandran"
                },
                "author": "Rahul Ramachandran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15690v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15690v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.2; H.5.2; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10297v1",
                "updated": "2025-09-12T14:37:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    37,
                    57,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T14:37:57Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    37,
                    57,
                    4,
                    255,
                    0
                ],
                "title": "The Morality of Probability: How Implicit Moral Biases in LLMs May Shape\n  the Future of Human-AI Symbiosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Morality of Probability: How Implicit Moral Biases in LLMs May Shape\n  the Future of Human-AI Symbiosis"
                },
                "summary": "Artificial intelligence (AI) is advancing at a pace that raises urgent\nquestions about how to align machine decision-making with human moral values.\nThis working paper investigates how leading AI systems prioritize moral\noutcomes and what this reveals about the prospects for human-AI symbiosis. We\naddress two central questions: (1) What moral values do state-of-the-art large\nlanguage models (LLMs) implicitly favour when confronted with dilemmas? (2) How\ndo differences in model architecture, cultural origin, and explainability\naffect these moral preferences? To explore these questions, we conduct a\nquantitative experiment with six LLMs, ranking and scoring outcomes across 18\ndilemmas representing five moral frameworks. Our findings uncover strikingly\nconsistent value biases. Across all models, Care and Virtue values outcomes\nwere rated most moral, while libertarian choices were consistently penalized.\nReasoning-enabled models exhibited greater sensitivity to context and provided\nricher explanations, whereas non-reasoning models produced more uniform but\nopaque judgments. This research makes three contributions: (i) Empirically, it\ndelivers a large-scale comparison of moral reasoning across culturally distinct\nLLMs; (ii) Theoretically, it links probabilistic model behaviour with\nunderlying value encodings; (iii) Practically, it highlights the need for\nexplainability and cultural awareness as critical design principles to guide AI\ntoward a transparent, aligned, and symbiotic future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence (AI) is advancing at a pace that raises urgent\nquestions about how to align machine decision-making with human moral values.\nThis working paper investigates how leading AI systems prioritize moral\noutcomes and what this reveals about the prospects for human-AI symbiosis. We\naddress two central questions: (1) What moral values do state-of-the-art large\nlanguage models (LLMs) implicitly favour when confronted with dilemmas? (2) How\ndo differences in model architecture, cultural origin, and explainability\naffect these moral preferences? To explore these questions, we conduct a\nquantitative experiment with six LLMs, ranking and scoring outcomes across 18\ndilemmas representing five moral frameworks. Our findings uncover strikingly\nconsistent value biases. Across all models, Care and Virtue values outcomes\nwere rated most moral, while libertarian choices were consistently penalized.\nReasoning-enabled models exhibited greater sensitivity to context and provided\nricher explanations, whereas non-reasoning models produced more uniform but\nopaque judgments. This research makes three contributions: (i) Empirically, it\ndelivers a large-scale comparison of moral reasoning across culturally distinct\nLLMs; (ii) Theoretically, it links probabilistic model behaviour with\nunderlying value encodings; (iii) Practically, it highlights the need for\nexplainability and cultural awareness as critical design principles to guide AI\ntoward a transparent, aligned, and symbiotic future."
                },
                "authors": [
                    {
                        "name": "Eoin O'Doherty"
                    },
                    {
                        "name": "Nicole Weinrauch"
                    },
                    {
                        "name": "Andrew Talone"
                    },
                    {
                        "name": "Uri Klempner"
                    },
                    {
                        "name": "Xiaoyuan Yi"
                    },
                    {
                        "name": "Xing Xie"
                    },
                    {
                        "name": "Yi Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zeng"
                },
                "author": "Yi Zeng",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13335v2",
                "updated": "2025-09-12T14:23:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    23,
                    5,
                    4,
                    255,
                    0
                ],
                "published": "2025-07-17T17:51:20Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    51,
                    20,
                    3,
                    198,
                    0
                ],
                "title": "Comparing Apples to Oranges: A Dataset & Analysis of LLM Humour\n  Understanding from Traditional Puns to Topical Jokes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing Apples to Oranges: A Dataset & Analysis of LLM Humour\n  Understanding from Traditional Puns to Topical Jokes"
                },
                "summary": "Humour, as a complex language form, is derived from myriad aspects of life.\nWhilst existing work on computational humour has focussed almost exclusively on\nshort pun-based jokes, we investigate whether the ability of Large Language\nModels (LLMs) to explain humour depends on the particular form. We compare\nmodels' joke explanation abilities from simple puns to complex topical humour\nthat requires esoteric knowledge of real-world entities and events. To this\nend, we curate a dataset of 600 jokes across 4 joke types and manually write\nhigh-quality explanations. These jokes include heterographic and homographic\npuns, contemporary internet humour, and topical jokes. Using this dataset, we\ncompare the zero-shot abilities of a range of LLMs to accurately and\ncomprehensively explain jokes of different types, identifying key research gaps\nin the task of humour explanation. We find that none of the tested models\n(including reasoning models) are capable of reliably generating adequate\nexplanations of all joke types, further highlighting the narrow focus of most\nexisting works on overly simple joke forms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humour, as a complex language form, is derived from myriad aspects of life.\nWhilst existing work on computational humour has focussed almost exclusively on\nshort pun-based jokes, we investigate whether the ability of Large Language\nModels (LLMs) to explain humour depends on the particular form. We compare\nmodels' joke explanation abilities from simple puns to complex topical humour\nthat requires esoteric knowledge of real-world entities and events. To this\nend, we curate a dataset of 600 jokes across 4 joke types and manually write\nhigh-quality explanations. These jokes include heterographic and homographic\npuns, contemporary internet humour, and topical jokes. Using this dataset, we\ncompare the zero-shot abilities of a range of LLMs to accurately and\ncomprehensively explain jokes of different types, identifying key research gaps\nin the task of humour explanation. We find that none of the tested models\n(including reasoning models) are capable of reliably generating adequate\nexplanations of all joke types, further highlighting the narrow focus of most\nexisting works on overly simple joke forms."
                },
                "authors": [
                    {
                        "name": "Tyler Loakman"
                    },
                    {
                        "name": "William Thorne"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "arxiv_comment": "Accepted to Findings of EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09448v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09448v2",
                "updated": "2025-09-12T14:00:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    0,
                    8,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-11T13:31:35Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    31,
                    35,
                    3,
                    254,
                    0
                ],
                "title": "TORSO: Template-Oriented Reasoning Towards General Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TORSO: Template-Oriented Reasoning Towards General Tasks"
                },
                "summary": "The approaches that guide Large Language Models (LLMs) to emulate human\nreasoning during response generation have emerged as an effective method for\nenabling them to solve complex problems in a step-by-step manner, thereby\nachieving superior performance. However, most existing approaches using\nfew-shot prompts to generate responses heavily depend on the provided examples,\nlimiting the utilization of the model's inherent reasoning capabilities.\nMoreover, constructing task-specific few-shot prompts is often costly and may\nlead to inconsistencies across different tasks. In this work, we introduce\nTemplate-Oriented Reasoning (TORSO), which elicits the model to utilize\ninternal reasoning abilities to generate proper responses across various tasks\nwithout the need for manually crafted few-shot examples. Our experimental\nresults demonstrate that TORSO achieves strong performance on diverse LLMs\nbenchmarks with reasonable rationales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The approaches that guide Large Language Models (LLMs) to emulate human\nreasoning during response generation have emerged as an effective method for\nenabling them to solve complex problems in a step-by-step manner, thereby\nachieving superior performance. However, most existing approaches using\nfew-shot prompts to generate responses heavily depend on the provided examples,\nlimiting the utilization of the model's inherent reasoning capabilities.\nMoreover, constructing task-specific few-shot prompts is often costly and may\nlead to inconsistencies across different tasks. In this work, we introduce\nTemplate-Oriented Reasoning (TORSO), which elicits the model to utilize\ninternal reasoning abilities to generate proper responses across various tasks\nwithout the need for manually crafted few-shot examples. Our experimental\nresults demonstrate that TORSO achieves strong performance on diverse LLMs\nbenchmarks with reasonable rationales."
                },
                "authors": [
                    {
                        "name": "Minhyuk Kim"
                    },
                    {
                        "name": "Seungyoon Lee"
                    },
                    {
                        "name": "Heuiseok Lim"
                    }
                ],
                "author_detail": {
                    "name": "Heuiseok Lim"
                },
                "author": "Heuiseok Lim",
                "arxiv_comment": "Accepted to EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09448v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09448v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20600v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20600v2",
                "updated": "2025-09-12T13:52:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    52,
                    45,
                    4,
                    255,
                    0
                ],
                "published": "2024-10-27T21:20:18Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    21,
                    20,
                    18,
                    6,
                    301,
                    0
                ],
                "title": "Multi-Turn Human-LLM Interaction Through the Lens of a Two-Way\n  Intelligibility Protocol",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Turn Human-LLM Interaction Through the Lens of a Two-Way\n  Intelligibility Protocol"
                },
                "summary": "Our interest is in the design of software systems involving a human-expert\ninteracting -- using natural language -- with a large language model (LLM) on\ndata analysis tasks. For complex problems, it is possible that LLMs can harness\nhuman expertise and creativity to find solutions that were otherwise elusive.\nOn one level, this interaction takes place through multiple turns of prompts\nfrom the human and responses from the LLM. Here we investigate a more\nstructured approach based on an abstract protocol described in [3] for\ninteraction between agents. The protocol is motivated by a notion of \"two-way\nintelligibility\" and is modelled by a pair of communicating finite-state\nmachines. We provide an implementation of the protocol, and provide empirical\nevidence of using the implementation to mediate interactions between an LLM and\na human-agent in two areas of scientific interest (radiology and drug design).\nWe conduct controlled experiments with a human proxy (a database), and\nuncontrolled experiments with human subjects. The results provide evidence in\nsupport of the protocol's capability of capturing one- and two-way\nintelligibility in human-LLM interaction; and for the utility of two-way\nintelligibility in the design of human-machine systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our interest is in the design of software systems involving a human-expert\ninteracting -- using natural language -- with a large language model (LLM) on\ndata analysis tasks. For complex problems, it is possible that LLMs can harness\nhuman expertise and creativity to find solutions that were otherwise elusive.\nOn one level, this interaction takes place through multiple turns of prompts\nfrom the human and responses from the LLM. Here we investigate a more\nstructured approach based on an abstract protocol described in [3] for\ninteraction between agents. The protocol is motivated by a notion of \"two-way\nintelligibility\" and is modelled by a pair of communicating finite-state\nmachines. We provide an implementation of the protocol, and provide empirical\nevidence of using the implementation to mediate interactions between an LLM and\na human-agent in two areas of scientific interest (radiology and drug design).\nWe conduct controlled experiments with a human proxy (a database), and\nuncontrolled experiments with human subjects. The results provide evidence in\nsupport of the protocol's capability of capturing one- and two-way\nintelligibility in human-LLM interaction; and for the utility of two-way\nintelligibility in the design of human-machine systems."
                },
                "authors": [
                    {
                        "name": "Harshvardhan Mestha"
                    },
                    {
                        "name": "Karan Bania"
                    },
                    {
                        "name": "Shreyas V"
                    },
                    {
                        "name": "Sidong Liu"
                    },
                    {
                        "name": "Ashwin Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Ashwin Srinivasan"
                },
                "author": "Ashwin Srinivasan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20600v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20600v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21182v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21182v2",
                "updated": "2025-09-12T13:52:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    52,
                    21,
                    4,
                    255,
                    0
                ],
                "published": "2025-08-28T19:47:36Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    19,
                    47,
                    36,
                    3,
                    240,
                    0
                ],
                "title": "The Impact of Spectroscopic Redshift Errors on Cosmological Measurements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Spectroscopic Redshift Errors on Cosmological Measurements"
                },
                "summary": "Spectroscopic redshift errors, including redshift uncertainty and\ncatastrophic failures, can bias cosmological measurements from galaxy redshift\nsurveys at sub-percent level. In this work, we investigate their impact on\nfull-shape clustering analysis using contaminated mock catalogs. We find that\nredshift uncertainty introduces a scale-dependent damping effect on the power\nspectrum, which is absorbed by counterterms in clustering model, keeping\nparameter biases below $5\\%$. Catastrophic failures suppress the power spectrum\namplitude by an approximately constant factor that scales with the catastrophic\nrate~$f_c$. While this effect is negligible for DESI galaxy populations\n($f_c=1\\%$), the slitless-like errors, combining redshift uncertainty with\n$f_c=5\\%$ catastrophics, introduce significant biases in cosmological\nconstraints. In this case, we observe $6\\%$ to $16\\%$ shifts ($\\sim2.2\\sigma$\nlevel) in estimating the fractional growth rate $df\\equiv f/f^{\\rm{fid}}$ and\nthe log primordial amplitude $\\ln(10^{10} A_{s})$. Applying a correction factor\n$(1-f_c)^2$ on the galaxy power spectrum mitigates the bias but weakens the\nparameter constraints due to new degeneracies. Alternatively, fixing $f_c$ to\nits expected value during fitting successfully restores the unbiased posterior\nwithout loss of constraint. Our results indicate that for space-based slitless\nsurveys such as \\textit{Euclid}, at minimum accurate estimation of $f_c$ and\nits incorporation into the clustering model are essential to get unbiased\ncosmological inference. Extending to evolving dark energy and massive neutrino\ncosmologies, redshift errors do not bias the dark energy properties\nparametrized by $w_0$ and $w_a$, but can degrade constraints on the summed\nneutrino mass $\\sum m_\\nu$ by up to $80\\%$ in the worst case.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectroscopic redshift errors, including redshift uncertainty and\ncatastrophic failures, can bias cosmological measurements from galaxy redshift\nsurveys at sub-percent level. In this work, we investigate their impact on\nfull-shape clustering analysis using contaminated mock catalogs. We find that\nredshift uncertainty introduces a scale-dependent damping effect on the power\nspectrum, which is absorbed by counterterms in clustering model, keeping\nparameter biases below $5\\%$. Catastrophic failures suppress the power spectrum\namplitude by an approximately constant factor that scales with the catastrophic\nrate~$f_c$. While this effect is negligible for DESI galaxy populations\n($f_c=1\\%$), the slitless-like errors, combining redshift uncertainty with\n$f_c=5\\%$ catastrophics, introduce significant biases in cosmological\nconstraints. In this case, we observe $6\\%$ to $16\\%$ shifts ($\\sim2.2\\sigma$\nlevel) in estimating the fractional growth rate $df\\equiv f/f^{\\rm{fid}}$ and\nthe log primordial amplitude $\\ln(10^{10} A_{s})$. Applying a correction factor\n$(1-f_c)^2$ on the galaxy power spectrum mitigates the bias but weakens the\nparameter constraints due to new degeneracies. Alternatively, fixing $f_c$ to\nits expected value during fitting successfully restores the unbiased posterior\nwithout loss of constraint. Our results indicate that for space-based slitless\nsurveys such as \\textit{Euclid}, at minimum accurate estimation of $f_c$ and\nits incorporation into the clustering model are essential to get unbiased\ncosmological inference. Extending to evolving dark energy and massive neutrino\ncosmologies, redshift errors do not bias the dark energy properties\nparametrized by $w_0$ and $w_a$, but can degrade constraints on the summed\nneutrino mass $\\sum m_\\nu$ by up to $80\\%$ in the worst case."
                },
                "authors": [
                    {
                        "name": "Shengyu He"
                    },
                    {
                        "name": "Jiaxi Yu"
                    },
                    {
                        "name": "Antoine Rocher"
                    },
                    {
                        "name": "Daniel Forero-Sánchez"
                    },
                    {
                        "name": "Jean-Paul Kneib"
                    },
                    {
                        "name": "Cheng Zhao"
                    },
                    {
                        "name": "Etienne Burtin"
                    },
                    {
                        "name": "Jiamin Hou"
                    }
                ],
                "author_detail": {
                    "name": "Jiamin Hou"
                },
                "author": "Jiamin Hou",
                "arxiv_comment": "25 pages, 9 figures, submitted to JCAP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21182v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21182v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10248v1",
                "updated": "2025-09-12T13:45:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    45,
                    24,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T13:45:24Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    45,
                    24,
                    4,
                    255,
                    0
                ],
                "title": "Prompt Injection Attacks on LLM Generated Reviews of Scientific\n  Publications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Injection Attacks on LLM Generated Reviews of Scientific\n  Publications"
                },
                "summary": "The ongoing intense discussion on rising LLM usage in the scientific\npeer-review process has recently been mingled by reports of authors using\nhidden prompt injections to manipulate review scores. Since the existence of\nsuch \"attacks\" - although seen by some commentators as \"self-defense\" - would\nhave a great impact on the further debate, this paper investigates the\npracticability and technical success of the described manipulations. Our\nsystematic evaluation uses 1k reviews of 2024 ICLR papers generated by a wide\nrange of LLMs shows two distinct results: I) very simple prompt injections are\nindeed highly effective, reaching up to 100% acceptance scores. II) LLM reviews\nare generally biased toward acceptance (>95% in many models). Both results have\ngreat impact on the ongoing discussions on LLM usage in peer-review.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ongoing intense discussion on rising LLM usage in the scientific\npeer-review process has recently been mingled by reports of authors using\nhidden prompt injections to manipulate review scores. Since the existence of\nsuch \"attacks\" - although seen by some commentators as \"self-defense\" - would\nhave a great impact on the further debate, this paper investigates the\npracticability and technical success of the described manipulations. Our\nsystematic evaluation uses 1k reviews of 2024 ICLR papers generated by a wide\nrange of LLMs shows two distinct results: I) very simple prompt injections are\nindeed highly effective, reaching up to 100% acceptance scores. II) LLM reviews\nare generally biased toward acceptance (>95% in many models). Both results have\ngreat impact on the ongoing discussions on LLM usage in peer-review."
                },
                "authors": [
                    {
                        "name": "Janis Keuper"
                    }
                ],
                "author_detail": {
                    "name": "Janis Keuper"
                },
                "author": "Janis Keuper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10222v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10222v1",
                "updated": "2025-09-12T13:14:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    14,
                    47,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T13:14:47Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    14,
                    47,
                    4,
                    255,
                    0
                ],
                "title": "Compartmentalised Agentic Reasoning for Clinical NLI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compartmentalised Agentic Reasoning for Clinical NLI"
                },
                "summary": "A common assumption holds that scaling data and parameters yields\nincreasingly structured, generalisable internal representations. We interrogate\nthis assumption in clinical natural language inference (NLI) by adopting a\nbenchmark decomposed into four reasoning families, Causal Attribution,\nCompositional Grounding, Epistemic Verification, and Risk State Abstraction,\nand introducing CARENLI, a Compartmentalised Agentic Reasoning for Clinical NLI\nthat separates knowledge access from principled inference. CARENLI routes each\npremise, statement pair to a family specific solver and enforces auditable\nprocedures via a planner, verifier, and refiner.\n  Across four LLMs, CARENLI improves fidelity by up to 42 points, reaching\n98.0% in Causal Attribution and 81.2% in Risk State Abstraction. Verifiers flag\nviolations with near-ceiling reliability, while refiners correct a substantial\nshare of epistemic errors. Remaining failures cluster in routing, identifying\nfamily classification as the main bottleneck. These results show that LLMs\noften retain relevant facts but default to heuristics when inference is\nunderspecified, a dissociation CARENLI makes explicit while offering a\nframework for safer, auditable reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A common assumption holds that scaling data and parameters yields\nincreasingly structured, generalisable internal representations. We interrogate\nthis assumption in clinical natural language inference (NLI) by adopting a\nbenchmark decomposed into four reasoning families, Causal Attribution,\nCompositional Grounding, Epistemic Verification, and Risk State Abstraction,\nand introducing CARENLI, a Compartmentalised Agentic Reasoning for Clinical NLI\nthat separates knowledge access from principled inference. CARENLI routes each\npremise, statement pair to a family specific solver and enforces auditable\nprocedures via a planner, verifier, and refiner.\n  Across four LLMs, CARENLI improves fidelity by up to 42 points, reaching\n98.0% in Causal Attribution and 81.2% in Risk State Abstraction. Verifiers flag\nviolations with near-ceiling reliability, while refiners correct a substantial\nshare of epistemic errors. Remaining failures cluster in routing, identifying\nfamily classification as the main bottleneck. These results show that LLMs\noften retain relevant facts but default to heuristics when inference is\nunderspecified, a dissociation CARENLI makes explicit while offering a\nframework for safer, auditable reasoning."
                },
                "authors": [
                    {
                        "name": "Maël Jullien"
                    },
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Marco Valentino"
                    },
                    {
                        "name": "André Freitas"
                    }
                ],
                "author_detail": {
                    "name": "André Freitas"
                },
                "author": "André Freitas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10222v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10222v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10221v1",
                "updated": "2025-09-12T13:12:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    12,
                    42,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T13:12:42Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    12,
                    42,
                    4,
                    255,
                    0
                ],
                "title": "Further evidence for natal kick segregation by spectral type in\n  high-mass X-ray binaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Further evidence for natal kick segregation by spectral type in\n  high-mass X-ray binaries"
                },
                "summary": "High-mass X-ray binaries (HMXBs) are systems in which a neutron star or black\nhole accretes material from a massive companion. HMXBs are expected to have\nexperienced a supernova in their evolution. The impulsive kick associated with\nthis event should affect the space velocity of the system in a way that depends\non the nature and state of the progenitor binary. Here, we test whether the\ndifferent evolutionary histories of HMXBs have left a detectable imprint on\ntheir peculiar velocities ($V_{\\rm pec}$). Using data from Gaia Data Release 3\n(Gaia DR3), we first calculate the $V_{\\rm pec}$ values for 63 well-known HMXBs\nhosting a black hole or neutron star and estimate the associated uncertainties\nvia Monte Carlo re-sampling. We then analyse their distribution and check for\ndifferences between classes. Overall, $V_{\\rm pec}$ estimates extend up to 100\nkm s$^{-1}$, but with Be/X-ray binaries (BeXRBs) favouring $V_{\\rm pec}$\n$\\lesssim 40$ km s$^{-1}$ and supergiant X-ray binaries (SgXRBs) favouring\n$V_{\\rm pec}$ $\\gtrsim 40$ km s$^{-1}$. Based on a Kolmogorov-Smirnov (K-S)\ntest, the null hypothesis that the peculiar velocities of both classes are\ndrawn from the same parent distribution can be robustly rejected, irrespective\nof the background stellar velocity dispersion. Tests with binary population\nsynthesis demonstrate that SgXRBs typically have shorter orbital periods and\nhigher fractional mass loss than BeXRBs at supernova. We argue that the\nmagnitude of $V_{\\rm pec}$ could be used as a complementary feature to\ndistinguish between Be and supergiant systems. These findings extend previous\ninferences based on two-dimensional kinematics from Hipparcos, and may be\nexplained by the differing nature of the respective progenitors systems between\nthe source classes at the instant of supernova.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-mass X-ray binaries (HMXBs) are systems in which a neutron star or black\nhole accretes material from a massive companion. HMXBs are expected to have\nexperienced a supernova in their evolution. The impulsive kick associated with\nthis event should affect the space velocity of the system in a way that depends\non the nature and state of the progenitor binary. Here, we test whether the\ndifferent evolutionary histories of HMXBs have left a detectable imprint on\ntheir peculiar velocities ($V_{\\rm pec}$). Using data from Gaia Data Release 3\n(Gaia DR3), we first calculate the $V_{\\rm pec}$ values for 63 well-known HMXBs\nhosting a black hole or neutron star and estimate the associated uncertainties\nvia Monte Carlo re-sampling. We then analyse their distribution and check for\ndifferences between classes. Overall, $V_{\\rm pec}$ estimates extend up to 100\nkm s$^{-1}$, but with Be/X-ray binaries (BeXRBs) favouring $V_{\\rm pec}$\n$\\lesssim 40$ km s$^{-1}$ and supergiant X-ray binaries (SgXRBs) favouring\n$V_{\\rm pec}$ $\\gtrsim 40$ km s$^{-1}$. Based on a Kolmogorov-Smirnov (K-S)\ntest, the null hypothesis that the peculiar velocities of both classes are\ndrawn from the same parent distribution can be robustly rejected, irrespective\nof the background stellar velocity dispersion. Tests with binary population\nsynthesis demonstrate that SgXRBs typically have shorter orbital periods and\nhigher fractional mass loss than BeXRBs at supernova. We argue that the\nmagnitude of $V_{\\rm pec}$ could be used as a complementary feature to\ndistinguish between Be and supergiant systems. These findings extend previous\ninferences based on two-dimensional kinematics from Hipparcos, and may be\nexplained by the differing nature of the respective progenitors systems between\nthe source classes at the instant of supernova."
                },
                "authors": [
                    {
                        "name": "Pornisara Nuchvanichakul"
                    },
                    {
                        "name": "Poshak Gandhi"
                    },
                    {
                        "name": "Christian Knigge"
                    },
                    {
                        "name": "Yue Zhao"
                    },
                    {
                        "name": "Puji Irawati"
                    },
                    {
                        "name": "Suwicha Wanawichian"
                    },
                    {
                        "name": "Cordelia Dashwood Brown"
                    }
                ],
                "author_detail": {
                    "name": "Cordelia Dashwood Brown"
                },
                "author": "Cordelia Dashwood Brown",
                "arxiv_doi": "10.1093/mnras/staf1506",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/staf1506",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.10221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages with 14 figures",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06806v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06806v4",
                "updated": "2025-09-12T13:11:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    11,
                    51,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-08T15:38:31Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    38,
                    31,
                    0,
                    251,
                    0
                ],
                "title": "MachineLearningLM: Scaling Many-shot In-context Learning via Continued\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MachineLearningLM: Scaling Many-shot In-context Learning via Continued\n  Pretraining"
                },
                "summary": "Large language models (LLMs) possess broad world knowledge and strong\ngeneral-purpose reasoning ability, yet they struggle to learn from many\nin-context examples on standard machine learning (ML) tasks, that is, to\nleverage many-shot demonstrations purely via in-context learning (ICL) without\ngradient descent. We introduce MachineLearningLM, a portable\ncontinued-pretraining framework that equips a general-purpose LLM with robust\nin-context ML capability while preserving its general knowledge and reasoning\nfor broader chat workflows.\n  Our pretraining procedure synthesizes ML tasks from millions of structural\ncausal models (SCMs), spanning shot counts up to 1,024. We begin with a\nrandom-forest teacher, distilling tree-based decision strategies into the LLM\nto strengthen robustness in numerical modeling. All tasks are serialized with a\ntoken-efficient prompt, enabling 3x to 6x more examples per context window and\ndelivering up to 50x amortized throughput via batch inference.\n  Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8),\nMachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an\naverage of about 15% on out-of-distribution tabular classification across\nfinance, physics, biology, and healthcare domains. It exhibits a striking\nmany-shot scaling law: accuracy increases monotonically as in-context\ndemonstrations grow from 8 to 1,024. Without any task-specific training, it\nattains random-forest-level accuracy across hundreds of shots. General chat\ncapabilities, including knowledge and reasoning, are preserved: it achieves\n75.4% on MMLU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) possess broad world knowledge and strong\ngeneral-purpose reasoning ability, yet they struggle to learn from many\nin-context examples on standard machine learning (ML) tasks, that is, to\nleverage many-shot demonstrations purely via in-context learning (ICL) without\ngradient descent. We introduce MachineLearningLM, a portable\ncontinued-pretraining framework that equips a general-purpose LLM with robust\nin-context ML capability while preserving its general knowledge and reasoning\nfor broader chat workflows.\n  Our pretraining procedure synthesizes ML tasks from millions of structural\ncausal models (SCMs), spanning shot counts up to 1,024. We begin with a\nrandom-forest teacher, distilling tree-based decision strategies into the LLM\nto strengthen robustness in numerical modeling. All tasks are serialized with a\ntoken-efficient prompt, enabling 3x to 6x more examples per context window and\ndelivering up to 50x amortized throughput via batch inference.\n  Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8),\nMachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an\naverage of about 15% on out-of-distribution tabular classification across\nfinance, physics, biology, and healthcare domains. It exhibits a striking\nmany-shot scaling law: accuracy increases monotonically as in-context\ndemonstrations grow from 8 to 1,024. Without any task-specific training, it\nattains random-forest-level accuracy across hundreds of shots. General chat\ncapabilities, including knowledge and reasoning, are preserved: it achieves\n75.4% on MMLU."
                },
                "authors": [
                    {
                        "name": "Haoyu Dong"
                    },
                    {
                        "name": "Pengkun Zhang"
                    },
                    {
                        "name": "Mingzhe Lu"
                    },
                    {
                        "name": "Yanzhen Shen"
                    },
                    {
                        "name": "Guolin Ke"
                    }
                ],
                "author_detail": {
                    "name": "Guolin Ke"
                },
                "author": "Guolin Ke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06806v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06806v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10216v1",
                "updated": "2025-09-12T13:08:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    8,
                    50,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T13:08:50Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    8,
                    50,
                    4,
                    255,
                    0
                ],
                "title": "RFSeek and Ye Shall Find",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RFSeek and Ye Shall Find"
                },
                "summary": "Requests for Comments (RFCs) are extensive specification documents for\nnetwork protocols, but their prose-based format and their considerable length\noften impede precise operational understanding. We present RFSeek, an\ninteractive tool that automatically extracts visual summaries of protocol logic\nfrom RFCs. RFSeek leverages large language models (LLMs) to generate\nprovenance-linked, explorable diagrams, surfacing both official state machines\nand additional logic found only in the RFC text. Compared to existing RFC\nvisualizations, RFSeek's visual summaries are more transparent and easier to\naudit against their textual source. We showcase the tool's potential through a\nseries of use cases, including guided knowledge extraction and semantic\ndiffing, applied to protocols such as TCP, QUIC, PPTP, and DCCP.\n  In practice, RFSeek not only reconstructs the RFC diagrams included in some\nspecifications, but, more interestingly, also uncovers important logic such as\nnodes or edges described in the text but missing from those diagrams. RFSeek\nfurther derives new visualization diagrams for complex RFCs, with QUIC as a\nrepresentative case. Our approach, which we term \\emph{Summary Visualization},\nhighlights a promising direction: combining LLMs with formal, user-customized\nvisualizations to enhance protocol comprehension and support robust\nimplementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Requests for Comments (RFCs) are extensive specification documents for\nnetwork protocols, but their prose-based format and their considerable length\noften impede precise operational understanding. We present RFSeek, an\ninteractive tool that automatically extracts visual summaries of protocol logic\nfrom RFCs. RFSeek leverages large language models (LLMs) to generate\nprovenance-linked, explorable diagrams, surfacing both official state machines\nand additional logic found only in the RFC text. Compared to existing RFC\nvisualizations, RFSeek's visual summaries are more transparent and easier to\naudit against their textual source. We showcase the tool's potential through a\nseries of use cases, including guided knowledge extraction and semantic\ndiffing, applied to protocols such as TCP, QUIC, PPTP, and DCCP.\n  In practice, RFSeek not only reconstructs the RFC diagrams included in some\nspecifications, but, more interestingly, also uncovers important logic such as\nnodes or edges described in the text but missing from those diagrams. RFSeek\nfurther derives new visualization diagrams for complex RFCs, with QUIC as a\nrepresentative case. Our approach, which we term \\emph{Summary Visualization},\nhighlights a promising direction: combining LLMs with formal, user-customized\nvisualizations to enhance protocol comprehension and support robust\nimplementations."
                },
                "authors": [
                    {
                        "name": "Noga H. Rotman"
                    },
                    {
                        "name": "Tiago Ferreira"
                    },
                    {
                        "name": "Hila Peleg"
                    },
                    {
                        "name": "Mark Silberstein"
                    },
                    {
                        "name": "Alexandra Silva"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Silva"
                },
                "author": "Alexandra Silva",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10210v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10210v1",
                "updated": "2025-09-12T12:56:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    12,
                    56,
                    47,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T12:56:47Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    12,
                    56,
                    47,
                    4,
                    255,
                    0
                ],
                "title": "Towards Fully Automated Molecular Simulations: Multi-Agent Framework for\n  Simulation Setup and Force Field Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Fully Automated Molecular Simulations: Multi-Agent Framework for\n  Simulation Setup and Force Field Extraction"
                },
                "summary": "Automated characterization of porous materials has the potential to\naccelerate materials discovery, but it remains limited by the complexity of\nsimulation setup and force field selection. We propose a multi-agent framework\nin which LLM-based agents can autonomously understand a characterization task,\nplan appropriate simulations, assemble relevant force fields, execute them and\ninterpret their results to guide subsequent steps. As a first step toward this\nvision, we present a multi-agent system for literature-informed force field\nextraction and automated RASPA simulation setup. Initial evaluations\ndemonstrate high correctness and reproducibility, highlighting this approach's\npotential to enable fully autonomous, scalable materials characterization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated characterization of porous materials has the potential to\naccelerate materials discovery, but it remains limited by the complexity of\nsimulation setup and force field selection. We propose a multi-agent framework\nin which LLM-based agents can autonomously understand a characterization task,\nplan appropriate simulations, assemble relevant force fields, execute them and\ninterpret their results to guide subsequent steps. As a first step toward this\nvision, we present a multi-agent system for literature-informed force field\nextraction and automated RASPA simulation setup. Initial evaluations\ndemonstrate high correctness and reproducibility, highlighting this approach's\npotential to enable fully autonomous, scalable materials characterization."
                },
                "authors": [
                    {
                        "name": "Marko Petković"
                    },
                    {
                        "name": "Vlado Menkovski"
                    },
                    {
                        "name": "Sofía Calero"
                    }
                ],
                "author_detail": {
                    "name": "Sofía Calero"
                },
                "author": "Sofía Calero",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10210v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10210v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10208v1",
                "updated": "2025-09-12T12:56:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    12,
                    56,
                    14,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T12:56:14Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    12,
                    56,
                    14,
                    4,
                    255,
                    0
                ],
                "title": "SI-FACT: Mitigating Knowledge Conflict via Self-Improving\n  Faithfulness-Aware Contrastive Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SI-FACT: Mitigating Knowledge Conflict via Self-Improving\n  Faithfulness-Aware Contrastive Tuning"
                },
                "summary": "Large Language Models often generate unfaithful responses in knowledge\nintensive tasks due to knowledge conflict,that is,a preference for relying on\ninternal parametric knowledge rather than the provided context.To address this\nissue,we propose a novel self improving framework,Self Improving Faithfulness\nAware Contrastive Tuning.The framework uses a self instruct mechanism that\nallows the base LLM to automatically generate high quality,structured\ncontrastive learning data,including anchor samples,semantically equivalent\npositive samples,and negative samples simulating unfaithful scenarios.This\napproach significantly reduces the cost of manual\nannotation.Subsequently,contrastive learning is applied to train the\nmodel,enabling it to pull faithful responses closer and push unfaithful\nresponses farther apart in the representation space.Experiments on knowledge\nconflict evaluation benchmarks ECARE KRE and COSE KRE show that the SI FACT\nmodel based on Llama3 8B Instruct improves the Contextual Recall Rate by 6.2%\nover the best baseline method,while significantly reducing dependence on\ninternal memory.The results indicate that SI FACT provides strong effectiveness\nand high data efficiency in enhancing the contextual faithfulness of\nLLMs,offering a practical pathway toward building more proactive and\ntrustworthy language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models often generate unfaithful responses in knowledge\nintensive tasks due to knowledge conflict,that is,a preference for relying on\ninternal parametric knowledge rather than the provided context.To address this\nissue,we propose a novel self improving framework,Self Improving Faithfulness\nAware Contrastive Tuning.The framework uses a self instruct mechanism that\nallows the base LLM to automatically generate high quality,structured\ncontrastive learning data,including anchor samples,semantically equivalent\npositive samples,and negative samples simulating unfaithful scenarios.This\napproach significantly reduces the cost of manual\nannotation.Subsequently,contrastive learning is applied to train the\nmodel,enabling it to pull faithful responses closer and push unfaithful\nresponses farther apart in the representation space.Experiments on knowledge\nconflict evaluation benchmarks ECARE KRE and COSE KRE show that the SI FACT\nmodel based on Llama3 8B Instruct improves the Contextual Recall Rate by 6.2%\nover the best baseline method,while significantly reducing dependence on\ninternal memory.The results indicate that SI FACT provides strong effectiveness\nand high data efficiency in enhancing the contextual faithfulness of\nLLMs,offering a practical pathway toward building more proactive and\ntrustworthy language models."
                },
                "authors": [
                    {
                        "name": "Shengqiang Fu"
                    }
                ],
                "author_detail": {
                    "name": "Shengqiang Fu"
                },
                "author": "Shengqiang Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07983v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07983v2",
                "updated": "2025-09-12T12:39:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    12,
                    39,
                    45,
                    4,
                    255,
                    0
                ],
                "published": "2025-07-01T16:03:55Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    16,
                    3,
                    55,
                    1,
                    182,
                    0
                ],
                "title": "Steering Protein Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering Protein Language Models"
                },
                "summary": "Protein Language Models (PLMs), pre-trained on extensive evolutionary data\nfrom natural proteins, have emerged as indispensable tools for protein design.\nWhile powerful, PLMs often struggle to produce proteins with precisely\nspecified functionalities or properties due to inherent challenges in\ncontrolling their outputs. In this work, we investigate the potential of\nActivation Steering, a technique originally developed for controlling text\ngeneration in Large Language Models (LLMs), to direct PLMs toward generating\nprotein sequences with targeted properties. We propose a simple yet effective\nmethod that employs activation editing to steer PLM outputs, and extend this\napproach to protein optimization through a novel editing site identification\nmodule. Through comprehensive experiments on lysozyme-like sequence generation\nand optimization, we demonstrate that our methods can be seamlessly integrated\ninto both auto-encoding and autoregressive PLMs without requiring additional\ntraining. These results highlight a promising direction for precise protein\nengineering using foundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protein Language Models (PLMs), pre-trained on extensive evolutionary data\nfrom natural proteins, have emerged as indispensable tools for protein design.\nWhile powerful, PLMs often struggle to produce proteins with precisely\nspecified functionalities or properties due to inherent challenges in\ncontrolling their outputs. In this work, we investigate the potential of\nActivation Steering, a technique originally developed for controlling text\ngeneration in Large Language Models (LLMs), to direct PLMs toward generating\nprotein sequences with targeted properties. We propose a simple yet effective\nmethod that employs activation editing to steer PLM outputs, and extend this\napproach to protein optimization through a novel editing site identification\nmodule. Through comprehensive experiments on lysozyme-like sequence generation\nand optimization, we demonstrate that our methods can be seamlessly integrated\ninto both auto-encoding and autoregressive PLMs without requiring additional\ntraining. These results highlight a promising direction for precise protein\nengineering using foundation models."
                },
                "authors": [
                    {
                        "name": "Long-Kai Huang"
                    },
                    {
                        "name": "Rongyi Zhu"
                    },
                    {
                        "name": "Bing He"
                    },
                    {
                        "name": "Jianhua Yao"
                    }
                ],
                "author_detail": {
                    "name": "Jianhua Yao"
                },
                "author": "Jianhua Yao",
                "arxiv_comment": "Accepted to ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07983v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07983v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10184v1",
                "updated": "2025-09-12T12:25:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    12,
                    25,
                    2,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T12:25:02Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    12,
                    25,
                    2,
                    4,
                    255,
                    0
                ],
                "title": "Incongruent Positivity: When Miscalibrated Positivity Undermines Online\n  Supportive Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incongruent Positivity: When Miscalibrated Positivity Undermines Online\n  Supportive Conversations"
                },
                "summary": "In emotionally supportive conversations, well-intended positivity can\nsometimes misfire, leading to responses that feel dismissive, minimizing, or\nunrealistically optimistic. We examine this phenomenon of incongruent\npositivity as miscalibrated expressions of positive support in both human and\nLLM generated responses. To this end, we collected real user-assistant\ndialogues from Reddit across a range of emotional intensities and generated\nadditional responses using large language models for the same context. We\ncategorize these conversations by intensity into two levels: Mild, which covers\nrelationship tension and general advice, and Severe, which covers grief and\nanxiety conversations. This level of categorization enables a comparative\nanalysis of how supportive responses vary across lower and higher stakes\ncontexts. Our analysis reveals that LLMs are more prone to unrealistic\npositivity through dismissive and minimizing tone, particularly in high-stakes\ncontexts. To further study the underlying dimensions of this phenomenon, we\nfinetune LLMs on datasets with strong and weak emotional reactions. Moreover,\nwe developed a weakly supervised multilabel classifier ensemble (DeBERTa and\nMentalBERT) that shows improved detection of incongruent positivity types\nacross two sorts of concerns (Mild and Severe). Our findings shed light on the\nneed to move beyond merely generating generic positive responses and instead\nstudy the congruent support measures to balance positive affect with emotional\nacknowledgment. This approach offers insights into aligning large language\nmodels with affective expectations in the online supportive dialogue, paving\nthe way toward context-aware and trust preserving online conversation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In emotionally supportive conversations, well-intended positivity can\nsometimes misfire, leading to responses that feel dismissive, minimizing, or\nunrealistically optimistic. We examine this phenomenon of incongruent\npositivity as miscalibrated expressions of positive support in both human and\nLLM generated responses. To this end, we collected real user-assistant\ndialogues from Reddit across a range of emotional intensities and generated\nadditional responses using large language models for the same context. We\ncategorize these conversations by intensity into two levels: Mild, which covers\nrelationship tension and general advice, and Severe, which covers grief and\nanxiety conversations. This level of categorization enables a comparative\nanalysis of how supportive responses vary across lower and higher stakes\ncontexts. Our analysis reveals that LLMs are more prone to unrealistic\npositivity through dismissive and minimizing tone, particularly in high-stakes\ncontexts. To further study the underlying dimensions of this phenomenon, we\nfinetune LLMs on datasets with strong and weak emotional reactions. Moreover,\nwe developed a weakly supervised multilabel classifier ensemble (DeBERTa and\nMentalBERT) that shows improved detection of incongruent positivity types\nacross two sorts of concerns (Mild and Severe). Our findings shed light on the\nneed to move beyond merely generating generic positive responses and instead\nstudy the congruent support measures to balance positive affect with emotional\nacknowledgment. This approach offers insights into aligning large language\nmodels with affective expectations in the online supportive dialogue, paving\nthe way toward context-aware and trust preserving online conversation systems."
                },
                "authors": [
                    {
                        "name": "Leen Almajed"
                    },
                    {
                        "name": "Abeer ALdayel"
                    }
                ],
                "author_detail": {
                    "name": "Abeer ALdayel"
                },
                "author": "Abeer ALdayel",
                "arxiv_comment": "This paper is under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14711v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14711v2",
                "updated": "2025-09-12T12:17:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    12,
                    17,
                    25,
                    4,
                    255,
                    0
                ],
                "published": "2024-05-23T15:45:21Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    15,
                    45,
                    21,
                    3,
                    144,
                    0
                ],
                "title": "Zero-inflation in the Multivariate Poisson Lognormal Family",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-inflation in the Multivariate Poisson Lognormal Family"
                },
                "summary": "Analyzing high-dimensional count data is a challenge and statistical\nmodel-based approaches provide an adequate and efficient framework that\npreserves explainability. The (multivariate) Poisson-Log-Normal (PLN) model is\none such model: it assumes count data are driven by an underlying structured\nlatent Gaussian variable, so that the dependencies between counts solely stems\nfrom the latent dependencies. However PLN doesn't account for zero-inflation, a\nfeature frequently observed in real-world datasets. Here we introduce the\nZero-Inflated PLN (ZIPLN) model, adding a multivariate zero-inflated component\nto the model, as an additional Bernoulli latent variable. The Zero-Inflation\ncan be fixed, site-specific, feature-specific or depends on covariates. We\nestimate model parameters using variational inference that scales up to\ndatasets with a few thousands variables and compare two approximations: (i)\nindependent Gaussian and Bernoulli variational distributions or (ii) Gaussian\nvariational distribution conditioned on the Bernoulli one. The method is\nassessed on synthetic data and the efficiency of ZIPLN is established even when\nzero-inflation concerns up to 90% of the observed counts. We then apply both\nZIPLN and PLN to a cow microbiome dataset, containing 90.6% of zeroes.\nAccounting for zero-inflation significantly increases log-likelihood and\nreduces dispersion in the latent space, thus leading to improved group\ndiscrimination.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing high-dimensional count data is a challenge and statistical\nmodel-based approaches provide an adequate and efficient framework that\npreserves explainability. The (multivariate) Poisson-Log-Normal (PLN) model is\none such model: it assumes count data are driven by an underlying structured\nlatent Gaussian variable, so that the dependencies between counts solely stems\nfrom the latent dependencies. However PLN doesn't account for zero-inflation, a\nfeature frequently observed in real-world datasets. Here we introduce the\nZero-Inflated PLN (ZIPLN) model, adding a multivariate zero-inflated component\nto the model, as an additional Bernoulli latent variable. The Zero-Inflation\ncan be fixed, site-specific, feature-specific or depends on covariates. We\nestimate model parameters using variational inference that scales up to\ndatasets with a few thousands variables and compare two approximations: (i)\nindependent Gaussian and Bernoulli variational distributions or (ii) Gaussian\nvariational distribution conditioned on the Bernoulli one. The method is\nassessed on synthetic data and the efficiency of ZIPLN is established even when\nzero-inflation concerns up to 90% of the observed counts. We then apply both\nZIPLN and PLN to a cow microbiome dataset, containing 90.6% of zeroes.\nAccounting for zero-inflation significantly increases log-likelihood and\nreduces dispersion in the latent space, thus leading to improved group\ndiscrimination."
                },
                "authors": [
                    {
                        "name": "Bastien Batardière"
                    },
                    {
                        "name": "Julien Chiquet"
                    },
                    {
                        "name": "François Gindraud"
                    },
                    {
                        "name": "Mahendra Mariadassou"
                    }
                ],
                "author_detail": {
                    "name": "Mahendra Mariadassou"
                },
                "author": "Mahendra Mariadassou",
                "arxiv_comment": "26 pages including appendices. 7 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14711v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14711v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10179v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10179v1",
                "updated": "2025-09-12T12:12:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    12,
                    12,
                    20,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T12:12:20Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    12,
                    12,
                    20,
                    4,
                    255,
                    0
                ],
                "title": "Benchmark of stylistic variation in LLM-generated texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmark of stylistic variation in LLM-generated texts"
                },
                "summary": "This study investigates the register variation in texts written by humans and\ncomparable texts produced by large language models (LLMs). Biber's\nmultidimensional analysis (MDA) is applied to a sample of human-written texts\nand AI-created texts generated to be their counterparts to find the dimensions\nof variation in which LLMs differ most significantly and most systematically\nfrom humans. As textual material, a new LLM-generated corpus AI-Brown is used,\nwhich is comparable to BE-21 (a Brown family corpus representing contemporary\nBritish English). Since all languages except English are underrepresented in\nthe training data of frontier LLMs, similar analysis is replicated on Czech\nusing AI-Koditex corpus and Czech multidimensional model. Examined were 16\nfrontier models in various settings and prompts, with emphasis placed on the\ndifference between base models and instruction-tuned models. Based on this, a\nbenchmark is created through which models can be compared with each other and\nranked in interpretable dimensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the register variation in texts written by humans and\ncomparable texts produced by large language models (LLMs). Biber's\nmultidimensional analysis (MDA) is applied to a sample of human-written texts\nand AI-created texts generated to be their counterparts to find the dimensions\nof variation in which LLMs differ most significantly and most systematically\nfrom humans. As textual material, a new LLM-generated corpus AI-Brown is used,\nwhich is comparable to BE-21 (a Brown family corpus representing contemporary\nBritish English). Since all languages except English are underrepresented in\nthe training data of frontier LLMs, similar analysis is replicated on Czech\nusing AI-Koditex corpus and Czech multidimensional model. Examined were 16\nfrontier models in various settings and prompts, with emphasis placed on the\ndifference between base models and instruction-tuned models. Based on this, a\nbenchmark is created through which models can be compared with each other and\nranked in interpretable dimensions."
                },
                "authors": [
                    {
                        "name": "Jiří Milička"
                    },
                    {
                        "name": "Anna Marklová"
                    },
                    {
                        "name": "Václav Cvrček"
                    }
                ],
                "author_detail": {
                    "name": "Václav Cvrček"
                },
                "author": "Václav Cvrček",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10179v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01340v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01340v3",
                "updated": "2025-09-12T12:10:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    12,
                    10,
                    6,
                    4,
                    255,
                    0
                ],
                "published": "2024-12-02T10:07:01Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    10,
                    7,
                    1,
                    0,
                    337,
                    0
                ],
                "title": "A 2-step Framework for Automated Literary Translation Evaluation: Its\n  Promises and Pitfalls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A 2-step Framework for Automated Literary Translation Evaluation: Its\n  Promises and Pitfalls"
                },
                "summary": "In this work, we propose and evaluate the feasibility of a two-stage pipeline\nto evaluate literary machine translation, in a fine-grained manner, from\nEnglish to Korean. The results show that our framework provides fine-grained,\ninterpretable metrics suited for literary translation and obtains a higher\ncorrelation with human judgment than traditional machine translation metrics.\nNonetheless, it still fails to match inter-human agreement, especially in\nmetrics like Korean Honorifics. We also observe that LLMs tend to favor\ntranslations generated by other LLMs, and we highlight the necessity of\ndeveloping more sophisticated evaluation methods to ensure accurate and\nculturally sensitive machine translation of literary works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose and evaluate the feasibility of a two-stage pipeline\nto evaluate literary machine translation, in a fine-grained manner, from\nEnglish to Korean. The results show that our framework provides fine-grained,\ninterpretable metrics suited for literary translation and obtains a higher\ncorrelation with human judgment than traditional machine translation metrics.\nNonetheless, it still fails to match inter-human agreement, especially in\nmetrics like Korean Honorifics. We also observe that LLMs tend to favor\ntranslations generated by other LLMs, and we highlight the necessity of\ndeveloping more sophisticated evaluation methods to ensure accurate and\nculturally sensitive machine translation of literary works."
                },
                "authors": [
                    {
                        "name": "Sheikh Shafayat"
                    },
                    {
                        "name": "Dongkeun Yoon"
                    },
                    {
                        "name": "Woori Jang"
                    },
                    {
                        "name": "Jiwoo Choi"
                    },
                    {
                        "name": "Alice Oh"
                    },
                    {
                        "name": "Seohyon Jung"
                    }
                ],
                "author_detail": {
                    "name": "Seohyon Jung"
                },
                "author": "Seohyon Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01340v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01340v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07531v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07531v2",
                "updated": "2025-09-12T12:03:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    12,
                    3,
                    22,
                    4,
                    255,
                    0
                ],
                "published": "2025-05-12T13:13:06Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    13,
                    13,
                    6,
                    0,
                    132,
                    0
                ],
                "title": "QuantX: A Framework for Hardware-Aware Quantization of Generative AI\n  Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuantX: A Framework for Hardware-Aware Quantization of Generative AI\n  Workloads"
                },
                "summary": "We present QuantX: a tailored suite of recipes for LLM and VLM quantization.\nIt is capable of quantizing down to 3-bit resolutions with minimal loss in\nperformance. The quantization strategies in QuantX take into account\nhardware-specific constraints to achieve efficient dequantization during\ninference ensuring flexible trade-off between runtime speed, memory requirement\nand model accuracy. Our results demonstrate that QuantX achieves performance\nwithin 6% of the unquantized model for LlaVa-v1.6 quantized down to 3-bits for\nmultiple end user tasks and outperforms recently published state-of-the-art\nquantization techniques. We further integrate one particular technique from\nQuantX into the popular Llama.cpp framework and show its feasibility in terms\nof runtime compared to the mainstream quantization techniques from Llama.cpp.\nLastly, this manuscript provides insights into the LLM quantization process\nthat motivated the range of recipes and options that are incorporated in\nQuantX.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present QuantX: a tailored suite of recipes for LLM and VLM quantization.\nIt is capable of quantizing down to 3-bit resolutions with minimal loss in\nperformance. The quantization strategies in QuantX take into account\nhardware-specific constraints to achieve efficient dequantization during\ninference ensuring flexible trade-off between runtime speed, memory requirement\nand model accuracy. Our results demonstrate that QuantX achieves performance\nwithin 6% of the unquantized model for LlaVa-v1.6 quantized down to 3-bits for\nmultiple end user tasks and outperforms recently published state-of-the-art\nquantization techniques. We further integrate one particular technique from\nQuantX into the popular Llama.cpp framework and show its feasibility in terms\nof runtime compared to the mainstream quantization techniques from Llama.cpp.\nLastly, this manuscript provides insights into the LLM quantization process\nthat motivated the range of recipes and options that are incorporated in\nQuantX."
                },
                "authors": [
                    {
                        "name": "Muhammad Ahmad"
                    },
                    {
                        "name": "Khurram Mazher"
                    },
                    {
                        "name": "Saqib Akram"
                    },
                    {
                        "name": "Ahmad Tameem"
                    },
                    {
                        "name": "Saad Bin Nasir"
                    }
                ],
                "author_detail": {
                    "name": "Saad Bin Nasir"
                },
                "author": "Saad Bin Nasir",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07531v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07531v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16913v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16913v2",
                "updated": "2025-09-12T11:56:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    11,
                    56,
                    27,
                    4,
                    255,
                    0
                ],
                "published": "2025-03-21T07:23:26Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    7,
                    23,
                    26,
                    4,
                    80,
                    0
                ],
                "title": "FGIT: Fault-Guided Fine-Tuning for Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FGIT: Fault-Guided Fine-Tuning for Code Generation"
                },
                "summary": "Modern instruction-tuned large language models (LLMs) have made remarkable\nprogress in code generation. However, these LLMs fine-tuned with standard\nsupervised fine-tuning (SFT) sometimes generate plausible-looking but\nfunctionally incorrect code variants. This issue likely stems from the\nlimitation of standard SFT, which treats all tokens equally during optimization\nand fails to emphasize the error-sensitive segments-specific code differences\nbetween correct implementations and similar incorrect variants. To address this\nproblem, we propose Fault-Guided Fine-Tuning (FGIT), a novel fine-tuning\ntechnique that enhances LLMs' code generation by (1) extracting\nmulti-granularity (line/token-level) differences between correct and incorrect\nyet similar implementations to identify error-sensitive segments, and (2)\ndynamically prioritizing those segments during training via dynamic loss\nweighting. Through extensive experiments on seven LLMs across three widely-used\nbenchmarks, our method achieves an average relative improvement of 6.9% on\npass@1 with some enhanced 6.7B LLMs outperforming closed-source models, e.g.,\nGPT-3.5-Turbo. Furthermore, our fine-tuning technique demonstrates strong\ngeneralization with performance improvements ranging from 3.8% to 19.1% across\ndiverse instruction-tuned LLMs, and our ablation studies confirm the\ncontributions of different granularities of differences and hyperparameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern instruction-tuned large language models (LLMs) have made remarkable\nprogress in code generation. However, these LLMs fine-tuned with standard\nsupervised fine-tuning (SFT) sometimes generate plausible-looking but\nfunctionally incorrect code variants. This issue likely stems from the\nlimitation of standard SFT, which treats all tokens equally during optimization\nand fails to emphasize the error-sensitive segments-specific code differences\nbetween correct implementations and similar incorrect variants. To address this\nproblem, we propose Fault-Guided Fine-Tuning (FGIT), a novel fine-tuning\ntechnique that enhances LLMs' code generation by (1) extracting\nmulti-granularity (line/token-level) differences between correct and incorrect\nyet similar implementations to identify error-sensitive segments, and (2)\ndynamically prioritizing those segments during training via dynamic loss\nweighting. Through extensive experiments on seven LLMs across three widely-used\nbenchmarks, our method achieves an average relative improvement of 6.9% on\npass@1 with some enhanced 6.7B LLMs outperforming closed-source models, e.g.,\nGPT-3.5-Turbo. Furthermore, our fine-tuning technique demonstrates strong\ngeneralization with performance improvements ranging from 3.8% to 19.1% across\ndiverse instruction-tuned LLMs, and our ablation studies confirm the\ncontributions of different granularities of differences and hyperparameters."
                },
                "authors": [
                    {
                        "name": "Lishui Fan"
                    },
                    {
                        "name": "Zhongxin Liu"
                    },
                    {
                        "name": "Haoye Wang"
                    },
                    {
                        "name": "Lingfeng Bao"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "Shanping Li"
                    }
                ],
                "author_detail": {
                    "name": "Shanping Li"
                },
                "author": "Shanping Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16913v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16913v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02908v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02908v2",
                "updated": "2025-09-12T11:49:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    11,
                    49,
                    57,
                    4,
                    255,
                    0
                ],
                "published": "2025-06-03T14:14:28Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    14,
                    14,
                    28,
                    1,
                    154,
                    0
                ],
                "title": "Diffusion Buffer: Online Diffusion-based Speech Enhancement with\n  Sub-Second Latency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Buffer: Online Diffusion-based Speech Enhancement with\n  Sub-Second Latency"
                },
                "summary": "Diffusion models are a class of generative models that have been recently\nused for speech enhancement with remarkable success but are computationally\nexpensive at inference time. Therefore, these models are impractical for\nprocessing streaming data in real-time. In this work, we adapt a sliding window\ndiffusion framework to the speech enhancement task. Our approach progressively\ncorrupts speech signals through time, assigning more noise to frames close to\nthe present in a buffer. This approach outputs denoised frames with a delay\nproportional to the chosen buffer size, enabling a trade-off between\nperformance and latency. Empirical results demonstrate that our method\noutperforms standard diffusion models and runs efficiently on a GPU, achieving\nan input-output latency in the order of 0.3 to 1 seconds. This marks the first\npractical diffusion-based solution for online speech enhancement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models are a class of generative models that have been recently\nused for speech enhancement with remarkable success but are computationally\nexpensive at inference time. Therefore, these models are impractical for\nprocessing streaming data in real-time. In this work, we adapt a sliding window\ndiffusion framework to the speech enhancement task. Our approach progressively\ncorrupts speech signals through time, assigning more noise to frames close to\nthe present in a buffer. This approach outputs denoised frames with a delay\nproportional to the chosen buffer size, enabling a trade-off between\nperformance and latency. Empirical results demonstrate that our method\noutperforms standard diffusion models and runs efficiently on a GPU, achieving\nan input-output latency in the order of 0.3 to 1 seconds. This marks the first\npractical diffusion-based solution for online speech enhancement."
                },
                "authors": [
                    {
                        "name": "Bunlong Lay"
                    },
                    {
                        "name": "Rostislav Makarov"
                    },
                    {
                        "name": "Timo Gerkmann"
                    }
                ],
                "author_detail": {
                    "name": "Timo Gerkmann"
                },
                "author": "Timo Gerkmann",
                "arxiv_comment": "5 pages, 2 figures, Accepted to Interspeech 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02908v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02908v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05271v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05271v3",
                "updated": "2025-09-12T11:28:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    11,
                    28,
                    52,
                    4,
                    255,
                    0
                ],
                "published": "2024-09-09T01:44:09Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    1,
                    44,
                    9,
                    0,
                    253,
                    0
                ],
                "title": "Priors from Envisioned Posterior Judgments: A Novel Elicitation Approach\n  With Application to Bayesian Clinical Trials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Priors from Envisioned Posterior Judgments: A Novel Elicitation Approach\n  With Application to Bayesian Clinical Trials"
                },
                "summary": "Background: The uptake of formalized prior elicitation from experts in\nBayesian clinical trials has been limited due to challenges such as complex\nstatistical modeling, lack of practical tools, and the cognitive burden placed\non experts arising from needing to quantify their uncertainty\nprobabilistically. Existing methods also fail to address prior-posterior\ncoherence, i.e., how do we ensure that the posterior distribution, obtained\nmathematically from combining the estimated prior with the trial data, reflects\nthe expert's actual posterior beliefs?\n  Method: In this study, we propose a new elicitation approach that effectuates\nprior-posterior coherence and reduces cognitive burden. This is achieved by\neliciting expert responses, comprising point estimates only, about envisioned\nposterior judgments under various data outcomes and inferring the prior\ndistribution by minimizing discrepancies between these responses and expected\nresponses derived from the posterior distribution. Via an iterative process,\nexperts receive feedback on the degree of coherency of their responses, and are\ninvited to revise their responses to achieve greater coherency. The feasibility\nand potential value of this new approach are illustrated through an application\nto an ongoing trial.\n  Results: We involved 10 experts from Walk 'n watch trial research team.\nExperts were presented with 16 hypothetical outcome scenarios to experts and\nelicit the priors followed by the developed elicitation framework. Following\ntwo rounds of elicitation, experts' judgments showed substantial improvement in\ncoherency, demonstrating the practical applicability of the proposed\nelicitation approach.\n  Conclusion: The proposed method provides a practical solution to the\nchallenges of formalized prior elicitation in Bayesian clinical trials by\naddressing prior-posterior coherence and reducing cognitive demands on experts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: The uptake of formalized prior elicitation from experts in\nBayesian clinical trials has been limited due to challenges such as complex\nstatistical modeling, lack of practical tools, and the cognitive burden placed\non experts arising from needing to quantify their uncertainty\nprobabilistically. Existing methods also fail to address prior-posterior\ncoherence, i.e., how do we ensure that the posterior distribution, obtained\nmathematically from combining the estimated prior with the trial data, reflects\nthe expert's actual posterior beliefs?\n  Method: In this study, we propose a new elicitation approach that effectuates\nprior-posterior coherence and reduces cognitive burden. This is achieved by\neliciting expert responses, comprising point estimates only, about envisioned\nposterior judgments under various data outcomes and inferring the prior\ndistribution by minimizing discrepancies between these responses and expected\nresponses derived from the posterior distribution. Via an iterative process,\nexperts receive feedback on the degree of coherency of their responses, and are\ninvited to revise their responses to achieve greater coherency. The feasibility\nand potential value of this new approach are illustrated through an application\nto an ongoing trial.\n  Results: We involved 10 experts from Walk 'n watch trial research team.\nExperts were presented with 16 hypothetical outcome scenarios to experts and\nelicit the priors followed by the developed elicitation framework. Following\ntwo rounds of elicitation, experts' judgments showed substantial improvement in\ncoherency, demonstrating the practical applicability of the proposed\nelicitation approach.\n  Conclusion: The proposed method provides a practical solution to the\nchallenges of formalized prior elicitation in Bayesian clinical trials by\naddressing prior-posterior coherence and reducing cognitive demands on experts."
                },
                "authors": [
                    {
                        "name": "Yongdong Ouyang"
                    },
                    {
                        "name": "Janice J Eng"
                    },
                    {
                        "name": "Denghuang Zhan"
                    },
                    {
                        "name": "Hubert Wong"
                    },
                    {
                        "name": "The WnW Research Team"
                    }
                ],
                "author_detail": {
                    "name": "The WnW Research Team"
                },
                "author": "The WnW Research Team",
                "arxiv_comment": "minor update",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05271v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05271v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21152v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21152v2",
                "updated": "2025-09-12T11:26:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    11,
                    26,
                    35,
                    4,
                    255,
                    0
                ],
                "published": "2025-06-26T11:22:06Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    11,
                    22,
                    6,
                    3,
                    177,
                    0
                ],
                "title": "Geometry and Perception Guided Gaussians for Multiview-consistent 3D\n  Generation from a Single Image",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometry and Perception Guided Gaussians for Multiview-consistent 3D\n  Generation from a Single Image"
                },
                "summary": "Generating realistic 3D objects from single-view images requires natural\nappearance, 3D consistency, and the ability to capture multiple plausible\ninterpretations of unseen regions. Existing approaches often rely on\nfine-tuning pretrained 2D diffusion models or directly generating 3D\ninformation through fast network inference or 3D Gaussian Splatting, but their\nresults generally suffer from poor multiview consistency and lack geometric\ndetail. To tackle these issues, we present a novel method that seamlessly\nintegrates geometry and perception information without requiring additional\nmodel training to reconstruct detailed 3D objects from a single image.\nSpecifically, we incorporate geometry and perception priors to initialize the\nGaussian branches and guide their parameter optimization. The geometry prior\ncaptures the rough 3D shapes, while the perception prior utilizes the 2D\npretrained diffusion model to enhance multiview information. Subsequently, we\nintroduce a stable Score Distillation Sampling for fine-grained prior\ndistillation to ensure effective knowledge transfer. The model is further\nenhanced by a reprojection-based strategy that enforces depth consistency.\nExperimental results show that we outperform existing methods on novel view\nsynthesis and 3D reconstruction, demonstrating robust and consistent 3D object\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating realistic 3D objects from single-view images requires natural\nappearance, 3D consistency, and the ability to capture multiple plausible\ninterpretations of unseen regions. Existing approaches often rely on\nfine-tuning pretrained 2D diffusion models or directly generating 3D\ninformation through fast network inference or 3D Gaussian Splatting, but their\nresults generally suffer from poor multiview consistency and lack geometric\ndetail. To tackle these issues, we present a novel method that seamlessly\nintegrates geometry and perception information without requiring additional\nmodel training to reconstruct detailed 3D objects from a single image.\nSpecifically, we incorporate geometry and perception priors to initialize the\nGaussian branches and guide their parameter optimization. The geometry prior\ncaptures the rough 3D shapes, while the perception prior utilizes the 2D\npretrained diffusion model to enhance multiview information. Subsequently, we\nintroduce a stable Score Distillation Sampling for fine-grained prior\ndistillation to ensure effective knowledge transfer. The model is further\nenhanced by a reprojection-based strategy that enforces depth consistency.\nExperimental results show that we outperform existing methods on novel view\nsynthesis and 3D reconstruction, demonstrating robust and consistent 3D object\ngeneration."
                },
                "authors": [
                    {
                        "name": "Pufan Li"
                    },
                    {
                        "name": "Bi'an Du"
                    },
                    {
                        "name": "Wei Hu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Hu"
                },
                "author": "Wei Hu",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21152v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21152v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10150v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10150v1",
                "updated": "2025-09-12T11:25:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    11,
                    25,
                    29,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T11:25:29Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    11,
                    25,
                    29,
                    4,
                    255,
                    0
                ],
                "title": "Magnetic effects on fundamental modes in rotating neutron stars with a\n  purely toroidal magnetic field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magnetic effects on fundamental modes in rotating neutron stars with a\n  purely toroidal magnetic field"
                },
                "summary": "Electromagnetic and gravitational-wave signals from neutron stars are shaped\nby rapid rotation and strong magnetic fields. Determining these properties is\nessential to interpret such signals, but current measurements are limited:\nrotation estimates rely on electromagnetic detections and assume uniform\nrotation, while inferring interior magnetic fields remains ambiguous due to a\nlack of direct observations. Measuring the excited fundamental modes of neutron\nstars in gravitational-wave signals offers a promising solution, as these modes\nencode information about stellar composition, structure, and dynamics. Previous\nstudies have examined the individual effects of rotation and magnetic fields on\nthese modes, identifying magnetic suppression and establishing linear relations\nfor the frequencies of the fundamental $l=0$ quasi-radial mode $f_F$ and $l=2$\nquadrupolar mode $f_{^2f}$. However, few have investigated the combined\ninfluence of rotation and magnetic fields. Here, for the first time, we\nconsider both rotation and a toroidal magnetic field to construct linear\nrelations for quantifying $f_F$ and $f_{^2f}$, showing that their combined\neffects can be constrained by detecting these modes. Using 2D axisymmetric\nsimulations, we demonstrate that quasi-linear relations between $f_F$,\n$f_{^2f}$, stellar compactness $M/R$, and kinetic-to-binding energy ratio\n$T/|W|$ persist even with a toroidal magnetic field. The slope of these\nrelations depends on the toroidal magnetization constant $K_\\mathrm{m}$.\nAdditionally, measuring the frequency ratio $f_{^2f}/f_F$ enables inference of\n$T/|W|$ and the maximum magnetic field strength $\\mathcal{B}_\\mathrm{max}$.\nLastly, we show that differential rotation causes only minor deviations from\npredictions for uniform rotation. Thus, this work demonstrates that rotational\nand magnetic properties of neutron stars can be inferred from their fundamental\nmodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electromagnetic and gravitational-wave signals from neutron stars are shaped\nby rapid rotation and strong magnetic fields. Determining these properties is\nessential to interpret such signals, but current measurements are limited:\nrotation estimates rely on electromagnetic detections and assume uniform\nrotation, while inferring interior magnetic fields remains ambiguous due to a\nlack of direct observations. Measuring the excited fundamental modes of neutron\nstars in gravitational-wave signals offers a promising solution, as these modes\nencode information about stellar composition, structure, and dynamics. Previous\nstudies have examined the individual effects of rotation and magnetic fields on\nthese modes, identifying magnetic suppression and establishing linear relations\nfor the frequencies of the fundamental $l=0$ quasi-radial mode $f_F$ and $l=2$\nquadrupolar mode $f_{^2f}$. However, few have investigated the combined\ninfluence of rotation and magnetic fields. Here, for the first time, we\nconsider both rotation and a toroidal magnetic field to construct linear\nrelations for quantifying $f_F$ and $f_{^2f}$, showing that their combined\neffects can be constrained by detecting these modes. Using 2D axisymmetric\nsimulations, we demonstrate that quasi-linear relations between $f_F$,\n$f_{^2f}$, stellar compactness $M/R$, and kinetic-to-binding energy ratio\n$T/|W|$ persist even with a toroidal magnetic field. The slope of these\nrelations depends on the toroidal magnetization constant $K_\\mathrm{m}$.\nAdditionally, measuring the frequency ratio $f_{^2f}/f_F$ enables inference of\n$T/|W|$ and the maximum magnetic field strength $\\mathcal{B}_\\mathrm{max}$.\nLastly, we show that differential rotation causes only minor deviations from\npredictions for uniform rotation. Thus, this work demonstrates that rotational\nand magnetic properties of neutron stars can be inferred from their fundamental\nmodes."
                },
                "authors": [
                    {
                        "name": "Anson Ka Long Yip"
                    },
                    {
                        "name": "Tjonnie Guang Feng Li"
                    }
                ],
                "author_detail": {
                    "name": "Tjonnie Guang Feng Li"
                },
                "author": "Tjonnie Guang Feng Li",
                "arxiv_comment": "14 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10150v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10150v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15546v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15546v3",
                "updated": "2025-09-12T11:24:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    11,
                    24,
                    8,
                    4,
                    255,
                    0
                ],
                "published": "2025-04-22T02:52:08Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    2,
                    52,
                    8,
                    1,
                    112,
                    0
                ],
                "title": "A Framework for Testing and Adapting REST APIs as LLM Tools",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Testing and Adapting REST APIs as LLM Tools"
                },
                "summary": "Large Language Models (LLMs) are increasingly used to build autonomous agents\nthat perform complex tasks with external tools, often exposed through APIs in\nenterprise systems. Direct use of these APIs is difficult due to the complex\ninput schema and verbose responses. Current benchmarks overlook these\nchallenges, leaving a gap in assessing API readiness for agent-driven\nautomation. We present a testing framework that systematically evaluates\nenterprise APIs when wrapped as Python tools for LLM-based agents. The\nframework generates data-aware test cases, translates them into natural\nlanguage instructions, and evaluates whether agents can correctly invoke the\ntool, handle their inputs, and process its responses. We apply the framework to\ngenerate over 2400 test cases across different domains and develop a taxonomy\nof common errors, including input misinterpretation, output failures, and\nschema mismatches. We further classify errors to support debugging and tool\nrefinement. Our framework provides a systematic approach to enabling enterprise\nAPIs as reliable tools for agent-based applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used to build autonomous agents\nthat perform complex tasks with external tools, often exposed through APIs in\nenterprise systems. Direct use of these APIs is difficult due to the complex\ninput schema and verbose responses. Current benchmarks overlook these\nchallenges, leaving a gap in assessing API readiness for agent-driven\nautomation. We present a testing framework that systematically evaluates\nenterprise APIs when wrapped as Python tools for LLM-based agents. The\nframework generates data-aware test cases, translates them into natural\nlanguage instructions, and evaluates whether agents can correctly invoke the\ntool, handle their inputs, and process its responses. We apply the framework to\ngenerate over 2400 test cases across different domains and develop a taxonomy\nof common errors, including input misinterpretation, output failures, and\nschema mismatches. We further classify errors to support debugging and tool\nrefinement. Our framework provides a systematic approach to enabling enterprise\nAPIs as reliable tools for agent-based applications."
                },
                "authors": [
                    {
                        "name": "Jayachandu Bandlamudi"
                    },
                    {
                        "name": "Ritwik Chaudhuri"
                    },
                    {
                        "name": "Neelamadhav Gantayat"
                    },
                    {
                        "name": "Sambit Ghosh"
                    },
                    {
                        "name": "Kushal Mukherjee"
                    },
                    {
                        "name": "Prerna Agarwal"
                    },
                    {
                        "name": "Renuka Sindhgatta"
                    },
                    {
                        "name": "Sameep Mehta"
                    }
                ],
                "author_detail": {
                    "name": "Sameep Mehta"
                },
                "author": "Sameep Mehta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15546v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15546v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10146v1",
                "updated": "2025-09-12T11:19:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    11,
                    19,
                    41,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T11:19:41Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    11,
                    19,
                    41,
                    4,
                    255,
                    0
                ],
                "title": "On Syntactical Simplification of Temporal Operators in Negation-free MTL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Syntactical Simplification of Temporal Operators in Negation-free MTL"
                },
                "summary": "Temporal reasoning in dynamic, data-intensive environments increasingly\ndemands expressive yet tractable logical frameworks. Traditional approaches\noften rely on negation to express absence or contradiction. In such contexts,\nNegation-as-Failure is commonly used to infer negative information from the\nlack of positive evidence. However, open and distributed systems such as IoT\nnetworks or the Semantic Web Negation-as-Failure semantics become unreliable\ndue to incomplete and asynchronous data. This has led to a growing interest in\nnegation-free fragments of temporal rule-based systems, which preserve\nmonotonicity and enable scalable reasoning.\n  This paper investigates the expressive power of negation-free MTL, a temporal\nlogic framework designed for rule-based reasoning over time. We show that the\n\"always\" operators of MTL, often treated as syntactic sugar for combinations of\nother temporal constructs, can be eliminated using \"once\", \"since\" and \"until\"\noperators. Remarkably, even the \"once\" operators can be removed, yielding a\nfragment based solely on \"until\" and \"since\". These results challenge the\nassumption that negation is necessary for expressing universal temporal\nconstraints, and reveal a robust fragment capable of capturing both existential\nand invariant temporal patterns. Furthermore, the results induce a reduction in\nthe syntax of MTL, which in turn can provide benefits for both theoretical\nstudy as well as implementation efforts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal reasoning in dynamic, data-intensive environments increasingly\ndemands expressive yet tractable logical frameworks. Traditional approaches\noften rely on negation to express absence or contradiction. In such contexts,\nNegation-as-Failure is commonly used to infer negative information from the\nlack of positive evidence. However, open and distributed systems such as IoT\nnetworks or the Semantic Web Negation-as-Failure semantics become unreliable\ndue to incomplete and asynchronous data. This has led to a growing interest in\nnegation-free fragments of temporal rule-based systems, which preserve\nmonotonicity and enable scalable reasoning.\n  This paper investigates the expressive power of negation-free MTL, a temporal\nlogic framework designed for rule-based reasoning over time. We show that the\n\"always\" operators of MTL, often treated as syntactic sugar for combinations of\nother temporal constructs, can be eliminated using \"once\", \"since\" and \"until\"\noperators. Remarkably, even the \"once\" operators can be removed, yielding a\nfragment based solely on \"until\" and \"since\". These results challenge the\nassumption that negation is necessary for expressing universal temporal\nconstraints, and reveal a robust fragment capable of capturing both existential\nand invariant temporal patterns. Furthermore, the results induce a reduction in\nthe syntax of MTL, which in turn can provide benefits for both theoretical\nstudy as well as implementation efforts."
                },
                "authors": [
                    {
                        "name": "Mathijs van Noort"
                    },
                    {
                        "name": "Femke Ongenae"
                    },
                    {
                        "name": "Pieter Bonte"
                    }
                ],
                "author_detail": {
                    "name": "Pieter Bonte"
                },
                "author": "Pieter Bonte",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10144v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10144v1",
                "updated": "2025-09-12T11:15:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    11,
                    15,
                    51,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T11:15:51Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    11,
                    15,
                    51,
                    4,
                    255,
                    0
                ],
                "title": "Cluster Ages to Reconstruct the Milky Way Assembly (CARMA) IV.\n  Chrono-dynamics of seven old star clusters in the Large Magellanic Cloud and\n  the peculiar origin of NGC 1841",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cluster Ages to Reconstruct the Milky Way Assembly (CARMA) IV.\n  Chrono-dynamics of seven old star clusters in the Large Magellanic Cloud and\n  the peculiar origin of NGC 1841"
                },
                "summary": "In this study, we report conclusive evidence for an ancient star cluster that\nhas been accreted by the Large Magellanic Cloud (LMC). By leveraging\nobservations from the Hubble Space Telescope (HST), we investigate the\nchrono-dynamical structure of a sample of seven old star clusters within the\nLMC in a self-consistent way. The multi-epoch nature of the dataset allowed the\ndetermination of high-precision proper motions for the clusters. Employing an\nisochrone-fitting methodology, we additionally infer from the deep\nhigh-resolution HST data homogeneous and robust estimates for their distances,\nages and metallicities. Supplementing these data with literature line-of-sight\nvelocities, we investigate the full 3-dimensional dynamics of the clusters\nwithin the frame of the LMC. With respect to the other clusters in our sample,\nNGC 1841 depicts a peculiar case. Its position in the age-metallicity plane,\nthat makes it about 1 Gyr younger than the other metal-poor LMC clusters, but\nalso its dynamical properties with a radial orbit almost perpendicular to the\nLMC disc plane, clearly advocates for a different origin. We thus conclude that\nNGC 1841 has likely been accreted by the LMC from a smaller galaxy. The other\nclusters in our sample show disc-like kinematics, with the case of NGC 2210\nbeing peculiar, based on its inclined orbit. Their coherent age-metallicity\nrelation closely resembles that of Gaia-Sausage-Enceladus globular clusters,\nthus suggesting a similar early evolution for the two dwarf galaxies. We do not\nfind clear-cut chrono-kinematic evidence that NGC 2005 has been accreted by the\nLMC as suggested by a previous study based on its chemical abundance pattern.\nRegardless of its nature, its very old age illustrates that peculiar chemical\nevolutions already emerge at very early times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we report conclusive evidence for an ancient star cluster that\nhas been accreted by the Large Magellanic Cloud (LMC). By leveraging\nobservations from the Hubble Space Telescope (HST), we investigate the\nchrono-dynamical structure of a sample of seven old star clusters within the\nLMC in a self-consistent way. The multi-epoch nature of the dataset allowed the\ndetermination of high-precision proper motions for the clusters. Employing an\nisochrone-fitting methodology, we additionally infer from the deep\nhigh-resolution HST data homogeneous and robust estimates for their distances,\nages and metallicities. Supplementing these data with literature line-of-sight\nvelocities, we investigate the full 3-dimensional dynamics of the clusters\nwithin the frame of the LMC. With respect to the other clusters in our sample,\nNGC 1841 depicts a peculiar case. Its position in the age-metallicity plane,\nthat makes it about 1 Gyr younger than the other metal-poor LMC clusters, but\nalso its dynamical properties with a radial orbit almost perpendicular to the\nLMC disc plane, clearly advocates for a different origin. We thus conclude that\nNGC 1841 has likely been accreted by the LMC from a smaller galaxy. The other\nclusters in our sample show disc-like kinematics, with the case of NGC 2210\nbeing peculiar, based on its inclined orbit. Their coherent age-metallicity\nrelation closely resembles that of Gaia-Sausage-Enceladus globular clusters,\nthus suggesting a similar early evolution for the two dwarf galaxies. We do not\nfind clear-cut chrono-kinematic evidence that NGC 2005 has been accreted by the\nLMC as suggested by a previous study based on its chemical abundance pattern.\nRegardless of its nature, its very old age illustrates that peculiar chemical\nevolutions already emerge at very early times."
                },
                "authors": [
                    {
                        "name": "F. Niederhofer"
                    },
                    {
                        "name": "D. Massari"
                    },
                    {
                        "name": "F. Aguado-Agelet"
                    },
                    {
                        "name": "S. Cassisi"
                    },
                    {
                        "name": "A. Bellini"
                    },
                    {
                        "name": "V. Kozhurina-Platais"
                    },
                    {
                        "name": "M. Libralato"
                    },
                    {
                        "name": "N. Kacharov"
                    },
                    {
                        "name": "A. Mucciarelli"
                    },
                    {
                        "name": "M. Monelli"
                    },
                    {
                        "name": "N. Bastian"
                    },
                    {
                        "name": "I. Cabrera-Ziri"
                    },
                    {
                        "name": "E. Ceccarelli"
                    },
                    {
                        "name": "M. -R. L. Cioni"
                    },
                    {
                        "name": "F. Dresbach"
                    },
                    {
                        "name": "M. Häberle"
                    },
                    {
                        "name": "S. Martocchia"
                    },
                    {
                        "name": "S. Saracino"
                    }
                ],
                "author_detail": {
                    "name": "S. Saracino"
                },
                "author": "S. Saracino",
                "arxiv_comment": "Accepted for publication in A&A, 22 pages, 13 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10144v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10144v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05367v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05367v2",
                "updated": "2025-09-12T11:05:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    11,
                    5,
                    8,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-04T05:53:20Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    5,
                    53,
                    20,
                    3,
                    247,
                    0
                ],
                "title": "Between a Rock and a Hard Place: Exploiting Ethical Reasoning to\n  Jailbreak LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Between a Rock and a Hard Place: Exploiting Ethical Reasoning to\n  Jailbreak LLMs"
                },
                "summary": "Large language models (LLMs) have undergone safety alignment efforts to\nmitigate harmful outputs. However, as LLMs become more sophisticated in\nreasoning, their intelligence may introduce new security risks. While\ntraditional jailbreak attacks relied on singlestep attacks, multi-turn\njailbreak strategies that adapt dynamically to context remain underexplored. In\nthis work, we introduce TRIAL (Trolley-problem Reasoning for Interactive Attack\nLogic), a framework that leverages LLMs ethical reasoning to bypass their\nsafeguards. TRIAL embeds adversarial goals within ethical dilemmas modeled on\nthe trolley problem. TRIAL demonstrates high jailbreak success rates towards\nboth open and close-source models. Our findings underscore a fundamental\nlimitation in AI safety: as models gain advanced reasoning abilities, the\nnature of their alignment may inadvertently allow for more covert security\nvulnerabilities to be exploited. TRIAL raises an urgent need in reevaluating\nsafety alignment oversight strategies, as current safeguards may prove\ninsufficient against context-aware adversarial attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have undergone safety alignment efforts to\nmitigate harmful outputs. However, as LLMs become more sophisticated in\nreasoning, their intelligence may introduce new security risks. While\ntraditional jailbreak attacks relied on singlestep attacks, multi-turn\njailbreak strategies that adapt dynamically to context remain underexplored. In\nthis work, we introduce TRIAL (Trolley-problem Reasoning for Interactive Attack\nLogic), a framework that leverages LLMs ethical reasoning to bypass their\nsafeguards. TRIAL embeds adversarial goals within ethical dilemmas modeled on\nthe trolley problem. TRIAL demonstrates high jailbreak success rates towards\nboth open and close-source models. Our findings underscore a fundamental\nlimitation in AI safety: as models gain advanced reasoning abilities, the\nnature of their alignment may inadvertently allow for more covert security\nvulnerabilities to be exploited. TRIAL raises an urgent need in reevaluating\nsafety alignment oversight strategies, as current safeguards may prove\ninsufficient against context-aware adversarial attack."
                },
                "authors": [
                    {
                        "name": "Shei Pern Chua"
                    },
                    {
                        "name": "Zhen Leng Thai"
                    },
                    {
                        "name": "Teh Kai Jun"
                    },
                    {
                        "name": "Xiao Li"
                    },
                    {
                        "name": "Xiaolin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaolin Hu"
                },
                "author": "Xiaolin Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05367v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05367v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10132v1",
                "updated": "2025-09-12T10:46:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    10,
                    46,
                    21,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T10:46:21Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    10,
                    46,
                    21,
                    4,
                    255,
                    0
                ],
                "title": "Cost-Free Personalization via Information-Geometric Projection in\n  Bayesian Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-Free Personalization via Information-Geometric Projection in\n  Bayesian Federated Learning"
                },
                "summary": "Bayesian Federated Learning (BFL) combines uncertainty modeling with\ndecentralized training, enabling the development of personalized and reliable\nmodels under data heterogeneity and privacy constraints. Existing approaches\ntypically rely on Markov Chain Monte Carlo (MCMC) sampling or variational\ninference, often incorporating personalization mechanisms to better adapt to\nlocal data distributions. In this work, we propose an information-geometric\nprojection framework for personalization in parametric BFL. By projecting the\nglobal model onto a neighborhood of the user's local model, our method enables\na tunable trade-off between global generalization and local specialization.\nUnder mild assumptions, we show that this projection step is equivalent to\ncomputing a barycenter on the statistical manifold, allowing us to derive\nclosed-form solutions and achieve cost-free personalization. We apply the\nproposed approach to a variational learning setup using the Improved\nVariational Online Newton (IVON) optimizer and extend its application to\ngeneral aggregation schemes in BFL. Empirical evaluations under heterogeneous\ndata distributions confirm that our method effectively balances global and\nlocal performance with minimal computational overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Federated Learning (BFL) combines uncertainty modeling with\ndecentralized training, enabling the development of personalized and reliable\nmodels under data heterogeneity and privacy constraints. Existing approaches\ntypically rely on Markov Chain Monte Carlo (MCMC) sampling or variational\ninference, often incorporating personalization mechanisms to better adapt to\nlocal data distributions. In this work, we propose an information-geometric\nprojection framework for personalization in parametric BFL. By projecting the\nglobal model onto a neighborhood of the user's local model, our method enables\na tunable trade-off between global generalization and local specialization.\nUnder mild assumptions, we show that this projection step is equivalent to\ncomputing a barycenter on the statistical manifold, allowing us to derive\nclosed-form solutions and achieve cost-free personalization. We apply the\nproposed approach to a variational learning setup using the Improved\nVariational Online Newton (IVON) optimizer and extend its application to\ngeneral aggregation schemes in BFL. Empirical evaluations under heterogeneous\ndata distributions confirm that our method effectively balances global and\nlocal performance with minimal computational overhead."
                },
                "authors": [
                    {
                        "name": "Nour Jamoussi"
                    },
                    {
                        "name": "Giuseppe Serra"
                    },
                    {
                        "name": "Photios A. Stavrou"
                    },
                    {
                        "name": "Marios Kountouris"
                    }
                ],
                "author_detail": {
                    "name": "Marios Kountouris"
                },
                "author": "Marios Kountouris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10127v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10127v1",
                "updated": "2025-09-12T10:43:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    10,
                    43,
                    47,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T10:43:47Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    10,
                    43,
                    47,
                    4,
                    255,
                    0
                ],
                "title": "Population-Aligned Persona Generation for LLM-based Social Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Population-Aligned Persona Generation for LLM-based Social Simulation"
                },
                "summary": "Recent advances in large language models (LLMs) have enabled human-like\nsocial simulations at unprecedented scale and fidelity, offering new\nopportunities for computational social science. A key challenge, however, is\nthe construction of persona sets that authentically represent the diversity and\ndistribution of real-world populations. Most existing LLM-based social\nsimulation studies focus primarily on designing agentic frameworks and\nsimulation environments, often overlooking the complexities of persona\ngeneration and the potential biases introduced by unrepresentative persona\nsets. In this paper, we propose a systematic framework for synthesizing\nhigh-quality, population-aligned persona sets for LLM-driven social simulation.\nOur approach begins by leveraging LLMs to generate narrative personas from\nlong-term social media data, followed by rigorous quality assessment to filter\nout low-fidelity profiles. We then apply importance sampling to achieve global\nalignment with reference psychometric distributions, such as the Big Five\npersonality traits. To address the needs of specific simulation contexts, we\nfurther introduce a task-specific module that adapts the globally aligned\npersona set to targeted subpopulations. Extensive experiments demonstrate that\nour method significantly reduces population-level bias and enables accurate,\nflexible social simulation for a wide range of research and policy\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have enabled human-like\nsocial simulations at unprecedented scale and fidelity, offering new\nopportunities for computational social science. A key challenge, however, is\nthe construction of persona sets that authentically represent the diversity and\ndistribution of real-world populations. Most existing LLM-based social\nsimulation studies focus primarily on designing agentic frameworks and\nsimulation environments, often overlooking the complexities of persona\ngeneration and the potential biases introduced by unrepresentative persona\nsets. In this paper, we propose a systematic framework for synthesizing\nhigh-quality, population-aligned persona sets for LLM-driven social simulation.\nOur approach begins by leveraging LLMs to generate narrative personas from\nlong-term social media data, followed by rigorous quality assessment to filter\nout low-fidelity profiles. We then apply importance sampling to achieve global\nalignment with reference psychometric distributions, such as the Big Five\npersonality traits. To address the needs of specific simulation contexts, we\nfurther introduce a task-specific module that adapts the globally aligned\npersona set to targeted subpopulations. Extensive experiments demonstrate that\nour method significantly reduces population-level bias and enables accurate,\nflexible social simulation for a wide range of research and policy\napplications."
                },
                "authors": [
                    {
                        "name": "Zhengyu Hu"
                    },
                    {
                        "name": "Zheyuan Xiao"
                    },
                    {
                        "name": "Max Xiong"
                    },
                    {
                        "name": "Yuxuan Lei"
                    },
                    {
                        "name": "Tianfu Wang"
                    },
                    {
                        "name": "Jianxun Lian"
                    },
                    {
                        "name": "Kaize Ding"
                    },
                    {
                        "name": "Ziang Xiao"
                    },
                    {
                        "name": "Nicholas Jing Yuan"
                    },
                    {
                        "name": "Xing Xie"
                    }
                ],
                "author_detail": {
                    "name": "Xing Xie"
                },
                "author": "Xing Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10127v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10127v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2201.10960v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2201.10960v4",
                "updated": "2025-09-12T10:38:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    10,
                    38,
                    58,
                    4,
                    255,
                    0
                ],
                "published": "2022-01-26T14:24:34Z",
                "published_parsed": [
                    2022,
                    1,
                    26,
                    14,
                    24,
                    34,
                    2,
                    26,
                    0
                ],
                "title": "Triplication: an important component of the modern scientific method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Triplication: an important component of the modern scientific method"
                },
                "summary": "A scientific-study protocol, as defined here, is designed to deliver results\nfrom which inductive inference is allowed. In the nineteenth century,\ntriplication was introduced into the plant sciences and Fisher's p<0.05 rule\n(1925) was incorporated into a triple-result protocol designed to counter\nrandom/systematic errors. Aims here were to: (1) classify replication\n(including non-replicated) protocols; (2) assess their prevalence in\nplant-science studies published in 2017 for a defined variable construct; and\n(3) explore the theoretical rationale for the use of triplication. Methods: the\nplant sciences were surveyed and a protocol-prevalence report produced;\nassociation versus experimental proportions analyzed; and real-world-data\nproxies were used to show confidence-interval-width patterns with increasing\nreplicate number. Results: triplication was found in ~70% of plant-science\nstudies analyzed, with triple-result protocols observed in ~15%. Theoretical\nconsiderations showed that, even if systematic errors predominate, \"square-root\nrules\" sometimes apply, contributing to the predominance of triplication\n(real-world-data proxy examples given). Conclusions: Triplication was\nextensively applied in the studies analyzed and there are strong methodological\nreasons why (1) triplication, rather than duplication/quadruplication, is the\nmost appropriate standard; (2) triple-result protocols are preferable to global\naveraging approaches; and (3) plant science methodological standards remain\nhigh, despite immense publication pressures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A scientific-study protocol, as defined here, is designed to deliver results\nfrom which inductive inference is allowed. In the nineteenth century,\ntriplication was introduced into the plant sciences and Fisher's p<0.05 rule\n(1925) was incorporated into a triple-result protocol designed to counter\nrandom/systematic errors. Aims here were to: (1) classify replication\n(including non-replicated) protocols; (2) assess their prevalence in\nplant-science studies published in 2017 for a defined variable construct; and\n(3) explore the theoretical rationale for the use of triplication. Methods: the\nplant sciences were surveyed and a protocol-prevalence report produced;\nassociation versus experimental proportions analyzed; and real-world-data\nproxies were used to show confidence-interval-width patterns with increasing\nreplicate number. Results: triplication was found in ~70% of plant-science\nstudies analyzed, with triple-result protocols observed in ~15%. Theoretical\nconsiderations showed that, even if systematic errors predominate, \"square-root\nrules\" sometimes apply, contributing to the predominance of triplication\n(real-world-data proxy examples given). Conclusions: Triplication was\nextensively applied in the studies analyzed and there are strong methodological\nreasons why (1) triplication, rather than duplication/quadruplication, is the\nmost appropriate standard; (2) triple-result protocols are preferable to global\naveraging approaches; and (3) plant science methodological standards remain\nhigh, despite immense publication pressures."
                },
                "authors": [
                    {
                        "name": "Jeremy S. C. Clark"
                    },
                    {
                        "name": "Karina Szczypiór-Piasecka"
                    },
                    {
                        "name": "Kamila Rydzewska"
                    },
                    {
                        "name": "Konrad Podsiadło"
                    }
                ],
                "author_detail": {
                    "name": "Konrad Podsiadło"
                },
                "author": "Konrad Podsiadło",
                "arxiv_comment": "41 pages, 6 figures. Supplemental files at\n  https://github.com/Abiologist/Triplication.git",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2201.10960v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2201.10960v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.15181v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.15181v2",
                "updated": "2025-09-12T10:35:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    10,
                    35,
                    35,
                    4,
                    255,
                    0
                ],
                "published": "2023-05-24T14:14:04Z",
                "published_parsed": [
                    2023,
                    5,
                    24,
                    14,
                    14,
                    4,
                    2,
                    144,
                    0
                ],
                "title": "Gravitational wave signatures from the phase-transition-induced collapse\n  of a magnetized neutron star",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational wave signatures from the phase-transition-induced collapse\n  of a magnetized neutron star"
                },
                "summary": "Strong magnetic fields make neutron stars potential sources of detectable\nelectromagnetic and gravitational-wave signals. Hence, inferring these magnetic\nfields is critical to understand the emissions of neutron stars. However, due\nto the lack of direct observational evidence, the interior magnetic field\nconfiguration remains ambiguous. Here, for the first time, we show that the\ninternal magnetic field strength along with the composition of a neutron star\ncan be directly constrained by detecting the gravitational waves from the\n\\emph{phase-transition-induced collapse} of a magnetized neutron star. By\ndynamically simulating this collapsing event, we first find that the dominant\npeaks in the gravitational waveform are the fundamental $l=0$ quasi-radial $F$\nmode and the fundamental $l=2$ quadrupolar $^2f$ mode. We next show that the\nmaximum gravitational wave amplitude $|h|_\\mathrm{max}$ increases with the\nmaximum magnetic field strength of the interior toroidal field\n$\\mathcal{B}_\\mathrm{max}$ until the maximum rest-mass density at bounce\n$\\rho_\\mathrm{max,b}$ decreases due to the increasing\n$\\mathcal{B}_\\mathrm{max}$. We then demonstrated that the magnetic suppression\nof fundamental modes found in our previous work remains valid for the hybrid\nstars formed after the phase-transition-induced collapses. We finally show that\nmeasuring the frequency ratio between the two fundamental modes $f_{^2f}/f_{F}$\nallows one to infer $\\mathcal{B}_\\mathrm{max}$ and the baryonic mass fraction\nof matter in the mixed phase $M_\\mathrm{mp} / M_{0}$ of the resulting hybrid\nstar. Consequently, taking $\\mathcal{B}_\\mathrm{max}$ and $M_\\mathrm{mp} /\nM_{0}$ as examples, this work has demonstrated that much information inside\nneutron stars could be extracted similarly through measuring the oscillation\nmodes of the stars.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strong magnetic fields make neutron stars potential sources of detectable\nelectromagnetic and gravitational-wave signals. Hence, inferring these magnetic\nfields is critical to understand the emissions of neutron stars. However, due\nto the lack of direct observational evidence, the interior magnetic field\nconfiguration remains ambiguous. Here, for the first time, we show that the\ninternal magnetic field strength along with the composition of a neutron star\ncan be directly constrained by detecting the gravitational waves from the\n\\emph{phase-transition-induced collapse} of a magnetized neutron star. By\ndynamically simulating this collapsing event, we first find that the dominant\npeaks in the gravitational waveform are the fundamental $l=0$ quasi-radial $F$\nmode and the fundamental $l=2$ quadrupolar $^2f$ mode. We next show that the\nmaximum gravitational wave amplitude $|h|_\\mathrm{max}$ increases with the\nmaximum magnetic field strength of the interior toroidal field\n$\\mathcal{B}_\\mathrm{max}$ until the maximum rest-mass density at bounce\n$\\rho_\\mathrm{max,b}$ decreases due to the increasing\n$\\mathcal{B}_\\mathrm{max}$. We then demonstrated that the magnetic suppression\nof fundamental modes found in our previous work remains valid for the hybrid\nstars formed after the phase-transition-induced collapses. We finally show that\nmeasuring the frequency ratio between the two fundamental modes $f_{^2f}/f_{F}$\nallows one to infer $\\mathcal{B}_\\mathrm{max}$ and the baryonic mass fraction\nof matter in the mixed phase $M_\\mathrm{mp} / M_{0}$ of the resulting hybrid\nstar. Consequently, taking $\\mathcal{B}_\\mathrm{max}$ and $M_\\mathrm{mp} /\nM_{0}$ as examples, this work has demonstrated that much information inside\nneutron stars could be extracted similarly through measuring the oscillation\nmodes of the stars."
                },
                "authors": [
                    {
                        "name": "Anson Ka Long Yip"
                    },
                    {
                        "name": "Patrick Chi-Kit Cheong"
                    },
                    {
                        "name": "Tjonnie Guang Feng Li"
                    }
                ],
                "author_detail": {
                    "name": "Tjonnie Guang Feng Li"
                },
                "author": "Tjonnie Guang Feng Li",
                "arxiv_doi": "10.1103/nz17-wwp3",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/nz17-wwp3",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2305.15181v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.15181v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages, 7 figures",
                "arxiv_journal_ref": "Phys. Rev. D 112, 043035 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03556v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03556v2",
                "updated": "2025-09-12T10:34:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    10,
                    34,
                    34,
                    4,
                    255,
                    0
                ],
                "published": "2025-03-05T14:44:53Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    14,
                    44,
                    53,
                    2,
                    64,
                    0
                ],
                "title": "Afford-X: Generalizable and Slim Affordance Reasoning for Task-oriented\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Afford-X: Generalizable and Slim Affordance Reasoning for Task-oriented\n  Manipulation"
                },
                "summary": "Object affordance reasoning, the ability to infer object functionalities\nbased on physical properties, is fundamental for task-oriented planning and\nactivities in both humans and Artificial Intelligence (AI). This capability,\nrequired for planning and executing daily activities in a task-oriented manner,\nrelies on commonsense knowledge of object physics and functionalities,\nextending beyond simple object recognition. Current computational models for\naffordance reasoning from perception lack generalizability, limiting their\napplicability in novel scenarios. Meanwhile, comprehensive Large Language\nModels (LLMs) with emerging reasoning capabilities are challenging to deploy on\nlocal devices for task-oriented manipulations. Here, we introduce LVIS-Aff, a\nlarge-scale dataset comprising 1,496 tasks and 119k images, designed to enhance\nthe generalizability of affordance reasoning from perception. Utilizing this\ndataset, we develop Afford-X, an end-to-end trainable affordance reasoning\nmodel that incorporates Verb Attention and Bi-Fusion modules to improve\nmulti-modal understanding. This model achieves up to a 12.1% performance\nimprovement over the best-reported results from non-LLM methods, while also\ndemonstrating a 1.2% enhancement compared to our previous conference paper.\nAdditionally, it maintains a compact 187M parameter size and infers nearly 50\ntimes faster than the GPT-4V API. Our work demonstrates the potential for\nefficient, generalizable affordance reasoning models that can be deployed on\nlocal devices for task-oriented manipulations. We showcase Afford-X's\neffectiveness in enabling task-oriented manipulations for robots across various\ntasks and environments, underscoring its efficiency and broad implications for\nadvancing robotics and AI systems in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object affordance reasoning, the ability to infer object functionalities\nbased on physical properties, is fundamental for task-oriented planning and\nactivities in both humans and Artificial Intelligence (AI). This capability,\nrequired for planning and executing daily activities in a task-oriented manner,\nrelies on commonsense knowledge of object physics and functionalities,\nextending beyond simple object recognition. Current computational models for\naffordance reasoning from perception lack generalizability, limiting their\napplicability in novel scenarios. Meanwhile, comprehensive Large Language\nModels (LLMs) with emerging reasoning capabilities are challenging to deploy on\nlocal devices for task-oriented manipulations. Here, we introduce LVIS-Aff, a\nlarge-scale dataset comprising 1,496 tasks and 119k images, designed to enhance\nthe generalizability of affordance reasoning from perception. Utilizing this\ndataset, we develop Afford-X, an end-to-end trainable affordance reasoning\nmodel that incorporates Verb Attention and Bi-Fusion modules to improve\nmulti-modal understanding. This model achieves up to a 12.1% performance\nimprovement over the best-reported results from non-LLM methods, while also\ndemonstrating a 1.2% enhancement compared to our previous conference paper.\nAdditionally, it maintains a compact 187M parameter size and infers nearly 50\ntimes faster than the GPT-4V API. Our work demonstrates the potential for\nefficient, generalizable affordance reasoning models that can be deployed on\nlocal devices for task-oriented manipulations. We showcase Afford-X's\neffectiveness in enabling task-oriented manipulations for robots across various\ntasks and environments, underscoring its efficiency and broad implications for\nadvancing robotics and AI systems in real-world applications."
                },
                "authors": [
                    {
                        "name": "Xiaomeng Zhu"
                    },
                    {
                        "name": "Yuyang Li"
                    },
                    {
                        "name": "Leiyao Cui"
                    },
                    {
                        "name": "Pengfei Li"
                    },
                    {
                        "name": "Huan-ang Gao"
                    },
                    {
                        "name": "Yixin Zhu"
                    },
                    {
                        "name": "Hao Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhao"
                },
                "author": "Hao Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03556v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03556v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10122v1",
                "updated": "2025-09-12T10:32:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    10,
                    32,
                    4,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T10:32:04Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    10,
                    32,
                    4,
                    4,
                    255,
                    0
                ],
                "title": "Realism Control One-step Diffusion for Real-World Image Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Realism Control One-step Diffusion for Real-World Image Super-Resolution"
                },
                "summary": "Pre-trained diffusion models have shown great potential in real-world image\nsuper-resolution (Real-ISR) tasks by enabling high-resolution reconstructions.\nWhile one-step diffusion (OSD) methods significantly improve efficiency\ncompared to traditional multi-step approaches, they still have limitations in\nbalancing fidelity and realism across diverse scenarios. Since the OSDs for SR\nare usually trained or distilled by a single timestep, they lack flexible\ncontrol mechanisms to adaptively prioritize these competing objectives, which\nare inherently manageable in multi-step methods through adjusting sampling\nsteps. To address this challenge, we propose a Realism Controlled One-step\nDiffusion (RCOD) framework for Real-ISR. RCOD provides a latent domain grouping\nstrategy that enables explicit control over fidelity-realism trade-offs during\nthe noise prediction phase with minimal training paradigm modifications and\noriginal training data. A degradation-aware sampling strategy is also\nintroduced to align distillation regularization with the grouping strategy and\nenhance the controlling of trade-offs. Moreover, a visual prompt injection\nmodule is used to replace conventional text prompts with degradation-aware\nvisual tokens, enhancing both restoration accuracy and semantic consistency.\nOur method achieves superior fidelity and perceptual quality while maintaining\ncomputational efficiency. Extensive experiments demonstrate that RCOD\noutperforms state-of-the-art OSD methods in both quantitative metrics and\nvisual qualities, with flexible realism control capabilities in the inference\nstage. The code will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-trained diffusion models have shown great potential in real-world image\nsuper-resolution (Real-ISR) tasks by enabling high-resolution reconstructions.\nWhile one-step diffusion (OSD) methods significantly improve efficiency\ncompared to traditional multi-step approaches, they still have limitations in\nbalancing fidelity and realism across diverse scenarios. Since the OSDs for SR\nare usually trained or distilled by a single timestep, they lack flexible\ncontrol mechanisms to adaptively prioritize these competing objectives, which\nare inherently manageable in multi-step methods through adjusting sampling\nsteps. To address this challenge, we propose a Realism Controlled One-step\nDiffusion (RCOD) framework for Real-ISR. RCOD provides a latent domain grouping\nstrategy that enables explicit control over fidelity-realism trade-offs during\nthe noise prediction phase with minimal training paradigm modifications and\noriginal training data. A degradation-aware sampling strategy is also\nintroduced to align distillation regularization with the grouping strategy and\nenhance the controlling of trade-offs. Moreover, a visual prompt injection\nmodule is used to replace conventional text prompts with degradation-aware\nvisual tokens, enhancing both restoration accuracy and semantic consistency.\nOur method achieves superior fidelity and perceptual quality while maintaining\ncomputational efficiency. Extensive experiments demonstrate that RCOD\noutperforms state-of-the-art OSD methods in both quantitative metrics and\nvisual qualities, with flexible realism control capabilities in the inference\nstage. The code will be released."
                },
                "authors": [
                    {
                        "name": "Zongliang Wu"
                    },
                    {
                        "name": "Siming Zheng"
                    },
                    {
                        "name": "Peng-Tao Jiang"
                    },
                    {
                        "name": "Xin Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Xin Yuan"
                },
                "author": "Xin Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10117v1",
                "updated": "2025-09-12T10:20:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    10,
                    20,
                    4,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T10:20:04Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    10,
                    20,
                    4,
                    4,
                    255,
                    0
                ],
                "title": "Impact of stochastic star-formation histories and dust on selecting\n  quiescent galaxies with JWST photometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impact of stochastic star-formation histories and dust on selecting\n  quiescent galaxies with JWST photometry"
                },
                "summary": "While the James Webb Space Telescope (JWST) now allows identifying quiescent\ngalaxies (QGs) out to early epochs, the photometric selection of quiescent\ngalaxy candidates (QGCs) and the derivation of key physical quantities are\nhighly sensitive to the assumed star-formation histories (SFHs). We aim to\nquantify how the inclusion of JWST/MIRI data and different SFH models impacts\nthe selection and characterisation of QGCs. We test the robustness of the\nphysical properties inferred from the spectral energy distribution (SED)\nfitting, such as M*, age, star formation rate (SFR), and AV, and study how they\nimpact the quiescence criteria of the galaxies across cosmic time. We perform\nSED fitting for ~13000 galaxies at z<6 from the CEERS/MIRI fields with up to 20\noptical-mid infrared (MIR) broadband coverage. We implement three SFH\nprescriptions: flexible delayed, NonParametric, and extended Regulator. For\neach model, we compare results obtained with and without MIRI photometry and\ndust emission models. We evaluate the impact of these configurations on the\nnumber of candidate QGCs, selected based on rest UVJ colours, sSFR and\nmain-sequence offset, and on their key physical properties such as M*, AV, and\nstellar ages. The number of QGCs selected varies significantly with the choice\nof SFH from 171 to 224 out of 13000 galaxies, depending on the model. This\nnumber increases to 222-327 when MIRI data are used (up to ~45% more QGCs).\nThis enhancement is driven by improved constraints on dust attenuation and M*.\nWe find a strong correlation between AV and M*, with massive galaxies (M*~10^11\nM\\odot) being 1.5-4.2 times more attenuated in magnitude than low-mass systems\n(M*~10^9 M\\odot), depending on SFH. Regardless of the SFH assumption, ~13% of\nQGCs exhibit significant attenuation (AV > 0.5) in support of recent JWST\nstudies challenging the notion that quiescent galaxies are uniformly dust-free.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the James Webb Space Telescope (JWST) now allows identifying quiescent\ngalaxies (QGs) out to early epochs, the photometric selection of quiescent\ngalaxy candidates (QGCs) and the derivation of key physical quantities are\nhighly sensitive to the assumed star-formation histories (SFHs). We aim to\nquantify how the inclusion of JWST/MIRI data and different SFH models impacts\nthe selection and characterisation of QGCs. We test the robustness of the\nphysical properties inferred from the spectral energy distribution (SED)\nfitting, such as M*, age, star formation rate (SFR), and AV, and study how they\nimpact the quiescence criteria of the galaxies across cosmic time. We perform\nSED fitting for ~13000 galaxies at z<6 from the CEERS/MIRI fields with up to 20\noptical-mid infrared (MIR) broadband coverage. We implement three SFH\nprescriptions: flexible delayed, NonParametric, and extended Regulator. For\neach model, we compare results obtained with and without MIRI photometry and\ndust emission models. We evaluate the impact of these configurations on the\nnumber of candidate QGCs, selected based on rest UVJ colours, sSFR and\nmain-sequence offset, and on their key physical properties such as M*, AV, and\nstellar ages. The number of QGCs selected varies significantly with the choice\nof SFH from 171 to 224 out of 13000 galaxies, depending on the model. This\nnumber increases to 222-327 when MIRI data are used (up to ~45% more QGCs).\nThis enhancement is driven by improved constraints on dust attenuation and M*.\nWe find a strong correlation between AV and M*, with massive galaxies (M*~10^11\nM\\odot) being 1.5-4.2 times more attenuated in magnitude than low-mass systems\n(M*~10^9 M\\odot), depending on SFH. Regardless of the SFH assumption, ~13% of\nQGCs exhibit significant attenuation (AV > 0.5) in support of recent JWST\nstudies challenging the notion that quiescent galaxies are uniformly dust-free."
                },
                "authors": [
                    {
                        "name": "K. Lisiecki"
                    },
                    {
                        "name": "D. Donevski"
                    },
                    {
                        "name": "A. W. S. Man"
                    },
                    {
                        "name": "I. Damjanov"
                    },
                    {
                        "name": "M. Romano"
                    },
                    {
                        "name": "S. Belli"
                    },
                    {
                        "name": "A. Long"
                    },
                    {
                        "name": "G. Lorenzon"
                    },
                    {
                        "name": "K. Małek"
                    },
                    {
                        "name": "Junais"
                    },
                    {
                        "name": "C. C. Lovell"
                    },
                    {
                        "name": "A. Nanni"
                    },
                    {
                        "name": "C. Bertemes"
                    },
                    {
                        "name": "W. Pearson"
                    },
                    {
                        "name": "O. Ryzhov"
                    },
                    {
                        "name": "M. Koprowski"
                    },
                    {
                        "name": "A. Pollo"
                    },
                    {
                        "name": "S. Dey"
                    },
                    {
                        "name": "H. Thuruthipilly"
                    }
                ],
                "author_detail": {
                    "name": "H. Thuruthipilly"
                },
                "author": "H. Thuruthipilly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10108v1",
                "updated": "2025-09-12T09:58:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    9,
                    58,
                    11,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T09:58:11Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    9,
                    58,
                    11,
                    4,
                    255,
                    0
                ],
                "title": "Scaling Arabic Medical Chatbots Using Synthetic Data: Enhancing\n  Generative AI with Synthetic Patient Records",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Arabic Medical Chatbots Using Synthetic Data: Enhancing\n  Generative AI with Synthetic Patient Records"
                },
                "summary": "The development of medical chatbots in Arabic is significantly constrained by\nthe scarcity of large-scale, high-quality annotated datasets. While prior\nefforts compiled a dataset of 20,000 Arabic patient-doctor interactions from\nsocial media to fine-tune large language models (LLMs), model scalability and\ngeneralization remained limited. In this study, we propose a scalable synthetic\ndata augmentation strategy to expand the training corpus to 100,000 records.\nUsing advanced generative AI systems ChatGPT-4o and Gemini 2.5 Pro we generated\n80,000 contextually relevant and medically coherent synthetic question-answer\npairs grounded in the structure of the original dataset. These synthetic\nsamples were semantically filtered, manually validated, and integrated into the\ntraining pipeline. We fine-tuned five LLMs, including Mistral-7B and AraGPT2,\nand evaluated their performance using BERTScore metrics and expert-driven\nqualitative assessments. To further analyze the effectiveness of synthetic\nsources, we conducted an ablation study comparing ChatGPT-4o and\nGemini-generated data independently. The results showed that ChatGPT-4o data\nconsistently led to higher F1-scores and fewer hallucinations across all\nmodels. Overall, our findings demonstrate the viability of synthetic\naugmentation as a practical solution for enhancing domain-specific language\nmodels in-low resource medical NLP, paving the way for more inclusive,\nscalable, and accurate Arabic healthcare chatbot systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of medical chatbots in Arabic is significantly constrained by\nthe scarcity of large-scale, high-quality annotated datasets. While prior\nefforts compiled a dataset of 20,000 Arabic patient-doctor interactions from\nsocial media to fine-tune large language models (LLMs), model scalability and\ngeneralization remained limited. In this study, we propose a scalable synthetic\ndata augmentation strategy to expand the training corpus to 100,000 records.\nUsing advanced generative AI systems ChatGPT-4o and Gemini 2.5 Pro we generated\n80,000 contextually relevant and medically coherent synthetic question-answer\npairs grounded in the structure of the original dataset. These synthetic\nsamples were semantically filtered, manually validated, and integrated into the\ntraining pipeline. We fine-tuned five LLMs, including Mistral-7B and AraGPT2,\nand evaluated their performance using BERTScore metrics and expert-driven\nqualitative assessments. To further analyze the effectiveness of synthetic\nsources, we conducted an ablation study comparing ChatGPT-4o and\nGemini-generated data independently. The results showed that ChatGPT-4o data\nconsistently led to higher F1-scores and fewer hallucinations across all\nmodels. Overall, our findings demonstrate the viability of synthetic\naugmentation as a practical solution for enhancing domain-specific language\nmodels in-low resource medical NLP, paving the way for more inclusive,\nscalable, and accurate Arabic healthcare chatbot systems."
                },
                "authors": [
                    {
                        "name": "Abdulrahman Allam"
                    },
                    {
                        "name": "Seif Ahmed"
                    },
                    {
                        "name": "Ali Hamdi"
                    },
                    {
                        "name": "Khaled Shaban"
                    }
                ],
                "author_detail": {
                    "name": "Khaled Shaban"
                },
                "author": "Khaled Shaban",
                "arxiv_comment": "Accepted in AICCSA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13519v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13519v2",
                "updated": "2025-09-12T09:57:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    9,
                    57,
                    33,
                    4,
                    255,
                    0
                ],
                "published": "2025-06-16T14:13:21Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    13,
                    21,
                    0,
                    167,
                    0
                ],
                "title": "General-relativistic magnetar magnetospheres in 3D with physics-informed\n  neural networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General-relativistic magnetar magnetospheres in 3D with physics-informed\n  neural networks"
                },
                "summary": "Magnetar phenomena are likely intertwined with the location and structure of\nmagnetospheric currents. General-relativistic effects are important in shaping\nthe force-free equilibria describing static configurations, though most studies\nhave quantified their impact only in cases of axial symmetry. Using a novel\nmethodology based on physics-informed neural networks, fully three-dimensional\nconfigurations of varying stellar compactness are constructed. Realistic\nprofiles for surface currents, qualitatively capturing the geometry of observed\nhotspots, are applied as boundary conditions to deduce the amount of free\nenergy available to fuel outburst activity. It is found that the lowest-energy\nsolution branches permit only a $\\approx 30\\%$ excess relative to\ncurrent-starved solutions in axisymmetric cases with global twists, regardless\nof compactness, reducing to $\\approx 5\\%$ in 3D models with localised spots.\nAccounting for redshift reductions to their inferred dipole moments from timing\ndata, explaining magnetar burst energetics therefore becomes more difficult\nunless the field hosts non-negligible multipoles. Discussions on other aspects\nof magnetar phenomena are also provided.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magnetar phenomena are likely intertwined with the location and structure of\nmagnetospheric currents. General-relativistic effects are important in shaping\nthe force-free equilibria describing static configurations, though most studies\nhave quantified their impact only in cases of axial symmetry. Using a novel\nmethodology based on physics-informed neural networks, fully three-dimensional\nconfigurations of varying stellar compactness are constructed. Realistic\nprofiles for surface currents, qualitatively capturing the geometry of observed\nhotspots, are applied as boundary conditions to deduce the amount of free\nenergy available to fuel outburst activity. It is found that the lowest-energy\nsolution branches permit only a $\\approx 30\\%$ excess relative to\ncurrent-starved solutions in axisymmetric cases with global twists, regardless\nof compactness, reducing to $\\approx 5\\%$ in 3D models with localised spots.\nAccounting for redshift reductions to their inferred dipole moments from timing\ndata, explaining magnetar burst energetics therefore becomes more difficult\nunless the field hosts non-negligible multipoles. Discussions on other aspects\nof magnetar phenomena are also provided."
                },
                "authors": [
                    {
                        "name": "Petros Stefanou"
                    },
                    {
                        "name": "Arthur G. Suvorov"
                    },
                    {
                        "name": "José A. Pons"
                    }
                ],
                "author_detail": {
                    "name": "José A. Pons"
                },
                "author": "José A. Pons",
                "arxiv_doi": "10.1093/mnras/staf1438",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/staf1438",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.13519v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13519v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 12 figures. v2 (accepted manuscript). Published by MNRAS.\n  Comments are welcome",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04191v2",
                "updated": "2025-09-12T09:55:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    9,
                    55,
                    20,
                    4,
                    255,
                    0
                ],
                "published": "2025-04-05T14:44:47Z",
                "published_parsed": [
                    2025,
                    4,
                    5,
                    14,
                    44,
                    47,
                    5,
                    95,
                    0
                ],
                "title": "GROVE: A Generalized Reward for Learning Open-Vocabulary Physical Skill",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GROVE: A Generalized Reward for Learning Open-Vocabulary Physical Skill"
                },
                "summary": "Learning open-vocabulary physical skills for simulated agents presents a\nsignificant challenge in artificial intelligence. Current reinforcement\nlearning approaches face critical limitations: manually designed rewards lack\nscalability across diverse tasks, while demonstration-based methods struggle to\ngeneralize beyond their training distribution. We introduce GROVE, a\ngeneralized reward framework that enables open-vocabulary physical skill\nlearning without manual engineering or task-specific demonstrations. Our key\ninsight is that Large Language Models(LLMs) and Vision Language Models(VLMs)\nprovide complementary guidance -- LLMs generate precise physical constraints\ncapturing task requirements, while VLMs evaluate motion semantics and\nnaturalness. Through an iterative design process, VLM-based feedback\ncontinuously refines LLM-generated constraints, creating a self-improving\nreward system. To bridge the domain gap between simulation and natural images,\nwe develop Pose2CLIP, a lightweight mapper that efficiently projects agent\nposes directly into semantic feature space without computationally expensive\nrendering. Extensive experiments across diverse embodiments and learning\nparadigms demonstrate GROVE's effectiveness, achieving 22.2% higher motion\nnaturalness and 25.7% better task completion scores while training 8.4x faster\nthan previous methods. These results establish a new foundation for scalable\nphysical skill acquisition in simulated environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning open-vocabulary physical skills for simulated agents presents a\nsignificant challenge in artificial intelligence. Current reinforcement\nlearning approaches face critical limitations: manually designed rewards lack\nscalability across diverse tasks, while demonstration-based methods struggle to\ngeneralize beyond their training distribution. We introduce GROVE, a\ngeneralized reward framework that enables open-vocabulary physical skill\nlearning without manual engineering or task-specific demonstrations. Our key\ninsight is that Large Language Models(LLMs) and Vision Language Models(VLMs)\nprovide complementary guidance -- LLMs generate precise physical constraints\ncapturing task requirements, while VLMs evaluate motion semantics and\nnaturalness. Through an iterative design process, VLM-based feedback\ncontinuously refines LLM-generated constraints, creating a self-improving\nreward system. To bridge the domain gap between simulation and natural images,\nwe develop Pose2CLIP, a lightweight mapper that efficiently projects agent\nposes directly into semantic feature space without computationally expensive\nrendering. Extensive experiments across diverse embodiments and learning\nparadigms demonstrate GROVE's effectiveness, achieving 22.2% higher motion\nnaturalness and 25.7% better task completion scores while training 8.4x faster\nthan previous methods. These results establish a new foundation for scalable\nphysical skill acquisition in simulated environments."
                },
                "authors": [
                    {
                        "name": "Jieming Cui"
                    },
                    {
                        "name": "Tengyu Liu"
                    },
                    {
                        "name": "Ziyu Meng"
                    },
                    {
                        "name": "Jiale Yu"
                    },
                    {
                        "name": "Ran Song"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Yixin Zhu"
                    },
                    {
                        "name": "Siyuan Huang"
                    }
                ],
                "author_detail": {
                    "name": "Siyuan Huang"
                },
                "author": "Siyuan Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10102v1",
                "updated": "2025-09-12T09:51:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    9,
                    51,
                    50,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T09:51:50Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    9,
                    51,
                    50,
                    4,
                    255,
                    0
                ],
                "title": "Generalizing thermodynamic efficiency of interactions: inferential,\n  information-geometric and computational perspectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalizing thermodynamic efficiency of interactions: inferential,\n  information-geometric and computational perspectives"
                },
                "summary": "Self-organizing systems consume energy to generate internal order. The\nconcept of thermodynamic efficiency, drawing from statistical physics and\ninformation theory, has previously been proposed to characterize a change in\ncontrol parameter by relating the resulting predictability gain to the required\namount of work. However, previous studies have taken a system-centric\nperspective and considered only single control parameters. Here, we generalize\nthermodynamic efficiency to multi-parameter settings and derive two\nobserver-centric formulations. The first, an inferential form, relates\nefficiency to fluctuations of macroscopic observables, interpreting\nthermodynamic efficiency in terms of how well the system parameters can be\ninferred from observable macroscopic behaviour. The second, an\ninformation-geometric form, expresses efficiency in terms of the Fisher\ninformation matrix, interpreting it with respect to how difficult it is to\nnavigate the statistical manifold defined by the control protocol. This\nobserver-centric perspective is contrasted with the existing system-centric\nview, where efficiency is considered an intrinsic property of the system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-organizing systems consume energy to generate internal order. The\nconcept of thermodynamic efficiency, drawing from statistical physics and\ninformation theory, has previously been proposed to characterize a change in\ncontrol parameter by relating the resulting predictability gain to the required\namount of work. However, previous studies have taken a system-centric\nperspective and considered only single control parameters. Here, we generalize\nthermodynamic efficiency to multi-parameter settings and derive two\nobserver-centric formulations. The first, an inferential form, relates\nefficiency to fluctuations of macroscopic observables, interpreting\nthermodynamic efficiency in terms of how well the system parameters can be\ninferred from observable macroscopic behaviour. The second, an\ninformation-geometric form, expresses efficiency in terms of the Fisher\ninformation matrix, interpreting it with respect to how difficult it is to\nnavigate the statistical manifold defined by the control protocol. This\nobserver-centric perspective is contrasted with the existing system-centric\nview, where efficiency is considered an intrinsic property of the system."
                },
                "authors": [
                    {
                        "name": "Qianyang Chen"
                    },
                    {
                        "name": "Nihat Ay"
                    },
                    {
                        "name": "Mikhail Prokopenko"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Prokopenko"
                },
                "author": "Mikhail Prokopenko",
                "arxiv_comment": "13 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nlin.AO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nlin.AO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05, 82B20, 82B27, 94A17",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16016v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16016v2",
                "updated": "2025-09-12T09:50:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    9,
                    50,
                    49,
                    4,
                    255,
                    0
                ],
                "published": "2025-08-22T00:27:59Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    0,
                    27,
                    59,
                    4,
                    234,
                    0
                ],
                "title": "DRespNeT: A UAV Dataset and YOLOv8-DRN Model for Aerial Instance\n  Segmentation of Building Access Points for Post-Earthquake Search-and-Rescue\n  Missions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRespNeT: A UAV Dataset and YOLOv8-DRN Model for Aerial Instance\n  Segmentation of Building Access Points for Post-Earthquake Search-and-Rescue\n  Missions"
                },
                "summary": "Recent advancements in computer vision and deep learning have enhanced\ndisaster-response capabilities, particularly in the rapid assessment of\nearthquake-affected urban environments. Timely identification of accessible\nentry points and structural obstacles is essential for effective\nsearch-and-rescue (SAR) operations. To address this need, we introduce\nDRespNeT, a high-resolution dataset specifically developed for aerial instance\nsegmentation of post-earthquake structural environments. Unlike existing\ndatasets, which rely heavily on satellite imagery or coarse semantic labeling,\nDRespNeT provides detailed polygon-level instance segmentation annotations\nderived from high-definition (1080p) aerial footage captured in disaster zones,\nincluding the 2023 Turkiye earthquake and other impacted regions. The dataset\ncomprises 28 operationally critical classes, including structurally compromised\nbuildings, access points such as doors, windows, and gaps, multiple debris\nlevels, rescue personnel, vehicles, and civilian visibility. A distinctive\nfeature of DRespNeT is its fine-grained annotation detail, enabling\ndifferentiation between accessible and obstructed areas, thereby improving\noperational planning and response efficiency. Performance evaluations using\nYOLO-based instance segmentation models, specifically YOLOv8-seg, demonstrate\nsignificant gains in real-time situational awareness and decision-making. Our\noptimized YOLOv8-DRN model achieves 92.7% mAP50 with an inference speed of 27\nFPS on an RTX-4090 GPU for multi-target detection, meeting real-time\noperational requirements. The dataset and models support SAR teams and robotic\nsystems, providing a foundation for enhancing human-robot collaboration,\nstreamlining emergency response, and improving survivor outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in computer vision and deep learning have enhanced\ndisaster-response capabilities, particularly in the rapid assessment of\nearthquake-affected urban environments. Timely identification of accessible\nentry points and structural obstacles is essential for effective\nsearch-and-rescue (SAR) operations. To address this need, we introduce\nDRespNeT, a high-resolution dataset specifically developed for aerial instance\nsegmentation of post-earthquake structural environments. Unlike existing\ndatasets, which rely heavily on satellite imagery or coarse semantic labeling,\nDRespNeT provides detailed polygon-level instance segmentation annotations\nderived from high-definition (1080p) aerial footage captured in disaster zones,\nincluding the 2023 Turkiye earthquake and other impacted regions. The dataset\ncomprises 28 operationally critical classes, including structurally compromised\nbuildings, access points such as doors, windows, and gaps, multiple debris\nlevels, rescue personnel, vehicles, and civilian visibility. A distinctive\nfeature of DRespNeT is its fine-grained annotation detail, enabling\ndifferentiation between accessible and obstructed areas, thereby improving\noperational planning and response efficiency. Performance evaluations using\nYOLO-based instance segmentation models, specifically YOLOv8-seg, demonstrate\nsignificant gains in real-time situational awareness and decision-making. Our\noptimized YOLOv8-DRN model achieves 92.7% mAP50 with an inference speed of 27\nFPS on an RTX-4090 GPU for multi-target detection, meeting real-time\noperational requirements. The dataset and models support SAR teams and robotic\nsystems, providing a foundation for enhancing human-robot collaboration,\nstreamlining emergency response, and improving survivor outcomes."
                },
                "authors": [
                    {
                        "name": "Aykut Sirma"
                    },
                    {
                        "name": "Angelos Plastropoulos"
                    },
                    {
                        "name": "Gilbert Tang"
                    },
                    {
                        "name": "Argyrios Zolotas"
                    }
                ],
                "author_detail": {
                    "name": "Argyrios Zolotas"
                },
                "author": "Argyrios Zolotas",
                "arxiv_comment": "Technical Paper of Scientific data paper: UAV imagery dataset from\n  2023 Turkiye earthquakes, annotated for instance segmentation to support SAR\n  robotics. Initial version of the Dataset is released:\n  https://figshare.com/s/66d3116a0de5b7d827fb and\n  https://universe.roboflow.com/cranfield-university-dwusz/phd-project-instance-segmentation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16016v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16016v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10099v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10099v1",
                "updated": "2025-09-12T09:49:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    9,
                    49,
                    46,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T09:49:46Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    9,
                    49,
                    46,
                    4,
                    255,
                    0
                ],
                "title": "Generating Energy-Efficient Code via Large-Language Models -- Where are\n  we now?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Energy-Efficient Code via Large-Language Models -- Where are\n  we now?"
                },
                "summary": "Context. The rise of Large Language Models (LLMs) has led to their widespread\nadoption in development pipelines. Goal. We empirically assess the energy\nefficiency of Python code generated by LLMs against human-written code and code\ndeveloped by a Green software expert. Method. We test 363 solutions to 9 coding\nproblems from the EvoEval benchmark using 6 widespread LLMs with 4 prompting\ntechniques, and comparing them to human-developed solutions. Energy consumption\nis measured on three different hardware platforms: a server, a PC, and a\nRaspberry Pi for a total of ~881h (36.7 days). Results. Human solutions are 16%\nmore energy-efficient on the server and 3% on the Raspberry Pi, while LLMs\noutperform human developers by 25% on the PC. Prompting does not consistently\nlead to energy savings, where the most energy-efficient prompts vary by\nhardware platform. The code developed by a Green software expert is\nconsistently more energy-efficient by at least 17% to 30% against all LLMs on\nall hardware platforms. Conclusions. Even though LLMs exhibit relatively good\ncode generation capabilities, no LLM-generated code was more energy-efficient\nthan that of an experienced Green software developer, suggesting that as of\ntoday there is still a great need of human expertise for developing\nenergy-efficient Python code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context. The rise of Large Language Models (LLMs) has led to their widespread\nadoption in development pipelines. Goal. We empirically assess the energy\nefficiency of Python code generated by LLMs against human-written code and code\ndeveloped by a Green software expert. Method. We test 363 solutions to 9 coding\nproblems from the EvoEval benchmark using 6 widespread LLMs with 4 prompting\ntechniques, and comparing them to human-developed solutions. Energy consumption\nis measured on three different hardware platforms: a server, a PC, and a\nRaspberry Pi for a total of ~881h (36.7 days). Results. Human solutions are 16%\nmore energy-efficient on the server and 3% on the Raspberry Pi, while LLMs\noutperform human developers by 25% on the PC. Prompting does not consistently\nlead to energy savings, where the most energy-efficient prompts vary by\nhardware platform. The code developed by a Green software expert is\nconsistently more energy-efficient by at least 17% to 30% against all LLMs on\nall hardware platforms. Conclusions. Even though LLMs exhibit relatively good\ncode generation capabilities, no LLM-generated code was more energy-efficient\nthan that of an experienced Green software developer, suggesting that as of\ntoday there is still a great need of human expertise for developing\nenergy-efficient Python code."
                },
                "authors": [
                    {
                        "name": "Radu Apsan"
                    },
                    {
                        "name": "Vincenzo Stoico"
                    },
                    {
                        "name": "Michel Albonico"
                    },
                    {
                        "name": "Rudra Dhar"
                    },
                    {
                        "name": "Karthik Vaidhyanathan"
                    },
                    {
                        "name": "Ivano Malavolta"
                    }
                ],
                "author_detail": {
                    "name": "Ivano Malavolta"
                },
                "author": "Ivano Malavolta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10099v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10099v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10095v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10095v1",
                "updated": "2025-09-12T09:37:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    9,
                    37,
                    26,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T09:37:26Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    9,
                    37,
                    26,
                    4,
                    255,
                    0
                ],
                "title": "Arabic Large Language Models for Medical Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arabic Large Language Models for Medical Text Generation"
                },
                "summary": "Efficient hospital management systems (HMS) are critical worldwide to address\nchallenges such as overcrowding, limited resources, and poor availability of\nurgent health care. Existing methods often lack the ability to provide\naccurate, real-time medical advice, particularly for irregular inputs and\nunderrepresented languages. To overcome these limitations, this study proposes\nan approach that fine-tunes large language models (LLMs) for Arabic medical\ntext generation. The system is designed to assist patients by providing\naccurate medical advice, diagnoses, drug recommendations, and treatment plans\nbased on user input. The research methodology required the collection of a\nunique dataset from social media platforms, capturing real-world medical\nconversations between patients and doctors. The dataset, which includes patient\ncomplaints together with medical advice, was properly cleaned and preprocessed\nto account for multiple Arabic dialects. Fine-tuning state-of-the-art\ngenerative models, such as Mistral-7B-Instruct-v0.2, LLaMA-2-7B, and GPT-2\nMedium, optimized the system's ability to generate reliable medical text.\nResults from evaluations indicate that the fine-tuned Mistral-7B model\noutperformed the other models, achieving average BERT (Bidirectional Encoder\nRepresentations from Transformers) Score values in precision, recall, and\nF1-scores of 68.5\\%, 69.08\\%, and 68.5\\%, respectively. Comparative\nbenchmarking and qualitative assessments validate the system's ability to\nproduce coherent and relevant medical replies to informal input. This study\nhighlights the potential of generative artificial intelligence (AI) in\nadvancing HMS, offering a scalable and adaptable solution for global healthcare\nchallenges, especially in linguistically and culturally diverse environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient hospital management systems (HMS) are critical worldwide to address\nchallenges such as overcrowding, limited resources, and poor availability of\nurgent health care. Existing methods often lack the ability to provide\naccurate, real-time medical advice, particularly for irregular inputs and\nunderrepresented languages. To overcome these limitations, this study proposes\nan approach that fine-tunes large language models (LLMs) for Arabic medical\ntext generation. The system is designed to assist patients by providing\naccurate medical advice, diagnoses, drug recommendations, and treatment plans\nbased on user input. The research methodology required the collection of a\nunique dataset from social media platforms, capturing real-world medical\nconversations between patients and doctors. The dataset, which includes patient\ncomplaints together with medical advice, was properly cleaned and preprocessed\nto account for multiple Arabic dialects. Fine-tuning state-of-the-art\ngenerative models, such as Mistral-7B-Instruct-v0.2, LLaMA-2-7B, and GPT-2\nMedium, optimized the system's ability to generate reliable medical text.\nResults from evaluations indicate that the fine-tuned Mistral-7B model\noutperformed the other models, achieving average BERT (Bidirectional Encoder\nRepresentations from Transformers) Score values in precision, recall, and\nF1-scores of 68.5\\%, 69.08\\%, and 68.5\\%, respectively. Comparative\nbenchmarking and qualitative assessments validate the system's ability to\nproduce coherent and relevant medical replies to informal input. This study\nhighlights the potential of generative artificial intelligence (AI) in\nadvancing HMS, offering a scalable and adaptable solution for global healthcare\nchallenges, especially in linguistically and culturally diverse environments."
                },
                "authors": [
                    {
                        "name": "Abdulrahman Allam"
                    },
                    {
                        "name": "Seif Ahmed"
                    },
                    {
                        "name": "Ali Hamdi"
                    },
                    {
                        "name": "Ammar Mohammed"
                    }
                ],
                "author_detail": {
                    "name": "Ammar Mohammed"
                },
                "author": "Ammar Mohammed",
                "arxiv_comment": "Published in 2025 4th International Conference on Computer\n  Technologies (ICCTech)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10095v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10095v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10078v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10078v1",
                "updated": "2025-09-12T09:14:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    9,
                    14,
                    42,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T09:14:42Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    9,
                    14,
                    42,
                    4,
                    255,
                    0
                ],
                "title": "Established Psychometric vs. Ecologically Valid Questionnaires:\n  Rethinking Psychological Assessments in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Established Psychometric vs. Ecologically Valid Questionnaires:\n  Rethinking Psychological Assessments in Large Language Models"
                },
                "summary": "Researchers have applied established psychometric questionnaires (e.g., BFI,\nPVQ) to measure the personality traits and values reflected in the responses of\nLarge Language Models (LLMs). However, concerns have been raised about applying\nthese human-designed questionnaires to LLMs. One such concern is their lack of\necological validity--the extent to which survey questions adequately reflect\nand resemble real-world contexts in which LLMs generate texts in response to\nuser queries. However, it remains unclear how established questionnaires and\necologically valid questionnaires differ in their outcomes, and what insights\nthese differences may provide. In this paper, we conduct a comprehensive\ncomparative analysis of the two types of questionnaires. Our analysis reveals\nthat established questionnaires (1) yield substantially different profiles of\nLLMs from ecologically valid ones, deviating from the psychological\ncharacteristics expressed in the context of user queries, (2) suffer from\ninsufficient items for stable measurement, (3) create misleading impressions\nthat LLMs possess stable constructs, and (4) yield exaggerated profiles for\npersona-prompted LLMs. Overall, our work cautions against the use of\nestablished psychological questionnaires for LLMs. Our code will be released\nupon publication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Researchers have applied established psychometric questionnaires (e.g., BFI,\nPVQ) to measure the personality traits and values reflected in the responses of\nLarge Language Models (LLMs). However, concerns have been raised about applying\nthese human-designed questionnaires to LLMs. One such concern is their lack of\necological validity--the extent to which survey questions adequately reflect\nand resemble real-world contexts in which LLMs generate texts in response to\nuser queries. However, it remains unclear how established questionnaires and\necologically valid questionnaires differ in their outcomes, and what insights\nthese differences may provide. In this paper, we conduct a comprehensive\ncomparative analysis of the two types of questionnaires. Our analysis reveals\nthat established questionnaires (1) yield substantially different profiles of\nLLMs from ecologically valid ones, deviating from the psychological\ncharacteristics expressed in the context of user queries, (2) suffer from\ninsufficient items for stable measurement, (3) create misleading impressions\nthat LLMs possess stable constructs, and (4) yield exaggerated profiles for\npersona-prompted LLMs. Overall, our work cautions against the use of\nestablished psychological questionnaires for LLMs. Our code will be released\nupon publication."
                },
                "authors": [
                    {
                        "name": "Dongmin Choi"
                    },
                    {
                        "name": "Woojung Song"
                    },
                    {
                        "name": "Jongwook Han"
                    },
                    {
                        "name": "Eun-Ju Lee"
                    },
                    {
                        "name": "Yohan Jo"
                    }
                ],
                "author_detail": {
                    "name": "Yohan Jo"
                },
                "author": "Yohan Jo",
                "arxiv_comment": "17 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10078v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10078v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13204v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13204v2",
                "updated": "2025-09-12T09:08:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    9,
                    8,
                    5,
                    4,
                    255,
                    0
                ],
                "published": "2025-05-19T14:55:41Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    55,
                    41,
                    0,
                    139,
                    0
                ],
                "title": "Alignment-Augmented Speculative Decoding with Alignment Sampling and\n  Conditional Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment-Augmented Speculative Decoding with Alignment Sampling and\n  Conditional Verification"
                },
                "summary": "Recent works have revealed the great potential of speculative decoding in\naccelerating the autoregressive generation process of large language models.\nThe success of these methods relies on the alignment between draft candidates\nand the sampled outputs of the target model. Existing methods mainly achieve\ndraft-target alignment with training-based methods, e.g., EAGLE, Medusa,\ninvolving considerable training costs. In this paper, we present a\ntraining-free alignment-augmented speculative decoding algorithm. We propose\nalignment sampling, which leverages output distribution obtained in the\nprefilling phase to provide more aligned draft candidates. To further benefit\nfrom high-quality but non-aligned draft candidates, we also introduce a simple\nyet effective flexible verification strategy. Through an adaptive probability\nthreshold, our approach can improve generation accuracy while further improving\ninference efficiency. Experiments on 8 datasets (including question answering,\nsummarization and code completion tasks) show that our approach increases the\naverage generation score by 3.3 points for the LLaMA3 model. Our method\nachieves a mean acceptance length up to 2.39 and speed up generation by 2.23.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent works have revealed the great potential of speculative decoding in\naccelerating the autoregressive generation process of large language models.\nThe success of these methods relies on the alignment between draft candidates\nand the sampled outputs of the target model. Existing methods mainly achieve\ndraft-target alignment with training-based methods, e.g., EAGLE, Medusa,\ninvolving considerable training costs. In this paper, we present a\ntraining-free alignment-augmented speculative decoding algorithm. We propose\nalignment sampling, which leverages output distribution obtained in the\nprefilling phase to provide more aligned draft candidates. To further benefit\nfrom high-quality but non-aligned draft candidates, we also introduce a simple\nyet effective flexible verification strategy. Through an adaptive probability\nthreshold, our approach can improve generation accuracy while further improving\ninference efficiency. Experiments on 8 datasets (including question answering,\nsummarization and code completion tasks) show that our approach increases the\naverage generation score by 3.3 points for the LLaMA3 model. Our method\nachieves a mean acceptance length up to 2.39 and speed up generation by 2.23."
                },
                "authors": [
                    {
                        "name": "Jikai Wang"
                    },
                    {
                        "name": "Zhenxu Tian"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Qingrong Xia"
                    },
                    {
                        "name": "Xinyu Duan"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Baoxing Huai"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Accepted at EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13204v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13204v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20241v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20241v2",
                "updated": "2025-09-12T09:08:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    9,
                    8,
                    2,
                    4,
                    255,
                    0
                ],
                "published": "2025-07-27T11:52:09Z",
                "published_parsed": [
                    2025,
                    7,
                    27,
                    11,
                    52,
                    9,
                    6,
                    208,
                    0
                ],
                "title": "Reframe Your Life Story: Interactive Narrative Therapist and Innovative\n  Moment Assessment with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reframe Your Life Story: Interactive Narrative Therapist and Innovative\n  Moment Assessment with Large Language Models"
                },
                "summary": "Recent progress in large language models (LLMs) has opened new possibilities\nfor mental health support, yet current approaches lack realism in simulating\nspecialized psychotherapy and fail to capture therapeutic progression over\ntime. Narrative therapy, which helps individuals transform problematic life\nstories into empowering alternatives, remains underutilized due to limited\naccess and social stigma. We address these limitations through a comprehensive\nframework with two core components. First, INT (Interactive Narrative\nTherapist) simulates expert narrative therapists by planning therapeutic\nstages, guiding reflection levels, and generating contextually appropriate\nexpert-like responses. Second, IMA (Innovative Moment Assessment) provides a\ntherapy-centric evaluation method that quantifies effectiveness by tracking\n\"Innovative Moments\" (IMs), critical narrative shifts in client speech\nsignaling therapy progress. Experimental results on 260 simulated clients and\n230 human participants reveal that INT consistently outperforms standard LLMs\nin therapeutic quality and depth. We further demonstrate the effectiveness of\nINT in synthesizing high-quality support conversations to facilitate social\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in large language models (LLMs) has opened new possibilities\nfor mental health support, yet current approaches lack realism in simulating\nspecialized psychotherapy and fail to capture therapeutic progression over\ntime. Narrative therapy, which helps individuals transform problematic life\nstories into empowering alternatives, remains underutilized due to limited\naccess and social stigma. We address these limitations through a comprehensive\nframework with two core components. First, INT (Interactive Narrative\nTherapist) simulates expert narrative therapists by planning therapeutic\nstages, guiding reflection levels, and generating contextually appropriate\nexpert-like responses. Second, IMA (Innovative Moment Assessment) provides a\ntherapy-centric evaluation method that quantifies effectiveness by tracking\n\"Innovative Moments\" (IMs), critical narrative shifts in client speech\nsignaling therapy progress. Experimental results on 260 simulated clients and\n230 human participants reveal that INT consistently outperforms standard LLMs\nin therapeutic quality and depth. We further demonstrate the effectiveness of\nINT in synthesizing high-quality support conversations to facilitate social\napplications."
                },
                "authors": [
                    {
                        "name": "Yi Feng"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Wenxuan Zhang"
                    },
                    {
                        "name": "Zhuang Chen"
                    },
                    {
                        "name": "Yutong Shen"
                    },
                    {
                        "name": "Xiyao Xiao"
                    },
                    {
                        "name": "Minlie Huang"
                    },
                    {
                        "name": "Liping Jing"
                    },
                    {
                        "name": "Jian Yu"
                    }
                ],
                "author_detail": {
                    "name": "Jian Yu"
                },
                "author": "Jian Yu",
                "arxiv_comment": "EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20241v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20241v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07841v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07841v2",
                "updated": "2025-09-12T08:58:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    8,
                    58,
                    30,
                    4,
                    255,
                    0
                ],
                "published": "2025-05-06T14:17:05Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    14,
                    17,
                    5,
                    1,
                    126,
                    0
                ],
                "title": "Task-Oriented Multimodal Token Transmission in Resource-Constrained\n  Multiuser Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-Oriented Multimodal Token Transmission in Resource-Constrained\n  Multiuser Networks"
                },
                "summary": "Despite the promising paradigm enabled by integrating semantic communication\n(SemCom) with multimodal large models (MLMs) for transmitting and utilizing\nmultimodal data, efficiently fusing and exploiting cross-modal information\nstill remain challenging. Moreover, widely adopted transformer-based\narchitectures inevitably produce excessively long token embeddings for\ntransmission, which result in higher bandwidth consumption, increased power\nusage, and greater latency, rendering them impractical in resource-constrained\nnetworks. In this letter, we propose a task-oriented multimodal token\ntransmission scheme for efficient multimodal information fusion and\nutilization. To improve inter-modal consistency and task-relevant token\ntransmission, we design a two-stage training algotithm which involves\ncross-modal alignment followed by task-oriented fine-tuning. Meanwhile, token\ncompression is performed using a sliding window pooling operation to conserve\nlimited communication resources. To balance the trade-off between latency\nreduction and performance degradation caused by compression, we formulate a\nweighted-sum optimization problem over latency and inference performance. We\njointly optimizes bandwidth, power allocation, and token length across users by\nusing an alternating optimization method. Simulation results demonstrate that\nthe proposed algorithm outperforms the baseline under different bandwidth and\npower budgets. Moreover, the two-stage training algorithm achieves higher\naccuracy across various signal-to-noise ratios than the method without\ncross-modal alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promising paradigm enabled by integrating semantic communication\n(SemCom) with multimodal large models (MLMs) for transmitting and utilizing\nmultimodal data, efficiently fusing and exploiting cross-modal information\nstill remain challenging. Moreover, widely adopted transformer-based\narchitectures inevitably produce excessively long token embeddings for\ntransmission, which result in higher bandwidth consumption, increased power\nusage, and greater latency, rendering them impractical in resource-constrained\nnetworks. In this letter, we propose a task-oriented multimodal token\ntransmission scheme for efficient multimodal information fusion and\nutilization. To improve inter-modal consistency and task-relevant token\ntransmission, we design a two-stage training algotithm which involves\ncross-modal alignment followed by task-oriented fine-tuning. Meanwhile, token\ncompression is performed using a sliding window pooling operation to conserve\nlimited communication resources. To balance the trade-off between latency\nreduction and performance degradation caused by compression, we formulate a\nweighted-sum optimization problem over latency and inference performance. We\njointly optimizes bandwidth, power allocation, and token length across users by\nusing an alternating optimization method. Simulation results demonstrate that\nthe proposed algorithm outperforms the baseline under different bandwidth and\npower budgets. Moreover, the two-stage training algorithm achieves higher\naccuracy across various signal-to-noise ratios than the method without\ncross-modal alignment."
                },
                "authors": [
                    {
                        "name": "Junhe Zhang"
                    },
                    {
                        "name": "Wanli Ni"
                    },
                    {
                        "name": "Pengwei Wang"
                    },
                    {
                        "name": "Dongyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dongyu Wang"
                },
                "author": "Dongyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07841v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07841v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10067v1",
                "updated": "2025-09-12T08:58:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    8,
                    58,
                    18,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T08:58:18Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    8,
                    58,
                    18,
                    4,
                    255,
                    0
                ],
                "title": "Weakening assumptions in the evaluation of treatment effects in\n  longitudinal randomized trials with truncation by death or other intercurrent\n  events",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weakening assumptions in the evaluation of treatment effects in\n  longitudinal randomized trials with truncation by death or other intercurrent\n  events"
                },
                "summary": "Intercurrent events, such as treatment switching, rescue medication, or\ntruncation by death, can complicate the interpretation of intention-to-treat\n(ITT) analyses in randomized clinical trials. Recent advances in causal\ninference address these challenges by targeting alternative estimands, such as\nhypothetical estimands or principal stratum estimands (e.g., survivor average\ncausal effects). However, such approaches often require strong, unverifiable\nassumptions, partly due to limited data on time-varying confounders and the\ndifficulty of adjusting for them. Additionally, strict trial protocols\nfrequently lead to (near) violations of the positivity assumption, resulting in\nlimited information for identifying these estimands.\n  In this paper, we propose a novel approach that sidesteps these difficulties\nby focusing on testing the null hypothesis of no treatment effect in the\npresence of arbitrary intercurrent events, including truncation by death, using\nlongitudinal trial data. Our key idea is to compare treated and untreated\nindividuals, matched on baseline covariates, at the most recent time point\nbefore either experiences an intercurrent event. We refer to such contrasts as\nPairwise Last Observation Time (PLOT) estimands. These estimands can be\nidentified in randomized clinical trials without requiring additional\nstructural assumptions, and even in the presence of the aforementioned\npositivity violations. However, they may still be susceptible to a form of\nresidual selection bias. We show that this bias vanishes under the conditions\ntypically required by alternative methods, and find it to be more generally\nsmall in extensive simulation studies. Building on this, we develop\nasymptotically efficient, model-free tests using data-adaptive estimation of\nnuisance parameters. We evaluate the method's performance via simulation\nstudies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intercurrent events, such as treatment switching, rescue medication, or\ntruncation by death, can complicate the interpretation of intention-to-treat\n(ITT) analyses in randomized clinical trials. Recent advances in causal\ninference address these challenges by targeting alternative estimands, such as\nhypothetical estimands or principal stratum estimands (e.g., survivor average\ncausal effects). However, such approaches often require strong, unverifiable\nassumptions, partly due to limited data on time-varying confounders and the\ndifficulty of adjusting for them. Additionally, strict trial protocols\nfrequently lead to (near) violations of the positivity assumption, resulting in\nlimited information for identifying these estimands.\n  In this paper, we propose a novel approach that sidesteps these difficulties\nby focusing on testing the null hypothesis of no treatment effect in the\npresence of arbitrary intercurrent events, including truncation by death, using\nlongitudinal trial data. Our key idea is to compare treated and untreated\nindividuals, matched on baseline covariates, at the most recent time point\nbefore either experiences an intercurrent event. We refer to such contrasts as\nPairwise Last Observation Time (PLOT) estimands. These estimands can be\nidentified in randomized clinical trials without requiring additional\nstructural assumptions, and even in the presence of the aforementioned\npositivity violations. However, they may still be susceptible to a form of\nresidual selection bias. We show that this bias vanishes under the conditions\ntypically required by alternative methods, and find it to be more generally\nsmall in extensive simulation studies. Building on this, we develop\nasymptotically efficient, model-free tests using data-adaptive estimation of\nnuisance parameters. We evaluate the method's performance via simulation\nstudies."
                },
                "authors": [
                    {
                        "name": "Georgi Baklicharov"
                    },
                    {
                        "name": "Kelly Van Lancker"
                    },
                    {
                        "name": "Stijn Vansteelandt"
                    }
                ],
                "author_detail": {
                    "name": "Stijn Vansteelandt"
                },
                "author": "Stijn Vansteelandt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10058v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10058v1",
                "updated": "2025-09-12T08:44:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    8,
                    44,
                    22,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T08:44:22Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    8,
                    44,
                    22,
                    4,
                    255,
                    0
                ],
                "title": "Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings\n  for Improved Diffusion Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings\n  for Improved Diffusion Generation"
                },
                "summary": "Accurate color alignment in text-to-image (T2I) generation is critical for\napplications such as fashion, product visualization, and interior design, yet\ncurrent diffusion models struggle with nuanced and compound color terms (e.g.,\nTiffany blue, lime green, hot pink), often producing images that are misaligned\nwith human intent. Existing approaches rely on cross-attention manipulation,\nreference images, or fine-tuning but fail to systematically resolve ambiguous\ncolor descriptions. To precisely render colors under prompt ambiguity, we\npropose a training-free framework that enhances color fidelity by leveraging a\nlarge language model (LLM) to disambiguate color-related prompts and guiding\ncolor blending operations directly in the text embedding space. Our method\nfirst employs a large language model (LLM) to resolve ambiguous color terms in\nthe text prompt, and then refines the text embeddings based on the spatial\nrelationships of the resulting color terms in the CIELAB color space. Unlike\nprior methods, our approach improves color accuracy without requiring\nadditional training or external reference images. Experimental results\ndemonstrate that our framework improves color alignment without compromising\nimage quality, bridging the gap between text semantics and visual generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate color alignment in text-to-image (T2I) generation is critical for\napplications such as fashion, product visualization, and interior design, yet\ncurrent diffusion models struggle with nuanced and compound color terms (e.g.,\nTiffany blue, lime green, hot pink), often producing images that are misaligned\nwith human intent. Existing approaches rely on cross-attention manipulation,\nreference images, or fine-tuning but fail to systematically resolve ambiguous\ncolor descriptions. To precisely render colors under prompt ambiguity, we\npropose a training-free framework that enhances color fidelity by leveraging a\nlarge language model (LLM) to disambiguate color-related prompts and guiding\ncolor blending operations directly in the text embedding space. Our method\nfirst employs a large language model (LLM) to resolve ambiguous color terms in\nthe text prompt, and then refines the text embeddings based on the spatial\nrelationships of the resulting color terms in the CIELAB color space. Unlike\nprior methods, our approach improves color accuracy without requiring\nadditional training or external reference images. Experimental results\ndemonstrate that our framework improves color alignment without compromising\nimage quality, bridging the gap between text semantics and visual generation."
                },
                "authors": [
                    {
                        "name": "Sung-Lin Tsai"
                    },
                    {
                        "name": "Bo-Lun Huang"
                    },
                    {
                        "name": "Yu Ting Shen"
                    },
                    {
                        "name": "Cheng Yu Yeo"
                    },
                    {
                        "name": "Chiang Tseng"
                    },
                    {
                        "name": "Bo-Kai Ruan"
                    },
                    {
                        "name": "Wen-Sheng Lien"
                    },
                    {
                        "name": "Hong-Han Shuai"
                    }
                ],
                "author_detail": {
                    "name": "Hong-Han Shuai"
                },
                "author": "Hong-Han Shuai",
                "arxiv_doi": "10.1145/3746027.3755385",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746027.3755385",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.10058v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10058v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to ACM Multimedia 2025 (MM '25)",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10054v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10054v1",
                "updated": "2025-09-12T08:40:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    8,
                    40,
                    58,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T08:40:58Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    8,
                    40,
                    58,
                    4,
                    255,
                    0
                ],
                "title": "XAgents: A Unified Framework for Multi-Agent Cooperation via IF-THEN\n  Rules and Multipolar Task Processing Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XAgents: A Unified Framework for Multi-Agent Cooperation via IF-THEN\n  Rules and Multipolar Task Processing Graph"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has significantly\nenhanced the capabilities of Multi-Agent Systems (MAS) in supporting humans\nwith complex, real-world tasks. However, MAS still face challenges in effective\ntask planning when handling highly complex tasks with uncertainty, often\nresulting in misleading or incorrect outputs that hinder task execution. To\naddress this, we propose XAgents, a unified multi-agent cooperative framework\nbuilt on a multipolar task processing graph and IF-THEN rules. XAgents uses the\nmultipolar task processing graph to enable dynamic task planning and handle\ntask uncertainty. During subtask processing, it integrates domain-specific\nIF-THEN rules to constrain agent behaviors, while global rules enhance\ninter-agent collaboration. We evaluate the performance of XAgents across three\ndistinct datasets, demonstrating that it consistently surpasses\nstate-of-the-art single-agent and multi-agent approaches in both\nknowledge-typed and logic-typed question-answering tasks. The codes for XAgents\nare available at: https://github.com/AGI-FHBC/XAgents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has significantly\nenhanced the capabilities of Multi-Agent Systems (MAS) in supporting humans\nwith complex, real-world tasks. However, MAS still face challenges in effective\ntask planning when handling highly complex tasks with uncertainty, often\nresulting in misleading or incorrect outputs that hinder task execution. To\naddress this, we propose XAgents, a unified multi-agent cooperative framework\nbuilt on a multipolar task processing graph and IF-THEN rules. XAgents uses the\nmultipolar task processing graph to enable dynamic task planning and handle\ntask uncertainty. During subtask processing, it integrates domain-specific\nIF-THEN rules to constrain agent behaviors, while global rules enhance\ninter-agent collaboration. We evaluate the performance of XAgents across three\ndistinct datasets, demonstrating that it consistently surpasses\nstate-of-the-art single-agent and multi-agent approaches in both\nknowledge-typed and logic-typed question-answering tasks. The codes for XAgents\nare available at: https://github.com/AGI-FHBC/XAgents."
                },
                "authors": [
                    {
                        "name": "Hailong Yang"
                    },
                    {
                        "name": "Mingxian Gu"
                    },
                    {
                        "name": "Jianqi Wang"
                    },
                    {
                        "name": "Guanjin Wang"
                    },
                    {
                        "name": "Zhaohong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhaohong Deng"
                },
                "author": "Zhaohong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10054v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10054v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04005v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04005v3",
                "updated": "2025-09-12T08:36:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    8,
                    36,
                    2,
                    4,
                    255,
                    0
                ],
                "published": "2025-07-05T11:17:20Z",
                "published_parsed": [
                    2025,
                    7,
                    5,
                    11,
                    17,
                    20,
                    5,
                    186,
                    0
                ],
                "title": "Exploring a Gamified Personality Assessment Method through Interaction\n  with LLM Agents Embodying Different Personalities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring a Gamified Personality Assessment Method through Interaction\n  with LLM Agents Embodying Different Personalities"
                },
                "summary": "The low-intrusion and automated personality assessment is receiving\nincreasing attention in psychology and human-computer interaction fields. This\nstudy explores an interactive approach for personality assessment, focusing on\nthe multiplicity of personality representation. We propose a framework of\nGamified Personality Assessment through Multi-Personality Representations\n(Multi-PR GPA). The framework leverages Large Language Models to empower\nvirtual agents with different personalities. These agents elicit multifaceted\nhuman personality representations through engaging in interactive games.\nDrawing upon the multi-type textual data generated throughout the interaction,\nit achieves two modes of personality assessment (i.e., Direct Assessment and\nQuestionnaire-based Assessment) and provides interpretable insights. Grounded\nin the classic Big Five personality theory, we developed a prototype system and\nconducted a user study to evaluate the efficacy of Multi-PR GPA. The results\naffirm the effectiveness of our approach in personality assessment and\ndemonstrate its superior performance when considering the multiplicity of\npersonality representation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The low-intrusion and automated personality assessment is receiving\nincreasing attention in psychology and human-computer interaction fields. This\nstudy explores an interactive approach for personality assessment, focusing on\nthe multiplicity of personality representation. We propose a framework of\nGamified Personality Assessment through Multi-Personality Representations\n(Multi-PR GPA). The framework leverages Large Language Models to empower\nvirtual agents with different personalities. These agents elicit multifaceted\nhuman personality representations through engaging in interactive games.\nDrawing upon the multi-type textual data generated throughout the interaction,\nit achieves two modes of personality assessment (i.e., Direct Assessment and\nQuestionnaire-based Assessment) and provides interpretable insights. Grounded\nin the classic Big Five personality theory, we developed a prototype system and\nconducted a user study to evaluate the efficacy of Multi-PR GPA. The results\naffirm the effectiveness of our approach in personality assessment and\ndemonstrate its superior performance when considering the multiplicity of\npersonality representation."
                },
                "authors": [
                    {
                        "name": "Baiqiao Zhang"
                    },
                    {
                        "name": "Xiangxian Li"
                    },
                    {
                        "name": "Chao Zhou"
                    },
                    {
                        "name": "Xinyu Gai"
                    },
                    {
                        "name": "Juan Liu"
                    },
                    {
                        "name": "Xue Yang"
                    },
                    {
                        "name": "Xiaojuan Ma"
                    },
                    {
                        "name": "Yong-jin Liu"
                    },
                    {
                        "name": "Yulong Bian"
                    }
                ],
                "author_detail": {
                    "name": "Yulong Bian"
                },
                "author": "Yulong Bian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04005v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04005v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13024v2",
                "updated": "2025-09-12T08:27:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    8,
                    27,
                    21,
                    4,
                    255,
                    0
                ],
                "published": "2025-07-17T11:52:27Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    11,
                    52,
                    27,
                    3,
                    198,
                    0
                ],
                "title": "When Pattern-by-Pattern Works: Theoretical and Empirical Insights for\n  Logistic Models with Missing Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Pattern-by-Pattern Works: Theoretical and Empirical Insights for\n  Logistic Models with Missing Values"
                },
                "summary": "Predicting a response with partially missing inputs remains a challenging\ntask even in parametric models, since parameter estimation in itself is not\nsufficient to predict on partially observed inputs. Several works study\nprediction in linear models. In this paper, we focus on logistic models, which\npresent their own difficulties. From a theoretical perspective, we prove that a\nPattern-by-Pattern strategy (PbP), which learns one logistic model per\nmissingness pattern, accurately approximates Bayes probabilities in various\nmissing data scenarios (MCAR, MAR and MNAR). Empirically, we thoroughly compare\nvarious methods (constant and iterative imputations, complete case analysis,\nPbP, and an EM algorithm) across classification, probability estimation,\ncalibration, and parameter inference. Our analysis provides a comprehensive\nview on the logistic regression with missing values. It reveals that mean\nimputation can be used as baseline for low sample sizes, and improved\nperformance is obtained via nonlinear multiple iterative imputation techniques\nwith the labels (MICE.RF.Y). For large sample sizes, PbP is the best method for\nGaussian mixtures, and we recommend MICE.RF.Y in presence of nonlinear\nfeatures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting a response with partially missing inputs remains a challenging\ntask even in parametric models, since parameter estimation in itself is not\nsufficient to predict on partially observed inputs. Several works study\nprediction in linear models. In this paper, we focus on logistic models, which\npresent their own difficulties. From a theoretical perspective, we prove that a\nPattern-by-Pattern strategy (PbP), which learns one logistic model per\nmissingness pattern, accurately approximates Bayes probabilities in various\nmissing data scenarios (MCAR, MAR and MNAR). Empirically, we thoroughly compare\nvarious methods (constant and iterative imputations, complete case analysis,\nPbP, and an EM algorithm) across classification, probability estimation,\ncalibration, and parameter inference. Our analysis provides a comprehensive\nview on the logistic regression with missing values. It reveals that mean\nimputation can be used as baseline for low sample sizes, and improved\nperformance is obtained via nonlinear multiple iterative imputation techniques\nwith the labels (MICE.RF.Y). For large sample sizes, PbP is the best method for\nGaussian mixtures, and we recommend MICE.RF.Y in presence of nonlinear\nfeatures."
                },
                "authors": [
                    {
                        "name": "Christophe Muller"
                    },
                    {
                        "name": "Erwan Scornet"
                    },
                    {
                        "name": "Julie Josse"
                    }
                ],
                "author_detail": {
                    "name": "Julie Josse"
                },
                "arxiv_affiliation": "PREMEDICAL",
                "author": "Julie Josse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08863v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08863v2",
                "updated": "2025-09-12T08:26:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    8,
                    26,
                    37,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-10T03:43:46Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    3,
                    43,
                    46,
                    2,
                    253,
                    0
                ],
                "title": "GeoJSON Agents:A Multi-Agent LLM Architecture for Geospatial\n  Analysis-Function Calling vs Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GeoJSON Agents:A Multi-Agent LLM Architecture for Geospatial\n  Analysis-Function Calling vs Code Generation"
                },
                "summary": "LLMs have made substantial progress in task automation and natural language\nunderstanding. However, without expertise in GIS, they continue to encounter\nlimitations. To address these issues, we propose GeoJSON Agents-a multi-agent\nLLM architecture. This framework transforms natural language tasks into\nstructured GeoJSON operation commands and processes spatial data using two\nwidely adopted LLM enhancement techniques: Function Calling and Code\nGeneration. The architecture consists of three components-task parsing, agent\ncollaboration, and result integration-aimed at enhancing both the performance\nand scalability of GIS automation. The Planner agent interprets natural\nlanguage tasks into structured GeoJSON commands. Then, specialized Worker\nagents collaborate according to assigned roles to perform spatial data\nprocessing and analysis, either by invoking predefined function APIs or by\ndynamically generating and executing Python-based spatial analysis code.\nFinally, the system integrates the outputs from multiple execution rounds into\nreusable, standards-compliant GeoJSON files. To systematically evaluate the\nperformance of the two approaches, we constructed a benchmark dataset of 70\ntasks with varying complexity and conducted experiments using OpenAI's GPT-4o\nas the core model. Results indicate that the Function Calling-based GeoJSON\nAgent achieved an accuracy of 85.71%, while the Code Generation-based agent\nreached 97.14%, both significantly outperforming the best-performing\ngeneral-purpose model (48.57%). Further analysis reveals that the Code\nGeneration provides greater flexibility, whereas the Function Calling approach\noffers more stable execution. This study is the first to introduce an LLM\nmulti-agent framework for GeoJSON data and to compare the strengths and\nlimitations of two mainstream LLM enhancement methods, offering new\nperspectives for improving GeoAI system performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have made substantial progress in task automation and natural language\nunderstanding. However, without expertise in GIS, they continue to encounter\nlimitations. To address these issues, we propose GeoJSON Agents-a multi-agent\nLLM architecture. This framework transforms natural language tasks into\nstructured GeoJSON operation commands and processes spatial data using two\nwidely adopted LLM enhancement techniques: Function Calling and Code\nGeneration. The architecture consists of three components-task parsing, agent\ncollaboration, and result integration-aimed at enhancing both the performance\nand scalability of GIS automation. The Planner agent interprets natural\nlanguage tasks into structured GeoJSON commands. Then, specialized Worker\nagents collaborate according to assigned roles to perform spatial data\nprocessing and analysis, either by invoking predefined function APIs or by\ndynamically generating and executing Python-based spatial analysis code.\nFinally, the system integrates the outputs from multiple execution rounds into\nreusable, standards-compliant GeoJSON files. To systematically evaluate the\nperformance of the two approaches, we constructed a benchmark dataset of 70\ntasks with varying complexity and conducted experiments using OpenAI's GPT-4o\nas the core model. Results indicate that the Function Calling-based GeoJSON\nAgent achieved an accuracy of 85.71%, while the Code Generation-based agent\nreached 97.14%, both significantly outperforming the best-performing\ngeneral-purpose model (48.57%). Further analysis reveals that the Code\nGeneration provides greater flexibility, whereas the Function Calling approach\noffers more stable execution. This study is the first to introduce an LLM\nmulti-agent framework for GeoJSON data and to compare the strengths and\nlimitations of two mainstream LLM enhancement methods, offering new\nperspectives for improving GeoAI system performance."
                },
                "authors": [
                    {
                        "name": "Qianqian Luo"
                    },
                    {
                        "name": "Liuchang Xu"
                    },
                    {
                        "name": "Qingming Lin"
                    },
                    {
                        "name": "Sensen Wu"
                    },
                    {
                        "name": "Ruichen Mao"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Hailin Feng"
                    },
                    {
                        "name": "Bo Huang"
                    },
                    {
                        "name": "Zhenhong Du"
                    }
                ],
                "author_detail": {
                    "name": "Zhenhong Du"
                },
                "author": "Zhenhong Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08863v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08863v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10045v1",
                "updated": "2025-09-12T08:21:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    8,
                    21,
                    59,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T08:21:59Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    8,
                    21,
                    59,
                    4,
                    255,
                    0
                ],
                "title": "A Bayesian Framework for Regularized Estimation in Multivariate Models\n  Integrating Approximate Computing Concepts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Bayesian Framework for Regularized Estimation in Multivariate Models\n  Integrating Approximate Computing Concepts"
                },
                "summary": "This paper discusses regularized estimators in the multivariate statistical\nmodel as tools naturally arising within a Bayesian framework. First, a link is\nestablished between Bayesian estimation and inference under parameter rounding\n(quantization), thereby connecting two distinct paradigms: Bayesian inference\nand approximate computing. Next, Bayesian estimation of the means from two\nindependent multivariate normal samples is employed to justify shrinkage\nestimators, i.e., means shrunk toward the pooled mean. Finally, regularized\nlinear discriminant analysis (LDA) is considered. Various shrinkage strategies\nfor the mean are justified from a Bayesian perspective, and novel algorithms\nfor their computation are proposed. The proposed methods are illustrated by\nnumerical experiments on real and simulated data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper discusses regularized estimators in the multivariate statistical\nmodel as tools naturally arising within a Bayesian framework. First, a link is\nestablished between Bayesian estimation and inference under parameter rounding\n(quantization), thereby connecting two distinct paradigms: Bayesian inference\nand approximate computing. Next, Bayesian estimation of the means from two\nindependent multivariate normal samples is employed to justify shrinkage\nestimators, i.e., means shrunk toward the pooled mean. Finally, regularized\nlinear discriminant analysis (LDA) is considered. Various shrinkage strategies\nfor the mean are justified from a Bayesian perspective, and novel algorithms\nfor their computation are proposed. The proposed methods are illustrated by\nnumerical experiments on real and simulated data."
                },
                "authors": [
                    {
                        "name": "Jan Kalina"
                    }
                ],
                "author_detail": {
                    "name": "Jan Kalina"
                },
                "author": "Jan Kalina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.6.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24298v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24298v3",
                "updated": "2025-09-12T07:59:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    59,
                    18,
                    4,
                    255,
                    0
                ],
                "published": "2025-05-30T07:18:25Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    7,
                    18,
                    25,
                    4,
                    150,
                    0
                ],
                "title": "AReaL: A Large-Scale Asynchronous Reinforcement Learning System for\n  Language Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AReaL: A Large-Scale Asynchronous Reinforcement Learning System for\n  Language Reasoning"
                },
                "summary": "Reinforcement learning (RL) has become a dominant paradigm for training large\nlanguage models (LLMs), particularly for reasoning tasks. Effective RL for LLMs\nrequires massive parallelization and poses an urgent need for efficient\ntraining systems. Most existing large-scale RL systems for LLMs are\nsynchronous, alternating generation and training in a batch setting where\nrollouts in each training batch are generated by the same model. This approach\nstabilizes RL training but suffers from severe system-level inefficiency:\ngeneration must wait until the longest output in the batch is completed before\nmodel updates, resulting in GPU underutilization. We present AReaL, a fully\nasynchronous RL system that completely decouples generation from training.\nRollout workers in AReaL continuously generate new outputs without waiting,\nwhile training workers update the model whenever a batch of data is collected.\nAReaL also incorporates a collection of system-level optimizations, leading to\nsubstantially higher GPU utilization. To stabilize RL training, AReaL balances\nthe workload of rollout and training workers to control data staleness, and\nadopts a staleness-enhanced PPO variant to better handle outdated training\nsamples. Extensive experiments on math and code reasoning benchmarks show that\nAReaL achieves up to 2.77$\\times$ training speedup compared to synchronous\nsystems with the same number of GPUs and matched or improved final performance.\nThe code of AReaL is available at https://github.com/inclusionAI/AReaL/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has become a dominant paradigm for training large\nlanguage models (LLMs), particularly for reasoning tasks. Effective RL for LLMs\nrequires massive parallelization and poses an urgent need for efficient\ntraining systems. Most existing large-scale RL systems for LLMs are\nsynchronous, alternating generation and training in a batch setting where\nrollouts in each training batch are generated by the same model. This approach\nstabilizes RL training but suffers from severe system-level inefficiency:\ngeneration must wait until the longest output in the batch is completed before\nmodel updates, resulting in GPU underutilization. We present AReaL, a fully\nasynchronous RL system that completely decouples generation from training.\nRollout workers in AReaL continuously generate new outputs without waiting,\nwhile training workers update the model whenever a batch of data is collected.\nAReaL also incorporates a collection of system-level optimizations, leading to\nsubstantially higher GPU utilization. To stabilize RL training, AReaL balances\nthe workload of rollout and training workers to control data staleness, and\nadopts a staleness-enhanced PPO variant to better handle outdated training\nsamples. Extensive experiments on math and code reasoning benchmarks show that\nAReaL achieves up to 2.77$\\times$ training speedup compared to synchronous\nsystems with the same number of GPUs and matched or improved final performance.\nThe code of AReaL is available at https://github.com/inclusionAI/AReaL/."
                },
                "authors": [
                    {
                        "name": "Wei Fu"
                    },
                    {
                        "name": "Jiaxuan Gao"
                    },
                    {
                        "name": "Xujie Shen"
                    },
                    {
                        "name": "Chen Zhu"
                    },
                    {
                        "name": "Zhiyu Mei"
                    },
                    {
                        "name": "Chuyi He"
                    },
                    {
                        "name": "Shusheng Xu"
                    },
                    {
                        "name": "Guo Wei"
                    },
                    {
                        "name": "Jun Mei"
                    },
                    {
                        "name": "Jiashu Wang"
                    },
                    {
                        "name": "Tongkai Yang"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Yi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Wu"
                },
                "author": "Yi Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24298v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24298v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01328v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01328v2",
                "updated": "2025-09-12T07:26:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    26,
                    28,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-01T10:11:56Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    10,
                    11,
                    56,
                    0,
                    244,
                    0
                ],
                "title": "Can Large Language Models Master Complex Card Games?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Master Complex Card Games?"
                },
                "summary": "Complex games have long been an important benchmark for testing the progress\nof artificial intelligence algorithms. AlphaGo, AlphaZero, and MuZero have\ndefeated top human players in Go and Chess, garnering widespread societal\nattention towards artificial intelligence. Concurrently, large language models\n(LLMs) have exhibited remarkable capabilities across various tasks, raising the\nquestion of whether LLMs can achieve similar success in complex games. In this\npaper, we explore the potential of LLMs in mastering complex card games. We\nsystematically assess the learning capabilities of LLMs across eight diverse\ncard games, evaluating the impact of fine-tuning on high-quality gameplay data,\nand examining the models' ability to retain general capabilities while\nmastering these games. Our findings indicate that: (1) LLMs can approach the\nperformance of strong game AIs through supervised fine-tuning on high-quality\ndata, (2) LLMs can master multiple complex card games simultaneously, with\nperformance augmentation for games with similar rules and conflicts for\ndissimilar ones, and (3) LLMs experience a decline in general capabilities when\nmastering complex games, but this decline can be mitigated by integrating a\ncertain amount of general instruction data. The evaluation results demonstrate\nstrong learning ability and versatility of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complex games have long been an important benchmark for testing the progress\nof artificial intelligence algorithms. AlphaGo, AlphaZero, and MuZero have\ndefeated top human players in Go and Chess, garnering widespread societal\nattention towards artificial intelligence. Concurrently, large language models\n(LLMs) have exhibited remarkable capabilities across various tasks, raising the\nquestion of whether LLMs can achieve similar success in complex games. In this\npaper, we explore the potential of LLMs in mastering complex card games. We\nsystematically assess the learning capabilities of LLMs across eight diverse\ncard games, evaluating the impact of fine-tuning on high-quality gameplay data,\nand examining the models' ability to retain general capabilities while\nmastering these games. Our findings indicate that: (1) LLMs can approach the\nperformance of strong game AIs through supervised fine-tuning on high-quality\ndata, (2) LLMs can master multiple complex card games simultaneously, with\nperformance augmentation for games with similar rules and conflicts for\ndissimilar ones, and (3) LLMs experience a decline in general capabilities when\nmastering complex games, but this decline can be mitigated by integrating a\ncertain amount of general instruction data. The evaluation results demonstrate\nstrong learning ability and versatility of LLMs."
                },
                "authors": [
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Felix Henry"
                    },
                    {
                        "name": "Junzhe Chen"
                    },
                    {
                        "name": "Dan Zhang"
                    },
                    {
                        "name": "Shiyu Huang"
                    },
                    {
                        "name": "Evgeny Kharlamov"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01328v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01328v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10018v1",
                "updated": "2025-09-12T07:22:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    22,
                    49,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T07:22:49Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    22,
                    49,
                    4,
                    255,
                    0
                ],
                "title": "GAMA: A General Anonymizing Multi-Agent System for Privacy Preservation\n  Enhanced by Domain Rules and Disproof Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GAMA: A General Anonymizing Multi-Agent System for Privacy Preservation\n  Enhanced by Domain Rules and Disproof Method"
                },
                "summary": "With the rapid advancement of Large Language Model (LLM), LLM-based agents\nexhibit exceptional abilities in understanding and generating natural language,\nfacilitating human-like collaboration and information transmission in LLM-based\nMulti-Agent System (MAS). High-performance LLMs are often hosted on remote\nservers in public spaces. When tasks involve privacy data, MAS cannot securely\nutilize these LLMs without implementing privacy-preserving mechanisms. To\naddress this challenge, we propose a General Anonymizing Multi-Agent system\n(GAMA), which divides the agents' workspace into private and public spaces and\nprotects privacy through the anonymizing mechanism. In the private space,\nagents handle sensitive data, while in the public space, only anonymized data\nis utilized. GAMA incorporates two key modules to mitigate semantic loss caused\nby anonymization: Domain-Rule-based Knowledge Enhancement (DRKE) and\nDisproof-based Logic Enhancement (DLE). We evaluate GAMA on two public\nquestion-answering datasets: Trivia Creative Writing and Logic Grid Puzzle. The\nresults demonstrate that GAMA has superior performance compared to the\nstate-of-the-art models. To further assess its privacy-preserving capabilities,\nwe designed two new datasets: Knowledge Privacy Preservation and Logic Privacy\nPreservation. The final results highlight GAMA's exceptional effectiveness in\nboth task processing and privacy preservation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of Large Language Model (LLM), LLM-based agents\nexhibit exceptional abilities in understanding and generating natural language,\nfacilitating human-like collaboration and information transmission in LLM-based\nMulti-Agent System (MAS). High-performance LLMs are often hosted on remote\nservers in public spaces. When tasks involve privacy data, MAS cannot securely\nutilize these LLMs without implementing privacy-preserving mechanisms. To\naddress this challenge, we propose a General Anonymizing Multi-Agent system\n(GAMA), which divides the agents' workspace into private and public spaces and\nprotects privacy through the anonymizing mechanism. In the private space,\nagents handle sensitive data, while in the public space, only anonymized data\nis utilized. GAMA incorporates two key modules to mitigate semantic loss caused\nby anonymization: Domain-Rule-based Knowledge Enhancement (DRKE) and\nDisproof-based Logic Enhancement (DLE). We evaluate GAMA on two public\nquestion-answering datasets: Trivia Creative Writing and Logic Grid Puzzle. The\nresults demonstrate that GAMA has superior performance compared to the\nstate-of-the-art models. To further assess its privacy-preserving capabilities,\nwe designed two new datasets: Knowledge Privacy Preservation and Logic Privacy\nPreservation. The final results highlight GAMA's exceptional effectiveness in\nboth task processing and privacy preservation."
                },
                "authors": [
                    {
                        "name": "Hailong Yang"
                    },
                    {
                        "name": "Renhuo Zhao"
                    },
                    {
                        "name": "Guanjin Wang"
                    },
                    {
                        "name": "Zhaohong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhaohong Deng"
                },
                "author": "Zhaohong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03498v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03498v2",
                "updated": "2025-09-12T07:12:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    12,
                    41,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-03T17:29:50Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    29,
                    50,
                    2,
                    246,
                    0
                ],
                "title": "OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and\n  Generation"
                },
                "summary": "We introduce OneCAT, a unified multimodal model that seamlessly integrates\nunderstanding, generation, and editing within a novel, pure decoder-only\ntransformer architecture. Our framework uniquely eliminates the need for\nexternal components such as Vision Transformers (ViT) or vision tokenizer\nduring inference, leading to significant efficiency gains, especially for\nhigh-resolution inputs. This is achieved through a modality-specific\nMixture-of-Experts (MoE) structure trained with a single autoregressive (AR)\nobjective, which also natively supports dynamic resolutions. Furthermore, we\npioneer a multi-scale visual autoregressive mechanism within the Large Language\nModel (LLM) that drastically reduces decoding steps compared to diffusion-based\nmethods while maintaining state-of-the-art performance. Our findings\ndemonstrate the powerful potential of pure autoregressive modeling as a\nsufficient and elegant foundation for unified multimodal intelligence. As a\nresult, OneCAT sets a new performance standard, outperforming existing\nopen-source unified multimodal models across benchmarks for multimodal\ngeneration, editing, and understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce OneCAT, a unified multimodal model that seamlessly integrates\nunderstanding, generation, and editing within a novel, pure decoder-only\ntransformer architecture. Our framework uniquely eliminates the need for\nexternal components such as Vision Transformers (ViT) or vision tokenizer\nduring inference, leading to significant efficiency gains, especially for\nhigh-resolution inputs. This is achieved through a modality-specific\nMixture-of-Experts (MoE) structure trained with a single autoregressive (AR)\nobjective, which also natively supports dynamic resolutions. Furthermore, we\npioneer a multi-scale visual autoregressive mechanism within the Large Language\nModel (LLM) that drastically reduces decoding steps compared to diffusion-based\nmethods while maintaining state-of-the-art performance. Our findings\ndemonstrate the powerful potential of pure autoregressive modeling as a\nsufficient and elegant foundation for unified multimodal intelligence. As a\nresult, OneCAT sets a new performance standard, outperforming existing\nopen-source unified multimodal models across benchmarks for multimodal\ngeneration, editing, and understanding."
                },
                "authors": [
                    {
                        "name": "Han Li"
                    },
                    {
                        "name": "Xinyu Peng"
                    },
                    {
                        "name": "Yaoming Wang"
                    },
                    {
                        "name": "Zelin Peng"
                    },
                    {
                        "name": "Xin Chen"
                    },
                    {
                        "name": "Rongxiang Weng"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Wenrui Dai"
                    },
                    {
                        "name": "Hongkai Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hongkai Xiong"
                },
                "author": "Hongkai Xiong",
                "arxiv_comment": "technical report, project url:https://onecat-ai.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03498v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03498v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10010v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10010v1",
                "updated": "2025-09-12T07:10:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    10,
                    55,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T07:10:55Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    10,
                    55,
                    4,
                    255,
                    0
                ],
                "title": "Multi-Intent Recognition in Dialogue Understanding: A Comparison Between\n  Smaller Open-Source LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Intent Recognition in Dialogue Understanding: A Comparison Between\n  Smaller Open-Source LLMs"
                },
                "summary": "In this paper, we provide an extensive analysis of multi-label intent\nclassification using Large Language Models (LLMs) that are open-source,\npublicly available, and can be run in consumer hardware. We use the MultiWOZ\n2.1 dataset, a benchmark in the dialogue system domain, to investigate the\nefficacy of three popular open-source pre-trained LLMs, namely LLama2-7B-hf,\nMistral-7B-v0.1, and Yi-6B. We perform the classification task in a few-shot\nsetup, giving 20 examples in the prompt with some instructions. Our approach\nfocuses on the differences in performance of these models across several\nperformance metrics by methodically assessing these models on multi-label\nintent classification tasks. Additionally, we compare the performance of the\ninstruction-based fine-tuning approach with supervised learning using the\nsmaller transformer model BertForSequenceClassification as a baseline. To\nevaluate the performance of the models, we use evaluation metrics like\naccuracy, precision, and recall as well as micro, macro, and weighted F1 score.\nWe also report the inference time, VRAM requirements, etc. The Mistral-7B-v0.1\noutperforms two other generative models on 11 intent classes out of 14 in terms\nof F-Score, with a weighted average of 0.50. It also has relatively lower\nHumming Loss and higher Jaccard Similarity, making it the winning model in the\nfew-shot setting. We find BERT based supervised classifier having superior\nperformance compared to the best performing few-shot generative LLM. The study\nprovides a framework for small open-source LLMs in detecting complex\nmulti-intent dialogues, enhancing the Natural Language Understanding aspect of\ntask-oriented chatbots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we provide an extensive analysis of multi-label intent\nclassification using Large Language Models (LLMs) that are open-source,\npublicly available, and can be run in consumer hardware. We use the MultiWOZ\n2.1 dataset, a benchmark in the dialogue system domain, to investigate the\nefficacy of three popular open-source pre-trained LLMs, namely LLama2-7B-hf,\nMistral-7B-v0.1, and Yi-6B. We perform the classification task in a few-shot\nsetup, giving 20 examples in the prompt with some instructions. Our approach\nfocuses on the differences in performance of these models across several\nperformance metrics by methodically assessing these models on multi-label\nintent classification tasks. Additionally, we compare the performance of the\ninstruction-based fine-tuning approach with supervised learning using the\nsmaller transformer model BertForSequenceClassification as a baseline. To\nevaluate the performance of the models, we use evaluation metrics like\naccuracy, precision, and recall as well as micro, macro, and weighted F1 score.\nWe also report the inference time, VRAM requirements, etc. The Mistral-7B-v0.1\noutperforms two other generative models on 11 intent classes out of 14 in terms\nof F-Score, with a weighted average of 0.50. It also has relatively lower\nHumming Loss and higher Jaccard Similarity, making it the winning model in the\nfew-shot setting. We find BERT based supervised classifier having superior\nperformance compared to the best performing few-shot generative LLM. The study\nprovides a framework for small open-source LLMs in detecting complex\nmulti-intent dialogues, enhancing the Natural Language Understanding aspect of\ntask-oriented chatbots."
                },
                "authors": [
                    {
                        "name": "Adnan Ahmad"
                    },
                    {
                        "name": "Philine Kowol"
                    },
                    {
                        "name": "Stefan Hillmann"
                    },
                    {
                        "name": "Sebastian Möller"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Möller"
                },
                "author": "Sebastian Möller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10010v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13654v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13654v4",
                "updated": "2025-09-12T07:04:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    4,
                    59,
                    4,
                    255,
                    0
                ],
                "published": "2025-08-19T09:04:13Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    4,
                    13,
                    1,
                    231,
                    0
                ],
                "title": "Input-Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Input-Time Scaling"
                },
                "summary": "Current Large Language Models (LLMs) are usually post-trained on large-scale\ncarefully curated datasets (data & training scaling) and doing reasoning in\ntest time (inference time scaling). In this work, we present a new scaling\nparadigm, Input-Time Scaling, to complement previous scaling methods by putting\nresources on queries (input time). During training and testing, we utilize\nmeta-knowledge from LLMs to refine inputs with different strategies. We also\ndiscover a new phenomenon, train-test co-design. It requires us to apply query\nstrategies during training and testing as a whole. Only applying strategies on\ntraining or testing would seriously degrade the performance gained. We are also\nsurprised to find that seemingly low data quality datasets can perform better.\nWe can get the best performance even by adding irrelevant information to the\nqueries, with randomly selected 1k examples from a minimally filtered dataset.\nThese findings contradict the widely held inductive bias, \"garbage in, garbage\nout\". Curating datasets with seemingly high-quality data can even potentially\nlimit the performance ceiling. In addition, models trained on more data with\nsimilar quality (15k VS 1k) perform worse, the intuition of simply scaling the\nsize should also be carefully inspected. The good news is that our findings are\ncompatible with the Less is More phenomenon. 1K examples are enough to invoke\nhigh-level reasoning ability. With experiments on Qwen2.5-32B-Instruct, we are\nable to reach SOTA performance among 32B models on AIME24(76.7%) and\nAIME25(76.7%) pass@1. We can further achieve AIME24(76.7%) and AIME25(80%) with\na majority vote of three models. Starting from DeepSeek-R1-Distill-Qwen-32B,\nthe result would be 90.0% on AIME24 and 80.0% on AIME25. To facilitate\nreproducibility and further research, we are working on open-source our\ndatasets, data pipelines, evaluation results, and checkpoints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Large Language Models (LLMs) are usually post-trained on large-scale\ncarefully curated datasets (data & training scaling) and doing reasoning in\ntest time (inference time scaling). In this work, we present a new scaling\nparadigm, Input-Time Scaling, to complement previous scaling methods by putting\nresources on queries (input time). During training and testing, we utilize\nmeta-knowledge from LLMs to refine inputs with different strategies. We also\ndiscover a new phenomenon, train-test co-design. It requires us to apply query\nstrategies during training and testing as a whole. Only applying strategies on\ntraining or testing would seriously degrade the performance gained. We are also\nsurprised to find that seemingly low data quality datasets can perform better.\nWe can get the best performance even by adding irrelevant information to the\nqueries, with randomly selected 1k examples from a minimally filtered dataset.\nThese findings contradict the widely held inductive bias, \"garbage in, garbage\nout\". Curating datasets with seemingly high-quality data can even potentially\nlimit the performance ceiling. In addition, models trained on more data with\nsimilar quality (15k VS 1k) perform worse, the intuition of simply scaling the\nsize should also be carefully inspected. The good news is that our findings are\ncompatible with the Less is More phenomenon. 1K examples are enough to invoke\nhigh-level reasoning ability. With experiments on Qwen2.5-32B-Instruct, we are\nable to reach SOTA performance among 32B models on AIME24(76.7%) and\nAIME25(76.7%) pass@1. We can further achieve AIME24(76.7%) and AIME25(80%) with\na majority vote of three models. Starting from DeepSeek-R1-Distill-Qwen-32B,\nthe result would be 90.0% on AIME24 and 80.0% on AIME25. To facilitate\nreproducibility and further research, we are working on open-source our\ndatasets, data pipelines, evaluation results, and checkpoints."
                },
                "authors": [
                    {
                        "name": "Rapheal Huang"
                    },
                    {
                        "name": "Weilong Guo"
                    }
                ],
                "author_detail": {
                    "name": "Weilong Guo"
                },
                "arxiv_affiliation": "Yuming",
                "author": "Weilong Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13654v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13654v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10005v1",
                "updated": "2025-09-12T07:02:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    2,
                    45,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T07:02:45Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    2,
                    45,
                    4,
                    255,
                    0
                ],
                "title": "TUNI: Real-time RGB-T Semantic Segmentation with Unified Multi-Modal\n  Feature Extraction and Cross-Modal Feature Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TUNI: Real-time RGB-T Semantic Segmentation with Unified Multi-Modal\n  Feature Extraction and Cross-Modal Feature Fusion"
                },
                "summary": "RGB-thermal (RGB-T) semantic segmentation improves the environmental\nperception of autonomous platforms in challenging conditions. Prevailing models\nemploy encoders pre-trained on RGB images to extract features from both RGB and\ninfrared inputs, and design additional modules to achieve cross-modal feature\nfusion. This results in limited thermal feature extraction and suboptimal\ncross-modal fusion, while the redundant encoders further compromises the\nmodel's real-time efficiency. To address the above issues, we propose TUNI,\nwith an RGB-T encoder consisting of multiple stacked blocks that simultaneously\nperform multi-modal feature extraction and cross-modal fusion. By leveraging\nlarge-scale pre-training with RGB and pseudo-thermal data, the RGB-T encoder\nlearns to integrate feature extraction and fusion in a unified manner. By\nslimming down the thermal branch, the encoder achieves a more compact\narchitecture. Moreover, we introduce an RGB-T local module to strengthen the\nencoder's capacity for cross-modal local feature fusion. The RGB-T local module\nemploys adaptive cosine similarity to selectively emphasize salient consistent\nand distinct local features across RGB-T modalities. Experimental results show\nthat TUNI achieves competitive performance with state-of-the-art models on FMB,\nPST900 and CART, with fewer parameters and lower computational cost. Meanwhile,\nit achieves an inference speed of 27 FPS on a Jetson Orin NX, demonstrating its\nreal-time capability in deployment. Codes are available at\nhttps://github.com/xiaodonguo/TUNI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RGB-thermal (RGB-T) semantic segmentation improves the environmental\nperception of autonomous platforms in challenging conditions. Prevailing models\nemploy encoders pre-trained on RGB images to extract features from both RGB and\ninfrared inputs, and design additional modules to achieve cross-modal feature\nfusion. This results in limited thermal feature extraction and suboptimal\ncross-modal fusion, while the redundant encoders further compromises the\nmodel's real-time efficiency. To address the above issues, we propose TUNI,\nwith an RGB-T encoder consisting of multiple stacked blocks that simultaneously\nperform multi-modal feature extraction and cross-modal fusion. By leveraging\nlarge-scale pre-training with RGB and pseudo-thermal data, the RGB-T encoder\nlearns to integrate feature extraction and fusion in a unified manner. By\nslimming down the thermal branch, the encoder achieves a more compact\narchitecture. Moreover, we introduce an RGB-T local module to strengthen the\nencoder's capacity for cross-modal local feature fusion. The RGB-T local module\nemploys adaptive cosine similarity to selectively emphasize salient consistent\nand distinct local features across RGB-T modalities. Experimental results show\nthat TUNI achieves competitive performance with state-of-the-art models on FMB,\nPST900 and CART, with fewer parameters and lower computational cost. Meanwhile,\nit achieves an inference speed of 27 FPS on a Jetson Orin NX, demonstrating its\nreal-time capability in deployment. Codes are available at\nhttps://github.com/xiaodonguo/TUNI."
                },
                "authors": [
                    {
                        "name": "Xiaodong Guo"
                    },
                    {
                        "name": "Tong Liu"
                    },
                    {
                        "name": "Yike Li"
                    },
                    {
                        "name": "Zi'ang Lin"
                    },
                    {
                        "name": "Zhihong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhihong Deng"
                },
                "author": "Zhihong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10004v1",
                "updated": "2025-09-12T06:58:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    6,
                    58,
                    17,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T06:58:17Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    6,
                    58,
                    17,
                    4,
                    255,
                    0
                ],
                "title": "Unsupervised Hallucination Detection by Inspecting Reasoning Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised Hallucination Detection by Inspecting Reasoning Processes"
                },
                "summary": "Unsupervised hallucination detection aims to identify hallucinated content\ngenerated by large language models (LLMs) without relying on labeled data.\nWhile unsupervised methods have gained popularity by eliminating\nlabor-intensive human annotations, they frequently rely on proxy signals\nunrelated to factual correctness. This misalignment biases detection probes\ntoward superficial or non-truth-related aspects, limiting generalizability\nacross datasets and scenarios. To overcome these limitations, we propose IRIS,\nan unsupervised hallucination detection framework, leveraging internal\nrepresentations intrinsic to factual correctness. IRIS prompts the LLM to\ncarefully verify the truthfulness of a given statement, and obtain its\ncontextualized embedding as informative features for training. Meanwhile, the\nuncertainty of each response is considered a soft pseudolabel for truthfulness.\nExperimental results demonstrate that IRIS consistently outperforms existing\nunsupervised methods. Our approach is fully unsupervised, computationally low\ncost, and works well even with few training data, making it suitable for\nreal-time detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised hallucination detection aims to identify hallucinated content\ngenerated by large language models (LLMs) without relying on labeled data.\nWhile unsupervised methods have gained popularity by eliminating\nlabor-intensive human annotations, they frequently rely on proxy signals\nunrelated to factual correctness. This misalignment biases detection probes\ntoward superficial or non-truth-related aspects, limiting generalizability\nacross datasets and scenarios. To overcome these limitations, we propose IRIS,\nan unsupervised hallucination detection framework, leveraging internal\nrepresentations intrinsic to factual correctness. IRIS prompts the LLM to\ncarefully verify the truthfulness of a given statement, and obtain its\ncontextualized embedding as informative features for training. Meanwhile, the\nuncertainty of each response is considered a soft pseudolabel for truthfulness.\nExperimental results demonstrate that IRIS consistently outperforms existing\nunsupervised methods. Our approach is fully unsupervised, computationally low\ncost, and works well even with few training data, making it suitable for\nreal-time detection."
                },
                "authors": [
                    {
                        "name": "Ponhvoan Srey"
                    },
                    {
                        "name": "Xiaobao Wu"
                    },
                    {
                        "name": "Anh Tuan Luu"
                    }
                ],
                "author_detail": {
                    "name": "Anh Tuan Luu"
                },
                "author": "Anh Tuan Luu",
                "arxiv_comment": "To appear in EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10001v1",
                "updated": "2025-09-12T06:52:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    6,
                    52,
                    39,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T06:52:39Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    6,
                    52,
                    39,
                    4,
                    255,
                    0
                ],
                "title": "Service Function Chaining Architecture for Multi-hop Split Inference and\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Service Function Chaining Architecture for Multi-hop Split Inference and\n  Learning"
                },
                "summary": "Service Function Chaining (SFC) is a networking technique that ensures\ntraffic traverses a predefined sequence of service functions, realizing\narbitrary network services through dynamic and efficient communication paths.\nInspired by this concept, we propose an SFC-based architecture for Multi-hop\nSplit Inference (MSI), where split sub-models are interpreted as service\nfunctions and their composition forms a service chain representing the global\nmodel. By leveraging SFC, the proposed architecture dynamically establishes\ncommunication paths for split sub-models, ensuring efficient and adaptive\nexecution. Furthermore, we extend this architecture to Multi-hop Split Learning\n(MSL) by applying SFC to the bidirectional communication required for training\ntasks. To realize the proposed architecture, we design Neural Service Functions\n(NSFs) to execute split sub-models as transparent TCP proxies and integrate\nthem with Segment Routing over IPv6 (SRv6) and the extended Berkeley Packet\nFilter (eBPF)-based SFC proxy. This integration ensures efficient ML processing\nover dynamic routing while maintaining compatibility with existing\napplications. Evaluation results demonstrate that (1) the proposed architecture\nis feasible for both MSI and MSL; (2) it is particularly suitable for real-time\ninference in MSI scenarios with small mini-batch sizes; (3) it supports dynamic\npath reconfiguration, enabling adaptive responses to changing network\nconditions while minimizing the impact of control mechanisms on inference and\nlearning processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Service Function Chaining (SFC) is a networking technique that ensures\ntraffic traverses a predefined sequence of service functions, realizing\narbitrary network services through dynamic and efficient communication paths.\nInspired by this concept, we propose an SFC-based architecture for Multi-hop\nSplit Inference (MSI), where split sub-models are interpreted as service\nfunctions and their composition forms a service chain representing the global\nmodel. By leveraging SFC, the proposed architecture dynamically establishes\ncommunication paths for split sub-models, ensuring efficient and adaptive\nexecution. Furthermore, we extend this architecture to Multi-hop Split Learning\n(MSL) by applying SFC to the bidirectional communication required for training\ntasks. To realize the proposed architecture, we design Neural Service Functions\n(NSFs) to execute split sub-models as transparent TCP proxies and integrate\nthem with Segment Routing over IPv6 (SRv6) and the extended Berkeley Packet\nFilter (eBPF)-based SFC proxy. This integration ensures efficient ML processing\nover dynamic routing while maintaining compatibility with existing\napplications. Evaluation results demonstrate that (1) the proposed architecture\nis feasible for both MSI and MSL; (2) it is particularly suitable for real-time\ninference in MSI scenarios with small mini-batch sizes; (3) it supports dynamic\npath reconfiguration, enabling adaptive responses to changing network\nconditions while minimizing the impact of control mechanisms on inference and\nlearning processes."
                },
                "authors": [
                    {
                        "name": "Takanori Hara"
                    },
                    {
                        "name": "Masahiro Sasabe"
                    }
                ],
                "author_detail": {
                    "name": "Masahiro Sasabe"
                },
                "author": "Masahiro Sasabe",
                "arxiv_comment": "11 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00559v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00559v2",
                "updated": "2025-09-12T06:49:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    6,
                    49,
                    21,
                    4,
                    255,
                    0
                ],
                "published": "2024-11-30T19:02:34Z",
                "published_parsed": [
                    2024,
                    11,
                    30,
                    19,
                    2,
                    34,
                    5,
                    335,
                    0
                ],
                "title": "Polish-English medical knowledge transfer: A new benchmark and results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Polish-English medical knowledge transfer: A new benchmark and results"
                },
                "summary": "Large Language Models (LLMs) have demonstrated significant potential in\nhandling specialized tasks, including medical problem-solving. However, most\nstudies predominantly focus on English-language contexts. This study introduces\na novel benchmark dataset based on Polish medical licensing and specialization\nexams (LEK, LDEK, PES) taken by medical doctor candidates and practicing\ndoctors pursuing specialization. The dataset was web-scraped from publicly\navailable resources provided by the Medical Examination Center and the Chief\nMedical Chamber. It comprises over 24,000 exam questions, including a subset of\nparallel Polish-English corpora, where the English portion was professionally\ntranslated by the examination center for foreign candidates. By creating a\nstructured benchmark from these existing exam questions, we systematically\nevaluate state-of-the-art LLMs, including general-purpose, domain-specific, and\nPolish-specific models, and compare their performance against human medical\nstudents. Our analysis reveals that while models like GPT-4o achieve near-human\nperformance, significant challenges persist in cross-lingual translation and\ndomain-specific understanding. These findings underscore disparities in model\nperformance across languages and medical specialties, highlighting the\nlimitations and ethical considerations of deploying LLMs in clinical practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated significant potential in\nhandling specialized tasks, including medical problem-solving. However, most\nstudies predominantly focus on English-language contexts. This study introduces\na novel benchmark dataset based on Polish medical licensing and specialization\nexams (LEK, LDEK, PES) taken by medical doctor candidates and practicing\ndoctors pursuing specialization. The dataset was web-scraped from publicly\navailable resources provided by the Medical Examination Center and the Chief\nMedical Chamber. It comprises over 24,000 exam questions, including a subset of\nparallel Polish-English corpora, where the English portion was professionally\ntranslated by the examination center for foreign candidates. By creating a\nstructured benchmark from these existing exam questions, we systematically\nevaluate state-of-the-art LLMs, including general-purpose, domain-specific, and\nPolish-specific models, and compare their performance against human medical\nstudents. Our analysis reveals that while models like GPT-4o achieve near-human\nperformance, significant challenges persist in cross-lingual translation and\ndomain-specific understanding. These findings underscore disparities in model\nperformance across languages and medical specialties, highlighting the\nlimitations and ethical considerations of deploying LLMs in clinical practice."
                },
                "authors": [
                    {
                        "name": "Łukasz Grzybowski"
                    },
                    {
                        "name": "Jakub Pokrywka"
                    },
                    {
                        "name": "Michał Ciesiółka"
                    },
                    {
                        "name": "Jeremi I. Kaczmarek"
                    },
                    {
                        "name": "Marek Kubis"
                    }
                ],
                "author_detail": {
                    "name": "Marek Kubis"
                },
                "author": "Marek Kubis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00559v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00559v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09550v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09550v2",
                "updated": "2025-09-12T06:43:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    6,
                    43,
                    25,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-11T15:39:59Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    39,
                    59,
                    3,
                    254,
                    0
                ],
                "title": "Finite Scalar Quantization Enables Redundant and Transmission-Robust\n  Neural Audio Compression at Low Bit-rates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finite Scalar Quantization Enables Redundant and Transmission-Robust\n  Neural Audio Compression at Low Bit-rates"
                },
                "summary": "Neural Audio Codecs (NACs) have become increasingly adopted in speech\nprocessing tasks due to their excellent rate-distortion performance and\ncompatibility with Large Language Models (LLMs) as discrete feature\nrepresentations for audio generation. While most existing codecs rely on\nResidual Vector Quantization (RVQ), Finite Scalar Quantization (FSQ) has\nrecently emerged as a compelling alternative that simplifies training and\nnatively supports single codebooks. We introduce NeuCodec, an FSQ-based NAC,\nand show that FSQ encodes baked-in redundancy which produces an encoding which\nis robust when transmitted through noisy channels. First, through an encoder\ndistillation experiment, we show that two different encoders can learn to\nencode identical audio into vastly different code sequences whilst maintaining\ncomparable reconstruction quality with the same quantizer and decoder. Second,\nwe demonstrate that FSQ has vastly superior bit-level perturbation robustness\nby comparing the performance of RVQ and FSQ codecs when simulating the\ntransmission of code sequences through a noisy channel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Audio Codecs (NACs) have become increasingly adopted in speech\nprocessing tasks due to their excellent rate-distortion performance and\ncompatibility with Large Language Models (LLMs) as discrete feature\nrepresentations for audio generation. While most existing codecs rely on\nResidual Vector Quantization (RVQ), Finite Scalar Quantization (FSQ) has\nrecently emerged as a compelling alternative that simplifies training and\nnatively supports single codebooks. We introduce NeuCodec, an FSQ-based NAC,\nand show that FSQ encodes baked-in redundancy which produces an encoding which\nis robust when transmitted through noisy channels. First, through an encoder\ndistillation experiment, we show that two different encoders can learn to\nencode identical audio into vastly different code sequences whilst maintaining\ncomparable reconstruction quality with the same quantizer and decoder. Second,\nwe demonstrate that FSQ has vastly superior bit-level perturbation robustness\nby comparing the performance of RVQ and FSQ codecs when simulating the\ntransmission of code sequences through a noisy channel."
                },
                "authors": [
                    {
                        "name": "Harry Julian"
                    },
                    {
                        "name": "Rachel Beeson"
                    },
                    {
                        "name": "Lohith Konathala"
                    },
                    {
                        "name": "Johanna Ulin"
                    },
                    {
                        "name": "Jiameng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jiameng Gao"
                },
                "author": "Jiameng Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09550v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09550v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.10446v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10446v1",
                "updated": "2025-09-12T17:52:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    17,
                    52,
                    35,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T17:52:35Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    17,
                    52,
                    35,
                    4,
                    255,
                    0
                ],
                "title": "DeepDive: Advancing Deep Search Agents with Knowledge Graphs and\n  Multi-Turn RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepDive: Advancing Deep Search Agents with Knowledge Graphs and\n  Multi-Turn RL"
                },
                "summary": "Augmenting large language models (LLMs) with browsing tools substantially\nimproves their potential as deep search agents to solve complex, real-world\ntasks. Yet, open LLMs still perform poorly in such settings due to limited\nlong-horizon reasoning capacity with browsing tools and the lack of\nsufficiently difficult supervised data. To address these challenges, we present\nDeepDive to advance deep search agents. First, we propose a strategy to\nautomatically synthesize complex, difficult, and hard-to-find questions from\nopen knowledge graphs. Second, we apply end-to-end multi-turn reinforcement\nlearning (RL) to enhance LLMs' long-horizon reasoning with deep search.\nExperiments show that DeepDive-32B achieves a new open-source competitive\nresult on BrowseComp, outperforming WebSailor, DeepSeek-R1-Browse, and\nSearch-o1. We demonstrate that multi-turn RL training improves deep search\nability and significantly contributes to the performance improvements across\nmultiple benchmarks. We observe that DeepDive enables test-time scaling of tool\ncalls and parallel sampling. All datasets, models, and code are publicly\navailable at https://github.com/THUDM/DeepDive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmenting large language models (LLMs) with browsing tools substantially\nimproves their potential as deep search agents to solve complex, real-world\ntasks. Yet, open LLMs still perform poorly in such settings due to limited\nlong-horizon reasoning capacity with browsing tools and the lack of\nsufficiently difficult supervised data. To address these challenges, we present\nDeepDive to advance deep search agents. First, we propose a strategy to\nautomatically synthesize complex, difficult, and hard-to-find questions from\nopen knowledge graphs. Second, we apply end-to-end multi-turn reinforcement\nlearning (RL) to enhance LLMs' long-horizon reasoning with deep search.\nExperiments show that DeepDive-32B achieves a new open-source competitive\nresult on BrowseComp, outperforming WebSailor, DeepSeek-R1-Browse, and\nSearch-o1. We demonstrate that multi-turn RL training improves deep search\nability and significantly contributes to the performance improvements across\nmultiple benchmarks. We observe that DeepDive enables test-time scaling of tool\ncalls and parallel sampling. All datasets, models, and code are publicly\navailable at https://github.com/THUDM/DeepDive."
                },
                "authors": [
                    {
                        "name": "Rui Lu"
                    },
                    {
                        "name": "Zhenyu Hou"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Hanchen Zhang"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Yujiang Li"
                    },
                    {
                        "name": "Shi Feng"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Yuxiao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiao Dong"
                },
                "author": "Yuxiao Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10446v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10446v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10436v1",
                "updated": "2025-09-12T17:44:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    17,
                    44,
                    22,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T17:44:22Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    17,
                    44,
                    22,
                    4,
                    255,
                    0
                ],
                "title": "RefactorCoderQA: Benchmarking LLMs for Multi-Domain Coding Question\n  Solutions in Cloud and Edge Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RefactorCoderQA: Benchmarking LLMs for Multi-Domain Coding Question\n  Solutions in Cloud and Edge Deployment"
                },
                "summary": "To optimize the reasoning and problem-solving capabilities of Large Language\nModels (LLMs), we propose a novel cloud-edge collaborative architecture that\nenables a structured, multi-agent prompting framework. This framework comprises\nthree specialized components: GuideLLM, a lightweight model deployed at the\nedge to provide methodological guidance; SolverLLM, a more powerful model\nhosted in the cloud responsible for generating code solutions; and JudgeLLM, an\nautomated evaluator for assessing solution correctness and quality. To evaluate\nand demonstrate the effectiveness of this architecture in realistic settings,\nwe introduce RefactorCoderQA, a comprehensive benchmark designed to evaluate\nand enhance the performance of Large Language Models (LLMs) across multi-domain\ncoding tasks. Motivated by the limitations of existing benchmarks,\nRefactorCoderQA systematically covers various technical domains, including\nSoftware Engineering, Data Science, Machine Learning, and Natural Language\nProcessing, using authentic coding challenges from Stack Overflow. Extensive\nexperiments reveal that our fine-tuned model, RefactorCoder-MoE, achieves\nstate-of-the-art performance, significantly outperforming leading open-source\nand commercial baselines with an overall accuracy of 76.84%. Human evaluations\nfurther validate the interpretability, accuracy, and practical relevance of the\ngenerated solutions. In addition, we evaluate system-level metrics, such as\nthroughput and latency, to gain deeper insights into the performance\ncharacteristics and trade-offs of the proposed architecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To optimize the reasoning and problem-solving capabilities of Large Language\nModels (LLMs), we propose a novel cloud-edge collaborative architecture that\nenables a structured, multi-agent prompting framework. This framework comprises\nthree specialized components: GuideLLM, a lightweight model deployed at the\nedge to provide methodological guidance; SolverLLM, a more powerful model\nhosted in the cloud responsible for generating code solutions; and JudgeLLM, an\nautomated evaluator for assessing solution correctness and quality. To evaluate\nand demonstrate the effectiveness of this architecture in realistic settings,\nwe introduce RefactorCoderQA, a comprehensive benchmark designed to evaluate\nand enhance the performance of Large Language Models (LLMs) across multi-domain\ncoding tasks. Motivated by the limitations of existing benchmarks,\nRefactorCoderQA systematically covers various technical domains, including\nSoftware Engineering, Data Science, Machine Learning, and Natural Language\nProcessing, using authentic coding challenges from Stack Overflow. Extensive\nexperiments reveal that our fine-tuned model, RefactorCoder-MoE, achieves\nstate-of-the-art performance, significantly outperforming leading open-source\nand commercial baselines with an overall accuracy of 76.84%. Human evaluations\nfurther validate the interpretability, accuracy, and practical relevance of the\ngenerated solutions. In addition, we evaluate system-level metrics, such as\nthroughput and latency, to gain deeper insights into the performance\ncharacteristics and trade-offs of the proposed architecture."
                },
                "authors": [
                    {
                        "name": "Shadikur Rahman"
                    },
                    {
                        "name": "Aroosa Hameed"
                    },
                    {
                        "name": "Gautam Srivastava"
                    },
                    {
                        "name": "Syed Muhammad Danish"
                    }
                ],
                "author_detail": {
                    "name": "Syed Muhammad Danish"
                },
                "author": "Syed Muhammad Danish",
                "arxiv_comment": "12 pages, 5 figures, submitted to IEEE Transactions on Services\n  Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05672v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05672v2",
                "updated": "2025-09-12T17:32:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    17,
                    32,
                    41,
                    4,
                    255,
                    0
                ],
                "published": "2025-08-04T16:59:43Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    59,
                    43,
                    0,
                    216,
                    0
                ],
                "title": "LMAR: Language Model Augmented Retriever for Domain-specific Knowledge\n  Indexing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LMAR: Language Model Augmented Retriever for Domain-specific Knowledge\n  Indexing"
                },
                "summary": "Retrieval Augmented Generation (RAG) systems often struggle with\ndomain-specific knowledge due to performance deterioration of pre-trained\nembeddings and prohibitive computational costs of large language model\n(LLM)-based retrievers. While fine-tuning data augmentation embedding models\noffers a promising direction, its effectiveness is limited by the need for\nhigh-quality training data and reliable chunking strategies that preserve\ncontextual integrity. We propose LMAR (Language Model Augmented Retriever), a\nmodel-agnostic framework that addresses these challenges by combining\nLLM-guided data synthesis with contrastive embedding adaptation and efficient\ntext clustering. LMAR consists of a two-stage pipeline: (1) Triplet sampling\nand synthetic data augmentation, where LLMs act as both labeler and validator\nto ensure high-fidelity supervision throughout the pipeline. Experimental\nresults across multiple domain-specific benchmark datasets demonstrate that\nLMAR outperforms multiple baseline models, while maintaining moderate hardware\nrequirements and low latency. Its model-agnostic nature further enables\nseamless integration with emerging RAG architectures and text embedding models,\nensuring continual improvements without redesigning the pipeline. These results\nhighlight LMAR as a practical and cost-effective solution for scalable\ndomain-specific adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) systems often struggle with\ndomain-specific knowledge due to performance deterioration of pre-trained\nembeddings and prohibitive computational costs of large language model\n(LLM)-based retrievers. While fine-tuning data augmentation embedding models\noffers a promising direction, its effectiveness is limited by the need for\nhigh-quality training data and reliable chunking strategies that preserve\ncontextual integrity. We propose LMAR (Language Model Augmented Retriever), a\nmodel-agnostic framework that addresses these challenges by combining\nLLM-guided data synthesis with contrastive embedding adaptation and efficient\ntext clustering. LMAR consists of a two-stage pipeline: (1) Triplet sampling\nand synthetic data augmentation, where LLMs act as both labeler and validator\nto ensure high-fidelity supervision throughout the pipeline. Experimental\nresults across multiple domain-specific benchmark datasets demonstrate that\nLMAR outperforms multiple baseline models, while maintaining moderate hardware\nrequirements and low latency. Its model-agnostic nature further enables\nseamless integration with emerging RAG architectures and text embedding models,\nensuring continual improvements without redesigning the pipeline. These results\nhighlight LMAR as a practical and cost-effective solution for scalable\ndomain-specific adaptation."
                },
                "authors": [
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Yantian Ding"
                    },
                    {
                        "name": "Zhiyue Zhang"
                    },
                    {
                        "name": "Dapeng Yao"
                    },
                    {
                        "name": "Yanxun Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yanxun Xu"
                },
                "author": "Yanxun Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05672v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05672v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10427v1",
                "updated": "2025-09-12T17:31:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    17,
                    31,
                    40,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T17:31:40Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    17,
                    31,
                    40,
                    4,
                    255,
                    0
                ],
                "title": "My Favorite Streamer is an LLM: Discovering, Bonding, and Co-Creating in\n  AI VTuber Fandom",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "My Favorite Streamer is an LLM: Discovering, Bonding, and Co-Creating in\n  AI VTuber Fandom"
                },
                "summary": "AI VTubers, where the performer is not human but algorithmically generated,\nintroduce a new context for fandom. While human VTubers have been substantially\nstudied for their cultural appeal, parasocial dynamics, and community\neconomies, little is known about how audiences engage with their AI\ncounterparts. To address this gap, we present a qualitative study of\nNeuro-sama, the most prominent AI VTuber. Our findings show that engagement is\nanchored in active co-creation: audiences are drawn by the AI's unpredictable\nyet entertaining interactions, cement loyalty through collective emotional\nevents that trigger anthropomorphic projection, and sustain attachment via the\nAI's consistent persona. Financial support emerges not as a reward for\nperformance but as a participatory mechanism for shaping livestream content,\nestablishing a resilient fan economy built on ongoing interaction. These\ndynamics reveal how AI Vtuber fandom reshapes fan-creator relationships and\noffer implications for designing transparent and sustainable AI-mediated\ncommunities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI VTubers, where the performer is not human but algorithmically generated,\nintroduce a new context for fandom. While human VTubers have been substantially\nstudied for their cultural appeal, parasocial dynamics, and community\neconomies, little is known about how audiences engage with their AI\ncounterparts. To address this gap, we present a qualitative study of\nNeuro-sama, the most prominent AI VTuber. Our findings show that engagement is\nanchored in active co-creation: audiences are drawn by the AI's unpredictable\nyet entertaining interactions, cement loyalty through collective emotional\nevents that trigger anthropomorphic projection, and sustain attachment via the\nAI's consistent persona. Financial support emerges not as a reward for\nperformance but as a participatory mechanism for shaping livestream content,\nestablishing a resilient fan economy built on ongoing interaction. These\ndynamics reveal how AI Vtuber fandom reshapes fan-creator relationships and\noffer implications for designing transparent and sustainable AI-mediated\ncommunities."
                },
                "authors": [
                    {
                        "name": "Jiayi Ye"
                    },
                    {
                        "name": "Chaoran Chen"
                    },
                    {
                        "name": "Yue Huang"
                    },
                    {
                        "name": "Yanfang Ye"
                    },
                    {
                        "name": "Toby Jia-Jun Li"
                    },
                    {
                        "name": "Xiangliang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangliang Zhang"
                },
                "author": "Xiangliang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08765v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08765v2",
                "updated": "2025-09-12T17:29:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    17,
                    29,
                    48,
                    4,
                    255,
                    0
                ],
                "published": "2025-08-12T09:11:31Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    9,
                    11,
                    31,
                    1,
                    224,
                    0
                ],
                "title": "Bridging the Gap: A Framework for Real-World Video Deepfake Detection\n  via Social Network Compression Emulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Gap: A Framework for Real-World Video Deepfake Detection\n  via Social Network Compression Emulation"
                },
                "summary": "The growing presence of AI-generated videos on social networks poses new\nchallenges for deepfake detection, as detectors trained under controlled\nconditions often fail to generalize to real-world scenarios. A key factor\nbehind this gap is the aggressive, proprietary compression applied by platforms\nlike YouTube and Facebook, which launder low-level forensic cues. However,\nreplicating these transformations at scale is difficult due to API limitations\nand data-sharing constraints. For these reasons, we propose a first framework\nthat emulates the video sharing pipelines of social networks by estimating\ncompression and resizing parameters from a small set of uploaded videos. These\nparameters enable a local emulator capable of reproducing platform-specific\nartifacts on large datasets without direct API access. Experiments on\nFaceForensics++ videos shared via social networks demonstrate that our emulated\ndata closely matches the degradation patterns of real uploads. Furthermore,\ndetectors fine-tuned on emulated videos achieve comparable performance to those\ntrained on actual shared media. Our approach offers a scalable and practical\nsolution for bridging the gap between lab-based training and real-world\ndeployment of deepfake detectors, particularly in the underexplored domain of\ncompressed video content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing presence of AI-generated videos on social networks poses new\nchallenges for deepfake detection, as detectors trained under controlled\nconditions often fail to generalize to real-world scenarios. A key factor\nbehind this gap is the aggressive, proprietary compression applied by platforms\nlike YouTube and Facebook, which launder low-level forensic cues. However,\nreplicating these transformations at scale is difficult due to API limitations\nand data-sharing constraints. For these reasons, we propose a first framework\nthat emulates the video sharing pipelines of social networks by estimating\ncompression and resizing parameters from a small set of uploaded videos. These\nparameters enable a local emulator capable of reproducing platform-specific\nartifacts on large datasets without direct API access. Experiments on\nFaceForensics++ videos shared via social networks demonstrate that our emulated\ndata closely matches the degradation patterns of real uploads. Furthermore,\ndetectors fine-tuned on emulated videos achieve comparable performance to those\ntrained on actual shared media. Our approach offers a scalable and practical\nsolution for bridging the gap between lab-based training and real-world\ndeployment of deepfake detectors, particularly in the underexplored domain of\ncompressed video content."
                },
                "authors": [
                    {
                        "name": "Andrea Montibeller"
                    },
                    {
                        "name": "Dasara Shullani"
                    },
                    {
                        "name": "Daniele Baracchi"
                    },
                    {
                        "name": "Alessandro Piva"
                    },
                    {
                        "name": "Giulia Boato"
                    }
                ],
                "author_detail": {
                    "name": "Giulia Boato"
                },
                "author": "Giulia Boato",
                "arxiv_doi": "10.1145/3746265.3759670",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746265.3759670",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.08765v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08765v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10423v1",
                "updated": "2025-09-12T17:24:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    17,
                    24,
                    20,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T17:24:20Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    17,
                    24,
                    20,
                    4,
                    255,
                    0
                ],
                "title": "Mutual Information Tracks Policy Coherence in Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mutual Information Tracks Policy Coherence in Reinforcement Learning"
                },
                "summary": "Reinforcement Learning (RL) agents deployed in real-world environments face\ndegradation from sensor faults, actuator wear, and environmental shifts, yet\nlack intrinsic mechanisms to detect and diagnose these failures. We present an\ninformation-theoretic framework that reveals both the fundamental dynamics of\nRL and provides practical methods for diagnosing deployment-time anomalies.\nThrough analysis of state-action mutual information patterns in a robotic\ncontrol task, we first demonstrate that successful learning exhibits\ncharacteristic information signatures: mutual information between states and\nactions steadily increases from 0.84 to 2.83 bits (238% growth) despite growing\nstate entropy, indicating that agents develop increasingly selective attention\nto task-relevant patterns. Intriguingly, states, actions and next states joint\nmutual information, MI(S,A;S'), follows an inverted U-curve, peaking during\nearly learning before declining as the agent specializes suggesting a\ntransition from broad exploration to efficient exploitation. More immediately\nactionable, we show that information metrics can differentially diagnose system\nfailures: observation-space, i.e., states noise (sensor faults) produces broad\ncollapses across all information channels with pronounced drops in state-action\ncoupling, while action-space noise (actuator faults) selectively disrupts\naction-outcome predictability while preserving state-action relationships. This\ndifferential diagnostic capability demonstrated through controlled perturbation\nexperiments enables precise fault localization without architectural\nmodifications or performance degradation. By establishing information patterns\nas both signatures of learning and diagnostic for system health, we provide the\nfoundation for adaptive RL systems capable of autonomous fault detection and\npolicy adjustment based on information-theoretic principles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) agents deployed in real-world environments face\ndegradation from sensor faults, actuator wear, and environmental shifts, yet\nlack intrinsic mechanisms to detect and diagnose these failures. We present an\ninformation-theoretic framework that reveals both the fundamental dynamics of\nRL and provides practical methods for diagnosing deployment-time anomalies.\nThrough analysis of state-action mutual information patterns in a robotic\ncontrol task, we first demonstrate that successful learning exhibits\ncharacteristic information signatures: mutual information between states and\nactions steadily increases from 0.84 to 2.83 bits (238% growth) despite growing\nstate entropy, indicating that agents develop increasingly selective attention\nto task-relevant patterns. Intriguingly, states, actions and next states joint\nmutual information, MI(S,A;S'), follows an inverted U-curve, peaking during\nearly learning before declining as the agent specializes suggesting a\ntransition from broad exploration to efficient exploitation. More immediately\nactionable, we show that information metrics can differentially diagnose system\nfailures: observation-space, i.e., states noise (sensor faults) produces broad\ncollapses across all information channels with pronounced drops in state-action\ncoupling, while action-space noise (actuator faults) selectively disrupts\naction-outcome predictability while preserving state-action relationships. This\ndifferential diagnostic capability demonstrated through controlled perturbation\nexperiments enables precise fault localization without architectural\nmodifications or performance degradation. By establishing information patterns\nas both signatures of learning and diagnostic for system health, we provide the\nfoundation for adaptive RL systems capable of autonomous fault detection and\npolicy adjustment based on information-theoretic principles."
                },
                "authors": [
                    {
                        "name": "Cameron Reid"
                    },
                    {
                        "name": "Wael Hafez"
                    },
                    {
                        "name": "Amirhossein Nazeri"
                    }
                ],
                "author_detail": {
                    "name": "Amirhossein Nazeri"
                },
                "author": "Amirhossein Nazeri",
                "arxiv_comment": "10 pages, 4 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14664v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14664v3",
                "updated": "2025-09-12T17:21:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    17,
                    21,
                    39,
                    4,
                    255,
                    0
                ],
                "published": "2024-09-23T02:08:20Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    2,
                    8,
                    20,
                    0,
                    267,
                    0
                ],
                "title": "Direct Judgement Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Judgement Preference Optimization"
                },
                "summary": "Auto-evaluation is crucial for assessing response quality and offering\nfeedback for model development. Recent studies have explored training large\nlanguage models (LLMs) as generative judges to evaluate and critique other\nmodels' outputs. In this work, we investigate the idea of learning from both\npositive and negative data with preference optimization to enhance the\nevaluation capabilities of LLM judges across an array of different use cases.\nWe achieve this by employing three approaches to collect the preference pairs\nfor different use cases, each aimed at improving our generative judge from a\ndifferent perspective. Our comprehensive study over a wide range of benchmarks\ndemonstrates the effectiveness of our method. In particular, our generative\njudge achieves the best performance on 10 out of 13 benchmarks, outperforming\nstrong baselines like GPT-4o and specialized judge models. Further analysis\nshow that our judge model robustly counters inherent biases such as position\nand length bias, flexibly adapts to any evaluation protocol specified by\npractitioners, and provides helpful language feedback for improving downstream\ngenerator models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-evaluation is crucial for assessing response quality and offering\nfeedback for model development. Recent studies have explored training large\nlanguage models (LLMs) as generative judges to evaluate and critique other\nmodels' outputs. In this work, we investigate the idea of learning from both\npositive and negative data with preference optimization to enhance the\nevaluation capabilities of LLM judges across an array of different use cases.\nWe achieve this by employing three approaches to collect the preference pairs\nfor different use cases, each aimed at improving our generative judge from a\ndifferent perspective. Our comprehensive study over a wide range of benchmarks\ndemonstrates the effectiveness of our method. In particular, our generative\njudge achieves the best performance on 10 out of 13 benchmarks, outperforming\nstrong baselines like GPT-4o and specialized judge models. Further analysis\nshow that our judge model robustly counters inherent biases such as position\nand length bias, flexibly adapts to any evaluation protocol specified by\npractitioners, and provides helpful language feedback for improving downstream\ngenerator models."
                },
                "authors": [
                    {
                        "name": "Peifeng Wang"
                    },
                    {
                        "name": "Austin Xu"
                    },
                    {
                        "name": "Yilun Zhou"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Shafiq Joty"
                    }
                ],
                "author_detail": {
                    "name": "Shafiq Joty"
                },
                "author": "Shafiq Joty",
                "arxiv_comment": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14664v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14664v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18889v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18889v2",
                "updated": "2025-09-12T17:18:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    17,
                    18,
                    27,
                    4,
                    255,
                    0
                ],
                "published": "2024-10-24T16:27:03Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    27,
                    3,
                    3,
                    298,
                    0
                ],
                "title": "Are LLMs Better than Reported? Detecting Label Errors and Mitigating\n  Their Effect on Model Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are LLMs Better than Reported? Detecting Label Errors and Mitigating\n  Their Effect on Model Performance"
                },
                "summary": "NLP benchmarks rely on standardized datasets for training and evaluating\nmodels and are crucial for advancing the field. Traditionally, expert\nannotations ensure high-quality labels; however, the cost of expert annotation\ndoes not scale well with the growing demand for larger datasets required by\nmodern models. While crowd-sourcing provides a more scalable solution, it often\ncomes at the expense of annotation precision and consistency. Recent\nadvancements in large language models (LLMs) offer new opportunities to enhance\nthe annotation process, particularly for detecting label errors in existing\ndatasets. In this work, we consider the recent approach of LLM-as-a-judge,\nleveraging an ensemble of LLMs to flag potentially mislabeled examples. We\nconduct a case study on four factual consistency datasets from the TRUE\nbenchmark, spanning diverse NLP tasks, and on SummEval, which uses Likert-scale\nratings of summary quality across multiple dimensions. We empirically analyze\nthe labeling quality of existing datasets and compare expert, crowd-sourced,\nand LLM-based annotations in terms of the agreement, label quality, and\nefficiency, demonstrating the strengths and limitations of each annotation\nmethod. Our findings reveal a substantial number of label errors, which, when\ncorrected, induce a significant upward shift in reported model performance.\nThis suggests that many of the LLMs' so-called mistakes are due to label errors\nrather than genuine model failures. Additionally, we discuss the implications\nof mislabeled data and propose methods to mitigate them in training to improve\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NLP benchmarks rely on standardized datasets for training and evaluating\nmodels and are crucial for advancing the field. Traditionally, expert\nannotations ensure high-quality labels; however, the cost of expert annotation\ndoes not scale well with the growing demand for larger datasets required by\nmodern models. While crowd-sourcing provides a more scalable solution, it often\ncomes at the expense of annotation precision and consistency. Recent\nadvancements in large language models (LLMs) offer new opportunities to enhance\nthe annotation process, particularly for detecting label errors in existing\ndatasets. In this work, we consider the recent approach of LLM-as-a-judge,\nleveraging an ensemble of LLMs to flag potentially mislabeled examples. We\nconduct a case study on four factual consistency datasets from the TRUE\nbenchmark, spanning diverse NLP tasks, and on SummEval, which uses Likert-scale\nratings of summary quality across multiple dimensions. We empirically analyze\nthe labeling quality of existing datasets and compare expert, crowd-sourced,\nand LLM-based annotations in terms of the agreement, label quality, and\nefficiency, demonstrating the strengths and limitations of each annotation\nmethod. Our findings reveal a substantial number of label errors, which, when\ncorrected, induce a significant upward shift in reported model performance.\nThis suggests that many of the LLMs' so-called mistakes are due to label errors\nrather than genuine model failures. Additionally, we discuss the implications\nof mislabeled data and propose methods to mitigate them in training to improve\nperformance."
                },
                "authors": [
                    {
                        "name": "Omer Nahum"
                    },
                    {
                        "name": "Nitay Calderon"
                    },
                    {
                        "name": "Orgad Keller"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Roi Reichart"
                    }
                ],
                "author_detail": {
                    "name": "Roi Reichart"
                },
                "author": "Roi Reichart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18889v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18889v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07980v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07980v2",
                "updated": "2025-09-12T17:15:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    17,
                    15,
                    56,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-09T17:59:35Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    17,
                    59,
                    35,
                    1,
                    252,
                    0
                ],
                "title": "Parallel-R1: Towards Parallel Thinking via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel-R1: Towards Parallel Thinking via Reinforcement Learning"
                },
                "summary": "Parallel thinking has emerged as a novel approach for enhancing the reasoning\ncapabilities of large language models (LLMs) by exploring multiple reasoning\npaths concurrently. However, activating such capabilities through training\nremains challenging, as existing methods predominantly rely on supervised\nfine-tuning (SFT) over synthetic data, which encourages teacher-forced\nimitation rather than exploration and generalization. Different from them, we\npropose \\textbf{Parallel-R1}, the first reinforcement learning (RL) framework\nthat enables parallel thinking behaviors for complex real-world reasoning\ntasks. Our framework employs a progressive curriculum that explicitly addresses\nthe cold-start problem in training parallel thinking with RL. We first use SFT\non prompt-generated trajectories from easier tasks to instill the parallel\nthinking ability, then transition to RL to explore and generalize this skill on\nharder problems. Experiments on various math benchmarks, including MATH, AMC23,\nand AIME, show that Parallel-R1 successfully instills parallel thinking,\nleading to 8.4% accuracy improvements over the sequential thinking model\ntrained directly on challenging tasks with RL. Further analysis reveals a clear\nshift in the model's thinking behavior: at an early stage, it uses parallel\nthinking as an exploration strategy, while in a later stage, it uses the same\ncapability for multi-perspective verification. Most significantly, we validate\nparallel thinking as a \\textbf{mid-training exploration scaffold}, where this\ntemporary exploratory phase unlocks a higher performance ceiling after RL,\nyielding a 42.9% improvement over the baseline on AIME25. Our model, data, and\ncode will be open-source at https://github.com/zhengkid/Parallel-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel thinking has emerged as a novel approach for enhancing the reasoning\ncapabilities of large language models (LLMs) by exploring multiple reasoning\npaths concurrently. However, activating such capabilities through training\nremains challenging, as existing methods predominantly rely on supervised\nfine-tuning (SFT) over synthetic data, which encourages teacher-forced\nimitation rather than exploration and generalization. Different from them, we\npropose \\textbf{Parallel-R1}, the first reinforcement learning (RL) framework\nthat enables parallel thinking behaviors for complex real-world reasoning\ntasks. Our framework employs a progressive curriculum that explicitly addresses\nthe cold-start problem in training parallel thinking with RL. We first use SFT\non prompt-generated trajectories from easier tasks to instill the parallel\nthinking ability, then transition to RL to explore and generalize this skill on\nharder problems. Experiments on various math benchmarks, including MATH, AMC23,\nand AIME, show that Parallel-R1 successfully instills parallel thinking,\nleading to 8.4% accuracy improvements over the sequential thinking model\ntrained directly on challenging tasks with RL. Further analysis reveals a clear\nshift in the model's thinking behavior: at an early stage, it uses parallel\nthinking as an exploration strategy, while in a later stage, it uses the same\ncapability for multi-perspective verification. Most significantly, we validate\nparallel thinking as a \\textbf{mid-training exploration scaffold}, where this\ntemporary exploratory phase unlocks a higher performance ceiling after RL,\nyielding a 42.9% improvement over the baseline on AIME25. Our model, data, and\ncode will be open-source at https://github.com/zhengkid/Parallel-R1."
                },
                "authors": [
                    {
                        "name": "Tong Zheng"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Wenhao Yu"
                    },
                    {
                        "name": "Xiaoyang Wang"
                    },
                    {
                        "name": "Runpeng Dai"
                    },
                    {
                        "name": "Rui Liu"
                    },
                    {
                        "name": "Huiwen Bao"
                    },
                    {
                        "name": "Chengsong Huang"
                    },
                    {
                        "name": "Heng Huang"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "arxiv_comment": "Project website: https://zhengkid.github.io/Parallel_R1.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07980v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07980v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10407v1",
                "updated": "2025-09-12T16:58:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    58,
                    20,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T16:58:20Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    58,
                    20,
                    4,
                    255,
                    0
                ],
                "title": "Compressed Video Quality Enhancement: Classifying and Benchmarking over\n  Standards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressed Video Quality Enhancement: Classifying and Benchmarking over\n  Standards"
                },
                "summary": "Compressed video quality enhancement (CVQE) is crucial for improving user\nexperience with lossy video codecs like H.264/AVC, H.265/HEVC, and H.266/VVC.\nWhile deep learning based CVQE has driven significant progress, existing\nsurveys still suffer from limitations: lack of systematic classification\nlinking methods to specific standards and artifacts, insufficient comparative\nanalysis of architectural paradigms across coding types, and underdeveloped\nbenchmarking practices. To address these gaps, this paper presents three key\ncontributions. First, it introduces a novel taxonomy classifying CVQE methods\nacross architectural paradigms, coding standards, and compressed-domain feature\nutilization. Second, it proposes a unified benchmarking framework integrating\nmodern compression protocols and standard test sequences for fair\nmulti-criteria evaluation. Third, it provides a systematic analysis of the\ncritical trade-offs between reconstruction performance and computational\ncomplexity observed in state-of-the-art methods and highlighting promising\ndirections for future research. This comprehensive review aims to establish a\nfoundation for consistent assessment and informed model selection in CVQE\nresearch and deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressed video quality enhancement (CVQE) is crucial for improving user\nexperience with lossy video codecs like H.264/AVC, H.265/HEVC, and H.266/VVC.\nWhile deep learning based CVQE has driven significant progress, existing\nsurveys still suffer from limitations: lack of systematic classification\nlinking methods to specific standards and artifacts, insufficient comparative\nanalysis of architectural paradigms across coding types, and underdeveloped\nbenchmarking practices. To address these gaps, this paper presents three key\ncontributions. First, it introduces a novel taxonomy classifying CVQE methods\nacross architectural paradigms, coding standards, and compressed-domain feature\nutilization. Second, it proposes a unified benchmarking framework integrating\nmodern compression protocols and standard test sequences for fair\nmulti-criteria evaluation. Third, it provides a systematic analysis of the\ncritical trade-offs between reconstruction performance and computational\ncomplexity observed in state-of-the-art methods and highlighting promising\ndirections for future research. This comprehensive review aims to establish a\nfoundation for consistent assessment and informed model selection in CVQE\nresearch and deployment."
                },
                "authors": [
                    {
                        "name": "Xiem HoangVan"
                    },
                    {
                        "name": "Dang BuiDinh"
                    },
                    {
                        "name": "Sang NguyenQuang"
                    },
                    {
                        "name": "Wen-Hsiao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Wen-Hsiao Peng"
                },
                "author": "Wen-Hsiao Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10402v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10402v1",
                "updated": "2025-09-12T16:52:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    52,
                    49,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T16:52:49Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    52,
                    49,
                    4,
                    255,
                    0
                ],
                "title": "Developer-LLM Conversations: An Empirical Study of Interactions and\n  Generated Code Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developer-LLM Conversations: An Empirical Study of Interactions and\n  Generated Code Quality"
                },
                "summary": "Large Language Models (LLMs) are becoming integral to modern software\ndevelopment workflows, assisting developers with code generation, API\nexplanation, and iterative problem-solving through natural language\nconversations. Despite widespread adoption, there is limited understanding of\nhow developers interact with LLMs in practice and how these conversational\ndynamics influence task outcomes, code quality, and software engineering\nworkflows. To address this, we leverage CodeChat, a large dataset comprising\n82,845 real-world developer-LLM conversations, containing 368,506 code snippets\ngenerated across over 20 programming languages, derived from the WildChat\ndataset. We find that LLM responses are substantially longer than developer\nprompts, with a median token-length ratio of 14:1. Multi-turn conversations\naccount for 68% of the dataset and often evolve due to shifting requirements,\nincomplete prompts, or clarification requests. Topic analysis identifies web\ndesign (9.6% of conversations) and neural network training (8.7% of\nconversations) as the most frequent LLM-assisted tasks. Evaluation across five\nlanguages (i.e., Python, JavaScript, C++, Java, and C#) reveals prevalent and\nlanguage-specific issues in LLM-generated code: generated Python and JavaScript\ncode often include undefined variables (83.4% and 75.3% of code snippets,\nrespectively); Java code lacks required comments (75.9%); C++ code frequently\nomits headers (41.1%) and C# code shows unresolved namespaces (49.2%). During a\nconversation, syntax and import errors persist across turns; however,\ndocumentation quality in Java improves by up to 14.7%, and import handling in\nPython improves by 3.7% over 5 turns. Prompts that point out mistakes in code\ngenerated in prior turns and explicitly request a fix are most effective for\nresolving errors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are becoming integral to modern software\ndevelopment workflows, assisting developers with code generation, API\nexplanation, and iterative problem-solving through natural language\nconversations. Despite widespread adoption, there is limited understanding of\nhow developers interact with LLMs in practice and how these conversational\ndynamics influence task outcomes, code quality, and software engineering\nworkflows. To address this, we leverage CodeChat, a large dataset comprising\n82,845 real-world developer-LLM conversations, containing 368,506 code snippets\ngenerated across over 20 programming languages, derived from the WildChat\ndataset. We find that LLM responses are substantially longer than developer\nprompts, with a median token-length ratio of 14:1. Multi-turn conversations\naccount for 68% of the dataset and often evolve due to shifting requirements,\nincomplete prompts, or clarification requests. Topic analysis identifies web\ndesign (9.6% of conversations) and neural network training (8.7% of\nconversations) as the most frequent LLM-assisted tasks. Evaluation across five\nlanguages (i.e., Python, JavaScript, C++, Java, and C#) reveals prevalent and\nlanguage-specific issues in LLM-generated code: generated Python and JavaScript\ncode often include undefined variables (83.4% and 75.3% of code snippets,\nrespectively); Java code lacks required comments (75.9%); C++ code frequently\nomits headers (41.1%) and C# code shows unresolved namespaces (49.2%). During a\nconversation, syntax and import errors persist across turns; however,\ndocumentation quality in Java improves by up to 14.7%, and import handling in\nPython improves by 3.7% over 5 turns. Prompts that point out mistakes in code\ngenerated in prior turns and explicitly request a fix are most effective for\nresolving errors."
                },
                "authors": [
                    {
                        "name": "Suzhen Zhong"
                    },
                    {
                        "name": "Ying Zou"
                    },
                    {
                        "name": "Bram Adams"
                    }
                ],
                "author_detail": {
                    "name": "Bram Adams"
                },
                "author": "Bram Adams",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10402v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10402v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.0; D.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10397v1",
                "updated": "2025-09-12T16:44:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    44,
                    34,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T16:44:34Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    44,
                    34,
                    4,
                    255,
                    0
                ],
                "title": "RecoWorld: Building Simulated Environments for Agentic Recommender\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RecoWorld: Building Simulated Environments for Agentic Recommender\n  Systems"
                },
                "summary": "We present RecoWorld, a blueprint for building simulated environments\ntailored to agentic recommender systems. Such environments give agents a proper\ntraining space where they can learn from errors without impacting real users.\nRecoWorld distinguishes itself with a dual-view architecture: a simulated user\nand an agentic recommender engage in multi-turn interactions aimed at\nmaximizing user retention. The user simulator reviews recommended items,\nupdates its mindset, and when sensing potential user disengagement, generates\nreflective instructions. The agentic recommender adapts its recommendations by\nincorporating these user instructions and reasoning traces, creating a dynamic\nfeedback loop that actively engages users. This process leverages the\nexceptional reasoning capabilities of modern LLMs. We explore diverse content\nrepresentations within the simulator, including text-based, multimodal, and\nsemantic ID modeling, and discuss how multi-turn RL enables the recommender to\nrefine its strategies through iterative interactions. RecoWorld also supports\nmulti-agent simulations, allowing creators to simulate the responses of\ntargeted user populations. It marks an important first step toward recommender\nsystems where users and agents collaboratively shape personalized information\nstreams. We envision new interaction paradigms where \"user instructs,\nrecommender responds,\" jointly optimizing user retention and engagement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present RecoWorld, a blueprint for building simulated environments\ntailored to agentic recommender systems. Such environments give agents a proper\ntraining space where they can learn from errors without impacting real users.\nRecoWorld distinguishes itself with a dual-view architecture: a simulated user\nand an agentic recommender engage in multi-turn interactions aimed at\nmaximizing user retention. The user simulator reviews recommended items,\nupdates its mindset, and when sensing potential user disengagement, generates\nreflective instructions. The agentic recommender adapts its recommendations by\nincorporating these user instructions and reasoning traces, creating a dynamic\nfeedback loop that actively engages users. This process leverages the\nexceptional reasoning capabilities of modern LLMs. We explore diverse content\nrepresentations within the simulator, including text-based, multimodal, and\nsemantic ID modeling, and discuss how multi-turn RL enables the recommender to\nrefine its strategies through iterative interactions. RecoWorld also supports\nmulti-agent simulations, allowing creators to simulate the responses of\ntargeted user populations. It marks an important first step toward recommender\nsystems where users and agents collaboratively shape personalized information\nstreams. We envision new interaction paradigms where \"user instructs,\nrecommender responds,\" jointly optimizing user retention and engagement."
                },
                "authors": [
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Xinyu Lin"
                    },
                    {
                        "name": "Hanchao Yu"
                    },
                    {
                        "name": "Mingyuan Wu"
                    },
                    {
                        "name": "Jianyu Wang"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Zhuokai Zhao"
                    },
                    {
                        "name": "Yinglong Xia"
                    },
                    {
                        "name": "Yao Zhang"
                    },
                    {
                        "name": "Weiwei Li"
                    },
                    {
                        "name": "Mingze Gao"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Lizhu Zhang"
                    },
                    {
                        "name": "Benyu Zhang"
                    },
                    {
                        "name": "Xiangjun Fan"
                    }
                ],
                "author_detail": {
                    "name": "Xiangjun Fan"
                },
                "author": "Xiangjun Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10396v1",
                "updated": "2025-09-12T16:44:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    44,
                    31,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T16:44:31Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    44,
                    31,
                    4,
                    255,
                    0
                ],
                "title": "Inpainting-Guided Policy Optimization for Diffusion Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inpainting-Guided Policy Optimization for Diffusion Large Language\n  Models"
                },
                "summary": "Masked diffusion large language models (dLLMs) are emerging as promising\nalternatives to autoregressive LLMs, offering competitive performance while\nsupporting unique generation capabilities such as inpainting. We explore how\ninpainting can inform RL algorithm design for dLLMs. Aligning LLMs with\nreinforcement learning faces an exploration challenge: sparse reward signals\nand sample waste when models fail to discover correct solutions. While this\ninefficiency affects LLMs broadly, dLLMs offer a distinctive opportunity--their\ninpainting ability can guide exploration. We introduce IGPO (Inpainting Guided\nPolicy Optimization), an RL framework that strategically inserts partial\nground-truth reasoning traces during online sampling. Unlike providing full\nsolutions, inpainting steers exploration toward promising trajectory spaces\nwhile preserving self-generated reasoning, bridging supervised fine-tuning and\nreinforcement learning. We apply IGPO to group-based optimization methods such\nas GRPO, where exploration failures cause zero advantages and gradients. IGPO\nrestores meaningful gradients while improving sample efficiency. We also\npropose supervised fine-tuning on synthetically rewritten concise traces that\nbetter align with dLLM generation patterns. With additional techniques\nincluding entropy-based filtering, our training recipe yields substantial gains\nacross three mathematical benchmarks--GSM8K, Math500, and AMC--achieving new\nstate-of-the-art results for full-attention masked dLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked diffusion large language models (dLLMs) are emerging as promising\nalternatives to autoregressive LLMs, offering competitive performance while\nsupporting unique generation capabilities such as inpainting. We explore how\ninpainting can inform RL algorithm design for dLLMs. Aligning LLMs with\nreinforcement learning faces an exploration challenge: sparse reward signals\nand sample waste when models fail to discover correct solutions. While this\ninefficiency affects LLMs broadly, dLLMs offer a distinctive opportunity--their\ninpainting ability can guide exploration. We introduce IGPO (Inpainting Guided\nPolicy Optimization), an RL framework that strategically inserts partial\nground-truth reasoning traces during online sampling. Unlike providing full\nsolutions, inpainting steers exploration toward promising trajectory spaces\nwhile preserving self-generated reasoning, bridging supervised fine-tuning and\nreinforcement learning. We apply IGPO to group-based optimization methods such\nas GRPO, where exploration failures cause zero advantages and gradients. IGPO\nrestores meaningful gradients while improving sample efficiency. We also\npropose supervised fine-tuning on synthetically rewritten concise traces that\nbetter align with dLLM generation patterns. With additional techniques\nincluding entropy-based filtering, our training recipe yields substantial gains\nacross three mathematical benchmarks--GSM8K, Math500, and AMC--achieving new\nstate-of-the-art results for full-attention masked dLLMs."
                },
                "authors": [
                    {
                        "name": "Siyan Zhao"
                    },
                    {
                        "name": "Mengchen Liu"
                    },
                    {
                        "name": "Jing Huang"
                    },
                    {
                        "name": "Miao Liu"
                    },
                    {
                        "name": "Chenyu Wang"
                    },
                    {
                        "name": "Bo Liu"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Guan Pang"
                    },
                    {
                        "name": "Sean Bell"
                    },
                    {
                        "name": "Aditya Grover"
                    },
                    {
                        "name": "Feiyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Feiyu Chen"
                },
                "author": "Feiyu Chen",
                "arxiv_comment": "preprint; 21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02896v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02896v2",
                "updated": "2025-09-12T16:30:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    30,
                    38,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-02T23:41:50Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    23,
                    41,
                    50,
                    1,
                    245,
                    0
                ],
                "title": "Cut Costs, Not Accuracy: LLM-Powered Data Processing with Guarantees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cut Costs, Not Accuracy: LLM-Powered Data Processing with Guarantees"
                },
                "summary": "Large Language Models (LLMs) are being increasingly used as a building block\nin data systems to process large text datasets. To do so, LLM model providers\noffer multiple LLMs with different sizes, spanning various cost-quality\ntrade-offs when processing text at scale. Top-of-the-line LLMs (e.g., GPT-4o,\nClaude Sonnet) operate with high accuracy but are prohibitively expensive when\nprocessing many records. To avoid high costs, more affordable but lower quality\nLLMs (e.g., GPT-4o-mini, Claude Haiku) can be used to process records, but we\nneed to ensure that the overall accuracy does not deviate substantially from\nthat of the top-of-the-line LLMs. The model cascade framework provides a\nblueprint to manage this trade-off, by using the confidence of LLMs in their\noutput (e.g., log-probabilities) to decide on which records to use the\naffordable LLM. However, existing solutions following this framework provide\nonly marginal cost savings and weak theoretical guarantees because of poor\nestimation of the quality of the affordable LLM's outputs. We present BARGAIN,\na method that judiciously uses affordable LLMs in data processing to\nsignificantly reduce cost while providing strong theoretical guarantees on the\nsolution quality. BARGAIN employs a novel adaptive sampling strategy and\nstatistical estimation procedure that uses data and task characteristics and\nbuilds on recent statistical tools to make accurate estimations with tight\ntheoretical guarantees. Variants of BARGAIN can support guarantees on accuracy,\nprecision, or recall of the output. Experimental results across 8 real-world\ndatasets show that BARGAIN reduces cost, on average, by up to 86% more than\nstate-of-the-art, while providing stronger theoretical guarantees on accuracy\nof output, with similar gains when guaranteeing a desired level of precision or\nrecall.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are being increasingly used as a building block\nin data systems to process large text datasets. To do so, LLM model providers\noffer multiple LLMs with different sizes, spanning various cost-quality\ntrade-offs when processing text at scale. Top-of-the-line LLMs (e.g., GPT-4o,\nClaude Sonnet) operate with high accuracy but are prohibitively expensive when\nprocessing many records. To avoid high costs, more affordable but lower quality\nLLMs (e.g., GPT-4o-mini, Claude Haiku) can be used to process records, but we\nneed to ensure that the overall accuracy does not deviate substantially from\nthat of the top-of-the-line LLMs. The model cascade framework provides a\nblueprint to manage this trade-off, by using the confidence of LLMs in their\noutput (e.g., log-probabilities) to decide on which records to use the\naffordable LLM. However, existing solutions following this framework provide\nonly marginal cost savings and weak theoretical guarantees because of poor\nestimation of the quality of the affordable LLM's outputs. We present BARGAIN,\na method that judiciously uses affordable LLMs in data processing to\nsignificantly reduce cost while providing strong theoretical guarantees on the\nsolution quality. BARGAIN employs a novel adaptive sampling strategy and\nstatistical estimation procedure that uses data and task characteristics and\nbuilds on recent statistical tools to make accurate estimations with tight\ntheoretical guarantees. Variants of BARGAIN can support guarantees on accuracy,\nprecision, or recall of the output. Experimental results across 8 real-world\ndatasets show that BARGAIN reduces cost, on average, by up to 86% more than\nstate-of-the-art, while providing stronger theoretical guarantees on accuracy\nof output, with similar gains when guaranteeing a desired level of precision or\nrecall."
                },
                "authors": [
                    {
                        "name": "Sepanta Zeighami"
                    },
                    {
                        "name": "Shreya Shankar"
                    },
                    {
                        "name": "Aditya Parameswaran"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Parameswaran"
                },
                "author": "Aditya Parameswaran",
                "arxiv_comment": "To appear in SIGMOD'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02896v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02896v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10377v1",
                "updated": "2025-09-12T16:09:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    9,
                    39,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T16:09:39Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    9,
                    39,
                    4,
                    255,
                    0
                ],
                "title": "Dropping Experts, Recombining Neurons: Retraining-Free Pruning for\n  Sparse Mixture-of-Experts LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dropping Experts, Recombining Neurons: Retraining-Free Pruning for\n  Sparse Mixture-of-Experts LLMs"
                },
                "summary": "Sparse Mixture-of-Experts (SMoE) architectures are widely used in large\nlanguage models (LLMs) due to their computational efficiency. However, though\nonly a few experts are activated for each token, SMoE still requires loading\nall expert parameters, leading to high memory usage and challenges in\ndeployment. Previous work has tried to reduce the overhead by pruning and\nmerging experts, but primarily focused on expert-level operations, leaving\nneuron-level structure underexplored. We propose DERN (Dropping Experts,\nRecombining Neurons), a task-agnostic and retraining-free framework for expert\npruning and reconstruction. We observe that experts are often misaligned and\ncontain semantic conflicts at the neuron level, which poses challenges for\ndirect merging. To solve this, DERN works in three steps: it first prunes\nredundant experts using router statistics; then it decomposes them into\nneuron-level expert segments, assigning each segment to its most compatible\nretained expert; and finally, it merges segments within each retained expert to\nbuild a compact representation. Experiments on Mixtral, Qwen, and DeepSeek SMoE\nmodels show that DERN improves performance by more than 5% on commonsense\nreasoning and MMLU benchmarks under 50% expert sparsity, without extra\ntraining. It also greatly reduces the number of experts and memory usage,\nmaking SMoE LLMs easier to deploy in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Mixture-of-Experts (SMoE) architectures are widely used in large\nlanguage models (LLMs) due to their computational efficiency. However, though\nonly a few experts are activated for each token, SMoE still requires loading\nall expert parameters, leading to high memory usage and challenges in\ndeployment. Previous work has tried to reduce the overhead by pruning and\nmerging experts, but primarily focused on expert-level operations, leaving\nneuron-level structure underexplored. We propose DERN (Dropping Experts,\nRecombining Neurons), a task-agnostic and retraining-free framework for expert\npruning and reconstruction. We observe that experts are often misaligned and\ncontain semantic conflicts at the neuron level, which poses challenges for\ndirect merging. To solve this, DERN works in three steps: it first prunes\nredundant experts using router statistics; then it decomposes them into\nneuron-level expert segments, assigning each segment to its most compatible\nretained expert; and finally, it merges segments within each retained expert to\nbuild a compact representation. Experiments on Mixtral, Qwen, and DeepSeek SMoE\nmodels show that DERN improves performance by more than 5% on commonsense\nreasoning and MMLU benchmarks under 50% expert sparsity, without extra\ntraining. It also greatly reduces the number of experts and memory usage,\nmaking SMoE LLMs easier to deploy in practice."
                },
                "authors": [
                    {
                        "name": "Yixiao Zhou"
                    },
                    {
                        "name": "Ziyu Zhao"
                    },
                    {
                        "name": "Dongzhou Cheng"
                    },
                    {
                        "name": "zhiliang wu"
                    },
                    {
                        "name": "Jie Gui"
                    },
                    {
                        "name": "Yi Yang"
                    },
                    {
                        "name": "Fei Wu"
                    },
                    {
                        "name": "Yu Cheng"
                    },
                    {
                        "name": "Hehe Fan"
                    }
                ],
                "author_detail": {
                    "name": "Hehe Fan"
                },
                "author": "Hehe Fan",
                "arxiv_comment": "Accepted to EMNLP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10372v1",
                "updated": "2025-09-12T16:05:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    5,
                    27,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T16:05:27Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    5,
                    27,
                    4,
                    255,
                    0
                ],
                "title": "MCBP: A Memory-Compute Efficient LLM Inference Accelerator Leveraging\n  Bit-Slice-enabled Sparsity and Repetitiveness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCBP: A Memory-Compute Efficient LLM Inference Accelerator Leveraging\n  Bit-Slice-enabled Sparsity and Repetitiveness"
                },
                "summary": "Large language models (LLMs) face significant inference latency due to\ninefficiencies in GEMM operations, weight access, and KV cache access,\nespecially in real-time scenarios. This highlights the need for a versatile\ncompute-memory efficient accelerator. Unfortunately, existing Transformer\naccelerators struggle to address both aspects simultaneously, as they focus on\nvalue-level processing, missing fine-grained opportunities to optimize\ncomputation and memory collaboratively. This paper introduces MCBP, a\nbit-grained compute-memory efficient algorithm-hardware co-design that\nleverages bit-slice (BS) enabled repetitiveness and sparsity to accelerate LLM\ninference. MCBP features three key innovations: 1) BS-repetitiveness-enabled\ncomputation reduction (BRCR), which eliminates redundant GEMM computations via\nleveraging redundancy hidden among BS vectors; 2) BS-sparsity-enabled two-state\ncoding (BSTC), which reduces weight access via exploiting significant sparsity\nin high-order bit-slice weight; 3) Bit-grained progressive prediction (BGPP),\nwhich reduces KV cache access by leveraging early-termination-based bit-grained\nprediction. These techniques, supported by custom accelerator designs,\neffectively alleviate the burden in GEMM, weight access, and KV cache access.\nExtensive experiments on 26 benchmarks show that MCBP achieves 9.43x speed up\nand 31.1x higher energy efficiency than Nvidia A100 GPU. Compared to SOTA\nTransformer accelerators, MCBP achieves 35x, 5.2x and 3.2x energy saving than\nSpatten, FACT and SOFA, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face significant inference latency due to\ninefficiencies in GEMM operations, weight access, and KV cache access,\nespecially in real-time scenarios. This highlights the need for a versatile\ncompute-memory efficient accelerator. Unfortunately, existing Transformer\naccelerators struggle to address both aspects simultaneously, as they focus on\nvalue-level processing, missing fine-grained opportunities to optimize\ncomputation and memory collaboratively. This paper introduces MCBP, a\nbit-grained compute-memory efficient algorithm-hardware co-design that\nleverages bit-slice (BS) enabled repetitiveness and sparsity to accelerate LLM\ninference. MCBP features three key innovations: 1) BS-repetitiveness-enabled\ncomputation reduction (BRCR), which eliminates redundant GEMM computations via\nleveraging redundancy hidden among BS vectors; 2) BS-sparsity-enabled two-state\ncoding (BSTC), which reduces weight access via exploiting significant sparsity\nin high-order bit-slice weight; 3) Bit-grained progressive prediction (BGPP),\nwhich reduces KV cache access by leveraging early-termination-based bit-grained\nprediction. These techniques, supported by custom accelerator designs,\neffectively alleviate the burden in GEMM, weight access, and KV cache access.\nExtensive experiments on 26 benchmarks show that MCBP achieves 9.43x speed up\nand 31.1x higher energy efficiency than Nvidia A100 GPU. Compared to SOTA\nTransformer accelerators, MCBP achieves 35x, 5.2x and 3.2x energy saving than\nSpatten, FACT and SOFA, respectively."
                },
                "authors": [
                    {
                        "name": "Huizheng Wang"
                    },
                    {
                        "name": "Zichuan Wang"
                    },
                    {
                        "name": "Zhiheng Yue"
                    },
                    {
                        "name": "Yousheng Long"
                    },
                    {
                        "name": "Taiquan Wei"
                    },
                    {
                        "name": "Jianxun Yang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Shaojun Wei"
                    },
                    {
                        "name": "Yang Hu"
                    },
                    {
                        "name": "Shouyi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Shouyi Yin"
                },
                "author": "Shouyi Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10371v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10371v1",
                "updated": "2025-09-12T16:05:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    5,
                    7,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T16:05:07Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    5,
                    7,
                    4,
                    255,
                    0
                ],
                "title": "Characterizing the Efficiency of Distributed Training: A Power,\n  Performance, and Thermal Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing the Efficiency of Distributed Training: A Power,\n  Performance, and Thermal Perspective"
                },
                "summary": "The rapid scaling of Large Language Models (LLMs) has pushed training\nworkloads far beyond the limits of single-node analysis, demanding a deeper\nunderstanding of how these models behave across large-scale, multi-GPU systems.\nIn this paper, we present a comprehensive characterization of LLM training\nacross diverse real-world workloads and hardware platforms, including NVIDIA\nH100/H200 and AMD MI250 GPUs. We analyze dense and sparse models under various\nparallelism strategies -- tensor, pipeline, data, and expert -- and evaluate\ntheir effects on hardware utilization, power consumption, and thermal behavior.\nWe further evaluate the effectiveness of optimizations such as activation\nrecomputation and compute-communication overlap. Our findings show that\nperformance is not determined solely by scaling hardware capacity. Scale-up\nsystems with fewer, higher-memory GPUs can outperform scale-out systems in\ncommunication-bound regimes, but only under carefully tuned configurations; in\nother cases, scale-out deployments achieve superior throughput. We also show\nthat certain parallelism combinations, such as tensor with pipeline, lead to\nbandwidth underutilization due to inefficient data chunking, while increasing\nmicrobatch sizes beyond a certain point induces bursty execution and peak power\nexcursions that worsen thermal throttling. These insights reveal how training\nperformance is shaped by complex interactions between hardware, system\ntopology, and model execution. We conclude by offering recommendations for\nsystem and hardware design to improve the scalability and reliability of future\nLLM systems and workloads. The source code of this project is available at\nhttps://github.com/sitar-lab/CharLLM-PPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid scaling of Large Language Models (LLMs) has pushed training\nworkloads far beyond the limits of single-node analysis, demanding a deeper\nunderstanding of how these models behave across large-scale, multi-GPU systems.\nIn this paper, we present a comprehensive characterization of LLM training\nacross diverse real-world workloads and hardware platforms, including NVIDIA\nH100/H200 and AMD MI250 GPUs. We analyze dense and sparse models under various\nparallelism strategies -- tensor, pipeline, data, and expert -- and evaluate\ntheir effects on hardware utilization, power consumption, and thermal behavior.\nWe further evaluate the effectiveness of optimizations such as activation\nrecomputation and compute-communication overlap. Our findings show that\nperformance is not determined solely by scaling hardware capacity. Scale-up\nsystems with fewer, higher-memory GPUs can outperform scale-out systems in\ncommunication-bound regimes, but only under carefully tuned configurations; in\nother cases, scale-out deployments achieve superior throughput. We also show\nthat certain parallelism combinations, such as tensor with pipeline, lead to\nbandwidth underutilization due to inefficient data chunking, while increasing\nmicrobatch sizes beyond a certain point induces bursty execution and peak power\nexcursions that worsen thermal throttling. These insights reveal how training\nperformance is shaped by complex interactions between hardware, system\ntopology, and model execution. We conclude by offering recommendations for\nsystem and hardware design to improve the scalability and reliability of future\nLLM systems and workloads. The source code of this project is available at\nhttps://github.com/sitar-lab/CharLLM-PPT."
                },
                "authors": [
                    {
                        "name": "Seokjin Go"
                    },
                    {
                        "name": "Joongun Park"
                    },
                    {
                        "name": "Spandan More"
                    },
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Irene Wang"
                    },
                    {
                        "name": "Aaron Jezghani"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Divya Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Divya Mahajan"
                },
                "author": "Divya Mahajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10371v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10371v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10366v1",
                "updated": "2025-09-12T15:59:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    15,
                    59,
                    55,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T15:59:55Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    15,
                    59,
                    55,
                    4,
                    255,
                    0
                ],
                "title": "Efficient Learned Image Compression Through Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Learned Image Compression Through Knowledge Distillation"
                },
                "summary": "Learned image compression sits at the intersection of machine learning and\nimage processing. With advances in deep learning, neural network-based\ncompression methods have emerged. In this process, an encoder maps the image to\na low-dimensional latent space, which is then quantized, entropy-coded into a\nbinary bitstream, and transmitted to the receiver. At the receiver end, the\nbitstream is entropy-decoded, and a decoder reconstructs an approximation of\nthe original image. Recent research suggests that these models consistently\noutperform conventional codecs. However, they require significant processing\npower, making them unsuitable for real-time use on resource-constrained\nplatforms, which hinders their deployment in mainstream applications. This\nstudy aims to reduce the resource requirements of neural networks used for\nimage compression by leveraging knowledge distillation, a training paradigm\nwhere smaller neural networks, partially trained on the outputs of larger, more\ncomplex models, can achieve better performance than when trained independently.\nOur work demonstrates that knowledge distillation can be effectively applied to\nimage compression tasks: i) across various architecture sizes, ii) to achieve\ndifferent image quality/bit rate tradeoffs, and iii) to save processing and\nenergy resources. This approach introduces new settings and hyperparameters,\nand future research could explore the impact of different teacher models, as\nwell as alternative loss functions. Knowledge distillation could also be\nextended to transformer-based models. The code is publicly available at:\nhttps://github.com/FABallemand/PRIM .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned image compression sits at the intersection of machine learning and\nimage processing. With advances in deep learning, neural network-based\ncompression methods have emerged. In this process, an encoder maps the image to\na low-dimensional latent space, which is then quantized, entropy-coded into a\nbinary bitstream, and transmitted to the receiver. At the receiver end, the\nbitstream is entropy-decoded, and a decoder reconstructs an approximation of\nthe original image. Recent research suggests that these models consistently\noutperform conventional codecs. However, they require significant processing\npower, making them unsuitable for real-time use on resource-constrained\nplatforms, which hinders their deployment in mainstream applications. This\nstudy aims to reduce the resource requirements of neural networks used for\nimage compression by leveraging knowledge distillation, a training paradigm\nwhere smaller neural networks, partially trained on the outputs of larger, more\ncomplex models, can achieve better performance than when trained independently.\nOur work demonstrates that knowledge distillation can be effectively applied to\nimage compression tasks: i) across various architecture sizes, ii) to achieve\ndifferent image quality/bit rate tradeoffs, and iii) to save processing and\nenergy resources. This approach introduces new settings and hyperparameters,\nand future research could explore the impact of different teacher models, as\nwell as alternative loss functions. Knowledge distillation could also be\nextended to transformer-based models. The code is publicly available at:\nhttps://github.com/FABallemand/PRIM ."
                },
                "authors": [
                    {
                        "name": "Fabien Allemand"
                    },
                    {
                        "name": "Attilio Fiandrotti"
                    },
                    {
                        "name": "Sumanta Chaudhuri"
                    },
                    {
                        "name": "Alaa Eddine Mazouz"
                    }
                ],
                "author_detail": {
                    "name": "Alaa Eddine Mazouz"
                },
                "author": "Alaa Eddine Mazouz",
                "arxiv_comment": "19 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18173v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18173v3",
                "updated": "2025-09-12T15:39:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    15,
                    39,
                    0,
                    4,
                    255,
                    0
                ],
                "published": "2024-06-26T08:44:36Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    8,
                    44,
                    36,
                    2,
                    178,
                    0
                ],
                "title": "UIO-LLMs: Unbiased Incremental Optimization for Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UIO-LLMs: Unbiased Incremental Optimization for Long-Context LLMs"
                },
                "summary": "Managing long texts is challenging for large language models (LLMs) due to\nlimited context window sizes. This study introduces UIO-LLMs, an unbiased\nincremental optimization approach for memory-enhanced transformers under\nlong-context settings. We initially conceptualize the process as a streamlined\nencoder-decoder framework where the weights-shared encoder and decoder\nrespectively encapsulate a context segment into memories and leverage these\nmemories to predict outputs of the subsequent segment. Subsequently, by\ntreating our memory-enhanced transformers as fully-connected recurrent neural\nnetworks (RNNs), we refine the training process using the Truncated\nBackpropagation Through Time (TBPTT) algorithm, which incorporates innovative\nincremental optimization techniques. These techniques not only diminish time\ncomplexity but also address the bias in gradient computation through an\nunbiased optimization process. UIO-LLMs successfully handle long context, such\nas extending the context window of Llama2-7b-chat from 4K to 100K tokens with\nminimal 2% additional parameters, while keeping the inference cost nearly\nlinear as context length increases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Managing long texts is challenging for large language models (LLMs) due to\nlimited context window sizes. This study introduces UIO-LLMs, an unbiased\nincremental optimization approach for memory-enhanced transformers under\nlong-context settings. We initially conceptualize the process as a streamlined\nencoder-decoder framework where the weights-shared encoder and decoder\nrespectively encapsulate a context segment into memories and leverage these\nmemories to predict outputs of the subsequent segment. Subsequently, by\ntreating our memory-enhanced transformers as fully-connected recurrent neural\nnetworks (RNNs), we refine the training process using the Truncated\nBackpropagation Through Time (TBPTT) algorithm, which incorporates innovative\nincremental optimization techniques. These techniques not only diminish time\ncomplexity but also address the bias in gradient computation through an\nunbiased optimization process. UIO-LLMs successfully handle long context, such\nas extending the context window of Llama2-7b-chat from 4K to 100K tokens with\nminimal 2% additional parameters, while keeping the inference cost nearly\nlinear as context length increases."
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Mingbao Lin"
                    },
                    {
                        "name": "Yunshan Zhong"
                    },
                    {
                        "name": "Shuicheng Yan"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "arxiv_comment": "This article was not accepted, and its quality is not very good.\n  Therefore, we have decided to withdraw the submission and will not resubmit\n  it elsewhere",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18173v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18173v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10348v1",
                "updated": "2025-09-12T15:36:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    15,
                    36,
                    26,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T15:36:26Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    15,
                    36,
                    26,
                    4,
                    255,
                    0
                ],
                "title": "Multi-pathology Chest X-ray Classification with Rejection Mechanisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-pathology Chest X-ray Classification with Rejection Mechanisms"
                },
                "summary": "Overconfidence in deep learning models poses a significant risk in\nhigh-stakes medical imaging tasks, particularly in multi-label classification\nof chest X-rays, where multiple co-occurring pathologies must be detected\nsimultaneously. This study introduces an uncertainty-aware framework for chest\nX-ray diagnosis based on a DenseNet-121 backbone, enhanced with two selective\nprediction mechanisms: entropy-based rejection and confidence interval-based\nrejection. Both methods enable the model to abstain from uncertain predictions,\nimproving reliability by deferring ambiguous cases to clinical experts. A\nquantile-based calibration procedure is employed to tune rejection thresholds\nusing either global or class-specific strategies. Experiments conducted on\nthree large public datasets (PadChest, NIH ChestX-ray14, and MIMIC-CXR)\ndemonstrate that selective rejection improves the trade-off between diagnostic\naccuracy and coverage, with entropy-based rejection yielding the highest\naverage AUC across all pathologies. These results support the integration of\nselective prediction into AI-assisted diagnostic workflows, providing a\npractical step toward safer, uncertainty-aware deployment of deep learning in\nclinical settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Overconfidence in deep learning models poses a significant risk in\nhigh-stakes medical imaging tasks, particularly in multi-label classification\nof chest X-rays, where multiple co-occurring pathologies must be detected\nsimultaneously. This study introduces an uncertainty-aware framework for chest\nX-ray diagnosis based on a DenseNet-121 backbone, enhanced with two selective\nprediction mechanisms: entropy-based rejection and confidence interval-based\nrejection. Both methods enable the model to abstain from uncertain predictions,\nimproving reliability by deferring ambiguous cases to clinical experts. A\nquantile-based calibration procedure is employed to tune rejection thresholds\nusing either global or class-specific strategies. Experiments conducted on\nthree large public datasets (PadChest, NIH ChestX-ray14, and MIMIC-CXR)\ndemonstrate that selective rejection improves the trade-off between diagnostic\naccuracy and coverage, with entropy-based rejection yielding the highest\naverage AUC across all pathologies. These results support the integration of\nselective prediction into AI-assisted diagnostic workflows, providing a\npractical step toward safer, uncertainty-aware deployment of deep learning in\nclinical settings."
                },
                "authors": [
                    {
                        "name": "Yehudit Aperstein"
                    },
                    {
                        "name": "Amit Tzahar"
                    },
                    {
                        "name": "Alon Gottlib"
                    },
                    {
                        "name": "Tal Verber"
                    },
                    {
                        "name": "Ravit Shagan Damti"
                    },
                    {
                        "name": "Alexander Apartsin"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Apartsin"
                },
                "author": "Alexander Apartsin",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10338v1",
                "updated": "2025-09-12T15:19:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    15,
                    19,
                    44,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T15:19:44Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    15,
                    19,
                    44,
                    4,
                    255,
                    0
                ],
                "title": "Trusted Repeater Placement in QKD-enabled Optical Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trusted Repeater Placement in QKD-enabled Optical Networks"
                },
                "summary": "Quantum Key Distribution (QKD) provides information-theoretic security, but\nis limited by distance in optical networks, thereby requiring repeater nodes to\nextend coverage. Existing works usually assume all repeater nodes and\nassociated Key Management Servers (KMSs) to be Trusted Repeater Nodes (TRNs),\nwhile ignoring risks from software exploits and insider threats. In this paper,\nwe propose a reliability-aware TRN placement framework for metro optical\nnetworks, which assigns each node a trust score and integrates it into the\nDijkstra algorithm via weighted links. We then rank the nodes using a composite\nscore, which is a weighted combination of betweenness centrality and\neigenvector centrality to enable a secure and scalable TRN deployment.\nSimulation results on a reference topology show that our method covers 10.77%\nmore shortest paths compared to traditional metrics like degree centrality,\nusing the same number (around eight) of TRNs, making it suitable for TRN\nselection to maximize secure connectivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Key Distribution (QKD) provides information-theoretic security, but\nis limited by distance in optical networks, thereby requiring repeater nodes to\nextend coverage. Existing works usually assume all repeater nodes and\nassociated Key Management Servers (KMSs) to be Trusted Repeater Nodes (TRNs),\nwhile ignoring risks from software exploits and insider threats. In this paper,\nwe propose a reliability-aware TRN placement framework for metro optical\nnetworks, which assigns each node a trust score and integrates it into the\nDijkstra algorithm via weighted links. We then rank the nodes using a composite\nscore, which is a weighted combination of betweenness centrality and\neigenvector centrality to enable a secure and scalable TRN deployment.\nSimulation results on a reference topology show that our method covers 10.77%\nmore shortest paths compared to traditional metrics like degree centrality,\nusing the same number (around eight) of TRNs, making it suitable for TRN\nselection to maximize secure connectivity."
                },
                "authors": [
                    {
                        "name": "Arup Kumar Marik"
                    },
                    {
                        "name": "Basabdatta Palit"
                    },
                    {
                        "name": "Sadananda Behera"
                    }
                ],
                "author_detail": {
                    "name": "Sadananda Behera"
                },
                "author": "Sadananda Behera",
                "arxiv_comment": "Paper accepted for the IEEE Global Communications Conference Workshop\n  on Quantum Computing for Communications and Learning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10334v1",
                "updated": "2025-09-12T15:14:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    15,
                    14,
                    19,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T15:14:19Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    15,
                    14,
                    19,
                    4,
                    255,
                    0
                ],
                "title": "I-Segmenter: Integer-Only Vision Transformer for Efficient Semantic\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I-Segmenter: Integer-Only Vision Transformer for Efficient Semantic\n  Segmentation"
                },
                "summary": "Vision Transformers (ViTs) have recently achieved strong results in semantic\nsegmentation, yet their deployment on resource-constrained devices remains\nlimited due to their high memory footprint and computational cost. Quantization\noffers an effective strategy to improve efficiency, but ViT-based segmentation\nmodels are notoriously fragile under low precision, as quantization errors\naccumulate across deep encoder-decoder pipelines. We introduce I-Segmenter, the\nfirst fully integer-only ViT segmentation framework. Building on the Segmenter\narchitecture, I-Segmenter systematically replaces floating-point operations\nwith integer-only counterparts. To further stabilize both training and\ninference, we propose $\\lambda$-ShiftGELU, a novel activation function that\nmitigates the limitations of uniform quantization in handling long-tailed\nactivation distributions. In addition, we remove the L2 normalization layer and\nreplace bilinear interpolation in the decoder with nearest neighbor upsampling,\nensuring integer-only execution throughout the computational graph. Extensive\nexperiments show that I-Segmenter achieves accuracy within a reasonable margin\nof its FP32 baseline (5.1 % on average), while reducing model size by up to\n3.8x and enabling up to 1.2x faster inference with optimized runtimes. Notably,\neven in one-shot PTQ with a single calibration image, I-Segmenter delivers\ncompetitive accuracy, underscoring its practicality for real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Transformers (ViTs) have recently achieved strong results in semantic\nsegmentation, yet their deployment on resource-constrained devices remains\nlimited due to their high memory footprint and computational cost. Quantization\noffers an effective strategy to improve efficiency, but ViT-based segmentation\nmodels are notoriously fragile under low precision, as quantization errors\naccumulate across deep encoder-decoder pipelines. We introduce I-Segmenter, the\nfirst fully integer-only ViT segmentation framework. Building on the Segmenter\narchitecture, I-Segmenter systematically replaces floating-point operations\nwith integer-only counterparts. To further stabilize both training and\ninference, we propose $\\lambda$-ShiftGELU, a novel activation function that\nmitigates the limitations of uniform quantization in handling long-tailed\nactivation distributions. In addition, we remove the L2 normalization layer and\nreplace bilinear interpolation in the decoder with nearest neighbor upsampling,\nensuring integer-only execution throughout the computational graph. Extensive\nexperiments show that I-Segmenter achieves accuracy within a reasonable margin\nof its FP32 baseline (5.1 % on average), while reducing model size by up to\n3.8x and enabling up to 1.2x faster inference with optimized runtimes. Notably,\neven in one-shot PTQ with a single calibration image, I-Segmenter delivers\ncompetitive accuracy, underscoring its practicality for real-world deployment."
                },
                "authors": [
                    {
                        "name": "Jordan Sassoon"
                    },
                    {
                        "name": "Michal Szczepanski"
                    },
                    {
                        "name": "Martyna Poreba"
                    }
                ],
                "author_detail": {
                    "name": "Martyna Poreba"
                },
                "author": "Martyna Poreba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17002v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17002v4",
                "updated": "2025-09-12T15:07:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    15,
                    7,
                    26,
                    4,
                    255,
                    0
                ],
                "published": "2024-06-24T14:37:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    14,
                    37,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Deep Survival Analysis from Adult and Pediatric Electrocardiograms: A\n  Multi-center Benchmark Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Survival Analysis from Adult and Pediatric Electrocardiograms: A\n  Multi-center Benchmark Study"
                },
                "summary": "Artificial intelligence applied to electrocardiography (AI-ECG) shows\npotential for mortality prediction, but heterogeneous approaches and private\ndatasets have limited generalizable insights. To address this, we\nsystematically evaluated model design choices across three large cohorts: Beth\nIsrael Deaconess (MIMIC-IV: n = 795,546 ECGs, United States), Telehealth\nNetwork of Minas Gerais (Code-15: n = 345,779, Brazil), and Boston Children's\nHospital (BCH: n = 255,379, United States). We evaluated models predicting\nall-cause mortality, comparing horizon-based classification and deep survival\nmethods with neural architectures including convolutional networks and\ntransformers, benchmarking against demographic-only and gradient boosting\nbaselines. Top models performed well (median concordance: Code-15, 0.83;\nMIMIC-IV, 0.78; BCH, 0.81). Incorporating age and sex improved performance\nacross all datasets. Classifier-Cox models showed site-dependent sensitivity to\nhorizon choice (median Pearson's R: Code-15, 0.35; MIMIC-IV, -0.71; BCH, 0.37).\nExternal validation reduced concordance, and in some cases demographic-only\nmodels outperformed externally trained AI-ECG models on Code-15. However,\nmodels trained on multi-site data outperformed site-specific models by 5-22%.\nFindings highlight factors for robust AI-ECG deployment: deep survival methods\noutperformed horizon-based classifiers, demographic covariates improved\npredictive performance, classifier-based models required site-specific\ncalibration, and cross-cohort training, even between adult and pediatric\ncohorts, substantially improved performance. These results emphasize the\nimportance of model type, demographics, and training diversity in developing\nAI-ECG models reliably applicable across populations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence applied to electrocardiography (AI-ECG) shows\npotential for mortality prediction, but heterogeneous approaches and private\ndatasets have limited generalizable insights. To address this, we\nsystematically evaluated model design choices across three large cohorts: Beth\nIsrael Deaconess (MIMIC-IV: n = 795,546 ECGs, United States), Telehealth\nNetwork of Minas Gerais (Code-15: n = 345,779, Brazil), and Boston Children's\nHospital (BCH: n = 255,379, United States). We evaluated models predicting\nall-cause mortality, comparing horizon-based classification and deep survival\nmethods with neural architectures including convolutional networks and\ntransformers, benchmarking against demographic-only and gradient boosting\nbaselines. Top models performed well (median concordance: Code-15, 0.83;\nMIMIC-IV, 0.78; BCH, 0.81). Incorporating age and sex improved performance\nacross all datasets. Classifier-Cox models showed site-dependent sensitivity to\nhorizon choice (median Pearson's R: Code-15, 0.35; MIMIC-IV, -0.71; BCH, 0.37).\nExternal validation reduced concordance, and in some cases demographic-only\nmodels outperformed externally trained AI-ECG models on Code-15. However,\nmodels trained on multi-site data outperformed site-specific models by 5-22%.\nFindings highlight factors for robust AI-ECG deployment: deep survival methods\noutperformed horizon-based classifiers, demographic covariates improved\npredictive performance, classifier-based models required site-specific\ncalibration, and cross-cohort training, even between adult and pediatric\ncohorts, substantially improved performance. These results emphasize the\nimportance of model type, demographics, and training diversity in developing\nAI-ECG models reliably applicable across populations."
                },
                "authors": [
                    {
                        "name": "Platon Lukyanenko"
                    },
                    {
                        "name": "Joshua Mayourian"
                    },
                    {
                        "name": "Mingxuan Liu"
                    },
                    {
                        "name": "John K. Triedman"
                    },
                    {
                        "name": "Sunil J. Ghelani"
                    },
                    {
                        "name": "William G. La Cava"
                    }
                ],
                "author_detail": {
                    "name": "William G. La Cava"
                },
                "author": "William G. La Cava",
                "arxiv_comment": "16 pages plus appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17002v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17002v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10317v1",
                "updated": "2025-09-12T14:59:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    59,
                    4,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T14:59:04Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    59,
                    4,
                    4,
                    255,
                    0
                ],
                "title": "Robot guide with multi-agent control and automatic scenario generation\n  with LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robot guide with multi-agent control and automatic scenario generation\n  with LLM"
                },
                "summary": "The work describes the development of a hybrid control architecture for an\nanthropomorphic tour guide robot, combining a multi-agent resource management\nsystem with automatic behavior scenario generation based on large language\nmodels. The proposed approach aims to overcome the limitations of traditional\nsystems, which rely on manual tuning of behavior scenarios. These limitations\ninclude manual configuration, low flexibility, and lack of naturalness in robot\nbehavior. The process of preparing tour scenarios is implemented through a\ntwo-stage generation: first, a stylized narrative is created, then non-verbal\naction tags are integrated into the text. The multi-agent system ensures\ncoordination and conflict resolution during the execution of parallel actions,\nas well as maintaining default behavior after the completion of main\noperations, contributing to more natural robot behavior. The results obtained\nfrom the trial demonstrate the potential of the proposed approach for\nautomating and scaling social robot control systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The work describes the development of a hybrid control architecture for an\nanthropomorphic tour guide robot, combining a multi-agent resource management\nsystem with automatic behavior scenario generation based on large language\nmodels. The proposed approach aims to overcome the limitations of traditional\nsystems, which rely on manual tuning of behavior scenarios. These limitations\ninclude manual configuration, low flexibility, and lack of naturalness in robot\nbehavior. The process of preparing tour scenarios is implemented through a\ntwo-stage generation: first, a stylized narrative is created, then non-verbal\naction tags are integrated into the text. The multi-agent system ensures\ncoordination and conflict resolution during the execution of parallel actions,\nas well as maintaining default behavior after the completion of main\noperations, contributing to more natural robot behavior. The results obtained\nfrom the trial demonstrate the potential of the proposed approach for\nautomating and scaling social robot control systems."
                },
                "authors": [
                    {
                        "name": "Elizaveta D. Moskovskaya"
                    },
                    {
                        "name": "Anton D. Moscowsky"
                    }
                ],
                "author_detail": {
                    "name": "Anton D. Moscowsky"
                },
                "author": "Anton D. Moscowsky",
                "arxiv_comment": "14 pages, 5 figures, 2 tables, 1 demo-video and repository link",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "93C85",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.9; I.2.7; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15690v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15690v2",
                "updated": "2025-09-12T14:40:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    40,
                    8,
                    4,
                    255,
                    0
                ],
                "published": "2025-05-21T16:05:29Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    5,
                    29,
                    2,
                    141,
                    0
                ],
                "title": "Toward Open Earth Science as Fast and Accessible as Natural Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Open Earth Science as Fast and Accessible as Natural Language"
                },
                "summary": "Is natural-language-driven earth observation data analysis now feasible with\nthe assistance of Large Language Models (LLMs)? For open science in service of\npublic interest, feasibility requires reliably high accuracy, interactive\nlatencies, low (sustainable) costs, open LLMs, and openly maintainable software\n-- hence, the challenge. What are the techniques and programming system\nrequirements necessary for satisfying these constraints, and what is the\ncorresponding development and maintenance burden in practice? This study lays\nthe groundwork for exploring these questions, introducing an impactful earth\nscience use-case, and providing a software framework with evaluation data and\nmetrics, along with initial results from employing model scaling,\nprompt-optimization, and inference-time scaling optimization techniques. While\nwe attain high accuracy (near 100%) across 10 of 11 metrics, the analysis\nfurther considers cost (token-spend), latency, and maintainability across this\nspace of techniques. Finally, we enumerate opportunities for further research,\ngeneral programming and evaluation framework development, and ongoing work for\na comprehensive, deployable solution. This is a call for collaboration and\ncontribution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is natural-language-driven earth observation data analysis now feasible with\nthe assistance of Large Language Models (LLMs)? For open science in service of\npublic interest, feasibility requires reliably high accuracy, interactive\nlatencies, low (sustainable) costs, open LLMs, and openly maintainable software\n-- hence, the challenge. What are the techniques and programming system\nrequirements necessary for satisfying these constraints, and what is the\ncorresponding development and maintenance burden in practice? This study lays\nthe groundwork for exploring these questions, introducing an impactful earth\nscience use-case, and providing a software framework with evaluation data and\nmetrics, along with initial results from employing model scaling,\nprompt-optimization, and inference-time scaling optimization techniques. While\nwe attain high accuracy (near 100%) across 10 of 11 metrics, the analysis\nfurther considers cost (token-spend), latency, and maintainability across this\nspace of techniques. Finally, we enumerate opportunities for further research,\ngeneral programming and evaluation framework development, and ongoing work for\na comprehensive, deployable solution. This is a call for collaboration and\ncontribution."
                },
                "authors": [
                    {
                        "name": "Marquita Ellis"
                    },
                    {
                        "name": "Iksha Gurung"
                    },
                    {
                        "name": "Muthukumaran Ramasubramanian"
                    },
                    {
                        "name": "Rahul Ramachandran"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Ramachandran"
                },
                "author": "Rahul Ramachandran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15690v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15690v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.2; H.5.2; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10297v1",
                "updated": "2025-09-12T14:37:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    37,
                    57,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T14:37:57Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    37,
                    57,
                    4,
                    255,
                    0
                ],
                "title": "The Morality of Probability: How Implicit Moral Biases in LLMs May Shape\n  the Future of Human-AI Symbiosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Morality of Probability: How Implicit Moral Biases in LLMs May Shape\n  the Future of Human-AI Symbiosis"
                },
                "summary": "Artificial intelligence (AI) is advancing at a pace that raises urgent\nquestions about how to align machine decision-making with human moral values.\nThis working paper investigates how leading AI systems prioritize moral\noutcomes and what this reveals about the prospects for human-AI symbiosis. We\naddress two central questions: (1) What moral values do state-of-the-art large\nlanguage models (LLMs) implicitly favour when confronted with dilemmas? (2) How\ndo differences in model architecture, cultural origin, and explainability\naffect these moral preferences? To explore these questions, we conduct a\nquantitative experiment with six LLMs, ranking and scoring outcomes across 18\ndilemmas representing five moral frameworks. Our findings uncover strikingly\nconsistent value biases. Across all models, Care and Virtue values outcomes\nwere rated most moral, while libertarian choices were consistently penalized.\nReasoning-enabled models exhibited greater sensitivity to context and provided\nricher explanations, whereas non-reasoning models produced more uniform but\nopaque judgments. This research makes three contributions: (i) Empirically, it\ndelivers a large-scale comparison of moral reasoning across culturally distinct\nLLMs; (ii) Theoretically, it links probabilistic model behaviour with\nunderlying value encodings; (iii) Practically, it highlights the need for\nexplainability and cultural awareness as critical design principles to guide AI\ntoward a transparent, aligned, and symbiotic future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence (AI) is advancing at a pace that raises urgent\nquestions about how to align machine decision-making with human moral values.\nThis working paper investigates how leading AI systems prioritize moral\noutcomes and what this reveals about the prospects for human-AI symbiosis. We\naddress two central questions: (1) What moral values do state-of-the-art large\nlanguage models (LLMs) implicitly favour when confronted with dilemmas? (2) How\ndo differences in model architecture, cultural origin, and explainability\naffect these moral preferences? To explore these questions, we conduct a\nquantitative experiment with six LLMs, ranking and scoring outcomes across 18\ndilemmas representing five moral frameworks. Our findings uncover strikingly\nconsistent value biases. Across all models, Care and Virtue values outcomes\nwere rated most moral, while libertarian choices were consistently penalized.\nReasoning-enabled models exhibited greater sensitivity to context and provided\nricher explanations, whereas non-reasoning models produced more uniform but\nopaque judgments. This research makes three contributions: (i) Empirically, it\ndelivers a large-scale comparison of moral reasoning across culturally distinct\nLLMs; (ii) Theoretically, it links probabilistic model behaviour with\nunderlying value encodings; (iii) Practically, it highlights the need for\nexplainability and cultural awareness as critical design principles to guide AI\ntoward a transparent, aligned, and symbiotic future."
                },
                "authors": [
                    {
                        "name": "Eoin O'Doherty"
                    },
                    {
                        "name": "Nicole Weinrauch"
                    },
                    {
                        "name": "Andrew Talone"
                    },
                    {
                        "name": "Uri Klempner"
                    },
                    {
                        "name": "Xiaoyuan Yi"
                    },
                    {
                        "name": "Xing Xie"
                    },
                    {
                        "name": "Yi Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zeng"
                },
                "author": "Yi Zeng",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10289v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10289v1",
                "updated": "2025-09-12T14:29:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    29,
                    14,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T14:29:14Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    29,
                    14,
                    4,
                    255,
                    0
                ],
                "title": "We Need a New Ethics for a World of AI Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We Need a New Ethics for a World of AI Agents"
                },
                "summary": "The deployment of capable AI agents raises fresh questions about safety,\nhuman-machine relationships and social coordination. We argue for greater\nengagement by scientists, scholars, engineers and policymakers with the\nimplications of a world increasingly populated by AI agents. We explore key\nchallenges that must be addressed to ensure that interactions between humans\nand agents, and among agents themselves, remain broadly beneficial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of capable AI agents raises fresh questions about safety,\nhuman-machine relationships and social coordination. We argue for greater\nengagement by scientists, scholars, engineers and policymakers with the\nimplications of a world increasingly populated by AI agents. We explore key\nchallenges that must be addressed to ensure that interactions between humans\nand agents, and among agents themselves, remain broadly beneficial."
                },
                "authors": [
                    {
                        "name": "Iason Gabriel"
                    },
                    {
                        "name": "Geoff Keeling"
                    },
                    {
                        "name": "Arianna Manzini"
                    },
                    {
                        "name": "James Evans"
                    }
                ],
                "author_detail": {
                    "name": "James Evans"
                },
                "author": "James Evans",
                "arxiv_doi": "10.1038/d41586-025-02454-5",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/d41586-025-02454-5",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.10289v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10289v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 pages, no figures",
                "arxiv_journal_ref": "Nature, 644 (8075), 2025, 38-40",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; K.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06830v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06830v2",
                "updated": "2025-09-12T14:28:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    28,
                    27,
                    4,
                    255,
                    0
                ],
                "published": "2025-04-09T12:42:58Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    12,
                    42,
                    58,
                    2,
                    99,
                    0
                ],
                "title": "Integrated Sensing and Communications Over the Years: An Evolution\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated Sensing and Communications Over the Years: An Evolution\n  Perspective"
                },
                "summary": "Integrated Sensing and Communications (ISAC) enables efficient spectrum\nutilization and reduces hardware costs for beyond 5G (B5G) and 6G networks,\nfacilitating intelligent applications that require both high-performance\ncommunication and precise sensing capabilities. This survey provides a\ncomprehensive review of the evolution of ISAC over the years. We examine the\nexpansion of the spectrum across RF and optical ISAC, highlighting the role of\nadvanced technologies, along with key challenges and synergies. We further\ndiscuss the advancements in network architecture from single-cell to multi-cell\nsystems, emphasizing the integration of collaborative sensing and interference\nmitigation strategies. Moreover, we analyze the progress from single-modal to\nmulti-modal sensing, with a focus on the integration of edge intelligence to\nenable real-time data processing, reduce latency, and enhance decision-making.\nFinally, we extensively review standardization efforts by 3GPP, IEEE, and ITU,\nexamining the transition of ISAC-related technologies and their implications\nfor the deployment of 6G networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated Sensing and Communications (ISAC) enables efficient spectrum\nutilization and reduces hardware costs for beyond 5G (B5G) and 6G networks,\nfacilitating intelligent applications that require both high-performance\ncommunication and precise sensing capabilities. This survey provides a\ncomprehensive review of the evolution of ISAC over the years. We examine the\nexpansion of the spectrum across RF and optical ISAC, highlighting the role of\nadvanced technologies, along with key challenges and synergies. We further\ndiscuss the advancements in network architecture from single-cell to multi-cell\nsystems, emphasizing the integration of collaborative sensing and interference\nmitigation strategies. Moreover, we analyze the progress from single-modal to\nmulti-modal sensing, with a focus on the integration of edge intelligence to\nenable real-time data processing, reduce latency, and enhance decision-making.\nFinally, we extensively review standardization efforts by 3GPP, IEEE, and ITU,\nexamining the transition of ISAC-related technologies and their implications\nfor the deployment of 6G networks."
                },
                "authors": [
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Yuanhao Cui"
                    },
                    {
                        "name": "Xiaowen Cao"
                    },
                    {
                        "name": "Nanchi Su"
                    },
                    {
                        "name": "Yi Gong"
                    },
                    {
                        "name": "Fan Liu"
                    },
                    {
                        "name": "Weijie Yuan"
                    },
                    {
                        "name": "Xiaojun Jing"
                    },
                    {
                        "name": "J. Andrew Zhang"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Christos Masouros"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Marco Di Renzo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Di Renzo"
                },
                "author": "Marco Di Renzo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06830v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06830v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13335v2",
                "updated": "2025-09-12T14:23:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    23,
                    5,
                    4,
                    255,
                    0
                ],
                "published": "2025-07-17T17:51:20Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    51,
                    20,
                    3,
                    198,
                    0
                ],
                "title": "Comparing Apples to Oranges: A Dataset & Analysis of LLM Humour\n  Understanding from Traditional Puns to Topical Jokes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing Apples to Oranges: A Dataset & Analysis of LLM Humour\n  Understanding from Traditional Puns to Topical Jokes"
                },
                "summary": "Humour, as a complex language form, is derived from myriad aspects of life.\nWhilst existing work on computational humour has focussed almost exclusively on\nshort pun-based jokes, we investigate whether the ability of Large Language\nModels (LLMs) to explain humour depends on the particular form. We compare\nmodels' joke explanation abilities from simple puns to complex topical humour\nthat requires esoteric knowledge of real-world entities and events. To this\nend, we curate a dataset of 600 jokes across 4 joke types and manually write\nhigh-quality explanations. These jokes include heterographic and homographic\npuns, contemporary internet humour, and topical jokes. Using this dataset, we\ncompare the zero-shot abilities of a range of LLMs to accurately and\ncomprehensively explain jokes of different types, identifying key research gaps\nin the task of humour explanation. We find that none of the tested models\n(including reasoning models) are capable of reliably generating adequate\nexplanations of all joke types, further highlighting the narrow focus of most\nexisting works on overly simple joke forms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humour, as a complex language form, is derived from myriad aspects of life.\nWhilst existing work on computational humour has focussed almost exclusively on\nshort pun-based jokes, we investigate whether the ability of Large Language\nModels (LLMs) to explain humour depends on the particular form. We compare\nmodels' joke explanation abilities from simple puns to complex topical humour\nthat requires esoteric knowledge of real-world entities and events. To this\nend, we curate a dataset of 600 jokes across 4 joke types and manually write\nhigh-quality explanations. These jokes include heterographic and homographic\npuns, contemporary internet humour, and topical jokes. Using this dataset, we\ncompare the zero-shot abilities of a range of LLMs to accurately and\ncomprehensively explain jokes of different types, identifying key research gaps\nin the task of humour explanation. We find that none of the tested models\n(including reasoning models) are capable of reliably generating adequate\nexplanations of all joke types, further highlighting the narrow focus of most\nexisting works on overly simple joke forms."
                },
                "authors": [
                    {
                        "name": "Tyler Loakman"
                    },
                    {
                        "name": "William Thorne"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "arxiv_comment": "Accepted to Findings of EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10280v1",
                "updated": "2025-09-12T14:20:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    20,
                    55,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T14:20:55Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    20,
                    55,
                    4,
                    255,
                    0
                ],
                "title": "Large-scale Aerial Reconfigurable Intelligent Surface-aided Robust\n  Anti-jamming Transmission",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale Aerial Reconfigurable Intelligent Surface-aided Robust\n  Anti-jamming Transmission"
                },
                "summary": "Aerial reconfigurable intelligent surfaces (ARIS), deployed on unmanned\naerial vehicles (UAVs), could enhance anti-jamming communication performance by\ndynamically configuring channel conditions and establishing reliable air-ground\nlinks. However, large-scale ARIS faces critical deployment challenges due to\nthe prohibitive computational complexity of conventional discrete optimization\nmethods and sophisticated jamming threats. In this paper, we introduce a mean\nfield modeling approach to design the spatial configuration of ARIS by a\ncontinuous density function, thus bypassing high-dimensional combinatorial\noptimization. We consider an adaptive jammer which adjusts its position and\nbeamforming to minimize the sum-rate. A key finding reveals that the jammer's\noptimal strategy is governed by a proximity-directivity trade-off between\nreducing path loss and enhancing spatial focusing. To combat the jamming, we\npropose a robust anti-jamming transmission framework that jointly optimizes the\nBS beamforming, the ARIS reflection, and the ARIS spatial distribution to\nmaximize the worst-case sum-rate. By leveraging variational optimization and\nRiemannian manifold methods, we efficiently solve the functional optimization\nproblems. Our analysis further unveils that the optimal ARIS deployment follows\na spatial water-filling principle, concentrating resources in high-gain regions\nwhile avoiding interference-prone areas. Simulation results demonstrate that\nthe proposed framework remarkably improves the sum-rate. Furthermore, the\ncomputational complexity of the proposed algorithm is independent of the number\nof UAVs, validating its effectiveness for scalable ARIS-assisted anti-jamming\ncommunications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aerial reconfigurable intelligent surfaces (ARIS), deployed on unmanned\naerial vehicles (UAVs), could enhance anti-jamming communication performance by\ndynamically configuring channel conditions and establishing reliable air-ground\nlinks. However, large-scale ARIS faces critical deployment challenges due to\nthe prohibitive computational complexity of conventional discrete optimization\nmethods and sophisticated jamming threats. In this paper, we introduce a mean\nfield modeling approach to design the spatial configuration of ARIS by a\ncontinuous density function, thus bypassing high-dimensional combinatorial\noptimization. We consider an adaptive jammer which adjusts its position and\nbeamforming to minimize the sum-rate. A key finding reveals that the jammer's\noptimal strategy is governed by a proximity-directivity trade-off between\nreducing path loss and enhancing spatial focusing. To combat the jamming, we\npropose a robust anti-jamming transmission framework that jointly optimizes the\nBS beamforming, the ARIS reflection, and the ARIS spatial distribution to\nmaximize the worst-case sum-rate. By leveraging variational optimization and\nRiemannian manifold methods, we efficiently solve the functional optimization\nproblems. Our analysis further unveils that the optimal ARIS deployment follows\na spatial water-filling principle, concentrating resources in high-gain regions\nwhile avoiding interference-prone areas. Simulation results demonstrate that\nthe proposed framework remarkably improves the sum-rate. Furthermore, the\ncomputational complexity of the proposed algorithm is independent of the number\nof UAVs, validating its effectiveness for scalable ARIS-assisted anti-jamming\ncommunications."
                },
                "authors": [
                    {
                        "name": "Junshan Luo"
                    },
                    {
                        "name": "Shilian Wang"
                    },
                    {
                        "name": "Boxiang He"
                    }
                ],
                "author_detail": {
                    "name": "Boxiang He"
                },
                "author": "Boxiang He",
                "arxiv_comment": "13 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09448v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09448v2",
                "updated": "2025-09-12T14:00:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    0,
                    8,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-11T13:31:35Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    31,
                    35,
                    3,
                    254,
                    0
                ],
                "title": "TORSO: Template-Oriented Reasoning Towards General Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TORSO: Template-Oriented Reasoning Towards General Tasks"
                },
                "summary": "The approaches that guide Large Language Models (LLMs) to emulate human\nreasoning during response generation have emerged as an effective method for\nenabling them to solve complex problems in a step-by-step manner, thereby\nachieving superior performance. However, most existing approaches using\nfew-shot prompts to generate responses heavily depend on the provided examples,\nlimiting the utilization of the model's inherent reasoning capabilities.\nMoreover, constructing task-specific few-shot prompts is often costly and may\nlead to inconsistencies across different tasks. In this work, we introduce\nTemplate-Oriented Reasoning (TORSO), which elicits the model to utilize\ninternal reasoning abilities to generate proper responses across various tasks\nwithout the need for manually crafted few-shot examples. Our experimental\nresults demonstrate that TORSO achieves strong performance on diverse LLMs\nbenchmarks with reasonable rationales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The approaches that guide Large Language Models (LLMs) to emulate human\nreasoning during response generation have emerged as an effective method for\nenabling them to solve complex problems in a step-by-step manner, thereby\nachieving superior performance. However, most existing approaches using\nfew-shot prompts to generate responses heavily depend on the provided examples,\nlimiting the utilization of the model's inherent reasoning capabilities.\nMoreover, constructing task-specific few-shot prompts is often costly and may\nlead to inconsistencies across different tasks. In this work, we introduce\nTemplate-Oriented Reasoning (TORSO), which elicits the model to utilize\ninternal reasoning abilities to generate proper responses across various tasks\nwithout the need for manually crafted few-shot examples. Our experimental\nresults demonstrate that TORSO achieves strong performance on diverse LLMs\nbenchmarks with reasonable rationales."
                },
                "authors": [
                    {
                        "name": "Minhyuk Kim"
                    },
                    {
                        "name": "Seungyoon Lee"
                    },
                    {
                        "name": "Heuiseok Lim"
                    }
                ],
                "author_detail": {
                    "name": "Heuiseok Lim"
                },
                "author": "Heuiseok Lim",
                "arxiv_comment": "Accepted to EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09448v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09448v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20600v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20600v2",
                "updated": "2025-09-12T13:52:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    52,
                    45,
                    4,
                    255,
                    0
                ],
                "published": "2024-10-27T21:20:18Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    21,
                    20,
                    18,
                    6,
                    301,
                    0
                ],
                "title": "Multi-Turn Human-LLM Interaction Through the Lens of a Two-Way\n  Intelligibility Protocol",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Turn Human-LLM Interaction Through the Lens of a Two-Way\n  Intelligibility Protocol"
                },
                "summary": "Our interest is in the design of software systems involving a human-expert\ninteracting -- using natural language -- with a large language model (LLM) on\ndata analysis tasks. For complex problems, it is possible that LLMs can harness\nhuman expertise and creativity to find solutions that were otherwise elusive.\nOn one level, this interaction takes place through multiple turns of prompts\nfrom the human and responses from the LLM. Here we investigate a more\nstructured approach based on an abstract protocol described in [3] for\ninteraction between agents. The protocol is motivated by a notion of \"two-way\nintelligibility\" and is modelled by a pair of communicating finite-state\nmachines. We provide an implementation of the protocol, and provide empirical\nevidence of using the implementation to mediate interactions between an LLM and\na human-agent in two areas of scientific interest (radiology and drug design).\nWe conduct controlled experiments with a human proxy (a database), and\nuncontrolled experiments with human subjects. The results provide evidence in\nsupport of the protocol's capability of capturing one- and two-way\nintelligibility in human-LLM interaction; and for the utility of two-way\nintelligibility in the design of human-machine systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our interest is in the design of software systems involving a human-expert\ninteracting -- using natural language -- with a large language model (LLM) on\ndata analysis tasks. For complex problems, it is possible that LLMs can harness\nhuman expertise and creativity to find solutions that were otherwise elusive.\nOn one level, this interaction takes place through multiple turns of prompts\nfrom the human and responses from the LLM. Here we investigate a more\nstructured approach based on an abstract protocol described in [3] for\ninteraction between agents. The protocol is motivated by a notion of \"two-way\nintelligibility\" and is modelled by a pair of communicating finite-state\nmachines. We provide an implementation of the protocol, and provide empirical\nevidence of using the implementation to mediate interactions between an LLM and\na human-agent in two areas of scientific interest (radiology and drug design).\nWe conduct controlled experiments with a human proxy (a database), and\nuncontrolled experiments with human subjects. The results provide evidence in\nsupport of the protocol's capability of capturing one- and two-way\nintelligibility in human-LLM interaction; and for the utility of two-way\nintelligibility in the design of human-machine systems."
                },
                "authors": [
                    {
                        "name": "Harshvardhan Mestha"
                    },
                    {
                        "name": "Karan Bania"
                    },
                    {
                        "name": "Shreyas V"
                    },
                    {
                        "name": "Sidong Liu"
                    },
                    {
                        "name": "Ashwin Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Ashwin Srinivasan"
                },
                "author": "Ashwin Srinivasan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20600v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20600v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06591v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06591v4",
                "updated": "2025-09-12T13:48:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    48,
                    25,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-08T12:02:38Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    12,
                    2,
                    38,
                    0,
                    251,
                    0
                ],
                "title": "Hybrid Swin Attention Networks for Simultaneously Low-Dose PET and CT\n  Denoising",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Swin Attention Networks for Simultaneously Low-Dose PET and CT\n  Denoising"
                },
                "summary": "Low-dose computed tomography (LDCT) and positron emission tomography (PET)\nhave emerged as safer alternatives to conventional imaging modalities by\nsignificantly reducing radiation exposure. However, this reduction often\nresults in increased noise and artifacts, which can compromise diagnostic\naccuracy. Consequently, denoising for LDCT/PET has become a vital area of\nresearch aimed at enhancing image quality while maintaining radiation safety.\nIn this study, we introduce a novel Hybrid Swin Attention Network (HSANet),\nwhich incorporates Efficient Global Attention (EGA) modules and a hybrid\nupsampling module. The EGA modules enhance both spatial and channel-wise\ninteraction, improving the network's capacity to capture relevant features,\nwhile the hybrid upsampling module mitigates the risk of overfitting to noise.\nWe validate the proposed approach using a publicly available LDCT/PET dataset.\nExperimental results demonstrate that HSANet achieves superior denoising\nperformance compared to existing methods, while maintaining a lightweight model\nsize suitable for deployment on GPUs with standard memory configurations. This\nmakes our approach highly practical for real-world clinical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-dose computed tomography (LDCT) and positron emission tomography (PET)\nhave emerged as safer alternatives to conventional imaging modalities by\nsignificantly reducing radiation exposure. However, this reduction often\nresults in increased noise and artifacts, which can compromise diagnostic\naccuracy. Consequently, denoising for LDCT/PET has become a vital area of\nresearch aimed at enhancing image quality while maintaining radiation safety.\nIn this study, we introduce a novel Hybrid Swin Attention Network (HSANet),\nwhich incorporates Efficient Global Attention (EGA) modules and a hybrid\nupsampling module. The EGA modules enhance both spatial and channel-wise\ninteraction, improving the network's capacity to capture relevant features,\nwhile the hybrid upsampling module mitigates the risk of overfitting to noise.\nWe validate the proposed approach using a publicly available LDCT/PET dataset.\nExperimental results demonstrate that HSANet achieves superior denoising\nperformance compared to existing methods, while maintaining a lightweight model\nsize suitable for deployment on GPUs with standard memory configurations. This\nmakes our approach highly practical for real-world clinical applications."
                },
                "authors": [
                    {
                        "name": "Yichao Liu"
                    },
                    {
                        "name": "Hengzhi Xue"
                    },
                    {
                        "name": "YueYang Teng"
                    }
                ],
                "author_detail": {
                    "name": "YueYang Teng"
                },
                "author": "YueYang Teng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06591v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06591v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10248v1",
                "updated": "2025-09-12T13:45:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    45,
                    24,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T13:45:24Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    45,
                    24,
                    4,
                    255,
                    0
                ],
                "title": "Prompt Injection Attacks on LLM Generated Reviews of Scientific\n  Publications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Injection Attacks on LLM Generated Reviews of Scientific\n  Publications"
                },
                "summary": "The ongoing intense discussion on rising LLM usage in the scientific\npeer-review process has recently been mingled by reports of authors using\nhidden prompt injections to manipulate review scores. Since the existence of\nsuch \"attacks\" - although seen by some commentators as \"self-defense\" - would\nhave a great impact on the further debate, this paper investigates the\npracticability and technical success of the described manipulations. Our\nsystematic evaluation uses 1k reviews of 2024 ICLR papers generated by a wide\nrange of LLMs shows two distinct results: I) very simple prompt injections are\nindeed highly effective, reaching up to 100% acceptance scores. II) LLM reviews\nare generally biased toward acceptance (>95% in many models). Both results have\ngreat impact on the ongoing discussions on LLM usage in peer-review.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ongoing intense discussion on rising LLM usage in the scientific\npeer-review process has recently been mingled by reports of authors using\nhidden prompt injections to manipulate review scores. Since the existence of\nsuch \"attacks\" - although seen by some commentators as \"self-defense\" - would\nhave a great impact on the further debate, this paper investigates the\npracticability and technical success of the described manipulations. Our\nsystematic evaluation uses 1k reviews of 2024 ICLR papers generated by a wide\nrange of LLMs shows two distinct results: I) very simple prompt injections are\nindeed highly effective, reaching up to 100% acceptance scores. II) LLM reviews\nare generally biased toward acceptance (>95% in many models). Both results have\ngreat impact on the ongoing discussions on LLM usage in peer-review."
                },
                "authors": [
                    {
                        "name": "Janis Keuper"
                    }
                ],
                "author_detail": {
                    "name": "Janis Keuper"
                },
                "author": "Janis Keuper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10222v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10222v1",
                "updated": "2025-09-12T13:14:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    14,
                    47,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T13:14:47Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    14,
                    47,
                    4,
                    255,
                    0
                ],
                "title": "Compartmentalised Agentic Reasoning for Clinical NLI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compartmentalised Agentic Reasoning for Clinical NLI"
                },
                "summary": "A common assumption holds that scaling data and parameters yields\nincreasingly structured, generalisable internal representations. We interrogate\nthis assumption in clinical natural language inference (NLI) by adopting a\nbenchmark decomposed into four reasoning families, Causal Attribution,\nCompositional Grounding, Epistemic Verification, and Risk State Abstraction,\nand introducing CARENLI, a Compartmentalised Agentic Reasoning for Clinical NLI\nthat separates knowledge access from principled inference. CARENLI routes each\npremise, statement pair to a family specific solver and enforces auditable\nprocedures via a planner, verifier, and refiner.\n  Across four LLMs, CARENLI improves fidelity by up to 42 points, reaching\n98.0% in Causal Attribution and 81.2% in Risk State Abstraction. Verifiers flag\nviolations with near-ceiling reliability, while refiners correct a substantial\nshare of epistemic errors. Remaining failures cluster in routing, identifying\nfamily classification as the main bottleneck. These results show that LLMs\noften retain relevant facts but default to heuristics when inference is\nunderspecified, a dissociation CARENLI makes explicit while offering a\nframework for safer, auditable reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A common assumption holds that scaling data and parameters yields\nincreasingly structured, generalisable internal representations. We interrogate\nthis assumption in clinical natural language inference (NLI) by adopting a\nbenchmark decomposed into four reasoning families, Causal Attribution,\nCompositional Grounding, Epistemic Verification, and Risk State Abstraction,\nand introducing CARENLI, a Compartmentalised Agentic Reasoning for Clinical NLI\nthat separates knowledge access from principled inference. CARENLI routes each\npremise, statement pair to a family specific solver and enforces auditable\nprocedures via a planner, verifier, and refiner.\n  Across four LLMs, CARENLI improves fidelity by up to 42 points, reaching\n98.0% in Causal Attribution and 81.2% in Risk State Abstraction. Verifiers flag\nviolations with near-ceiling reliability, while refiners correct a substantial\nshare of epistemic errors. Remaining failures cluster in routing, identifying\nfamily classification as the main bottleneck. These results show that LLMs\noften retain relevant facts but default to heuristics when inference is\nunderspecified, a dissociation CARENLI makes explicit while offering a\nframework for safer, auditable reasoning."
                },
                "authors": [
                    {
                        "name": "Maël Jullien"
                    },
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Marco Valentino"
                    },
                    {
                        "name": "André Freitas"
                    }
                ],
                "author_detail": {
                    "name": "André Freitas"
                },
                "author": "André Freitas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10222v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10222v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06806v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06806v4",
                "updated": "2025-09-12T13:11:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    11,
                    51,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-08T15:38:31Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    38,
                    31,
                    0,
                    251,
                    0
                ],
                "title": "MachineLearningLM: Scaling Many-shot In-context Learning via Continued\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MachineLearningLM: Scaling Many-shot In-context Learning via Continued\n  Pretraining"
                },
                "summary": "Large language models (LLMs) possess broad world knowledge and strong\ngeneral-purpose reasoning ability, yet they struggle to learn from many\nin-context examples on standard machine learning (ML) tasks, that is, to\nleverage many-shot demonstrations purely via in-context learning (ICL) without\ngradient descent. We introduce MachineLearningLM, a portable\ncontinued-pretraining framework that equips a general-purpose LLM with robust\nin-context ML capability while preserving its general knowledge and reasoning\nfor broader chat workflows.\n  Our pretraining procedure synthesizes ML tasks from millions of structural\ncausal models (SCMs), spanning shot counts up to 1,024. We begin with a\nrandom-forest teacher, distilling tree-based decision strategies into the LLM\nto strengthen robustness in numerical modeling. All tasks are serialized with a\ntoken-efficient prompt, enabling 3x to 6x more examples per context window and\ndelivering up to 50x amortized throughput via batch inference.\n  Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8),\nMachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an\naverage of about 15% on out-of-distribution tabular classification across\nfinance, physics, biology, and healthcare domains. It exhibits a striking\nmany-shot scaling law: accuracy increases monotonically as in-context\ndemonstrations grow from 8 to 1,024. Without any task-specific training, it\nattains random-forest-level accuracy across hundreds of shots. General chat\ncapabilities, including knowledge and reasoning, are preserved: it achieves\n75.4% on MMLU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) possess broad world knowledge and strong\ngeneral-purpose reasoning ability, yet they struggle to learn from many\nin-context examples on standard machine learning (ML) tasks, that is, to\nleverage many-shot demonstrations purely via in-context learning (ICL) without\ngradient descent. We introduce MachineLearningLM, a portable\ncontinued-pretraining framework that equips a general-purpose LLM with robust\nin-context ML capability while preserving its general knowledge and reasoning\nfor broader chat workflows.\n  Our pretraining procedure synthesizes ML tasks from millions of structural\ncausal models (SCMs), spanning shot counts up to 1,024. We begin with a\nrandom-forest teacher, distilling tree-based decision strategies into the LLM\nto strengthen robustness in numerical modeling. All tasks are serialized with a\ntoken-efficient prompt, enabling 3x to 6x more examples per context window and\ndelivering up to 50x amortized throughput via batch inference.\n  Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8),\nMachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an\naverage of about 15% on out-of-distribution tabular classification across\nfinance, physics, biology, and healthcare domains. It exhibits a striking\nmany-shot scaling law: accuracy increases monotonically as in-context\ndemonstrations grow from 8 to 1,024. Without any task-specific training, it\nattains random-forest-level accuracy across hundreds of shots. General chat\ncapabilities, including knowledge and reasoning, are preserved: it achieves\n75.4% on MMLU."
                },
                "authors": [
                    {
                        "name": "Haoyu Dong"
                    },
                    {
                        "name": "Pengkun Zhang"
                    },
                    {
                        "name": "Mingzhe Lu"
                    },
                    {
                        "name": "Yanzhen Shen"
                    },
                    {
                        "name": "Guolin Ke"
                    }
                ],
                "author_detail": {
                    "name": "Guolin Ke"
                },
                "author": "Guolin Ke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06806v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06806v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10216v1",
                "updated": "2025-09-12T13:08:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    8,
                    50,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T13:08:50Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    8,
                    50,
                    4,
                    255,
                    0
                ],
                "title": "RFSeek and Ye Shall Find",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RFSeek and Ye Shall Find"
                },
                "summary": "Requests for Comments (RFCs) are extensive specification documents for\nnetwork protocols, but their prose-based format and their considerable length\noften impede precise operational understanding. We present RFSeek, an\ninteractive tool that automatically extracts visual summaries of protocol logic\nfrom RFCs. RFSeek leverages large language models (LLMs) to generate\nprovenance-linked, explorable diagrams, surfacing both official state machines\nand additional logic found only in the RFC text. Compared to existing RFC\nvisualizations, RFSeek's visual summaries are more transparent and easier to\naudit against their textual source. We showcase the tool's potential through a\nseries of use cases, including guided knowledge extraction and semantic\ndiffing, applied to protocols such as TCP, QUIC, PPTP, and DCCP.\n  In practice, RFSeek not only reconstructs the RFC diagrams included in some\nspecifications, but, more interestingly, also uncovers important logic such as\nnodes or edges described in the text but missing from those diagrams. RFSeek\nfurther derives new visualization diagrams for complex RFCs, with QUIC as a\nrepresentative case. Our approach, which we term \\emph{Summary Visualization},\nhighlights a promising direction: combining LLMs with formal, user-customized\nvisualizations to enhance protocol comprehension and support robust\nimplementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Requests for Comments (RFCs) are extensive specification documents for\nnetwork protocols, but their prose-based format and their considerable length\noften impede precise operational understanding. We present RFSeek, an\ninteractive tool that automatically extracts visual summaries of protocol logic\nfrom RFCs. RFSeek leverages large language models (LLMs) to generate\nprovenance-linked, explorable diagrams, surfacing both official state machines\nand additional logic found only in the RFC text. Compared to existing RFC\nvisualizations, RFSeek's visual summaries are more transparent and easier to\naudit against their textual source. We showcase the tool's potential through a\nseries of use cases, including guided knowledge extraction and semantic\ndiffing, applied to protocols such as TCP, QUIC, PPTP, and DCCP.\n  In practice, RFSeek not only reconstructs the RFC diagrams included in some\nspecifications, but, more interestingly, also uncovers important logic such as\nnodes or edges described in the text but missing from those diagrams. RFSeek\nfurther derives new visualization diagrams for complex RFCs, with QUIC as a\nrepresentative case. Our approach, which we term \\emph{Summary Visualization},\nhighlights a promising direction: combining LLMs with formal, user-customized\nvisualizations to enhance protocol comprehension and support robust\nimplementations."
                },
                "authors": [
                    {
                        "name": "Noga H. Rotman"
                    },
                    {
                        "name": "Tiago Ferreira"
                    },
                    {
                        "name": "Hila Peleg"
                    },
                    {
                        "name": "Mark Silberstein"
                    },
                    {
                        "name": "Alexandra Silva"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Silva"
                },
                "author": "Alexandra Silva",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10210v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10210v1",
                "updated": "2025-09-12T12:56:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    12,
                    56,
                    47,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T12:56:47Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    12,
                    56,
                    47,
                    4,
                    255,
                    0
                ],
                "title": "Towards Fully Automated Molecular Simulations: Multi-Agent Framework for\n  Simulation Setup and Force Field Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Fully Automated Molecular Simulations: Multi-Agent Framework for\n  Simulation Setup and Force Field Extraction"
                },
                "summary": "Automated characterization of porous materials has the potential to\naccelerate materials discovery, but it remains limited by the complexity of\nsimulation setup and force field selection. We propose a multi-agent framework\nin which LLM-based agents can autonomously understand a characterization task,\nplan appropriate simulations, assemble relevant force fields, execute them and\ninterpret their results to guide subsequent steps. As a first step toward this\nvision, we present a multi-agent system for literature-informed force field\nextraction and automated RASPA simulation setup. Initial evaluations\ndemonstrate high correctness and reproducibility, highlighting this approach's\npotential to enable fully autonomous, scalable materials characterization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated characterization of porous materials has the potential to\naccelerate materials discovery, but it remains limited by the complexity of\nsimulation setup and force field selection. We propose a multi-agent framework\nin which LLM-based agents can autonomously understand a characterization task,\nplan appropriate simulations, assemble relevant force fields, execute them and\ninterpret their results to guide subsequent steps. As a first step toward this\nvision, we present a multi-agent system for literature-informed force field\nextraction and automated RASPA simulation setup. Initial evaluations\ndemonstrate high correctness and reproducibility, highlighting this approach's\npotential to enable fully autonomous, scalable materials characterization."
                },
                "authors": [
                    {
                        "name": "Marko Petković"
                    },
                    {
                        "name": "Vlado Menkovski"
                    },
                    {
                        "name": "Sofía Calero"
                    }
                ],
                "author_detail": {
                    "name": "Sofía Calero"
                },
                "author": "Sofía Calero",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10210v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10210v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10208v1",
                "updated": "2025-09-12T12:56:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    12,
                    56,
                    14,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T12:56:14Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    12,
                    56,
                    14,
                    4,
                    255,
                    0
                ],
                "title": "SI-FACT: Mitigating Knowledge Conflict via Self-Improving\n  Faithfulness-Aware Contrastive Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SI-FACT: Mitigating Knowledge Conflict via Self-Improving\n  Faithfulness-Aware Contrastive Tuning"
                },
                "summary": "Large Language Models often generate unfaithful responses in knowledge\nintensive tasks due to knowledge conflict,that is,a preference for relying on\ninternal parametric knowledge rather than the provided context.To address this\nissue,we propose a novel self improving framework,Self Improving Faithfulness\nAware Contrastive Tuning.The framework uses a self instruct mechanism that\nallows the base LLM to automatically generate high quality,structured\ncontrastive learning data,including anchor samples,semantically equivalent\npositive samples,and negative samples simulating unfaithful scenarios.This\napproach significantly reduces the cost of manual\nannotation.Subsequently,contrastive learning is applied to train the\nmodel,enabling it to pull faithful responses closer and push unfaithful\nresponses farther apart in the representation space.Experiments on knowledge\nconflict evaluation benchmarks ECARE KRE and COSE KRE show that the SI FACT\nmodel based on Llama3 8B Instruct improves the Contextual Recall Rate by 6.2%\nover the best baseline method,while significantly reducing dependence on\ninternal memory.The results indicate that SI FACT provides strong effectiveness\nand high data efficiency in enhancing the contextual faithfulness of\nLLMs,offering a practical pathway toward building more proactive and\ntrustworthy language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models often generate unfaithful responses in knowledge\nintensive tasks due to knowledge conflict,that is,a preference for relying on\ninternal parametric knowledge rather than the provided context.To address this\nissue,we propose a novel self improving framework,Self Improving Faithfulness\nAware Contrastive Tuning.The framework uses a self instruct mechanism that\nallows the base LLM to automatically generate high quality,structured\ncontrastive learning data,including anchor samples,semantically equivalent\npositive samples,and negative samples simulating unfaithful scenarios.This\napproach significantly reduces the cost of manual\nannotation.Subsequently,contrastive learning is applied to train the\nmodel,enabling it to pull faithful responses closer and push unfaithful\nresponses farther apart in the representation space.Experiments on knowledge\nconflict evaluation benchmarks ECARE KRE and COSE KRE show that the SI FACT\nmodel based on Llama3 8B Instruct improves the Contextual Recall Rate by 6.2%\nover the best baseline method,while significantly reducing dependence on\ninternal memory.The results indicate that SI FACT provides strong effectiveness\nand high data efficiency in enhancing the contextual faithfulness of\nLLMs,offering a practical pathway toward building more proactive and\ntrustworthy language models."
                },
                "authors": [
                    {
                        "name": "Shengqiang Fu"
                    }
                ],
                "author_detail": {
                    "name": "Shengqiang Fu"
                },
                "author": "Shengqiang Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07983v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07983v2",
                "updated": "2025-09-12T12:39:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    12,
                    39,
                    45,
                    4,
                    255,
                    0
                ],
                "published": "2025-07-01T16:03:55Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    16,
                    3,
                    55,
                    1,
                    182,
                    0
                ],
                "title": "Steering Protein Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering Protein Language Models"
                },
                "summary": "Protein Language Models (PLMs), pre-trained on extensive evolutionary data\nfrom natural proteins, have emerged as indispensable tools for protein design.\nWhile powerful, PLMs often struggle to produce proteins with precisely\nspecified functionalities or properties due to inherent challenges in\ncontrolling their outputs. In this work, we investigate the potential of\nActivation Steering, a technique originally developed for controlling text\ngeneration in Large Language Models (LLMs), to direct PLMs toward generating\nprotein sequences with targeted properties. We propose a simple yet effective\nmethod that employs activation editing to steer PLM outputs, and extend this\napproach to protein optimization through a novel editing site identification\nmodule. Through comprehensive experiments on lysozyme-like sequence generation\nand optimization, we demonstrate that our methods can be seamlessly integrated\ninto both auto-encoding and autoregressive PLMs without requiring additional\ntraining. These results highlight a promising direction for precise protein\nengineering using foundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protein Language Models (PLMs), pre-trained on extensive evolutionary data\nfrom natural proteins, have emerged as indispensable tools for protein design.\nWhile powerful, PLMs often struggle to produce proteins with precisely\nspecified functionalities or properties due to inherent challenges in\ncontrolling their outputs. In this work, we investigate the potential of\nActivation Steering, a technique originally developed for controlling text\ngeneration in Large Language Models (LLMs), to direct PLMs toward generating\nprotein sequences with targeted properties. We propose a simple yet effective\nmethod that employs activation editing to steer PLM outputs, and extend this\napproach to protein optimization through a novel editing site identification\nmodule. Through comprehensive experiments on lysozyme-like sequence generation\nand optimization, we demonstrate that our methods can be seamlessly integrated\ninto both auto-encoding and autoregressive PLMs without requiring additional\ntraining. These results highlight a promising direction for precise protein\nengineering using foundation models."
                },
                "authors": [
                    {
                        "name": "Long-Kai Huang"
                    },
                    {
                        "name": "Rongyi Zhu"
                    },
                    {
                        "name": "Bing He"
                    },
                    {
                        "name": "Jianhua Yao"
                    }
                ],
                "author_detail": {
                    "name": "Jianhua Yao"
                },
                "author": "Jianhua Yao",
                "arxiv_comment": "Accepted to ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07983v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07983v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10184v1",
                "updated": "2025-09-12T12:25:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    12,
                    25,
                    2,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T12:25:02Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    12,
                    25,
                    2,
                    4,
                    255,
                    0
                ],
                "title": "Incongruent Positivity: When Miscalibrated Positivity Undermines Online\n  Supportive Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incongruent Positivity: When Miscalibrated Positivity Undermines Online\n  Supportive Conversations"
                },
                "summary": "In emotionally supportive conversations, well-intended positivity can\nsometimes misfire, leading to responses that feel dismissive, minimizing, or\nunrealistically optimistic. We examine this phenomenon of incongruent\npositivity as miscalibrated expressions of positive support in both human and\nLLM generated responses. To this end, we collected real user-assistant\ndialogues from Reddit across a range of emotional intensities and generated\nadditional responses using large language models for the same context. We\ncategorize these conversations by intensity into two levels: Mild, which covers\nrelationship tension and general advice, and Severe, which covers grief and\nanxiety conversations. This level of categorization enables a comparative\nanalysis of how supportive responses vary across lower and higher stakes\ncontexts. Our analysis reveals that LLMs are more prone to unrealistic\npositivity through dismissive and minimizing tone, particularly in high-stakes\ncontexts. To further study the underlying dimensions of this phenomenon, we\nfinetune LLMs on datasets with strong and weak emotional reactions. Moreover,\nwe developed a weakly supervised multilabel classifier ensemble (DeBERTa and\nMentalBERT) that shows improved detection of incongruent positivity types\nacross two sorts of concerns (Mild and Severe). Our findings shed light on the\nneed to move beyond merely generating generic positive responses and instead\nstudy the congruent support measures to balance positive affect with emotional\nacknowledgment. This approach offers insights into aligning large language\nmodels with affective expectations in the online supportive dialogue, paving\nthe way toward context-aware and trust preserving online conversation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In emotionally supportive conversations, well-intended positivity can\nsometimes misfire, leading to responses that feel dismissive, minimizing, or\nunrealistically optimistic. We examine this phenomenon of incongruent\npositivity as miscalibrated expressions of positive support in both human and\nLLM generated responses. To this end, we collected real user-assistant\ndialogues from Reddit across a range of emotional intensities and generated\nadditional responses using large language models for the same context. We\ncategorize these conversations by intensity into two levels: Mild, which covers\nrelationship tension and general advice, and Severe, which covers grief and\nanxiety conversations. This level of categorization enables a comparative\nanalysis of how supportive responses vary across lower and higher stakes\ncontexts. Our analysis reveals that LLMs are more prone to unrealistic\npositivity through dismissive and minimizing tone, particularly in high-stakes\ncontexts. To further study the underlying dimensions of this phenomenon, we\nfinetune LLMs on datasets with strong and weak emotional reactions. Moreover,\nwe developed a weakly supervised multilabel classifier ensemble (DeBERTa and\nMentalBERT) that shows improved detection of incongruent positivity types\nacross two sorts of concerns (Mild and Severe). Our findings shed light on the\nneed to move beyond merely generating generic positive responses and instead\nstudy the congruent support measures to balance positive affect with emotional\nacknowledgment. This approach offers insights into aligning large language\nmodels with affective expectations in the online supportive dialogue, paving\nthe way toward context-aware and trust preserving online conversation systems."
                },
                "authors": [
                    {
                        "name": "Leen Almajed"
                    },
                    {
                        "name": "Abeer ALdayel"
                    }
                ],
                "author_detail": {
                    "name": "Abeer ALdayel"
                },
                "author": "Abeer ALdayel",
                "arxiv_comment": "This paper is under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10179v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10179v1",
                "updated": "2025-09-12T12:12:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    12,
                    12,
                    20,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T12:12:20Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    12,
                    12,
                    20,
                    4,
                    255,
                    0
                ],
                "title": "Benchmark of stylistic variation in LLM-generated texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmark of stylistic variation in LLM-generated texts"
                },
                "summary": "This study investigates the register variation in texts written by humans and\ncomparable texts produced by large language models (LLMs). Biber's\nmultidimensional analysis (MDA) is applied to a sample of human-written texts\nand AI-created texts generated to be their counterparts to find the dimensions\nof variation in which LLMs differ most significantly and most systematically\nfrom humans. As textual material, a new LLM-generated corpus AI-Brown is used,\nwhich is comparable to BE-21 (a Brown family corpus representing contemporary\nBritish English). Since all languages except English are underrepresented in\nthe training data of frontier LLMs, similar analysis is replicated on Czech\nusing AI-Koditex corpus and Czech multidimensional model. Examined were 16\nfrontier models in various settings and prompts, with emphasis placed on the\ndifference between base models and instruction-tuned models. Based on this, a\nbenchmark is created through which models can be compared with each other and\nranked in interpretable dimensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the register variation in texts written by humans and\ncomparable texts produced by large language models (LLMs). Biber's\nmultidimensional analysis (MDA) is applied to a sample of human-written texts\nand AI-created texts generated to be their counterparts to find the dimensions\nof variation in which LLMs differ most significantly and most systematically\nfrom humans. As textual material, a new LLM-generated corpus AI-Brown is used,\nwhich is comparable to BE-21 (a Brown family corpus representing contemporary\nBritish English). Since all languages except English are underrepresented in\nthe training data of frontier LLMs, similar analysis is replicated on Czech\nusing AI-Koditex corpus and Czech multidimensional model. Examined were 16\nfrontier models in various settings and prompts, with emphasis placed on the\ndifference between base models and instruction-tuned models. Based on this, a\nbenchmark is created through which models can be compared with each other and\nranked in interpretable dimensions."
                },
                "authors": [
                    {
                        "name": "Jiří Milička"
                    },
                    {
                        "name": "Anna Marklová"
                    },
                    {
                        "name": "Václav Cvrček"
                    }
                ],
                "author_detail": {
                    "name": "Václav Cvrček"
                },
                "author": "Václav Cvrček",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10179v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01340v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01340v3",
                "updated": "2025-09-12T12:10:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    12,
                    10,
                    6,
                    4,
                    255,
                    0
                ],
                "published": "2024-12-02T10:07:01Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    10,
                    7,
                    1,
                    0,
                    337,
                    0
                ],
                "title": "A 2-step Framework for Automated Literary Translation Evaluation: Its\n  Promises and Pitfalls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A 2-step Framework for Automated Literary Translation Evaluation: Its\n  Promises and Pitfalls"
                },
                "summary": "In this work, we propose and evaluate the feasibility of a two-stage pipeline\nto evaluate literary machine translation, in a fine-grained manner, from\nEnglish to Korean. The results show that our framework provides fine-grained,\ninterpretable metrics suited for literary translation and obtains a higher\ncorrelation with human judgment than traditional machine translation metrics.\nNonetheless, it still fails to match inter-human agreement, especially in\nmetrics like Korean Honorifics. We also observe that LLMs tend to favor\ntranslations generated by other LLMs, and we highlight the necessity of\ndeveloping more sophisticated evaluation methods to ensure accurate and\nculturally sensitive machine translation of literary works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose and evaluate the feasibility of a two-stage pipeline\nto evaluate literary machine translation, in a fine-grained manner, from\nEnglish to Korean. The results show that our framework provides fine-grained,\ninterpretable metrics suited for literary translation and obtains a higher\ncorrelation with human judgment than traditional machine translation metrics.\nNonetheless, it still fails to match inter-human agreement, especially in\nmetrics like Korean Honorifics. We also observe that LLMs tend to favor\ntranslations generated by other LLMs, and we highlight the necessity of\ndeveloping more sophisticated evaluation methods to ensure accurate and\nculturally sensitive machine translation of literary works."
                },
                "authors": [
                    {
                        "name": "Sheikh Shafayat"
                    },
                    {
                        "name": "Dongkeun Yoon"
                    },
                    {
                        "name": "Woori Jang"
                    },
                    {
                        "name": "Jiwoo Choi"
                    },
                    {
                        "name": "Alice Oh"
                    },
                    {
                        "name": "Seohyon Jung"
                    }
                ],
                "author_detail": {
                    "name": "Seohyon Jung"
                },
                "author": "Seohyon Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01340v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01340v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07531v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07531v2",
                "updated": "2025-09-12T12:03:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    12,
                    3,
                    22,
                    4,
                    255,
                    0
                ],
                "published": "2025-05-12T13:13:06Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    13,
                    13,
                    6,
                    0,
                    132,
                    0
                ],
                "title": "QuantX: A Framework for Hardware-Aware Quantization of Generative AI\n  Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuantX: A Framework for Hardware-Aware Quantization of Generative AI\n  Workloads"
                },
                "summary": "We present QuantX: a tailored suite of recipes for LLM and VLM quantization.\nIt is capable of quantizing down to 3-bit resolutions with minimal loss in\nperformance. The quantization strategies in QuantX take into account\nhardware-specific constraints to achieve efficient dequantization during\ninference ensuring flexible trade-off between runtime speed, memory requirement\nand model accuracy. Our results demonstrate that QuantX achieves performance\nwithin 6% of the unquantized model for LlaVa-v1.6 quantized down to 3-bits for\nmultiple end user tasks and outperforms recently published state-of-the-art\nquantization techniques. We further integrate one particular technique from\nQuantX into the popular Llama.cpp framework and show its feasibility in terms\nof runtime compared to the mainstream quantization techniques from Llama.cpp.\nLastly, this manuscript provides insights into the LLM quantization process\nthat motivated the range of recipes and options that are incorporated in\nQuantX.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present QuantX: a tailored suite of recipes for LLM and VLM quantization.\nIt is capable of quantizing down to 3-bit resolutions with minimal loss in\nperformance. The quantization strategies in QuantX take into account\nhardware-specific constraints to achieve efficient dequantization during\ninference ensuring flexible trade-off between runtime speed, memory requirement\nand model accuracy. Our results demonstrate that QuantX achieves performance\nwithin 6% of the unquantized model for LlaVa-v1.6 quantized down to 3-bits for\nmultiple end user tasks and outperforms recently published state-of-the-art\nquantization techniques. We further integrate one particular technique from\nQuantX into the popular Llama.cpp framework and show its feasibility in terms\nof runtime compared to the mainstream quantization techniques from Llama.cpp.\nLastly, this manuscript provides insights into the LLM quantization process\nthat motivated the range of recipes and options that are incorporated in\nQuantX."
                },
                "authors": [
                    {
                        "name": "Muhammad Ahmad"
                    },
                    {
                        "name": "Khurram Mazher"
                    },
                    {
                        "name": "Saqib Akram"
                    },
                    {
                        "name": "Ahmad Tameem"
                    },
                    {
                        "name": "Saad Bin Nasir"
                    }
                ],
                "author_detail": {
                    "name": "Saad Bin Nasir"
                },
                "author": "Saad Bin Nasir",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07531v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07531v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12719v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12719v3",
                "updated": "2025-09-12T11:57:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    11,
                    57,
                    49,
                    4,
                    255,
                    0
                ],
                "published": "2025-04-17T07:48:50Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    7,
                    48,
                    50,
                    3,
                    107,
                    0
                ],
                "title": "B*: Efficient and Optimal Base Placement for Fixed-Base Manipulators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "B*: Efficient and Optimal Base Placement for Fixed-Base Manipulators"
                },
                "summary": "B* is a novel optimization framework that addresses a critical challenge in\nfixed-base manipulator robotics: optimal base placement. Current methods rely\non pre-computed kinematics databases generated through sampling to search for\nsolutions. However, they face an inherent trade-off between solution optimality\nand computational efficiency when determining sampling resolution. To address\nthese limitations, B* unifies multiple objectives without database dependence.\nThe framework employs a two-layer hierarchical approach. The outer layer\nsystematically manages terminal constraints through progressive tightening,\nparticularly for base mobility, enabling feasible initialization and broad\nsolution exploration. The inner layer addresses non-convexities in each\nouter-layer subproblem through sequential local linearization, converting the\noriginal problem into tractable sequential linear programming (SLP). Testing\nacross multiple robot platforms demonstrates B*'s effectiveness. The framework\nachieves solution optimality five orders of magnitude better than\nsampling-based approaches while maintaining perfect success rates and reduced\ncomputational overhead. Operating directly in configuration space, B* enables\nsimultaneous path planning with customizable optimization criteria. B* serves\nas a crucial initialization tool that bridges the gap between theoretical\nmotion planning and practical deployment, where feasible trajectory existence\nis fundamental.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "B* is a novel optimization framework that addresses a critical challenge in\nfixed-base manipulator robotics: optimal base placement. Current methods rely\non pre-computed kinematics databases generated through sampling to search for\nsolutions. However, they face an inherent trade-off between solution optimality\nand computational efficiency when determining sampling resolution. To address\nthese limitations, B* unifies multiple objectives without database dependence.\nThe framework employs a two-layer hierarchical approach. The outer layer\nsystematically manages terminal constraints through progressive tightening,\nparticularly for base mobility, enabling feasible initialization and broad\nsolution exploration. The inner layer addresses non-convexities in each\nouter-layer subproblem through sequential local linearization, converting the\noriginal problem into tractable sequential linear programming (SLP). Testing\nacross multiple robot platforms demonstrates B*'s effectiveness. The framework\nachieves solution optimality five orders of magnitude better than\nsampling-based approaches while maintaining perfect success rates and reduced\ncomputational overhead. Operating directly in configuration space, B* enables\nsimultaneous path planning with customizable optimization criteria. B* serves\nas a crucial initialization tool that bridges the gap between theoretical\nmotion planning and practical deployment, where feasible trajectory existence\nis fundamental."
                },
                "authors": [
                    {
                        "name": "Zihang Zhao"
                    },
                    {
                        "name": "Leiyao Cui"
                    },
                    {
                        "name": "Sirui Xie"
                    },
                    {
                        "name": "Saiyao Zhang"
                    },
                    {
                        "name": "Zhi Han"
                    },
                    {
                        "name": "Lecheng Ruan"
                    },
                    {
                        "name": "Yixin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yixin Zhu"
                },
                "author": "Yixin Zhu",
                "arxiv_doi": "10.1109/LRA.2025.3604741",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/LRA.2025.3604741",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.12719v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12719v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "accepted for publication in the IEEE Robotics and Automation Letters\n  (RA-L)",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16913v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16913v2",
                "updated": "2025-09-12T11:56:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    11,
                    56,
                    27,
                    4,
                    255,
                    0
                ],
                "published": "2025-03-21T07:23:26Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    7,
                    23,
                    26,
                    4,
                    80,
                    0
                ],
                "title": "FGIT: Fault-Guided Fine-Tuning for Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FGIT: Fault-Guided Fine-Tuning for Code Generation"
                },
                "summary": "Modern instruction-tuned large language models (LLMs) have made remarkable\nprogress in code generation. However, these LLMs fine-tuned with standard\nsupervised fine-tuning (SFT) sometimes generate plausible-looking but\nfunctionally incorrect code variants. This issue likely stems from the\nlimitation of standard SFT, which treats all tokens equally during optimization\nand fails to emphasize the error-sensitive segments-specific code differences\nbetween correct implementations and similar incorrect variants. To address this\nproblem, we propose Fault-Guided Fine-Tuning (FGIT), a novel fine-tuning\ntechnique that enhances LLMs' code generation by (1) extracting\nmulti-granularity (line/token-level) differences between correct and incorrect\nyet similar implementations to identify error-sensitive segments, and (2)\ndynamically prioritizing those segments during training via dynamic loss\nweighting. Through extensive experiments on seven LLMs across three widely-used\nbenchmarks, our method achieves an average relative improvement of 6.9% on\npass@1 with some enhanced 6.7B LLMs outperforming closed-source models, e.g.,\nGPT-3.5-Turbo. Furthermore, our fine-tuning technique demonstrates strong\ngeneralization with performance improvements ranging from 3.8% to 19.1% across\ndiverse instruction-tuned LLMs, and our ablation studies confirm the\ncontributions of different granularities of differences and hyperparameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern instruction-tuned large language models (LLMs) have made remarkable\nprogress in code generation. However, these LLMs fine-tuned with standard\nsupervised fine-tuning (SFT) sometimes generate plausible-looking but\nfunctionally incorrect code variants. This issue likely stems from the\nlimitation of standard SFT, which treats all tokens equally during optimization\nand fails to emphasize the error-sensitive segments-specific code differences\nbetween correct implementations and similar incorrect variants. To address this\nproblem, we propose Fault-Guided Fine-Tuning (FGIT), a novel fine-tuning\ntechnique that enhances LLMs' code generation by (1) extracting\nmulti-granularity (line/token-level) differences between correct and incorrect\nyet similar implementations to identify error-sensitive segments, and (2)\ndynamically prioritizing those segments during training via dynamic loss\nweighting. Through extensive experiments on seven LLMs across three widely-used\nbenchmarks, our method achieves an average relative improvement of 6.9% on\npass@1 with some enhanced 6.7B LLMs outperforming closed-source models, e.g.,\nGPT-3.5-Turbo. Furthermore, our fine-tuning technique demonstrates strong\ngeneralization with performance improvements ranging from 3.8% to 19.1% across\ndiverse instruction-tuned LLMs, and our ablation studies confirm the\ncontributions of different granularities of differences and hyperparameters."
                },
                "authors": [
                    {
                        "name": "Lishui Fan"
                    },
                    {
                        "name": "Zhongxin Liu"
                    },
                    {
                        "name": "Haoye Wang"
                    },
                    {
                        "name": "Lingfeng Bao"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "Shanping Li"
                    }
                ],
                "author_detail": {
                    "name": "Shanping Li"
                },
                "author": "Shanping Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16913v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16913v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15546v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15546v3",
                "updated": "2025-09-12T11:24:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    11,
                    24,
                    8,
                    4,
                    255,
                    0
                ],
                "published": "2025-04-22T02:52:08Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    2,
                    52,
                    8,
                    1,
                    112,
                    0
                ],
                "title": "A Framework for Testing and Adapting REST APIs as LLM Tools",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Testing and Adapting REST APIs as LLM Tools"
                },
                "summary": "Large Language Models (LLMs) are increasingly used to build autonomous agents\nthat perform complex tasks with external tools, often exposed through APIs in\nenterprise systems. Direct use of these APIs is difficult due to the complex\ninput schema and verbose responses. Current benchmarks overlook these\nchallenges, leaving a gap in assessing API readiness for agent-driven\nautomation. We present a testing framework that systematically evaluates\nenterprise APIs when wrapped as Python tools for LLM-based agents. The\nframework generates data-aware test cases, translates them into natural\nlanguage instructions, and evaluates whether agents can correctly invoke the\ntool, handle their inputs, and process its responses. We apply the framework to\ngenerate over 2400 test cases across different domains and develop a taxonomy\nof common errors, including input misinterpretation, output failures, and\nschema mismatches. We further classify errors to support debugging and tool\nrefinement. Our framework provides a systematic approach to enabling enterprise\nAPIs as reliable tools for agent-based applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used to build autonomous agents\nthat perform complex tasks with external tools, often exposed through APIs in\nenterprise systems. Direct use of these APIs is difficult due to the complex\ninput schema and verbose responses. Current benchmarks overlook these\nchallenges, leaving a gap in assessing API readiness for agent-driven\nautomation. We present a testing framework that systematically evaluates\nenterprise APIs when wrapped as Python tools for LLM-based agents. The\nframework generates data-aware test cases, translates them into natural\nlanguage instructions, and evaluates whether agents can correctly invoke the\ntool, handle their inputs, and process its responses. We apply the framework to\ngenerate over 2400 test cases across different domains and develop a taxonomy\nof common errors, including input misinterpretation, output failures, and\nschema mismatches. We further classify errors to support debugging and tool\nrefinement. Our framework provides a systematic approach to enabling enterprise\nAPIs as reliable tools for agent-based applications."
                },
                "authors": [
                    {
                        "name": "Jayachandu Bandlamudi"
                    },
                    {
                        "name": "Ritwik Chaudhuri"
                    },
                    {
                        "name": "Neelamadhav Gantayat"
                    },
                    {
                        "name": "Sambit Ghosh"
                    },
                    {
                        "name": "Kushal Mukherjee"
                    },
                    {
                        "name": "Prerna Agarwal"
                    },
                    {
                        "name": "Renuka Sindhgatta"
                    },
                    {
                        "name": "Sameep Mehta"
                    }
                ],
                "author_detail": {
                    "name": "Sameep Mehta"
                },
                "author": "Sameep Mehta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15546v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15546v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01607v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01607v3",
                "updated": "2025-09-12T11:19:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    11,
                    19,
                    56,
                    4,
                    255,
                    0
                ],
                "published": "2025-07-02T11:21:27Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    11,
                    21,
                    27,
                    2,
                    183,
                    0
                ],
                "title": "Survivability of Backdoor Attacks on Unconstrained Face Recognition\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Survivability of Backdoor Attacks on Unconstrained Face Recognition\n  Systems"
                },
                "summary": "The widespread deployment of Deep Learning-based Face Recognition Systems\nraises multiple security concerns. While prior research has identified backdoor\nvulnerabilities on isolated components, Backdoor Attacks on real-world,\nunconstrained pipelines remain underexplored. This paper presents the first\ncomprehensive system-level analysis of Backdoor Attacks targeting Face\nRecognition Systems and provides three contributions. We first show that face\nfeature extractors trained with large margin metric learning losses are\nsusceptible to Backdoor Attacks. By analyzing 20 pipeline configurations and 15\nattack scenarios, we then reveal that a single backdoor can compromise an\nentire Face Recognition System. Finally, we propose effective best practices\nand countermeasures for stakeholders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread deployment of Deep Learning-based Face Recognition Systems\nraises multiple security concerns. While prior research has identified backdoor\nvulnerabilities on isolated components, Backdoor Attacks on real-world,\nunconstrained pipelines remain underexplored. This paper presents the first\ncomprehensive system-level analysis of Backdoor Attacks targeting Face\nRecognition Systems and provides three contributions. We first show that face\nfeature extractors trained with large margin metric learning losses are\nsusceptible to Backdoor Attacks. By analyzing 20 pipeline configurations and 15\nattack scenarios, we then reveal that a single backdoor can compromise an\nentire Face Recognition System. Finally, we propose effective best practices\nand countermeasures for stakeholders."
                },
                "authors": [
                    {
                        "name": "Quentin Le Roux"
                    },
                    {
                        "name": "Yannick Teglia"
                    },
                    {
                        "name": "Teddy Furon"
                    },
                    {
                        "name": "Philippe Loubet-Moundi"
                    },
                    {
                        "name": "Eric Bourbao"
                    }
                ],
                "author_detail": {
                    "name": "Eric Bourbao"
                },
                "author": "Eric Bourbao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01607v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01607v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05367v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05367v2",
                "updated": "2025-09-12T11:05:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    11,
                    5,
                    8,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-04T05:53:20Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    5,
                    53,
                    20,
                    3,
                    247,
                    0
                ],
                "title": "Between a Rock and a Hard Place: Exploiting Ethical Reasoning to\n  Jailbreak LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Between a Rock and a Hard Place: Exploiting Ethical Reasoning to\n  Jailbreak LLMs"
                },
                "summary": "Large language models (LLMs) have undergone safety alignment efforts to\nmitigate harmful outputs. However, as LLMs become more sophisticated in\nreasoning, their intelligence may introduce new security risks. While\ntraditional jailbreak attacks relied on singlestep attacks, multi-turn\njailbreak strategies that adapt dynamically to context remain underexplored. In\nthis work, we introduce TRIAL (Trolley-problem Reasoning for Interactive Attack\nLogic), a framework that leverages LLMs ethical reasoning to bypass their\nsafeguards. TRIAL embeds adversarial goals within ethical dilemmas modeled on\nthe trolley problem. TRIAL demonstrates high jailbreak success rates towards\nboth open and close-source models. Our findings underscore a fundamental\nlimitation in AI safety: as models gain advanced reasoning abilities, the\nnature of their alignment may inadvertently allow for more covert security\nvulnerabilities to be exploited. TRIAL raises an urgent need in reevaluating\nsafety alignment oversight strategies, as current safeguards may prove\ninsufficient against context-aware adversarial attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have undergone safety alignment efforts to\nmitigate harmful outputs. However, as LLMs become more sophisticated in\nreasoning, their intelligence may introduce new security risks. While\ntraditional jailbreak attacks relied on singlestep attacks, multi-turn\njailbreak strategies that adapt dynamically to context remain underexplored. In\nthis work, we introduce TRIAL (Trolley-problem Reasoning for Interactive Attack\nLogic), a framework that leverages LLMs ethical reasoning to bypass their\nsafeguards. TRIAL embeds adversarial goals within ethical dilemmas modeled on\nthe trolley problem. TRIAL demonstrates high jailbreak success rates towards\nboth open and close-source models. Our findings underscore a fundamental\nlimitation in AI safety: as models gain advanced reasoning abilities, the\nnature of their alignment may inadvertently allow for more covert security\nvulnerabilities to be exploited. TRIAL raises an urgent need in reevaluating\nsafety alignment oversight strategies, as current safeguards may prove\ninsufficient against context-aware adversarial attack."
                },
                "authors": [
                    {
                        "name": "Shei Pern Chua"
                    },
                    {
                        "name": "Zhen Leng Thai"
                    },
                    {
                        "name": "Teh Kai Jun"
                    },
                    {
                        "name": "Xiao Li"
                    },
                    {
                        "name": "Xiaolin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaolin Hu"
                },
                "author": "Xiaolin Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05367v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05367v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08578v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08578v2",
                "updated": "2025-09-12T11:02:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    11,
                    2,
                    37,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-10T13:27:40Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    13,
                    27,
                    40,
                    2,
                    253,
                    0
                ],
                "title": "MAESTRO: Multi-modal Adaptive Estimation for Temporal Respiratory\n  Disease Outbreak",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAESTRO: Multi-modal Adaptive Estimation for Temporal Respiratory\n  Disease Outbreak"
                },
                "summary": "Timely and robust influenza incidence forecasting is critical for public\nhealth decision-making. This paper presents MAESTRO (Multi-modal Adaptive\nEstimation for Temporal Respiratory Disease Outbreak), a novel, unified\nframework that synergistically integrates advanced spectro-temporal modeling\nwith multi-modal data fusion, including surveillance, web search trends, and\nmeteorological data. By adaptively weighting heterogeneous data sources and\ndecomposing complex time series patterns, the model achieves robust and\naccurate forecasts. Evaluated on over 11 years of Hong Kong influenza data\n(excluding the COVID-19 period), MAESTRO demonstrates state-of-the-art\nperformance, achieving a superior model fit with an R-square of 0.956.\nExtensive ablations confirm the significant contributions of its multi-modal\nand spectro-temporal components. The modular and reproducible pipeline is made\npublicly available to facilitate deployment and extension to other regions and\npathogens, presenting a powerful tool for epidemiological forecasting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timely and robust influenza incidence forecasting is critical for public\nhealth decision-making. This paper presents MAESTRO (Multi-modal Adaptive\nEstimation for Temporal Respiratory Disease Outbreak), a novel, unified\nframework that synergistically integrates advanced spectro-temporal modeling\nwith multi-modal data fusion, including surveillance, web search trends, and\nmeteorological data. By adaptively weighting heterogeneous data sources and\ndecomposing complex time series patterns, the model achieves robust and\naccurate forecasts. Evaluated on over 11 years of Hong Kong influenza data\n(excluding the COVID-19 period), MAESTRO demonstrates state-of-the-art\nperformance, achieving a superior model fit with an R-square of 0.956.\nExtensive ablations confirm the significant contributions of its multi-modal\nand spectro-temporal components. The modular and reproducible pipeline is made\npublicly available to facilitate deployment and extension to other regions and\npathogens, presenting a powerful tool for epidemiological forecasting."
                },
                "authors": [
                    {
                        "name": "Hong Liu"
                    },
                    {
                        "name": "Kerui Cen"
                    },
                    {
                        "name": "Yanxing Chen"
                    },
                    {
                        "name": "Zige Liu"
                    },
                    {
                        "name": "Dong Chen"
                    },
                    {
                        "name": "Zifeng Yang"
                    },
                    {
                        "name": "Chitin Hon"
                    }
                ],
                "author_detail": {
                    "name": "Chitin Hon"
                },
                "author": "Chitin Hon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08578v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08578v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03900v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03900v2",
                "updated": "2025-09-12T10:44:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    10,
                    44,
                    36,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-04T05:48:54Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    5,
                    48,
                    54,
                    3,
                    247,
                    0
                ],
                "title": "The Auth Shim: A Lightweight Architectural Pattern for Integrating\n  Enterprise SSO with Standalone Open-Source Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Auth Shim: A Lightweight Architectural Pattern for Integrating\n  Enterprise SSO with Standalone Open-Source Applications"
                },
                "summary": "Open-source software OSS is widely adopted in enterprise settings, but\nstandalone tools often lack native support for protocols like SAML or OIDC,\ncreating a critical security integration gap. This paper introduces and\nformalizes the Auth Shim, a lightweight architectural pattern designed to solve\nthis problem. The Auth Shim is a minimal, external proxy service that acts as a\ncompatibility layer, translating requests from an enterprise Identity Provider\nIdP into the native session management mechanism of a target application. A key\nprerequisite for this pattern is that the target application must expose a\nprogrammatic, secure administrative API. We present a case study of the\npattern's implementation at Adobe to integrate a popular OSS BI tool with Okta\nSAML, which enabled automated Role-Based Access Control RBAC via IAM group\nmapping and eliminated manual user provisioning. By defining its components,\ninteractions, and production deployment considerations, this paper provides a\nreusable, secure, and cost-effective blueprint for integrating any standalone\nOSS tool into an enterprise SSO ecosystem, thereby enabling organizations to\nembrace open-source innovation without compromising on security governance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-source software OSS is widely adopted in enterprise settings, but\nstandalone tools often lack native support for protocols like SAML or OIDC,\ncreating a critical security integration gap. This paper introduces and\nformalizes the Auth Shim, a lightweight architectural pattern designed to solve\nthis problem. The Auth Shim is a minimal, external proxy service that acts as a\ncompatibility layer, translating requests from an enterprise Identity Provider\nIdP into the native session management mechanism of a target application. A key\nprerequisite for this pattern is that the target application must expose a\nprogrammatic, secure administrative API. We present a case study of the\npattern's implementation at Adobe to integrate a popular OSS BI tool with Okta\nSAML, which enabled automated Role-Based Access Control RBAC via IAM group\nmapping and eliminated manual user provisioning. By defining its components,\ninteractions, and production deployment considerations, this paper provides a\nreusable, secure, and cost-effective blueprint for integrating any standalone\nOSS tool into an enterprise SSO ecosystem, thereby enabling organizations to\nembrace open-source innovation without compromising on security governance."
                },
                "authors": [
                    {
                        "name": "Yuvraj Agrawal"
                    }
                ],
                "author_detail": {
                    "name": "Yuvraj Agrawal"
                },
                "author": "Yuvraj Agrawal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03900v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03900v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10127v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10127v1",
                "updated": "2025-09-12T10:43:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    10,
                    43,
                    47,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T10:43:47Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    10,
                    43,
                    47,
                    4,
                    255,
                    0
                ],
                "title": "Population-Aligned Persona Generation for LLM-based Social Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Population-Aligned Persona Generation for LLM-based Social Simulation"
                },
                "summary": "Recent advances in large language models (LLMs) have enabled human-like\nsocial simulations at unprecedented scale and fidelity, offering new\nopportunities for computational social science. A key challenge, however, is\nthe construction of persona sets that authentically represent the diversity and\ndistribution of real-world populations. Most existing LLM-based social\nsimulation studies focus primarily on designing agentic frameworks and\nsimulation environments, often overlooking the complexities of persona\ngeneration and the potential biases introduced by unrepresentative persona\nsets. In this paper, we propose a systematic framework for synthesizing\nhigh-quality, population-aligned persona sets for LLM-driven social simulation.\nOur approach begins by leveraging LLMs to generate narrative personas from\nlong-term social media data, followed by rigorous quality assessment to filter\nout low-fidelity profiles. We then apply importance sampling to achieve global\nalignment with reference psychometric distributions, such as the Big Five\npersonality traits. To address the needs of specific simulation contexts, we\nfurther introduce a task-specific module that adapts the globally aligned\npersona set to targeted subpopulations. Extensive experiments demonstrate that\nour method significantly reduces population-level bias and enables accurate,\nflexible social simulation for a wide range of research and policy\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have enabled human-like\nsocial simulations at unprecedented scale and fidelity, offering new\nopportunities for computational social science. A key challenge, however, is\nthe construction of persona sets that authentically represent the diversity and\ndistribution of real-world populations. Most existing LLM-based social\nsimulation studies focus primarily on designing agentic frameworks and\nsimulation environments, often overlooking the complexities of persona\ngeneration and the potential biases introduced by unrepresentative persona\nsets. In this paper, we propose a systematic framework for synthesizing\nhigh-quality, population-aligned persona sets for LLM-driven social simulation.\nOur approach begins by leveraging LLMs to generate narrative personas from\nlong-term social media data, followed by rigorous quality assessment to filter\nout low-fidelity profiles. We then apply importance sampling to achieve global\nalignment with reference psychometric distributions, such as the Big Five\npersonality traits. To address the needs of specific simulation contexts, we\nfurther introduce a task-specific module that adapts the globally aligned\npersona set to targeted subpopulations. Extensive experiments demonstrate that\nour method significantly reduces population-level bias and enables accurate,\nflexible social simulation for a wide range of research and policy\napplications."
                },
                "authors": [
                    {
                        "name": "Zhengyu Hu"
                    },
                    {
                        "name": "Zheyuan Xiao"
                    },
                    {
                        "name": "Max Xiong"
                    },
                    {
                        "name": "Yuxuan Lei"
                    },
                    {
                        "name": "Tianfu Wang"
                    },
                    {
                        "name": "Jianxun Lian"
                    },
                    {
                        "name": "Kaize Ding"
                    },
                    {
                        "name": "Ziang Xiao"
                    },
                    {
                        "name": "Nicholas Jing Yuan"
                    },
                    {
                        "name": "Xing Xie"
                    }
                ],
                "author_detail": {
                    "name": "Xing Xie"
                },
                "author": "Xing Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10127v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10127v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03556v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03556v2",
                "updated": "2025-09-12T10:34:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    10,
                    34,
                    34,
                    4,
                    255,
                    0
                ],
                "published": "2025-03-05T14:44:53Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    14,
                    44,
                    53,
                    2,
                    64,
                    0
                ],
                "title": "Afford-X: Generalizable and Slim Affordance Reasoning for Task-oriented\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Afford-X: Generalizable and Slim Affordance Reasoning for Task-oriented\n  Manipulation"
                },
                "summary": "Object affordance reasoning, the ability to infer object functionalities\nbased on physical properties, is fundamental for task-oriented planning and\nactivities in both humans and Artificial Intelligence (AI). This capability,\nrequired for planning and executing daily activities in a task-oriented manner,\nrelies on commonsense knowledge of object physics and functionalities,\nextending beyond simple object recognition. Current computational models for\naffordance reasoning from perception lack generalizability, limiting their\napplicability in novel scenarios. Meanwhile, comprehensive Large Language\nModels (LLMs) with emerging reasoning capabilities are challenging to deploy on\nlocal devices for task-oriented manipulations. Here, we introduce LVIS-Aff, a\nlarge-scale dataset comprising 1,496 tasks and 119k images, designed to enhance\nthe generalizability of affordance reasoning from perception. Utilizing this\ndataset, we develop Afford-X, an end-to-end trainable affordance reasoning\nmodel that incorporates Verb Attention and Bi-Fusion modules to improve\nmulti-modal understanding. This model achieves up to a 12.1% performance\nimprovement over the best-reported results from non-LLM methods, while also\ndemonstrating a 1.2% enhancement compared to our previous conference paper.\nAdditionally, it maintains a compact 187M parameter size and infers nearly 50\ntimes faster than the GPT-4V API. Our work demonstrates the potential for\nefficient, generalizable affordance reasoning models that can be deployed on\nlocal devices for task-oriented manipulations. We showcase Afford-X's\neffectiveness in enabling task-oriented manipulations for robots across various\ntasks and environments, underscoring its efficiency and broad implications for\nadvancing robotics and AI systems in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object affordance reasoning, the ability to infer object functionalities\nbased on physical properties, is fundamental for task-oriented planning and\nactivities in both humans and Artificial Intelligence (AI). This capability,\nrequired for planning and executing daily activities in a task-oriented manner,\nrelies on commonsense knowledge of object physics and functionalities,\nextending beyond simple object recognition. Current computational models for\naffordance reasoning from perception lack generalizability, limiting their\napplicability in novel scenarios. Meanwhile, comprehensive Large Language\nModels (LLMs) with emerging reasoning capabilities are challenging to deploy on\nlocal devices for task-oriented manipulations. Here, we introduce LVIS-Aff, a\nlarge-scale dataset comprising 1,496 tasks and 119k images, designed to enhance\nthe generalizability of affordance reasoning from perception. Utilizing this\ndataset, we develop Afford-X, an end-to-end trainable affordance reasoning\nmodel that incorporates Verb Attention and Bi-Fusion modules to improve\nmulti-modal understanding. This model achieves up to a 12.1% performance\nimprovement over the best-reported results from non-LLM methods, while also\ndemonstrating a 1.2% enhancement compared to our previous conference paper.\nAdditionally, it maintains a compact 187M parameter size and infers nearly 50\ntimes faster than the GPT-4V API. Our work demonstrates the potential for\nefficient, generalizable affordance reasoning models that can be deployed on\nlocal devices for task-oriented manipulations. We showcase Afford-X's\neffectiveness in enabling task-oriented manipulations for robots across various\ntasks and environments, underscoring its efficiency and broad implications for\nadvancing robotics and AI systems in real-world applications."
                },
                "authors": [
                    {
                        "name": "Xiaomeng Zhu"
                    },
                    {
                        "name": "Yuyang Li"
                    },
                    {
                        "name": "Leiyao Cui"
                    },
                    {
                        "name": "Pengfei Li"
                    },
                    {
                        "name": "Huan-ang Gao"
                    },
                    {
                        "name": "Yixin Zhu"
                    },
                    {
                        "name": "Hao Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhao"
                },
                "author": "Hao Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03556v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03556v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10114v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10114v1",
                "updated": "2025-09-12T10:13:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    10,
                    13,
                    38,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T10:13:38Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    10,
                    13,
                    38,
                    4,
                    255,
                    0
                ],
                "title": "A Lightweight Ensemble-Based Face Image Quality Assessment Method with\n  Correlation-Aware Loss",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Lightweight Ensemble-Based Face Image Quality Assessment Method with\n  Correlation-Aware Loss"
                },
                "summary": "Face image quality assessment (FIQA) plays a critical role in face\nrecognition and verification systems, especially in uncontrolled, real-world\nenvironments. Although several methods have been proposed, general-purpose\nno-reference image quality assessment techniques often fail to capture\nface-specific degradations. Meanwhile, state-of-the-art FIQA models tend to be\ncomputationally intensive, limiting their practical applicability. We propose a\nlightweight and efficient method for FIQA, designed for the perceptual\nevaluation of face images in the wild. Our approach integrates an ensemble of\ntwo compact convolutional neural networks, MobileNetV3-Small and ShuffleNetV2,\nwith prediction-level fusion via simple averaging. To enhance alignment with\nhuman perceptual judgments, we employ a correlation-aware loss (MSECorrLoss),\ncombining mean squared error (MSE) with a Pearson correlation regularizer. Our\nmethod achieves a strong balance between accuracy and computational cost,\nmaking it suitable for real-world deployment. Experiments on the VQualA FIQA\nbenchmark demonstrate that our model achieves a Spearman rank correlation\ncoefficient (SRCC) of 0.9829 and a Pearson linear correlation coefficient\n(PLCC) of 0.9894, remaining within competition efficiency constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Face image quality assessment (FIQA) plays a critical role in face\nrecognition and verification systems, especially in uncontrolled, real-world\nenvironments. Although several methods have been proposed, general-purpose\nno-reference image quality assessment techniques often fail to capture\nface-specific degradations. Meanwhile, state-of-the-art FIQA models tend to be\ncomputationally intensive, limiting their practical applicability. We propose a\nlightweight and efficient method for FIQA, designed for the perceptual\nevaluation of face images in the wild. Our approach integrates an ensemble of\ntwo compact convolutional neural networks, MobileNetV3-Small and ShuffleNetV2,\nwith prediction-level fusion via simple averaging. To enhance alignment with\nhuman perceptual judgments, we employ a correlation-aware loss (MSECorrLoss),\ncombining mean squared error (MSE) with a Pearson correlation regularizer. Our\nmethod achieves a strong balance between accuracy and computational cost,\nmaking it suitable for real-world deployment. Experiments on the VQualA FIQA\nbenchmark demonstrate that our model achieves a Spearman rank correlation\ncoefficient (SRCC) of 0.9829 and a Pearson linear correlation coefficient\n(PLCC) of 0.9894, remaining within competition efficiency constraints."
                },
                "authors": [
                    {
                        "name": "MohammadAli Hamidi"
                    },
                    {
                        "name": "Hadi Amirpour"
                    },
                    {
                        "name": "Luigi Atzori"
                    },
                    {
                        "name": "Christian Timmerer"
                    }
                ],
                "author_detail": {
                    "name": "Christian Timmerer"
                },
                "author": "Christian Timmerer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10114v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10114v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10108v1",
                "updated": "2025-09-12T09:58:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    9,
                    58,
                    11,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T09:58:11Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    9,
                    58,
                    11,
                    4,
                    255,
                    0
                ],
                "title": "Scaling Arabic Medical Chatbots Using Synthetic Data: Enhancing\n  Generative AI with Synthetic Patient Records",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Arabic Medical Chatbots Using Synthetic Data: Enhancing\n  Generative AI with Synthetic Patient Records"
                },
                "summary": "The development of medical chatbots in Arabic is significantly constrained by\nthe scarcity of large-scale, high-quality annotated datasets. While prior\nefforts compiled a dataset of 20,000 Arabic patient-doctor interactions from\nsocial media to fine-tune large language models (LLMs), model scalability and\ngeneralization remained limited. In this study, we propose a scalable synthetic\ndata augmentation strategy to expand the training corpus to 100,000 records.\nUsing advanced generative AI systems ChatGPT-4o and Gemini 2.5 Pro we generated\n80,000 contextually relevant and medically coherent synthetic question-answer\npairs grounded in the structure of the original dataset. These synthetic\nsamples were semantically filtered, manually validated, and integrated into the\ntraining pipeline. We fine-tuned five LLMs, including Mistral-7B and AraGPT2,\nand evaluated their performance using BERTScore metrics and expert-driven\nqualitative assessments. To further analyze the effectiveness of synthetic\nsources, we conducted an ablation study comparing ChatGPT-4o and\nGemini-generated data independently. The results showed that ChatGPT-4o data\nconsistently led to higher F1-scores and fewer hallucinations across all\nmodels. Overall, our findings demonstrate the viability of synthetic\naugmentation as a practical solution for enhancing domain-specific language\nmodels in-low resource medical NLP, paving the way for more inclusive,\nscalable, and accurate Arabic healthcare chatbot systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of medical chatbots in Arabic is significantly constrained by\nthe scarcity of large-scale, high-quality annotated datasets. While prior\nefforts compiled a dataset of 20,000 Arabic patient-doctor interactions from\nsocial media to fine-tune large language models (LLMs), model scalability and\ngeneralization remained limited. In this study, we propose a scalable synthetic\ndata augmentation strategy to expand the training corpus to 100,000 records.\nUsing advanced generative AI systems ChatGPT-4o and Gemini 2.5 Pro we generated\n80,000 contextually relevant and medically coherent synthetic question-answer\npairs grounded in the structure of the original dataset. These synthetic\nsamples were semantically filtered, manually validated, and integrated into the\ntraining pipeline. We fine-tuned five LLMs, including Mistral-7B and AraGPT2,\nand evaluated their performance using BERTScore metrics and expert-driven\nqualitative assessments. To further analyze the effectiveness of synthetic\nsources, we conducted an ablation study comparing ChatGPT-4o and\nGemini-generated data independently. The results showed that ChatGPT-4o data\nconsistently led to higher F1-scores and fewer hallucinations across all\nmodels. Overall, our findings demonstrate the viability of synthetic\naugmentation as a practical solution for enhancing domain-specific language\nmodels in-low resource medical NLP, paving the way for more inclusive,\nscalable, and accurate Arabic healthcare chatbot systems."
                },
                "authors": [
                    {
                        "name": "Abdulrahman Allam"
                    },
                    {
                        "name": "Seif Ahmed"
                    },
                    {
                        "name": "Ali Hamdi"
                    },
                    {
                        "name": "Khaled Shaban"
                    }
                ],
                "author_detail": {
                    "name": "Khaled Shaban"
                },
                "author": "Khaled Shaban",
                "arxiv_comment": "Accepted in AICCSA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10105v1",
                "updated": "2025-09-12T09:55:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    9,
                    55,
                    56,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T09:55:56Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    9,
                    55,
                    56,
                    4,
                    255,
                    0
                ],
                "title": "VARCO-VISION-2.0 Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VARCO-VISION-2.0 Technical Report"
                },
                "summary": "We introduce VARCO-VISION-2.0, an open-weight bilingual vision-language model\n(VLM) for Korean and English with improved capabilities compared to the\nprevious model VARCO-VISION-14B. The model supports multi-image understanding\nfor complex inputs such as documents, charts, and tables, and delivers\nlayoutaware OCR by predicting both textual content and its spatial location.\nTrained with a four-stage curriculum with memory-efficient techniques, the\nmodel achieves enhanced multimodal alignment, while preserving core language\nabilities and improving safety via preference optimization. Extensive benchmark\nevaluations demonstrate strong spatial grounding and competitive results for\nboth languages, with the 14B model achieving 8th place on the OpenCompass VLM\nleaderboard among models of comparable scale. Alongside the 14B-scale model, we\nrelease a 1.7B version optimized for on-device deployment. We believe these\nmodels advance the development of bilingual VLMs and their practical\napplications. Two variants of VARCO-VISION-2.0 are available at Hugging Face: a\nfull-scale 14B model and a lightweight 1.7B model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce VARCO-VISION-2.0, an open-weight bilingual vision-language model\n(VLM) for Korean and English with improved capabilities compared to the\nprevious model VARCO-VISION-14B. The model supports multi-image understanding\nfor complex inputs such as documents, charts, and tables, and delivers\nlayoutaware OCR by predicting both textual content and its spatial location.\nTrained with a four-stage curriculum with memory-efficient techniques, the\nmodel achieves enhanced multimodal alignment, while preserving core language\nabilities and improving safety via preference optimization. Extensive benchmark\nevaluations demonstrate strong spatial grounding and competitive results for\nboth languages, with the 14B model achieving 8th place on the OpenCompass VLM\nleaderboard among models of comparable scale. Alongside the 14B-scale model, we\nrelease a 1.7B version optimized for on-device deployment. We believe these\nmodels advance the development of bilingual VLMs and their practical\napplications. Two variants of VARCO-VISION-2.0 are available at Hugging Face: a\nfull-scale 14B model and a lightweight 1.7B model."
                },
                "authors": [
                    {
                        "name": "Young-rok Cha"
                    },
                    {
                        "name": "Jeongho Ju"
                    },
                    {
                        "name": "SunYoung Park"
                    },
                    {
                        "name": "Jong-Hyeon Lee"
                    },
                    {
                        "name": "Younghyun Yu"
                    },
                    {
                        "name": "Youngjune Kim"
                    }
                ],
                "author_detail": {
                    "name": "Youngjune Kim"
                },
                "author": "Youngjune Kim",
                "arxiv_comment": "19 pages, 1 figure, 14 tables. Technical report for VARCO-VISION-2.0,\n  a Korean-English bilingual VLM in 14B and 1.7B variants. Key features:\n  multi-image understanding, OCR with text localization, improved Korean\n  capabilities",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04191v2",
                "updated": "2025-09-12T09:55:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    9,
                    55,
                    20,
                    4,
                    255,
                    0
                ],
                "published": "2025-04-05T14:44:47Z",
                "published_parsed": [
                    2025,
                    4,
                    5,
                    14,
                    44,
                    47,
                    5,
                    95,
                    0
                ],
                "title": "GROVE: A Generalized Reward for Learning Open-Vocabulary Physical Skill",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GROVE: A Generalized Reward for Learning Open-Vocabulary Physical Skill"
                },
                "summary": "Learning open-vocabulary physical skills for simulated agents presents a\nsignificant challenge in artificial intelligence. Current reinforcement\nlearning approaches face critical limitations: manually designed rewards lack\nscalability across diverse tasks, while demonstration-based methods struggle to\ngeneralize beyond their training distribution. We introduce GROVE, a\ngeneralized reward framework that enables open-vocabulary physical skill\nlearning without manual engineering or task-specific demonstrations. Our key\ninsight is that Large Language Models(LLMs) and Vision Language Models(VLMs)\nprovide complementary guidance -- LLMs generate precise physical constraints\ncapturing task requirements, while VLMs evaluate motion semantics and\nnaturalness. Through an iterative design process, VLM-based feedback\ncontinuously refines LLM-generated constraints, creating a self-improving\nreward system. To bridge the domain gap between simulation and natural images,\nwe develop Pose2CLIP, a lightweight mapper that efficiently projects agent\nposes directly into semantic feature space without computationally expensive\nrendering. Extensive experiments across diverse embodiments and learning\nparadigms demonstrate GROVE's effectiveness, achieving 22.2% higher motion\nnaturalness and 25.7% better task completion scores while training 8.4x faster\nthan previous methods. These results establish a new foundation for scalable\nphysical skill acquisition in simulated environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning open-vocabulary physical skills for simulated agents presents a\nsignificant challenge in artificial intelligence. Current reinforcement\nlearning approaches face critical limitations: manually designed rewards lack\nscalability across diverse tasks, while demonstration-based methods struggle to\ngeneralize beyond their training distribution. We introduce GROVE, a\ngeneralized reward framework that enables open-vocabulary physical skill\nlearning without manual engineering or task-specific demonstrations. Our key\ninsight is that Large Language Models(LLMs) and Vision Language Models(VLMs)\nprovide complementary guidance -- LLMs generate precise physical constraints\ncapturing task requirements, while VLMs evaluate motion semantics and\nnaturalness. Through an iterative design process, VLM-based feedback\ncontinuously refines LLM-generated constraints, creating a self-improving\nreward system. To bridge the domain gap between simulation and natural images,\nwe develop Pose2CLIP, a lightweight mapper that efficiently projects agent\nposes directly into semantic feature space without computationally expensive\nrendering. Extensive experiments across diverse embodiments and learning\nparadigms demonstrate GROVE's effectiveness, achieving 22.2% higher motion\nnaturalness and 25.7% better task completion scores while training 8.4x faster\nthan previous methods. These results establish a new foundation for scalable\nphysical skill acquisition in simulated environments."
                },
                "authors": [
                    {
                        "name": "Jieming Cui"
                    },
                    {
                        "name": "Tengyu Liu"
                    },
                    {
                        "name": "Ziyu Meng"
                    },
                    {
                        "name": "Jiale Yu"
                    },
                    {
                        "name": "Ran Song"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Yixin Zhu"
                    },
                    {
                        "name": "Siyuan Huang"
                    }
                ],
                "author_detail": {
                    "name": "Siyuan Huang"
                },
                "author": "Siyuan Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10099v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10099v1",
                "updated": "2025-09-12T09:49:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    9,
                    49,
                    46,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T09:49:46Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    9,
                    49,
                    46,
                    4,
                    255,
                    0
                ],
                "title": "Generating Energy-Efficient Code via Large-Language Models -- Where are\n  we now?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Energy-Efficient Code via Large-Language Models -- Where are\n  we now?"
                },
                "summary": "Context. The rise of Large Language Models (LLMs) has led to their widespread\nadoption in development pipelines. Goal. We empirically assess the energy\nefficiency of Python code generated by LLMs against human-written code and code\ndeveloped by a Green software expert. Method. We test 363 solutions to 9 coding\nproblems from the EvoEval benchmark using 6 widespread LLMs with 4 prompting\ntechniques, and comparing them to human-developed solutions. Energy consumption\nis measured on three different hardware platforms: a server, a PC, and a\nRaspberry Pi for a total of ~881h (36.7 days). Results. Human solutions are 16%\nmore energy-efficient on the server and 3% on the Raspberry Pi, while LLMs\noutperform human developers by 25% on the PC. Prompting does not consistently\nlead to energy savings, where the most energy-efficient prompts vary by\nhardware platform. The code developed by a Green software expert is\nconsistently more energy-efficient by at least 17% to 30% against all LLMs on\nall hardware platforms. Conclusions. Even though LLMs exhibit relatively good\ncode generation capabilities, no LLM-generated code was more energy-efficient\nthan that of an experienced Green software developer, suggesting that as of\ntoday there is still a great need of human expertise for developing\nenergy-efficient Python code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context. The rise of Large Language Models (LLMs) has led to their widespread\nadoption in development pipelines. Goal. We empirically assess the energy\nefficiency of Python code generated by LLMs against human-written code and code\ndeveloped by a Green software expert. Method. We test 363 solutions to 9 coding\nproblems from the EvoEval benchmark using 6 widespread LLMs with 4 prompting\ntechniques, and comparing them to human-developed solutions. Energy consumption\nis measured on three different hardware platforms: a server, a PC, and a\nRaspberry Pi for a total of ~881h (36.7 days). Results. Human solutions are 16%\nmore energy-efficient on the server and 3% on the Raspberry Pi, while LLMs\noutperform human developers by 25% on the PC. Prompting does not consistently\nlead to energy savings, where the most energy-efficient prompts vary by\nhardware platform. The code developed by a Green software expert is\nconsistently more energy-efficient by at least 17% to 30% against all LLMs on\nall hardware platforms. Conclusions. Even though LLMs exhibit relatively good\ncode generation capabilities, no LLM-generated code was more energy-efficient\nthan that of an experienced Green software developer, suggesting that as of\ntoday there is still a great need of human expertise for developing\nenergy-efficient Python code."
                },
                "authors": [
                    {
                        "name": "Radu Apsan"
                    },
                    {
                        "name": "Vincenzo Stoico"
                    },
                    {
                        "name": "Michel Albonico"
                    },
                    {
                        "name": "Rudra Dhar"
                    },
                    {
                        "name": "Karthik Vaidhyanathan"
                    },
                    {
                        "name": "Ivano Malavolta"
                    }
                ],
                "author_detail": {
                    "name": "Ivano Malavolta"
                },
                "author": "Ivano Malavolta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10099v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10099v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10097v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10097v1",
                "updated": "2025-09-12T09:39:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    9,
                    39,
                    15,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T09:39:15Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    9,
                    39,
                    15,
                    4,
                    255,
                    0
                ],
                "title": "Maximising Energy Efficiency in Large-Scale Open RAN: Hybrid xApps and\n  Digital Twin Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maximising Energy Efficiency in Large-Scale Open RAN: Hybrid xApps and\n  Digital Twin Integration"
                },
                "summary": "The growing demand for high-speed, ultra-reliable, and low-latency\ncommunications in 5G and beyond networks has significantly driven up power\nconsumption, particularly within the Radio Access Network (RAN). This surge in\nenergy demand poses critical operational and sustainability challenges for\nmobile network operators, necessitating innovative solutions that enhance\nenergy efficiency without compromising Quality of Service (QoS). Open Radio\nAccess Network (O-RAN), spearheaded by the O-RAN Alliance, offers\ndisaggregated, programmable, and intelligent architectures, promoting\nflexibility, interoperability, and cost-effectiveness. However, this\ndisaggregated approach adds complexity, particularly in managing power\nconsumption across diverse network components such as Open Radio Units (RUs).\nIn this paper, we propose a hybrid xApp leveraging heuristic methods and\nunsupervised machine learning, integrated with digital twin technology through\nthe TeraVM AI RAN Scenario Generator (AI-RSG). This approach dynamically\nmanages RU sleep modes to effectively reduce energy consumption. Our\nexperimental evaluation in a realistic, large-scale emulated Open RAN scenario\ndemonstrates that the hybrid xApp achieves approximately 13% energy savings,\nhighlighting its practicality and significant potential for real-world\ndeployments without compromising user QoS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for high-speed, ultra-reliable, and low-latency\ncommunications in 5G and beyond networks has significantly driven up power\nconsumption, particularly within the Radio Access Network (RAN). This surge in\nenergy demand poses critical operational and sustainability challenges for\nmobile network operators, necessitating innovative solutions that enhance\nenergy efficiency without compromising Quality of Service (QoS). Open Radio\nAccess Network (O-RAN), spearheaded by the O-RAN Alliance, offers\ndisaggregated, programmable, and intelligent architectures, promoting\nflexibility, interoperability, and cost-effectiveness. However, this\ndisaggregated approach adds complexity, particularly in managing power\nconsumption across diverse network components such as Open Radio Units (RUs).\nIn this paper, we propose a hybrid xApp leveraging heuristic methods and\nunsupervised machine learning, integrated with digital twin technology through\nthe TeraVM AI RAN Scenario Generator (AI-RSG). This approach dynamically\nmanages RU sleep modes to effectively reduce energy consumption. Our\nexperimental evaluation in a realistic, large-scale emulated Open RAN scenario\ndemonstrates that the hybrid xApp achieves approximately 13% energy savings,\nhighlighting its practicality and significant potential for real-world\ndeployments without compromising user QoS."
                },
                "authors": [
                    {
                        "name": "Ahmed Al-Tahmeesschi"
                    },
                    {
                        "name": "Yi Chu"
                    },
                    {
                        "name": "Gurdeep Singh"
                    },
                    {
                        "name": "Charles Turyagyenda"
                    },
                    {
                        "name": "Dritan Kaleshi"
                    },
                    {
                        "name": "David Grace"
                    },
                    {
                        "name": "Hamed Ahmadi"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Ahmadi"
                },
                "author": "Hamed Ahmadi",
                "arxiv_comment": "Accepted in GLOBECOM WS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10097v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10097v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10095v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10095v1",
                "updated": "2025-09-12T09:37:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    9,
                    37,
                    26,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T09:37:26Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    9,
                    37,
                    26,
                    4,
                    255,
                    0
                ],
                "title": "Arabic Large Language Models for Medical Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arabic Large Language Models for Medical Text Generation"
                },
                "summary": "Efficient hospital management systems (HMS) are critical worldwide to address\nchallenges such as overcrowding, limited resources, and poor availability of\nurgent health care. Existing methods often lack the ability to provide\naccurate, real-time medical advice, particularly for irregular inputs and\nunderrepresented languages. To overcome these limitations, this study proposes\nan approach that fine-tunes large language models (LLMs) for Arabic medical\ntext generation. The system is designed to assist patients by providing\naccurate medical advice, diagnoses, drug recommendations, and treatment plans\nbased on user input. The research methodology required the collection of a\nunique dataset from social media platforms, capturing real-world medical\nconversations between patients and doctors. The dataset, which includes patient\ncomplaints together with medical advice, was properly cleaned and preprocessed\nto account for multiple Arabic dialects. Fine-tuning state-of-the-art\ngenerative models, such as Mistral-7B-Instruct-v0.2, LLaMA-2-7B, and GPT-2\nMedium, optimized the system's ability to generate reliable medical text.\nResults from evaluations indicate that the fine-tuned Mistral-7B model\noutperformed the other models, achieving average BERT (Bidirectional Encoder\nRepresentations from Transformers) Score values in precision, recall, and\nF1-scores of 68.5\\%, 69.08\\%, and 68.5\\%, respectively. Comparative\nbenchmarking and qualitative assessments validate the system's ability to\nproduce coherent and relevant medical replies to informal input. This study\nhighlights the potential of generative artificial intelligence (AI) in\nadvancing HMS, offering a scalable and adaptable solution for global healthcare\nchallenges, especially in linguistically and culturally diverse environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient hospital management systems (HMS) are critical worldwide to address\nchallenges such as overcrowding, limited resources, and poor availability of\nurgent health care. Existing methods often lack the ability to provide\naccurate, real-time medical advice, particularly for irregular inputs and\nunderrepresented languages. To overcome these limitations, this study proposes\nan approach that fine-tunes large language models (LLMs) for Arabic medical\ntext generation. The system is designed to assist patients by providing\naccurate medical advice, diagnoses, drug recommendations, and treatment plans\nbased on user input. The research methodology required the collection of a\nunique dataset from social media platforms, capturing real-world medical\nconversations between patients and doctors. The dataset, which includes patient\ncomplaints together with medical advice, was properly cleaned and preprocessed\nto account for multiple Arabic dialects. Fine-tuning state-of-the-art\ngenerative models, such as Mistral-7B-Instruct-v0.2, LLaMA-2-7B, and GPT-2\nMedium, optimized the system's ability to generate reliable medical text.\nResults from evaluations indicate that the fine-tuned Mistral-7B model\noutperformed the other models, achieving average BERT (Bidirectional Encoder\nRepresentations from Transformers) Score values in precision, recall, and\nF1-scores of 68.5\\%, 69.08\\%, and 68.5\\%, respectively. Comparative\nbenchmarking and qualitative assessments validate the system's ability to\nproduce coherent and relevant medical replies to informal input. This study\nhighlights the potential of generative artificial intelligence (AI) in\nadvancing HMS, offering a scalable and adaptable solution for global healthcare\nchallenges, especially in linguistically and culturally diverse environments."
                },
                "authors": [
                    {
                        "name": "Abdulrahman Allam"
                    },
                    {
                        "name": "Seif Ahmed"
                    },
                    {
                        "name": "Ali Hamdi"
                    },
                    {
                        "name": "Ammar Mohammed"
                    }
                ],
                "author_detail": {
                    "name": "Ammar Mohammed"
                },
                "author": "Ammar Mohammed",
                "arxiv_comment": "Published in 2025 4th International Conference on Computer\n  Technologies (ICCTech)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10095v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10095v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10082v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10082v1",
                "updated": "2025-09-12T09:19:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    9,
                    19,
                    4,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T09:19:04Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    9,
                    19,
                    4,
                    4,
                    255,
                    0
                ],
                "title": "FetalSleepNet: A Transfer Learning Framework with Spectral Equalisation\n  Domain Adaptation for Fetal Sleep Stage Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FetalSleepNet: A Transfer Learning Framework with Spectral Equalisation\n  Domain Adaptation for Fetal Sleep Stage Classification"
                },
                "summary": "Introduction: This study presents FetalSleepNet, the first published deep\nlearning approach to classifying sleep states from the ovine\nelectroencephalogram (EEG). Fetal EEG is complex to acquire and difficult and\nlaborious to interpret consistently. However, accurate sleep stage\nclassification may aid in the early detection of abnormal brain maturation\nassociated with pregnancy complications (e.g. hypoxia or intrauterine growth\nrestriction).\n  Methods: EEG electrodes were secured onto the ovine dura over the parietal\ncortices of 24 late gestation fetal sheep. A lightweight deep neural network\noriginally developed for adult EEG sleep staging was trained on the ovine EEG\nusing transfer learning from adult EEG. A spectral equalisation-based domain\nadaptation strategy was used to reduce cross-domain mismatch.\n  Results: We demonstrated that while direct transfer performed poorly, full\nfine tuning combined with spectral equalisation achieved the best overall\nperformance (accuracy: 86.6 percent, macro F1-score: 62.5), outperforming\nbaseline models.\n  Conclusions: To the best of our knowledge, FetalSleepNet is the first deep\nlearning framework specifically developed for automated sleep staging from the\nfetal EEG. Beyond the laboratory, the EEG-based sleep stage classifier\nfunctions as a label engine, enabling large scale weak/semi supervised labeling\nand distillation to facilitate training on less invasive signals that can be\nacquired in the clinic, such as Doppler Ultrasound or electrocardiogram data.\nFetalSleepNet's lightweight design makes it well suited for deployment in low\npower, real time, and wearable fetal monitoring systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introduction: This study presents FetalSleepNet, the first published deep\nlearning approach to classifying sleep states from the ovine\nelectroencephalogram (EEG). Fetal EEG is complex to acquire and difficult and\nlaborious to interpret consistently. However, accurate sleep stage\nclassification may aid in the early detection of abnormal brain maturation\nassociated with pregnancy complications (e.g. hypoxia or intrauterine growth\nrestriction).\n  Methods: EEG electrodes were secured onto the ovine dura over the parietal\ncortices of 24 late gestation fetal sheep. A lightweight deep neural network\noriginally developed for adult EEG sleep staging was trained on the ovine EEG\nusing transfer learning from adult EEG. A spectral equalisation-based domain\nadaptation strategy was used to reduce cross-domain mismatch.\n  Results: We demonstrated that while direct transfer performed poorly, full\nfine tuning combined with spectral equalisation achieved the best overall\nperformance (accuracy: 86.6 percent, macro F1-score: 62.5), outperforming\nbaseline models.\n  Conclusions: To the best of our knowledge, FetalSleepNet is the first deep\nlearning framework specifically developed for automated sleep staging from the\nfetal EEG. Beyond the laboratory, the EEG-based sleep stage classifier\nfunctions as a label engine, enabling large scale weak/semi supervised labeling\nand distillation to facilitate training on less invasive signals that can be\nacquired in the clinic, such as Doppler Ultrasound or electrocardiogram data.\nFetalSleepNet's lightweight design makes it well suited for deployment in low\npower, real time, and wearable fetal monitoring systems."
                },
                "authors": [
                    {
                        "name": "Weitao Tang"
                    },
                    {
                        "name": "Johann Vargas-Calixto"
                    },
                    {
                        "name": "Nasim Katebi"
                    },
                    {
                        "name": "Nhi Tran"
                    },
                    {
                        "name": "Sharmony B. Kelly"
                    },
                    {
                        "name": "Gari D. Clifford"
                    },
                    {
                        "name": "Robert Galinsky"
                    },
                    {
                        "name": "Faezeh Marzbanrad"
                    }
                ],
                "author_detail": {
                    "name": "Faezeh Marzbanrad"
                },
                "author": "Faezeh Marzbanrad",
                "arxiv_comment": "13 pages, 4 tables, 5 figures, submitted to IEEE Journal of\n  Biomedical and Health Informatics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10082v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10082v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10078v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10078v1",
                "updated": "2025-09-12T09:14:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    9,
                    14,
                    42,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T09:14:42Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    9,
                    14,
                    42,
                    4,
                    255,
                    0
                ],
                "title": "Established Psychometric vs. Ecologically Valid Questionnaires:\n  Rethinking Psychological Assessments in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Established Psychometric vs. Ecologically Valid Questionnaires:\n  Rethinking Psychological Assessments in Large Language Models"
                },
                "summary": "Researchers have applied established psychometric questionnaires (e.g., BFI,\nPVQ) to measure the personality traits and values reflected in the responses of\nLarge Language Models (LLMs). However, concerns have been raised about applying\nthese human-designed questionnaires to LLMs. One such concern is their lack of\necological validity--the extent to which survey questions adequately reflect\nand resemble real-world contexts in which LLMs generate texts in response to\nuser queries. However, it remains unclear how established questionnaires and\necologically valid questionnaires differ in their outcomes, and what insights\nthese differences may provide. In this paper, we conduct a comprehensive\ncomparative analysis of the two types of questionnaires. Our analysis reveals\nthat established questionnaires (1) yield substantially different profiles of\nLLMs from ecologically valid ones, deviating from the psychological\ncharacteristics expressed in the context of user queries, (2) suffer from\ninsufficient items for stable measurement, (3) create misleading impressions\nthat LLMs possess stable constructs, and (4) yield exaggerated profiles for\npersona-prompted LLMs. Overall, our work cautions against the use of\nestablished psychological questionnaires for LLMs. Our code will be released\nupon publication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Researchers have applied established psychometric questionnaires (e.g., BFI,\nPVQ) to measure the personality traits and values reflected in the responses of\nLarge Language Models (LLMs). However, concerns have been raised about applying\nthese human-designed questionnaires to LLMs. One such concern is their lack of\necological validity--the extent to which survey questions adequately reflect\nand resemble real-world contexts in which LLMs generate texts in response to\nuser queries. However, it remains unclear how established questionnaires and\necologically valid questionnaires differ in their outcomes, and what insights\nthese differences may provide. In this paper, we conduct a comprehensive\ncomparative analysis of the two types of questionnaires. Our analysis reveals\nthat established questionnaires (1) yield substantially different profiles of\nLLMs from ecologically valid ones, deviating from the psychological\ncharacteristics expressed in the context of user queries, (2) suffer from\ninsufficient items for stable measurement, (3) create misleading impressions\nthat LLMs possess stable constructs, and (4) yield exaggerated profiles for\npersona-prompted LLMs. Overall, our work cautions against the use of\nestablished psychological questionnaires for LLMs. Our code will be released\nupon publication."
                },
                "authors": [
                    {
                        "name": "Dongmin Choi"
                    },
                    {
                        "name": "Woojung Song"
                    },
                    {
                        "name": "Jongwook Han"
                    },
                    {
                        "name": "Eun-Ju Lee"
                    },
                    {
                        "name": "Yohan Jo"
                    }
                ],
                "author_detail": {
                    "name": "Yohan Jo"
                },
                "author": "Yohan Jo",
                "arxiv_comment": "17 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10078v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10078v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20241v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20241v2",
                "updated": "2025-09-12T09:08:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    9,
                    8,
                    2,
                    4,
                    255,
                    0
                ],
                "published": "2025-07-27T11:52:09Z",
                "published_parsed": [
                    2025,
                    7,
                    27,
                    11,
                    52,
                    9,
                    6,
                    208,
                    0
                ],
                "title": "Reframe Your Life Story: Interactive Narrative Therapist and Innovative\n  Moment Assessment with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reframe Your Life Story: Interactive Narrative Therapist and Innovative\n  Moment Assessment with Large Language Models"
                },
                "summary": "Recent progress in large language models (LLMs) has opened new possibilities\nfor mental health support, yet current approaches lack realism in simulating\nspecialized psychotherapy and fail to capture therapeutic progression over\ntime. Narrative therapy, which helps individuals transform problematic life\nstories into empowering alternatives, remains underutilized due to limited\naccess and social stigma. We address these limitations through a comprehensive\nframework with two core components. First, INT (Interactive Narrative\nTherapist) simulates expert narrative therapists by planning therapeutic\nstages, guiding reflection levels, and generating contextually appropriate\nexpert-like responses. Second, IMA (Innovative Moment Assessment) provides a\ntherapy-centric evaluation method that quantifies effectiveness by tracking\n\"Innovative Moments\" (IMs), critical narrative shifts in client speech\nsignaling therapy progress. Experimental results on 260 simulated clients and\n230 human participants reveal that INT consistently outperforms standard LLMs\nin therapeutic quality and depth. We further demonstrate the effectiveness of\nINT in synthesizing high-quality support conversations to facilitate social\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in large language models (LLMs) has opened new possibilities\nfor mental health support, yet current approaches lack realism in simulating\nspecialized psychotherapy and fail to capture therapeutic progression over\ntime. Narrative therapy, which helps individuals transform problematic life\nstories into empowering alternatives, remains underutilized due to limited\naccess and social stigma. We address these limitations through a comprehensive\nframework with two core components. First, INT (Interactive Narrative\nTherapist) simulates expert narrative therapists by planning therapeutic\nstages, guiding reflection levels, and generating contextually appropriate\nexpert-like responses. Second, IMA (Innovative Moment Assessment) provides a\ntherapy-centric evaluation method that quantifies effectiveness by tracking\n\"Innovative Moments\" (IMs), critical narrative shifts in client speech\nsignaling therapy progress. Experimental results on 260 simulated clients and\n230 human participants reveal that INT consistently outperforms standard LLMs\nin therapeutic quality and depth. We further demonstrate the effectiveness of\nINT in synthesizing high-quality support conversations to facilitate social\napplications."
                },
                "authors": [
                    {
                        "name": "Yi Feng"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Wenxuan Zhang"
                    },
                    {
                        "name": "Zhuang Chen"
                    },
                    {
                        "name": "Yutong Shen"
                    },
                    {
                        "name": "Xiyao Xiao"
                    },
                    {
                        "name": "Minlie Huang"
                    },
                    {
                        "name": "Liping Jing"
                    },
                    {
                        "name": "Jian Yu"
                    }
                ],
                "author_detail": {
                    "name": "Jian Yu"
                },
                "author": "Jian Yu",
                "arxiv_comment": "EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20241v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20241v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10073v1",
                "updated": "2025-09-12T09:06:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    9,
                    6,
                    5,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T09:06:05Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    9,
                    6,
                    5,
                    4,
                    255,
                    0
                ],
                "title": "Benchmarking Classical, Machine Learning, and Bayesian Survival Models\n  for Clinical Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Classical, Machine Learning, and Bayesian Survival Models\n  for Clinical Prediction"
                },
                "summary": "Survival analysis is a statistical framework for modeling time-to-event data,\nparticularly valuable in healthcare for predicting outcomes like patient\ndischarge or recurrence. This study implements and compares several survival\nmodels - including Weibull, Weibull AFT, Weibull AFT with Gamma Frailty, Cox\nProportional Hazards (CoxPH), Random Survival Forest (RSF), and DeepSurv -\nusing a publicly available breast cancer dataset. This study aims to benchmark\nclassical, machine learning, and Bayesian survival models in terms of their\npredictive performance, interpretability, and suitability for clinical\ndeployment. The models are evaluated using performance metrics such as the\nConcordance Index (C-index) and the Root Mean Squared Error (RMSE). DeepSurv\nshowed the highest predictive performance, while interpretable models like RSF\nand Weibull AFT with Gamma Frailty offered competitive results. We also\nexplored the implementation of statistical models from a Bayesian perspective,\nincluding frailty models, due to their ability to properly quantify\nuncertainty. Notably, frailty models are not readily available in standard\nsurvival analysis libraries, necessitating custom implementation. Our results\ndemonstrate that interpretable statistical models, when correctly implemented\nusing parameters that are effectively estimated using a Bayesian approach, can\nperform competitively with modern black-box models. These findings illustrate\nthe trade-offs between model complexity, interpretability, and predictive\npower, highlighting the potential of Bayesian survival models in clinical\ndecision-making settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Survival analysis is a statistical framework for modeling time-to-event data,\nparticularly valuable in healthcare for predicting outcomes like patient\ndischarge or recurrence. This study implements and compares several survival\nmodels - including Weibull, Weibull AFT, Weibull AFT with Gamma Frailty, Cox\nProportional Hazards (CoxPH), Random Survival Forest (RSF), and DeepSurv -\nusing a publicly available breast cancer dataset. This study aims to benchmark\nclassical, machine learning, and Bayesian survival models in terms of their\npredictive performance, interpretability, and suitability for clinical\ndeployment. The models are evaluated using performance metrics such as the\nConcordance Index (C-index) and the Root Mean Squared Error (RMSE). DeepSurv\nshowed the highest predictive performance, while interpretable models like RSF\nand Weibull AFT with Gamma Frailty offered competitive results. We also\nexplored the implementation of statistical models from a Bayesian perspective,\nincluding frailty models, due to their ability to properly quantify\nuncertainty. Notably, frailty models are not readily available in standard\nsurvival analysis libraries, necessitating custom implementation. Our results\ndemonstrate that interpretable statistical models, when correctly implemented\nusing parameters that are effectively estimated using a Bayesian approach, can\nperform competitively with modern black-box models. These findings illustrate\nthe trade-offs between model complexity, interpretability, and predictive\npower, highlighting the potential of Bayesian survival models in clinical\ndecision-making settings."
                },
                "authors": [
                    {
                        "name": "Irving Gómez-Méndez"
                    },
                    {
                        "name": "Sivakorn Phromsiri"
                    },
                    {
                        "name": "Ittiphat Kijpaisansak"
                    },
                    {
                        "name": "Settawut Chaithurdthum"
                    }
                ],
                "author_detail": {
                    "name": "Settawut Chaithurdthum"
                },
                "author": "Settawut Chaithurdthum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62N02",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10058v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10058v1",
                "updated": "2025-09-12T08:44:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    8,
                    44,
                    22,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T08:44:22Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    8,
                    44,
                    22,
                    4,
                    255,
                    0
                ],
                "title": "Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings\n  for Improved Diffusion Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings\n  for Improved Diffusion Generation"
                },
                "summary": "Accurate color alignment in text-to-image (T2I) generation is critical for\napplications such as fashion, product visualization, and interior design, yet\ncurrent diffusion models struggle with nuanced and compound color terms (e.g.,\nTiffany blue, lime green, hot pink), often producing images that are misaligned\nwith human intent. Existing approaches rely on cross-attention manipulation,\nreference images, or fine-tuning but fail to systematically resolve ambiguous\ncolor descriptions. To precisely render colors under prompt ambiguity, we\npropose a training-free framework that enhances color fidelity by leveraging a\nlarge language model (LLM) to disambiguate color-related prompts and guiding\ncolor blending operations directly in the text embedding space. Our method\nfirst employs a large language model (LLM) to resolve ambiguous color terms in\nthe text prompt, and then refines the text embeddings based on the spatial\nrelationships of the resulting color terms in the CIELAB color space. Unlike\nprior methods, our approach improves color accuracy without requiring\nadditional training or external reference images. Experimental results\ndemonstrate that our framework improves color alignment without compromising\nimage quality, bridging the gap between text semantics and visual generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate color alignment in text-to-image (T2I) generation is critical for\napplications such as fashion, product visualization, and interior design, yet\ncurrent diffusion models struggle with nuanced and compound color terms (e.g.,\nTiffany blue, lime green, hot pink), often producing images that are misaligned\nwith human intent. Existing approaches rely on cross-attention manipulation,\nreference images, or fine-tuning but fail to systematically resolve ambiguous\ncolor descriptions. To precisely render colors under prompt ambiguity, we\npropose a training-free framework that enhances color fidelity by leveraging a\nlarge language model (LLM) to disambiguate color-related prompts and guiding\ncolor blending operations directly in the text embedding space. Our method\nfirst employs a large language model (LLM) to resolve ambiguous color terms in\nthe text prompt, and then refines the text embeddings based on the spatial\nrelationships of the resulting color terms in the CIELAB color space. Unlike\nprior methods, our approach improves color accuracy without requiring\nadditional training or external reference images. Experimental results\ndemonstrate that our framework improves color alignment without compromising\nimage quality, bridging the gap between text semantics and visual generation."
                },
                "authors": [
                    {
                        "name": "Sung-Lin Tsai"
                    },
                    {
                        "name": "Bo-Lun Huang"
                    },
                    {
                        "name": "Yu Ting Shen"
                    },
                    {
                        "name": "Cheng Yu Yeo"
                    },
                    {
                        "name": "Chiang Tseng"
                    },
                    {
                        "name": "Bo-Kai Ruan"
                    },
                    {
                        "name": "Wen-Sheng Lien"
                    },
                    {
                        "name": "Hong-Han Shuai"
                    }
                ],
                "author_detail": {
                    "name": "Hong-Han Shuai"
                },
                "author": "Hong-Han Shuai",
                "arxiv_doi": "10.1145/3746027.3755385",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746027.3755385",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.10058v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10058v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to ACM Multimedia 2025 (MM '25)",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10054v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10054v1",
                "updated": "2025-09-12T08:40:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    8,
                    40,
                    58,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T08:40:58Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    8,
                    40,
                    58,
                    4,
                    255,
                    0
                ],
                "title": "XAgents: A Unified Framework for Multi-Agent Cooperation via IF-THEN\n  Rules and Multipolar Task Processing Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XAgents: A Unified Framework for Multi-Agent Cooperation via IF-THEN\n  Rules and Multipolar Task Processing Graph"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has significantly\nenhanced the capabilities of Multi-Agent Systems (MAS) in supporting humans\nwith complex, real-world tasks. However, MAS still face challenges in effective\ntask planning when handling highly complex tasks with uncertainty, often\nresulting in misleading or incorrect outputs that hinder task execution. To\naddress this, we propose XAgents, a unified multi-agent cooperative framework\nbuilt on a multipolar task processing graph and IF-THEN rules. XAgents uses the\nmultipolar task processing graph to enable dynamic task planning and handle\ntask uncertainty. During subtask processing, it integrates domain-specific\nIF-THEN rules to constrain agent behaviors, while global rules enhance\ninter-agent collaboration. We evaluate the performance of XAgents across three\ndistinct datasets, demonstrating that it consistently surpasses\nstate-of-the-art single-agent and multi-agent approaches in both\nknowledge-typed and logic-typed question-answering tasks. The codes for XAgents\nare available at: https://github.com/AGI-FHBC/XAgents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has significantly\nenhanced the capabilities of Multi-Agent Systems (MAS) in supporting humans\nwith complex, real-world tasks. However, MAS still face challenges in effective\ntask planning when handling highly complex tasks with uncertainty, often\nresulting in misleading or incorrect outputs that hinder task execution. To\naddress this, we propose XAgents, a unified multi-agent cooperative framework\nbuilt on a multipolar task processing graph and IF-THEN rules. XAgents uses the\nmultipolar task processing graph to enable dynamic task planning and handle\ntask uncertainty. During subtask processing, it integrates domain-specific\nIF-THEN rules to constrain agent behaviors, while global rules enhance\ninter-agent collaboration. We evaluate the performance of XAgents across three\ndistinct datasets, demonstrating that it consistently surpasses\nstate-of-the-art single-agent and multi-agent approaches in both\nknowledge-typed and logic-typed question-answering tasks. The codes for XAgents\nare available at: https://github.com/AGI-FHBC/XAgents."
                },
                "authors": [
                    {
                        "name": "Hailong Yang"
                    },
                    {
                        "name": "Mingxian Gu"
                    },
                    {
                        "name": "Jianqi Wang"
                    },
                    {
                        "name": "Guanjin Wang"
                    },
                    {
                        "name": "Zhaohong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhaohong Deng"
                },
                "author": "Zhaohong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10054v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10054v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04005v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04005v3",
                "updated": "2025-09-12T08:36:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    8,
                    36,
                    2,
                    4,
                    255,
                    0
                ],
                "published": "2025-07-05T11:17:20Z",
                "published_parsed": [
                    2025,
                    7,
                    5,
                    11,
                    17,
                    20,
                    5,
                    186,
                    0
                ],
                "title": "Exploring a Gamified Personality Assessment Method through Interaction\n  with LLM Agents Embodying Different Personalities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring a Gamified Personality Assessment Method through Interaction\n  with LLM Agents Embodying Different Personalities"
                },
                "summary": "The low-intrusion and automated personality assessment is receiving\nincreasing attention in psychology and human-computer interaction fields. This\nstudy explores an interactive approach for personality assessment, focusing on\nthe multiplicity of personality representation. We propose a framework of\nGamified Personality Assessment through Multi-Personality Representations\n(Multi-PR GPA). The framework leverages Large Language Models to empower\nvirtual agents with different personalities. These agents elicit multifaceted\nhuman personality representations through engaging in interactive games.\nDrawing upon the multi-type textual data generated throughout the interaction,\nit achieves two modes of personality assessment (i.e., Direct Assessment and\nQuestionnaire-based Assessment) and provides interpretable insights. Grounded\nin the classic Big Five personality theory, we developed a prototype system and\nconducted a user study to evaluate the efficacy of Multi-PR GPA. The results\naffirm the effectiveness of our approach in personality assessment and\ndemonstrate its superior performance when considering the multiplicity of\npersonality representation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The low-intrusion and automated personality assessment is receiving\nincreasing attention in psychology and human-computer interaction fields. This\nstudy explores an interactive approach for personality assessment, focusing on\nthe multiplicity of personality representation. We propose a framework of\nGamified Personality Assessment through Multi-Personality Representations\n(Multi-PR GPA). The framework leverages Large Language Models to empower\nvirtual agents with different personalities. These agents elicit multifaceted\nhuman personality representations through engaging in interactive games.\nDrawing upon the multi-type textual data generated throughout the interaction,\nit achieves two modes of personality assessment (i.e., Direct Assessment and\nQuestionnaire-based Assessment) and provides interpretable insights. Grounded\nin the classic Big Five personality theory, we developed a prototype system and\nconducted a user study to evaluate the efficacy of Multi-PR GPA. The results\naffirm the effectiveness of our approach in personality assessment and\ndemonstrate its superior performance when considering the multiplicity of\npersonality representation."
                },
                "authors": [
                    {
                        "name": "Baiqiao Zhang"
                    },
                    {
                        "name": "Xiangxian Li"
                    },
                    {
                        "name": "Chao Zhou"
                    },
                    {
                        "name": "Xinyu Gai"
                    },
                    {
                        "name": "Juan Liu"
                    },
                    {
                        "name": "Xue Yang"
                    },
                    {
                        "name": "Xiaojuan Ma"
                    },
                    {
                        "name": "Yong-jin Liu"
                    },
                    {
                        "name": "Yulong Bian"
                    }
                ],
                "author_detail": {
                    "name": "Yulong Bian"
                },
                "author": "Yulong Bian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04005v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04005v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08863v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08863v2",
                "updated": "2025-09-12T08:26:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    8,
                    26,
                    37,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-10T03:43:46Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    3,
                    43,
                    46,
                    2,
                    253,
                    0
                ],
                "title": "GeoJSON Agents:A Multi-Agent LLM Architecture for Geospatial\n  Analysis-Function Calling vs Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GeoJSON Agents:A Multi-Agent LLM Architecture for Geospatial\n  Analysis-Function Calling vs Code Generation"
                },
                "summary": "LLMs have made substantial progress in task automation and natural language\nunderstanding. However, without expertise in GIS, they continue to encounter\nlimitations. To address these issues, we propose GeoJSON Agents-a multi-agent\nLLM architecture. This framework transforms natural language tasks into\nstructured GeoJSON operation commands and processes spatial data using two\nwidely adopted LLM enhancement techniques: Function Calling and Code\nGeneration. The architecture consists of three components-task parsing, agent\ncollaboration, and result integration-aimed at enhancing both the performance\nand scalability of GIS automation. The Planner agent interprets natural\nlanguage tasks into structured GeoJSON commands. Then, specialized Worker\nagents collaborate according to assigned roles to perform spatial data\nprocessing and analysis, either by invoking predefined function APIs or by\ndynamically generating and executing Python-based spatial analysis code.\nFinally, the system integrates the outputs from multiple execution rounds into\nreusable, standards-compliant GeoJSON files. To systematically evaluate the\nperformance of the two approaches, we constructed a benchmark dataset of 70\ntasks with varying complexity and conducted experiments using OpenAI's GPT-4o\nas the core model. Results indicate that the Function Calling-based GeoJSON\nAgent achieved an accuracy of 85.71%, while the Code Generation-based agent\nreached 97.14%, both significantly outperforming the best-performing\ngeneral-purpose model (48.57%). Further analysis reveals that the Code\nGeneration provides greater flexibility, whereas the Function Calling approach\noffers more stable execution. This study is the first to introduce an LLM\nmulti-agent framework for GeoJSON data and to compare the strengths and\nlimitations of two mainstream LLM enhancement methods, offering new\nperspectives for improving GeoAI system performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have made substantial progress in task automation and natural language\nunderstanding. However, without expertise in GIS, they continue to encounter\nlimitations. To address these issues, we propose GeoJSON Agents-a multi-agent\nLLM architecture. This framework transforms natural language tasks into\nstructured GeoJSON operation commands and processes spatial data using two\nwidely adopted LLM enhancement techniques: Function Calling and Code\nGeneration. The architecture consists of three components-task parsing, agent\ncollaboration, and result integration-aimed at enhancing both the performance\nand scalability of GIS automation. The Planner agent interprets natural\nlanguage tasks into structured GeoJSON commands. Then, specialized Worker\nagents collaborate according to assigned roles to perform spatial data\nprocessing and analysis, either by invoking predefined function APIs or by\ndynamically generating and executing Python-based spatial analysis code.\nFinally, the system integrates the outputs from multiple execution rounds into\nreusable, standards-compliant GeoJSON files. To systematically evaluate the\nperformance of the two approaches, we constructed a benchmark dataset of 70\ntasks with varying complexity and conducted experiments using OpenAI's GPT-4o\nas the core model. Results indicate that the Function Calling-based GeoJSON\nAgent achieved an accuracy of 85.71%, while the Code Generation-based agent\nreached 97.14%, both significantly outperforming the best-performing\ngeneral-purpose model (48.57%). Further analysis reveals that the Code\nGeneration provides greater flexibility, whereas the Function Calling approach\noffers more stable execution. This study is the first to introduce an LLM\nmulti-agent framework for GeoJSON data and to compare the strengths and\nlimitations of two mainstream LLM enhancement methods, offering new\nperspectives for improving GeoAI system performance."
                },
                "authors": [
                    {
                        "name": "Qianqian Luo"
                    },
                    {
                        "name": "Liuchang Xu"
                    },
                    {
                        "name": "Qingming Lin"
                    },
                    {
                        "name": "Sensen Wu"
                    },
                    {
                        "name": "Ruichen Mao"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Hailin Feng"
                    },
                    {
                        "name": "Bo Huang"
                    },
                    {
                        "name": "Zhenhong Du"
                    }
                ],
                "author_detail": {
                    "name": "Zhenhong Du"
                },
                "author": "Zhenhong Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08863v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08863v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24298v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24298v3",
                "updated": "2025-09-12T07:59:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    59,
                    18,
                    4,
                    255,
                    0
                ],
                "published": "2025-05-30T07:18:25Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    7,
                    18,
                    25,
                    4,
                    150,
                    0
                ],
                "title": "AReaL: A Large-Scale Asynchronous Reinforcement Learning System for\n  Language Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AReaL: A Large-Scale Asynchronous Reinforcement Learning System for\n  Language Reasoning"
                },
                "summary": "Reinforcement learning (RL) has become a dominant paradigm for training large\nlanguage models (LLMs), particularly for reasoning tasks. Effective RL for LLMs\nrequires massive parallelization and poses an urgent need for efficient\ntraining systems. Most existing large-scale RL systems for LLMs are\nsynchronous, alternating generation and training in a batch setting where\nrollouts in each training batch are generated by the same model. This approach\nstabilizes RL training but suffers from severe system-level inefficiency:\ngeneration must wait until the longest output in the batch is completed before\nmodel updates, resulting in GPU underutilization. We present AReaL, a fully\nasynchronous RL system that completely decouples generation from training.\nRollout workers in AReaL continuously generate new outputs without waiting,\nwhile training workers update the model whenever a batch of data is collected.\nAReaL also incorporates a collection of system-level optimizations, leading to\nsubstantially higher GPU utilization. To stabilize RL training, AReaL balances\nthe workload of rollout and training workers to control data staleness, and\nadopts a staleness-enhanced PPO variant to better handle outdated training\nsamples. Extensive experiments on math and code reasoning benchmarks show that\nAReaL achieves up to 2.77$\\times$ training speedup compared to synchronous\nsystems with the same number of GPUs and matched or improved final performance.\nThe code of AReaL is available at https://github.com/inclusionAI/AReaL/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has become a dominant paradigm for training large\nlanguage models (LLMs), particularly for reasoning tasks. Effective RL for LLMs\nrequires massive parallelization and poses an urgent need for efficient\ntraining systems. Most existing large-scale RL systems for LLMs are\nsynchronous, alternating generation and training in a batch setting where\nrollouts in each training batch are generated by the same model. This approach\nstabilizes RL training but suffers from severe system-level inefficiency:\ngeneration must wait until the longest output in the batch is completed before\nmodel updates, resulting in GPU underutilization. We present AReaL, a fully\nasynchronous RL system that completely decouples generation from training.\nRollout workers in AReaL continuously generate new outputs without waiting,\nwhile training workers update the model whenever a batch of data is collected.\nAReaL also incorporates a collection of system-level optimizations, leading to\nsubstantially higher GPU utilization. To stabilize RL training, AReaL balances\nthe workload of rollout and training workers to control data staleness, and\nadopts a staleness-enhanced PPO variant to better handle outdated training\nsamples. Extensive experiments on math and code reasoning benchmarks show that\nAReaL achieves up to 2.77$\\times$ training speedup compared to synchronous\nsystems with the same number of GPUs and matched or improved final performance.\nThe code of AReaL is available at https://github.com/inclusionAI/AReaL/."
                },
                "authors": [
                    {
                        "name": "Wei Fu"
                    },
                    {
                        "name": "Jiaxuan Gao"
                    },
                    {
                        "name": "Xujie Shen"
                    },
                    {
                        "name": "Chen Zhu"
                    },
                    {
                        "name": "Zhiyu Mei"
                    },
                    {
                        "name": "Chuyi He"
                    },
                    {
                        "name": "Shusheng Xu"
                    },
                    {
                        "name": "Guo Wei"
                    },
                    {
                        "name": "Jun Mei"
                    },
                    {
                        "name": "Jiashu Wang"
                    },
                    {
                        "name": "Tongkai Yang"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Yi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Wu"
                },
                "author": "Yi Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24298v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24298v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10026v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10026v1",
                "updated": "2025-09-12T07:45:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    45,
                    44,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T07:45:44Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    45,
                    44,
                    4,
                    255,
                    0
                ],
                "title": "LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization\n  for Real-World Multilingual VQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization\n  for Real-World Multilingual VQA"
                },
                "summary": "As large vision language models (VLMs) advance, their capabilities in\nmultilingual visual question answering (mVQA) have significantly improved.\nChain-of-thought (CoT) reasoning has been proven to enhance interpretability\nand complex reasoning. However, most existing approaches rely primarily on\ntextual CoT and provide limited support for multilingual multimodal reasoning,\nconstraining their deployment in real-world applications. To address this gap,\nwe introduce \\textbf{LaV-CoT}, the first Language-aware Visual CoT framework\nwith Multi-Aspect Reward Optimization. LaV-CoT incorporates an interpretable\nmulti-stage reasoning pipeline consisting of Text Summary with Bounding Box\n(BBox), Language Identification, Spatial Object-level Captioning, and\nStep-by-step Logical Reasoning. Following this reasoning pipeline, we design an\nautomated data curation method that generates multilingual CoT annotations\nthrough iterative generation, correction, and refinement, enabling scalable and\nhigh-quality training data. To improve reasoning and generalization, LaV-CoT\nadopts a two-stage training paradigm combining Supervised Fine-Tuning (SFT)\nwith Language-aware Group Relative Policy Optimization (GRPO), guided by\nverifiable multi-aspect rewards including language consistency, structural\naccuracy, and semantic alignment. Extensive evaluations on public datasets\nincluding MMMB, Multilingual MMBench, and MTVQA show that LaV-CoT achieves up\nto \\(\\sim\\)9.5\\% accuracy improvements over open-source baselines of similar\nsize and even surpasses models with 2$\\times$ larger scales by \\(\\sim\\)2.6\\%.\nMoreover, LaV-CoT outperforms advanced proprietary models such as GPT-4o-0513\nand Gemini-2.5-flash. We further conducted an online A/B test to validate our\nmethod on real-world data, highlighting its effectiveness for industrial\ndeployment. Our code is available at this link:\n\\href{https://github.com/HJNVR/LaV-CoT}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large vision language models (VLMs) advance, their capabilities in\nmultilingual visual question answering (mVQA) have significantly improved.\nChain-of-thought (CoT) reasoning has been proven to enhance interpretability\nand complex reasoning. However, most existing approaches rely primarily on\ntextual CoT and provide limited support for multilingual multimodal reasoning,\nconstraining their deployment in real-world applications. To address this gap,\nwe introduce \\textbf{LaV-CoT}, the first Language-aware Visual CoT framework\nwith Multi-Aspect Reward Optimization. LaV-CoT incorporates an interpretable\nmulti-stage reasoning pipeline consisting of Text Summary with Bounding Box\n(BBox), Language Identification, Spatial Object-level Captioning, and\nStep-by-step Logical Reasoning. Following this reasoning pipeline, we design an\nautomated data curation method that generates multilingual CoT annotations\nthrough iterative generation, correction, and refinement, enabling scalable and\nhigh-quality training data. To improve reasoning and generalization, LaV-CoT\nadopts a two-stage training paradigm combining Supervised Fine-Tuning (SFT)\nwith Language-aware Group Relative Policy Optimization (GRPO), guided by\nverifiable multi-aspect rewards including language consistency, structural\naccuracy, and semantic alignment. Extensive evaluations on public datasets\nincluding MMMB, Multilingual MMBench, and MTVQA show that LaV-CoT achieves up\nto \\(\\sim\\)9.5\\% accuracy improvements over open-source baselines of similar\nsize and even surpasses models with 2$\\times$ larger scales by \\(\\sim\\)2.6\\%.\nMoreover, LaV-CoT outperforms advanced proprietary models such as GPT-4o-0513\nand Gemini-2.5-flash. We further conducted an online A/B test to validate our\nmethod on real-world data, highlighting its effectiveness for industrial\ndeployment. Our code is available at this link:\n\\href{https://github.com/HJNVR/LaV-CoT}"
                },
                "authors": [
                    {
                        "name": "Jing Huang"
                    },
                    {
                        "name": "Zhiya Tan"
                    },
                    {
                        "name": "Shutao Gong"
                    },
                    {
                        "name": "Fanwei Zeng"
                    },
                    {
                        "name": "Jianshu Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianshu Li"
                },
                "author": "Jianshu Li",
                "arxiv_comment": "12 Pages, 12 Figures, 2 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10026v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10026v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19366v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19366v2",
                "updated": "2025-09-12T07:35:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    35,
                    3,
                    4,
                    255,
                    0
                ],
                "published": "2025-06-24T06:53:20Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    6,
                    53,
                    20,
                    1,
                    175,
                    0
                ],
                "title": "Fractality of Wireless Mesh Networks: Dimensional Effects on Network\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fractality of Wireless Mesh Networks: Dimensional Effects on Network\n  Performance"
                },
                "summary": "Wireless mesh networks (WMNs) depend on the spatial distribution of nodes,\nwhich directly influences connectivity, routing efficiency, and overall network\nperformance. Conventional models typically assume uniform or random node\nplacement, which inadequately represent the complex, hierarchical spatial\npatterns observed in practical deployments. In this study, we present a novel\nalgorithm that constructs WMN topologies with tunable fractal dimensions,\nallowing precise control over spatial self-similarity. By systematically\nvarying the fractal dimension, the algorithm generates network layouts spanning\na continuum of spatial complexities, ranging from sparse fragmented clusters to\ndense, cohesive structures. Through NS-3 simulations, Key performance metrics\nincluding throughput, latency, jitter, and packet delivery ratio were evaluated\nacross a range of fractal dimensions. Comparative evaluations against classical\nrandom, small-world, scale-free, grid and hierarchical tree networks models\nreveal that high-dimensional fractal topologies achieve enhanced resilience and\nthroughput under equivalent conditions. These findings demonstrate the\npotential of fractal geometry as a design paradigm for scalable and efficient\nWMN architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless mesh networks (WMNs) depend on the spatial distribution of nodes,\nwhich directly influences connectivity, routing efficiency, and overall network\nperformance. Conventional models typically assume uniform or random node\nplacement, which inadequately represent the complex, hierarchical spatial\npatterns observed in practical deployments. In this study, we present a novel\nalgorithm that constructs WMN topologies with tunable fractal dimensions,\nallowing precise control over spatial self-similarity. By systematically\nvarying the fractal dimension, the algorithm generates network layouts spanning\na continuum of spatial complexities, ranging from sparse fragmented clusters to\ndense, cohesive structures. Through NS-3 simulations, Key performance metrics\nincluding throughput, latency, jitter, and packet delivery ratio were evaluated\nacross a range of fractal dimensions. Comparative evaluations against classical\nrandom, small-world, scale-free, grid and hierarchical tree networks models\nreveal that high-dimensional fractal topologies achieve enhanced resilience and\nthroughput under equivalent conditions. These findings demonstrate the\npotential of fractal geometry as a design paradigm for scalable and efficient\nWMN architectures."
                },
                "authors": [
                    {
                        "name": "Marat Zaidyn"
                    },
                    {
                        "name": "Sayat Akhtanov"
                    },
                    {
                        "name": "Dana Turlykozhayeva"
                    },
                    {
                        "name": "Symbat Temesheva"
                    },
                    {
                        "name": "Almat Akhmetali"
                    },
                    {
                        "name": "Alisher Skabylov"
                    },
                    {
                        "name": "Nurzhan Ussipov"
                    }
                ],
                "author_detail": {
                    "name": "Nurzhan Ussipov"
                },
                "author": "Nurzhan Ussipov",
                "arxiv_comment": "14 pages, 8 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19366v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19366v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10810v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10810v2",
                "updated": "2025-09-12T07:33:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    33,
                    46,
                    4,
                    255,
                    0
                ],
                "published": "2024-06-16T06:20:06Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    6,
                    20,
                    6,
                    6,
                    168,
                    0
                ],
                "title": "RGBlimp-Q: Robotic Gliding Blimp With Moving Mass Control Based on a\n  Bird-Inspired Continuum Arm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RGBlimp-Q: Robotic Gliding Blimp With Moving Mass Control Based on a\n  Bird-Inspired Continuum Arm"
                },
                "summary": "Robotic blimps, as lighter-than-air aerial platforms, offer extended\noperational duration and enhanced safety in human-robot interactions due to\ntheir buoyant lift. However, achieving robust flight performance under\nenvironmental airflow disturbances remains a critical challenge, thereby\nlimiting their broader deployment. Inspired by avian flight mechanics,\nparticularly the ability of birds to perch and stabilize in turbulent wind\nconditions, this article introduces RGBlimp-Q -- a robotic gliding blimp\nequipped with a bird-inspired continuum arm featuring a novel moving mass\nactuation mechanism. This continuum arm enables flexible attitude regulation\nthrough internal mass redistribution, significantly enhancing the system's\nresilience to external disturbances. In addition, it facilitates aerial\nmanipulation by employing end-effector claws that interact with the environment\nin a manner analogous to avian perching behavior. This article presents the\ndesign, modeling, and prototyping of RGBlimp-Q, supported by comprehensive\nexperimental evaluation and comparative analysis. To the best of the authors'\nknowledge, this represents the first interdisciplinary integration of continuum\nmechanisms into a lighter-than-air robotic platform, where the continuum arm\nsimultaneously functions as both an actuation and manipulation module. This\ndesign establishes a novel paradigm for robotic blimps, expanding their\napplicability to complex and dynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic blimps, as lighter-than-air aerial platforms, offer extended\noperational duration and enhanced safety in human-robot interactions due to\ntheir buoyant lift. However, achieving robust flight performance under\nenvironmental airflow disturbances remains a critical challenge, thereby\nlimiting their broader deployment. Inspired by avian flight mechanics,\nparticularly the ability of birds to perch and stabilize in turbulent wind\nconditions, this article introduces RGBlimp-Q -- a robotic gliding blimp\nequipped with a bird-inspired continuum arm featuring a novel moving mass\nactuation mechanism. This continuum arm enables flexible attitude regulation\nthrough internal mass redistribution, significantly enhancing the system's\nresilience to external disturbances. In addition, it facilitates aerial\nmanipulation by employing end-effector claws that interact with the environment\nin a manner analogous to avian perching behavior. This article presents the\ndesign, modeling, and prototyping of RGBlimp-Q, supported by comprehensive\nexperimental evaluation and comparative analysis. To the best of the authors'\nknowledge, this represents the first interdisciplinary integration of continuum\nmechanisms into a lighter-than-air robotic platform, where the continuum arm\nsimultaneously functions as both an actuation and manipulation module. This\ndesign establishes a novel paradigm for robotic blimps, expanding their\napplicability to complex and dynamic environments."
                },
                "authors": [
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Feitian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Feitian Zhang"
                },
                "author": "Feitian Zhang",
                "arxiv_doi": "10.1109/TRO.2025.3600135",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TRO.2025.3600135",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.10810v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10810v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Transactions on Robotics, vol. 41, pp. 5097-5116, 2025",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01328v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01328v2",
                "updated": "2025-09-12T07:26:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    26,
                    28,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-01T10:11:56Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    10,
                    11,
                    56,
                    0,
                    244,
                    0
                ],
                "title": "Can Large Language Models Master Complex Card Games?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Master Complex Card Games?"
                },
                "summary": "Complex games have long been an important benchmark for testing the progress\nof artificial intelligence algorithms. AlphaGo, AlphaZero, and MuZero have\ndefeated top human players in Go and Chess, garnering widespread societal\nattention towards artificial intelligence. Concurrently, large language models\n(LLMs) have exhibited remarkable capabilities across various tasks, raising the\nquestion of whether LLMs can achieve similar success in complex games. In this\npaper, we explore the potential of LLMs in mastering complex card games. We\nsystematically assess the learning capabilities of LLMs across eight diverse\ncard games, evaluating the impact of fine-tuning on high-quality gameplay data,\nand examining the models' ability to retain general capabilities while\nmastering these games. Our findings indicate that: (1) LLMs can approach the\nperformance of strong game AIs through supervised fine-tuning on high-quality\ndata, (2) LLMs can master multiple complex card games simultaneously, with\nperformance augmentation for games with similar rules and conflicts for\ndissimilar ones, and (3) LLMs experience a decline in general capabilities when\nmastering complex games, but this decline can be mitigated by integrating a\ncertain amount of general instruction data. The evaluation results demonstrate\nstrong learning ability and versatility of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complex games have long been an important benchmark for testing the progress\nof artificial intelligence algorithms. AlphaGo, AlphaZero, and MuZero have\ndefeated top human players in Go and Chess, garnering widespread societal\nattention towards artificial intelligence. Concurrently, large language models\n(LLMs) have exhibited remarkable capabilities across various tasks, raising the\nquestion of whether LLMs can achieve similar success in complex games. In this\npaper, we explore the potential of LLMs in mastering complex card games. We\nsystematically assess the learning capabilities of LLMs across eight diverse\ncard games, evaluating the impact of fine-tuning on high-quality gameplay data,\nand examining the models' ability to retain general capabilities while\nmastering these games. Our findings indicate that: (1) LLMs can approach the\nperformance of strong game AIs through supervised fine-tuning on high-quality\ndata, (2) LLMs can master multiple complex card games simultaneously, with\nperformance augmentation for games with similar rules and conflicts for\ndissimilar ones, and (3) LLMs experience a decline in general capabilities when\nmastering complex games, but this decline can be mitigated by integrating a\ncertain amount of general instruction data. The evaluation results demonstrate\nstrong learning ability and versatility of LLMs."
                },
                "authors": [
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Felix Henry"
                    },
                    {
                        "name": "Junzhe Chen"
                    },
                    {
                        "name": "Dan Zhang"
                    },
                    {
                        "name": "Shiyu Huang"
                    },
                    {
                        "name": "Evgeny Kharlamov"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01328v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01328v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10018v1",
                "updated": "2025-09-12T07:22:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    22,
                    49,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T07:22:49Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    22,
                    49,
                    4,
                    255,
                    0
                ],
                "title": "GAMA: A General Anonymizing Multi-Agent System for Privacy Preservation\n  Enhanced by Domain Rules and Disproof Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GAMA: A General Anonymizing Multi-Agent System for Privacy Preservation\n  Enhanced by Domain Rules and Disproof Method"
                },
                "summary": "With the rapid advancement of Large Language Model (LLM), LLM-based agents\nexhibit exceptional abilities in understanding and generating natural language,\nfacilitating human-like collaboration and information transmission in LLM-based\nMulti-Agent System (MAS). High-performance LLMs are often hosted on remote\nservers in public spaces. When tasks involve privacy data, MAS cannot securely\nutilize these LLMs without implementing privacy-preserving mechanisms. To\naddress this challenge, we propose a General Anonymizing Multi-Agent system\n(GAMA), which divides the agents' workspace into private and public spaces and\nprotects privacy through the anonymizing mechanism. In the private space,\nagents handle sensitive data, while in the public space, only anonymized data\nis utilized. GAMA incorporates two key modules to mitigate semantic loss caused\nby anonymization: Domain-Rule-based Knowledge Enhancement (DRKE) and\nDisproof-based Logic Enhancement (DLE). We evaluate GAMA on two public\nquestion-answering datasets: Trivia Creative Writing and Logic Grid Puzzle. The\nresults demonstrate that GAMA has superior performance compared to the\nstate-of-the-art models. To further assess its privacy-preserving capabilities,\nwe designed two new datasets: Knowledge Privacy Preservation and Logic Privacy\nPreservation. The final results highlight GAMA's exceptional effectiveness in\nboth task processing and privacy preservation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of Large Language Model (LLM), LLM-based agents\nexhibit exceptional abilities in understanding and generating natural language,\nfacilitating human-like collaboration and information transmission in LLM-based\nMulti-Agent System (MAS). High-performance LLMs are often hosted on remote\nservers in public spaces. When tasks involve privacy data, MAS cannot securely\nutilize these LLMs without implementing privacy-preserving mechanisms. To\naddress this challenge, we propose a General Anonymizing Multi-Agent system\n(GAMA), which divides the agents' workspace into private and public spaces and\nprotects privacy through the anonymizing mechanism. In the private space,\nagents handle sensitive data, while in the public space, only anonymized data\nis utilized. GAMA incorporates two key modules to mitigate semantic loss caused\nby anonymization: Domain-Rule-based Knowledge Enhancement (DRKE) and\nDisproof-based Logic Enhancement (DLE). We evaluate GAMA on two public\nquestion-answering datasets: Trivia Creative Writing and Logic Grid Puzzle. The\nresults demonstrate that GAMA has superior performance compared to the\nstate-of-the-art models. To further assess its privacy-preserving capabilities,\nwe designed two new datasets: Knowledge Privacy Preservation and Logic Privacy\nPreservation. The final results highlight GAMA's exceptional effectiveness in\nboth task processing and privacy preservation."
                },
                "authors": [
                    {
                        "name": "Hailong Yang"
                    },
                    {
                        "name": "Renhuo Zhao"
                    },
                    {
                        "name": "Guanjin Wang"
                    },
                    {
                        "name": "Zhaohong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhaohong Deng"
                },
                "author": "Zhaohong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03498v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03498v2",
                "updated": "2025-09-12T07:12:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    12,
                    41,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-03T17:29:50Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    17,
                    29,
                    50,
                    2,
                    246,
                    0
                ],
                "title": "OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and\n  Generation"
                },
                "summary": "We introduce OneCAT, a unified multimodal model that seamlessly integrates\nunderstanding, generation, and editing within a novel, pure decoder-only\ntransformer architecture. Our framework uniquely eliminates the need for\nexternal components such as Vision Transformers (ViT) or vision tokenizer\nduring inference, leading to significant efficiency gains, especially for\nhigh-resolution inputs. This is achieved through a modality-specific\nMixture-of-Experts (MoE) structure trained with a single autoregressive (AR)\nobjective, which also natively supports dynamic resolutions. Furthermore, we\npioneer a multi-scale visual autoregressive mechanism within the Large Language\nModel (LLM) that drastically reduces decoding steps compared to diffusion-based\nmethods while maintaining state-of-the-art performance. Our findings\ndemonstrate the powerful potential of pure autoregressive modeling as a\nsufficient and elegant foundation for unified multimodal intelligence. As a\nresult, OneCAT sets a new performance standard, outperforming existing\nopen-source unified multimodal models across benchmarks for multimodal\ngeneration, editing, and understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce OneCAT, a unified multimodal model that seamlessly integrates\nunderstanding, generation, and editing within a novel, pure decoder-only\ntransformer architecture. Our framework uniquely eliminates the need for\nexternal components such as Vision Transformers (ViT) or vision tokenizer\nduring inference, leading to significant efficiency gains, especially for\nhigh-resolution inputs. This is achieved through a modality-specific\nMixture-of-Experts (MoE) structure trained with a single autoregressive (AR)\nobjective, which also natively supports dynamic resolutions. Furthermore, we\npioneer a multi-scale visual autoregressive mechanism within the Large Language\nModel (LLM) that drastically reduces decoding steps compared to diffusion-based\nmethods while maintaining state-of-the-art performance. Our findings\ndemonstrate the powerful potential of pure autoregressive modeling as a\nsufficient and elegant foundation for unified multimodal intelligence. As a\nresult, OneCAT sets a new performance standard, outperforming existing\nopen-source unified multimodal models across benchmarks for multimodal\ngeneration, editing, and understanding."
                },
                "authors": [
                    {
                        "name": "Han Li"
                    },
                    {
                        "name": "Xinyu Peng"
                    },
                    {
                        "name": "Yaoming Wang"
                    },
                    {
                        "name": "Zelin Peng"
                    },
                    {
                        "name": "Xin Chen"
                    },
                    {
                        "name": "Rongxiang Weng"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Wenrui Dai"
                    },
                    {
                        "name": "Hongkai Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hongkai Xiong"
                },
                "author": "Hongkai Xiong",
                "arxiv_comment": "technical report, project url:https://onecat-ai.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03498v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03498v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10010v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10010v1",
                "updated": "2025-09-12T07:10:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    10,
                    55,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T07:10:55Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    10,
                    55,
                    4,
                    255,
                    0
                ],
                "title": "Multi-Intent Recognition in Dialogue Understanding: A Comparison Between\n  Smaller Open-Source LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Intent Recognition in Dialogue Understanding: A Comparison Between\n  Smaller Open-Source LLMs"
                },
                "summary": "In this paper, we provide an extensive analysis of multi-label intent\nclassification using Large Language Models (LLMs) that are open-source,\npublicly available, and can be run in consumer hardware. We use the MultiWOZ\n2.1 dataset, a benchmark in the dialogue system domain, to investigate the\nefficacy of three popular open-source pre-trained LLMs, namely LLama2-7B-hf,\nMistral-7B-v0.1, and Yi-6B. We perform the classification task in a few-shot\nsetup, giving 20 examples in the prompt with some instructions. Our approach\nfocuses on the differences in performance of these models across several\nperformance metrics by methodically assessing these models on multi-label\nintent classification tasks. Additionally, we compare the performance of the\ninstruction-based fine-tuning approach with supervised learning using the\nsmaller transformer model BertForSequenceClassification as a baseline. To\nevaluate the performance of the models, we use evaluation metrics like\naccuracy, precision, and recall as well as micro, macro, and weighted F1 score.\nWe also report the inference time, VRAM requirements, etc. The Mistral-7B-v0.1\noutperforms two other generative models on 11 intent classes out of 14 in terms\nof F-Score, with a weighted average of 0.50. It also has relatively lower\nHumming Loss and higher Jaccard Similarity, making it the winning model in the\nfew-shot setting. We find BERT based supervised classifier having superior\nperformance compared to the best performing few-shot generative LLM. The study\nprovides a framework for small open-source LLMs in detecting complex\nmulti-intent dialogues, enhancing the Natural Language Understanding aspect of\ntask-oriented chatbots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we provide an extensive analysis of multi-label intent\nclassification using Large Language Models (LLMs) that are open-source,\npublicly available, and can be run in consumer hardware. We use the MultiWOZ\n2.1 dataset, a benchmark in the dialogue system domain, to investigate the\nefficacy of three popular open-source pre-trained LLMs, namely LLama2-7B-hf,\nMistral-7B-v0.1, and Yi-6B. We perform the classification task in a few-shot\nsetup, giving 20 examples in the prompt with some instructions. Our approach\nfocuses on the differences in performance of these models across several\nperformance metrics by methodically assessing these models on multi-label\nintent classification tasks. Additionally, we compare the performance of the\ninstruction-based fine-tuning approach with supervised learning using the\nsmaller transformer model BertForSequenceClassification as a baseline. To\nevaluate the performance of the models, we use evaluation metrics like\naccuracy, precision, and recall as well as micro, macro, and weighted F1 score.\nWe also report the inference time, VRAM requirements, etc. The Mistral-7B-v0.1\noutperforms two other generative models on 11 intent classes out of 14 in terms\nof F-Score, with a weighted average of 0.50. It also has relatively lower\nHumming Loss and higher Jaccard Similarity, making it the winning model in the\nfew-shot setting. We find BERT based supervised classifier having superior\nperformance compared to the best performing few-shot generative LLM. The study\nprovides a framework for small open-source LLMs in detecting complex\nmulti-intent dialogues, enhancing the Natural Language Understanding aspect of\ntask-oriented chatbots."
                },
                "authors": [
                    {
                        "name": "Adnan Ahmad"
                    },
                    {
                        "name": "Philine Kowol"
                    },
                    {
                        "name": "Stefan Hillmann"
                    },
                    {
                        "name": "Sebastian Möller"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Möller"
                },
                "author": "Sebastian Möller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10010v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13654v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13654v4",
                "updated": "2025-09-12T07:04:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    4,
                    59,
                    4,
                    255,
                    0
                ],
                "published": "2025-08-19T09:04:13Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    4,
                    13,
                    1,
                    231,
                    0
                ],
                "title": "Input-Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Input-Time Scaling"
                },
                "summary": "Current Large Language Models (LLMs) are usually post-trained on large-scale\ncarefully curated datasets (data & training scaling) and doing reasoning in\ntest time (inference time scaling). In this work, we present a new scaling\nparadigm, Input-Time Scaling, to complement previous scaling methods by putting\nresources on queries (input time). During training and testing, we utilize\nmeta-knowledge from LLMs to refine inputs with different strategies. We also\ndiscover a new phenomenon, train-test co-design. It requires us to apply query\nstrategies during training and testing as a whole. Only applying strategies on\ntraining or testing would seriously degrade the performance gained. We are also\nsurprised to find that seemingly low data quality datasets can perform better.\nWe can get the best performance even by adding irrelevant information to the\nqueries, with randomly selected 1k examples from a minimally filtered dataset.\nThese findings contradict the widely held inductive bias, \"garbage in, garbage\nout\". Curating datasets with seemingly high-quality data can even potentially\nlimit the performance ceiling. In addition, models trained on more data with\nsimilar quality (15k VS 1k) perform worse, the intuition of simply scaling the\nsize should also be carefully inspected. The good news is that our findings are\ncompatible with the Less is More phenomenon. 1K examples are enough to invoke\nhigh-level reasoning ability. With experiments on Qwen2.5-32B-Instruct, we are\nable to reach SOTA performance among 32B models on AIME24(76.7%) and\nAIME25(76.7%) pass@1. We can further achieve AIME24(76.7%) and AIME25(80%) with\na majority vote of three models. Starting from DeepSeek-R1-Distill-Qwen-32B,\nthe result would be 90.0% on AIME24 and 80.0% on AIME25. To facilitate\nreproducibility and further research, we are working on open-source our\ndatasets, data pipelines, evaluation results, and checkpoints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Large Language Models (LLMs) are usually post-trained on large-scale\ncarefully curated datasets (data & training scaling) and doing reasoning in\ntest time (inference time scaling). In this work, we present a new scaling\nparadigm, Input-Time Scaling, to complement previous scaling methods by putting\nresources on queries (input time). During training and testing, we utilize\nmeta-knowledge from LLMs to refine inputs with different strategies. We also\ndiscover a new phenomenon, train-test co-design. It requires us to apply query\nstrategies during training and testing as a whole. Only applying strategies on\ntraining or testing would seriously degrade the performance gained. We are also\nsurprised to find that seemingly low data quality datasets can perform better.\nWe can get the best performance even by adding irrelevant information to the\nqueries, with randomly selected 1k examples from a minimally filtered dataset.\nThese findings contradict the widely held inductive bias, \"garbage in, garbage\nout\". Curating datasets with seemingly high-quality data can even potentially\nlimit the performance ceiling. In addition, models trained on more data with\nsimilar quality (15k VS 1k) perform worse, the intuition of simply scaling the\nsize should also be carefully inspected. The good news is that our findings are\ncompatible with the Less is More phenomenon. 1K examples are enough to invoke\nhigh-level reasoning ability. With experiments on Qwen2.5-32B-Instruct, we are\nable to reach SOTA performance among 32B models on AIME24(76.7%) and\nAIME25(76.7%) pass@1. We can further achieve AIME24(76.7%) and AIME25(80%) with\na majority vote of three models. Starting from DeepSeek-R1-Distill-Qwen-32B,\nthe result would be 90.0% on AIME24 and 80.0% on AIME25. To facilitate\nreproducibility and further research, we are working on open-source our\ndatasets, data pipelines, evaluation results, and checkpoints."
                },
                "authors": [
                    {
                        "name": "Rapheal Huang"
                    },
                    {
                        "name": "Weilong Guo"
                    }
                ],
                "author_detail": {
                    "name": "Weilong Guo"
                },
                "arxiv_affiliation": "Yuming",
                "author": "Weilong Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13654v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13654v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10005v1",
                "updated": "2025-09-12T07:02:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    2,
                    45,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T07:02:45Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    2,
                    45,
                    4,
                    255,
                    0
                ],
                "title": "TUNI: Real-time RGB-T Semantic Segmentation with Unified Multi-Modal\n  Feature Extraction and Cross-Modal Feature Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TUNI: Real-time RGB-T Semantic Segmentation with Unified Multi-Modal\n  Feature Extraction and Cross-Modal Feature Fusion"
                },
                "summary": "RGB-thermal (RGB-T) semantic segmentation improves the environmental\nperception of autonomous platforms in challenging conditions. Prevailing models\nemploy encoders pre-trained on RGB images to extract features from both RGB and\ninfrared inputs, and design additional modules to achieve cross-modal feature\nfusion. This results in limited thermal feature extraction and suboptimal\ncross-modal fusion, while the redundant encoders further compromises the\nmodel's real-time efficiency. To address the above issues, we propose TUNI,\nwith an RGB-T encoder consisting of multiple stacked blocks that simultaneously\nperform multi-modal feature extraction and cross-modal fusion. By leveraging\nlarge-scale pre-training with RGB and pseudo-thermal data, the RGB-T encoder\nlearns to integrate feature extraction and fusion in a unified manner. By\nslimming down the thermal branch, the encoder achieves a more compact\narchitecture. Moreover, we introduce an RGB-T local module to strengthen the\nencoder's capacity for cross-modal local feature fusion. The RGB-T local module\nemploys adaptive cosine similarity to selectively emphasize salient consistent\nand distinct local features across RGB-T modalities. Experimental results show\nthat TUNI achieves competitive performance with state-of-the-art models on FMB,\nPST900 and CART, with fewer parameters and lower computational cost. Meanwhile,\nit achieves an inference speed of 27 FPS on a Jetson Orin NX, demonstrating its\nreal-time capability in deployment. Codes are available at\nhttps://github.com/xiaodonguo/TUNI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RGB-thermal (RGB-T) semantic segmentation improves the environmental\nperception of autonomous platforms in challenging conditions. Prevailing models\nemploy encoders pre-trained on RGB images to extract features from both RGB and\ninfrared inputs, and design additional modules to achieve cross-modal feature\nfusion. This results in limited thermal feature extraction and suboptimal\ncross-modal fusion, while the redundant encoders further compromises the\nmodel's real-time efficiency. To address the above issues, we propose TUNI,\nwith an RGB-T encoder consisting of multiple stacked blocks that simultaneously\nperform multi-modal feature extraction and cross-modal fusion. By leveraging\nlarge-scale pre-training with RGB and pseudo-thermal data, the RGB-T encoder\nlearns to integrate feature extraction and fusion in a unified manner. By\nslimming down the thermal branch, the encoder achieves a more compact\narchitecture. Moreover, we introduce an RGB-T local module to strengthen the\nencoder's capacity for cross-modal local feature fusion. The RGB-T local module\nemploys adaptive cosine similarity to selectively emphasize salient consistent\nand distinct local features across RGB-T modalities. Experimental results show\nthat TUNI achieves competitive performance with state-of-the-art models on FMB,\nPST900 and CART, with fewer parameters and lower computational cost. Meanwhile,\nit achieves an inference speed of 27 FPS on a Jetson Orin NX, demonstrating its\nreal-time capability in deployment. Codes are available at\nhttps://github.com/xiaodonguo/TUNI."
                },
                "authors": [
                    {
                        "name": "Xiaodong Guo"
                    },
                    {
                        "name": "Tong Liu"
                    },
                    {
                        "name": "Yike Li"
                    },
                    {
                        "name": "Zi'ang Lin"
                    },
                    {
                        "name": "Zhihong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhihong Deng"
                },
                "author": "Zhihong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10004v1",
                "updated": "2025-09-12T06:58:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    6,
                    58,
                    17,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T06:58:17Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    6,
                    58,
                    17,
                    4,
                    255,
                    0
                ],
                "title": "Unsupervised Hallucination Detection by Inspecting Reasoning Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised Hallucination Detection by Inspecting Reasoning Processes"
                },
                "summary": "Unsupervised hallucination detection aims to identify hallucinated content\ngenerated by large language models (LLMs) without relying on labeled data.\nWhile unsupervised methods have gained popularity by eliminating\nlabor-intensive human annotations, they frequently rely on proxy signals\nunrelated to factual correctness. This misalignment biases detection probes\ntoward superficial or non-truth-related aspects, limiting generalizability\nacross datasets and scenarios. To overcome these limitations, we propose IRIS,\nan unsupervised hallucination detection framework, leveraging internal\nrepresentations intrinsic to factual correctness. IRIS prompts the LLM to\ncarefully verify the truthfulness of a given statement, and obtain its\ncontextualized embedding as informative features for training. Meanwhile, the\nuncertainty of each response is considered a soft pseudolabel for truthfulness.\nExperimental results demonstrate that IRIS consistently outperforms existing\nunsupervised methods. Our approach is fully unsupervised, computationally low\ncost, and works well even with few training data, making it suitable for\nreal-time detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised hallucination detection aims to identify hallucinated content\ngenerated by large language models (LLMs) without relying on labeled data.\nWhile unsupervised methods have gained popularity by eliminating\nlabor-intensive human annotations, they frequently rely on proxy signals\nunrelated to factual correctness. This misalignment biases detection probes\ntoward superficial or non-truth-related aspects, limiting generalizability\nacross datasets and scenarios. To overcome these limitations, we propose IRIS,\nan unsupervised hallucination detection framework, leveraging internal\nrepresentations intrinsic to factual correctness. IRIS prompts the LLM to\ncarefully verify the truthfulness of a given statement, and obtain its\ncontextualized embedding as informative features for training. Meanwhile, the\nuncertainty of each response is considered a soft pseudolabel for truthfulness.\nExperimental results demonstrate that IRIS consistently outperforms existing\nunsupervised methods. Our approach is fully unsupervised, computationally low\ncost, and works well even with few training data, making it suitable for\nreal-time detection."
                },
                "authors": [
                    {
                        "name": "Ponhvoan Srey"
                    },
                    {
                        "name": "Xiaobao Wu"
                    },
                    {
                        "name": "Anh Tuan Luu"
                    }
                ],
                "author_detail": {
                    "name": "Anh Tuan Luu"
                },
                "author": "Anh Tuan Luu",
                "arxiv_comment": "To appear in EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00559v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00559v2",
                "updated": "2025-09-12T06:49:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    6,
                    49,
                    21,
                    4,
                    255,
                    0
                ],
                "published": "2024-11-30T19:02:34Z",
                "published_parsed": [
                    2024,
                    11,
                    30,
                    19,
                    2,
                    34,
                    5,
                    335,
                    0
                ],
                "title": "Polish-English medical knowledge transfer: A new benchmark and results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Polish-English medical knowledge transfer: A new benchmark and results"
                },
                "summary": "Large Language Models (LLMs) have demonstrated significant potential in\nhandling specialized tasks, including medical problem-solving. However, most\nstudies predominantly focus on English-language contexts. This study introduces\na novel benchmark dataset based on Polish medical licensing and specialization\nexams (LEK, LDEK, PES) taken by medical doctor candidates and practicing\ndoctors pursuing specialization. The dataset was web-scraped from publicly\navailable resources provided by the Medical Examination Center and the Chief\nMedical Chamber. It comprises over 24,000 exam questions, including a subset of\nparallel Polish-English corpora, where the English portion was professionally\ntranslated by the examination center for foreign candidates. By creating a\nstructured benchmark from these existing exam questions, we systematically\nevaluate state-of-the-art LLMs, including general-purpose, domain-specific, and\nPolish-specific models, and compare their performance against human medical\nstudents. Our analysis reveals that while models like GPT-4o achieve near-human\nperformance, significant challenges persist in cross-lingual translation and\ndomain-specific understanding. These findings underscore disparities in model\nperformance across languages and medical specialties, highlighting the\nlimitations and ethical considerations of deploying LLMs in clinical practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated significant potential in\nhandling specialized tasks, including medical problem-solving. However, most\nstudies predominantly focus on English-language contexts. This study introduces\na novel benchmark dataset based on Polish medical licensing and specialization\nexams (LEK, LDEK, PES) taken by medical doctor candidates and practicing\ndoctors pursuing specialization. The dataset was web-scraped from publicly\navailable resources provided by the Medical Examination Center and the Chief\nMedical Chamber. It comprises over 24,000 exam questions, including a subset of\nparallel Polish-English corpora, where the English portion was professionally\ntranslated by the examination center for foreign candidates. By creating a\nstructured benchmark from these existing exam questions, we systematically\nevaluate state-of-the-art LLMs, including general-purpose, domain-specific, and\nPolish-specific models, and compare their performance against human medical\nstudents. Our analysis reveals that while models like GPT-4o achieve near-human\nperformance, significant challenges persist in cross-lingual translation and\ndomain-specific understanding. These findings underscore disparities in model\nperformance across languages and medical specialties, highlighting the\nlimitations and ethical considerations of deploying LLMs in clinical practice."
                },
                "authors": [
                    {
                        "name": "Łukasz Grzybowski"
                    },
                    {
                        "name": "Jakub Pokrywka"
                    },
                    {
                        "name": "Michał Ciesiółka"
                    },
                    {
                        "name": "Jeremi I. Kaczmarek"
                    },
                    {
                        "name": "Marek Kubis"
                    }
                ],
                "author_detail": {
                    "name": "Marek Kubis"
                },
                "author": "Marek Kubis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00559v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00559v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09997v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09997v1",
                "updated": "2025-09-12T06:47:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    6,
                    47,
                    7,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T06:47:07Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    6,
                    47,
                    7,
                    4,
                    255,
                    0
                ],
                "title": "Taming Volatility: Stable and Private QUIC Classification with Federated\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taming Volatility: Stable and Private QUIC Classification with Federated\n  Learning"
                },
                "summary": "Federated Learning (FL) is a promising approach for privacy-preserving\nnetwork traffic analysis, but its practical deployment is challenged by the\nnon-IID nature of real-world data. While prior work has addressed statistical\nheterogeneity, the impact of temporal traffic volatility-the natural daily ebb\nand flow of network activity-on model stability remains largely unexplored.\nThis volatility can lead to inconsistent data availability at clients,\ndestabilizing the entire training process. In this paper, we systematically\naddress the problem of temporal volatility in federated QUIC classification. We\nfirst demonstrate the instability of standard FL in this dynamic setting. We\nthen propose and evaluate a client-side data buffer as a practical mechanism to\nensure stable and consistent local training, decoupling it from real-time\ntraffic fluctuations. Using the real-world CESNET-QUIC22 dataset partitioned\ninto 14 autonomous clients, we then demonstrate that this approach enables\nrobust convergence. Our results show that a stable federated system achieves a\n95.2% F1 score, a mere 2.3 percentage points below a non-private centralized\nmodel. This work establishes a blueprint for building operationally stable FL\nsystems for network management, proving that the challenges of dynamic network\nenvironments can be overcome with targeted architectural choices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is a promising approach for privacy-preserving\nnetwork traffic analysis, but its practical deployment is challenged by the\nnon-IID nature of real-world data. While prior work has addressed statistical\nheterogeneity, the impact of temporal traffic volatility-the natural daily ebb\nand flow of network activity-on model stability remains largely unexplored.\nThis volatility can lead to inconsistent data availability at clients,\ndestabilizing the entire training process. In this paper, we systematically\naddress the problem of temporal volatility in federated QUIC classification. We\nfirst demonstrate the instability of standard FL in this dynamic setting. We\nthen propose and evaluate a client-side data buffer as a practical mechanism to\nensure stable and consistent local training, decoupling it from real-time\ntraffic fluctuations. Using the real-world CESNET-QUIC22 dataset partitioned\ninto 14 autonomous clients, we then demonstrate that this approach enables\nrobust convergence. Our results show that a stable federated system achieves a\n95.2% F1 score, a mere 2.3 percentage points below a non-private centralized\nmodel. This work establishes a blueprint for building operationally stable FL\nsystems for network management, proving that the challenges of dynamic network\nenvironments can be overcome with targeted architectural choices."
                },
                "authors": [
                    {
                        "name": "Richard Jozsa"
                    },
                    {
                        "name": "Karel Hynek"
                    },
                    {
                        "name": "Adrian Pekar"
                    }
                ],
                "author_detail": {
                    "name": "Adrian Pekar"
                },
                "author": "Adrian Pekar",
                "arxiv_comment": "Accepted for presentation at CNSM2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09997v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09550v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09550v2",
                "updated": "2025-09-12T06:43:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    6,
                    43,
                    25,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-11T15:39:59Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    39,
                    59,
                    3,
                    254,
                    0
                ],
                "title": "Finite Scalar Quantization Enables Redundant and Transmission-Robust\n  Neural Audio Compression at Low Bit-rates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finite Scalar Quantization Enables Redundant and Transmission-Robust\n  Neural Audio Compression at Low Bit-rates"
                },
                "summary": "Neural Audio Codecs (NACs) have become increasingly adopted in speech\nprocessing tasks due to their excellent rate-distortion performance and\ncompatibility with Large Language Models (LLMs) as discrete feature\nrepresentations for audio generation. While most existing codecs rely on\nResidual Vector Quantization (RVQ), Finite Scalar Quantization (FSQ) has\nrecently emerged as a compelling alternative that simplifies training and\nnatively supports single codebooks. We introduce NeuCodec, an FSQ-based NAC,\nand show that FSQ encodes baked-in redundancy which produces an encoding which\nis robust when transmitted through noisy channels. First, through an encoder\ndistillation experiment, we show that two different encoders can learn to\nencode identical audio into vastly different code sequences whilst maintaining\ncomparable reconstruction quality with the same quantizer and decoder. Second,\nwe demonstrate that FSQ has vastly superior bit-level perturbation robustness\nby comparing the performance of RVQ and FSQ codecs when simulating the\ntransmission of code sequences through a noisy channel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Audio Codecs (NACs) have become increasingly adopted in speech\nprocessing tasks due to their excellent rate-distortion performance and\ncompatibility with Large Language Models (LLMs) as discrete feature\nrepresentations for audio generation. While most existing codecs rely on\nResidual Vector Quantization (RVQ), Finite Scalar Quantization (FSQ) has\nrecently emerged as a compelling alternative that simplifies training and\nnatively supports single codebooks. We introduce NeuCodec, an FSQ-based NAC,\nand show that FSQ encodes baked-in redundancy which produces an encoding which\nis robust when transmitted through noisy channels. First, through an encoder\ndistillation experiment, we show that two different encoders can learn to\nencode identical audio into vastly different code sequences whilst maintaining\ncomparable reconstruction quality with the same quantizer and decoder. Second,\nwe demonstrate that FSQ has vastly superior bit-level perturbation robustness\nby comparing the performance of RVQ and FSQ codecs when simulating the\ntransmission of code sequences through a noisy channel."
                },
                "authors": [
                    {
                        "name": "Harry Julian"
                    },
                    {
                        "name": "Rachel Beeson"
                    },
                    {
                        "name": "Lohith Konathala"
                    },
                    {
                        "name": "Johanna Ulin"
                    },
                    {
                        "name": "Jiameng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jiameng Gao"
                },
                "author": "Jiameng Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09550v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09550v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17222v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17222v2",
                "updated": "2025-09-12T06:41:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    6,
                    41,
                    58,
                    4,
                    255,
                    0
                ],
                "published": "2025-05-22T18:55:22Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    18,
                    55,
                    22,
                    3,
                    142,
                    0
                ],
                "title": "Humans Hallucinate Too: Language Models Identify and Correct Subjective\n  Annotation Errors With Label-in-a-Haystack Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans Hallucinate Too: Language Models Identify and Correct Subjective\n  Annotation Errors With Label-in-a-Haystack Prompts"
                },
                "summary": "Modeling complex subjective tasks in Natural Language Processing, such as\nrecognizing emotion and morality, is considerably challenging due to\nsignificant variation in human annotations. This variation often reflects\nreasonable differences in semantic interpretations rather than mere noise,\nnecessitating methods to distinguish between legitimate subjectivity and error.\nWe address this challenge by exploring label verification in these contexts\nusing Large Language Models (LLMs). First, we propose a simple In-Context\nLearning binary filtering baseline that estimates the reasonableness of a\ndocument-label pair. We then introduce the Label-in-a-Haystack setting: the\nquery and its label(s) are included in the demonstrations shown to LLMs, which\nare prompted to predict the label(s) again, while receiving task-specific\ninstructions (e.g., emotion recognition) rather than label copying. We show how\nthe failure to copy the label(s) to the output of the LLM are task-relevant and\ninformative. Building on this, we propose the Label-in-a-Haystack Rectification\n(LiaHR) framework for subjective label correction: when the model outputs\ndiverge from the reference gold labels, we assign the generated labels to the\nexample instead of discarding it. This approach can be integrated into\nannotation pipelines to enhance signal-to-noise ratios. Comprehensive analyses,\nhuman evaluations, and ecological validity studies verify the utility of LiaHR\nfor label correction. Code is available at https://github.com/gchochla/liahr.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling complex subjective tasks in Natural Language Processing, such as\nrecognizing emotion and morality, is considerably challenging due to\nsignificant variation in human annotations. This variation often reflects\nreasonable differences in semantic interpretations rather than mere noise,\nnecessitating methods to distinguish between legitimate subjectivity and error.\nWe address this challenge by exploring label verification in these contexts\nusing Large Language Models (LLMs). First, we propose a simple In-Context\nLearning binary filtering baseline that estimates the reasonableness of a\ndocument-label pair. We then introduce the Label-in-a-Haystack setting: the\nquery and its label(s) are included in the demonstrations shown to LLMs, which\nare prompted to predict the label(s) again, while receiving task-specific\ninstructions (e.g., emotion recognition) rather than label copying. We show how\nthe failure to copy the label(s) to the output of the LLM are task-relevant and\ninformative. Building on this, we propose the Label-in-a-Haystack Rectification\n(LiaHR) framework for subjective label correction: when the model outputs\ndiverge from the reference gold labels, we assign the generated labels to the\nexample instead of discarding it. This approach can be integrated into\nannotation pipelines to enhance signal-to-noise ratios. Comprehensive analyses,\nhuman evaluations, and ecological validity studies verify the utility of LiaHR\nfor label correction. Code is available at https://github.com/gchochla/liahr."
                },
                "authors": [
                    {
                        "name": "Georgios Chochlakis"
                    },
                    {
                        "name": "Peter Wu"
                    },
                    {
                        "name": "Arjun Bedi"
                    },
                    {
                        "name": "Marcus Ma"
                    },
                    {
                        "name": "Kristina Lerman"
                    },
                    {
                        "name": "Shrikanth Narayanan"
                    }
                ],
                "author_detail": {
                    "name": "Shrikanth Narayanan"
                },
                "author": "Shrikanth Narayanan",
                "arxiv_comment": "Accepted to the Main Proceedings of EMNLP, 2025. 20 pages, 16\n  figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17222v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17222v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09995v1",
                "updated": "2025-09-12T06:35:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    6,
                    35,
                    40,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T06:35:40Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    6,
                    35,
                    40,
                    4,
                    255,
                    0
                ],
                "title": "QuantAgent: Price-Driven Multi-Agent LLMs for High-Frequency Trading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuantAgent: Price-Driven Multi-Agent LLMs for High-Frequency Trading"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have demonstrated impressive\ncapabilities in financial reasoning and market understanding. Multi-agent LLM\nframeworks such as TradingAgent and FINMEM augment these models to long-horizon\ninvestment tasks, leveraging fundamental and sentiment-based inputs for\nstrategic decision-making. However, such systems are ill-suited for the\nhigh-speed, precision-critical demands of High-Frequency Trading (HFT). HFT\nrequires rapid, risk-aware decisions based on structured, short-horizon\nsignals, including technical indicators, chart patterns, and trend-based\nfeatures, distinct from the long-term semantic reasoning typical of traditional\nfinancial LLM applications. To this end, we introduce QuantAgent, the first\nmulti-agent LLM framework explicitly designed for high-frequency algorithmic\ntrading. The system decomposes trading into four specialized agents, Indicator,\nPattern, Trend, and Risk, each equipped with domain-specific tools and\nstructured reasoning capabilities to capture distinct aspects of market\ndynamics over short temporal windows. In zero-shot evaluations across ten\nfinancial instruments, including Bitcoin and Nasdaq futures, QuantAgent\ndemonstrates superior performance in both predictive accuracy and cumulative\nreturn over 4-hour trading intervals, outperforming strong neural and\nrule-based baselines. Our findings suggest that combining structured financial\npriors with language-native reasoning unlocks new potential for traceable,\nreal-time decision systems in high-frequency financial markets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have demonstrated impressive\ncapabilities in financial reasoning and market understanding. Multi-agent LLM\nframeworks such as TradingAgent and FINMEM augment these models to long-horizon\ninvestment tasks, leveraging fundamental and sentiment-based inputs for\nstrategic decision-making. However, such systems are ill-suited for the\nhigh-speed, precision-critical demands of High-Frequency Trading (HFT). HFT\nrequires rapid, risk-aware decisions based on structured, short-horizon\nsignals, including technical indicators, chart patterns, and trend-based\nfeatures, distinct from the long-term semantic reasoning typical of traditional\nfinancial LLM applications. To this end, we introduce QuantAgent, the first\nmulti-agent LLM framework explicitly designed for high-frequency algorithmic\ntrading. The system decomposes trading into four specialized agents, Indicator,\nPattern, Trend, and Risk, each equipped with domain-specific tools and\nstructured reasoning capabilities to capture distinct aspects of market\ndynamics over short temporal windows. In zero-shot evaluations across ten\nfinancial instruments, including Bitcoin and Nasdaq futures, QuantAgent\ndemonstrates superior performance in both predictive accuracy and cumulative\nreturn over 4-hour trading intervals, outperforming strong neural and\nrule-based baselines. Our findings suggest that combining structured financial\npriors with language-native reasoning unlocks new potential for traceable,\nreal-time decision systems in high-frequency financial markets."
                },
                "authors": [
                    {
                        "name": "Fei Xiong"
                    },
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Aosong Feng"
                    },
                    {
                        "name": "Siqi Sun"
                    },
                    {
                        "name": "Chenyu You"
                    }
                ],
                "author_detail": {
                    "name": "Chenyu You"
                },
                "author": "Chenyu You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03897v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03897v2",
                "updated": "2025-09-12T05:44:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    5,
                    44,
                    35,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-04T05:43:50Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    5,
                    43,
                    50,
                    3,
                    247,
                    0
                ],
                "title": "SPECS: Specificity-Enhanced CLIP-Score for Long Image Caption Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPECS: Specificity-Enhanced CLIP-Score for Long Image Caption Evaluation"
                },
                "summary": "As interest grows in generating long, detailed image captions, standard\nevaluation metrics become increasingly unreliable. N-gram-based metrics though\nefficient, fail to capture semantic correctness. Representational Similarity\n(RS) metrics, designed to address this, initially saw limited use due to high\ncomputational costs, while today, despite advances in hardware, they remain\nunpopular due to low correlation to human judgments. Meanwhile, metrics based\non large language models (LLMs) show strong correlation with human judgments,\nbut remain too expensive for iterative use during model development.\n  We introduce SPECS (Specificity-Enhanced CLIPScore), a reference-free RS\nmetric tailored to long image captioning. SPECS modifies CLIP with a new\nobjective that emphasizes specificity: rewarding correct details and penalizing\nincorrect ones. We show that SPECS matches the performance of open-source\nLLM-based metrics in correlation to human judgments, while being far more\nefficient. This makes it a practical alternative for iterative checkpoint\nevaluation during image captioning model development.Our code can be found at\nhttps://github.com/mbzuai-nlp/SPECS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As interest grows in generating long, detailed image captions, standard\nevaluation metrics become increasingly unreliable. N-gram-based metrics though\nefficient, fail to capture semantic correctness. Representational Similarity\n(RS) metrics, designed to address this, initially saw limited use due to high\ncomputational costs, while today, despite advances in hardware, they remain\nunpopular due to low correlation to human judgments. Meanwhile, metrics based\non large language models (LLMs) show strong correlation with human judgments,\nbut remain too expensive for iterative use during model development.\n  We introduce SPECS (Specificity-Enhanced CLIPScore), a reference-free RS\nmetric tailored to long image captioning. SPECS modifies CLIP with a new\nobjective that emphasizes specificity: rewarding correct details and penalizing\nincorrect ones. We show that SPECS matches the performance of open-source\nLLM-based metrics in correlation to human judgments, while being far more\nefficient. This makes it a practical alternative for iterative checkpoint\nevaluation during image captioning model development.Our code can be found at\nhttps://github.com/mbzuai-nlp/SPECS."
                },
                "authors": [
                    {
                        "name": "Xiaofu Chen"
                    },
                    {
                        "name": "Israfel Salazar"
                    },
                    {
                        "name": "Yova Kementchedjhieva"
                    }
                ],
                "author_detail": {
                    "name": "Yova Kementchedjhieva"
                },
                "author": "Yova Kementchedjhieva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03897v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03897v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09975v1",
                "updated": "2025-09-12T05:34:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    5,
                    34,
                    48,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T05:34:48Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    5,
                    34,
                    48,
                    4,
                    255,
                    0
                ],
                "title": "Development of Automated Software Design Document Review Methods Using\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development of Automated Software Design Document Review Methods Using\n  Large Language Models"
                },
                "summary": "In this study, we explored an approach to automate the review process of\nsoftware design documents by using LLM. We first analyzed the review methods of\ndesign documents and organized 11 review perspectives. Additionally, we\nanalyzed the issues of utilizing LLMs for these 11 review perspectives and\ndetermined which perspectives can be reviewed by current general-purpose LLMs\ninstead of humans. For the reviewable perspectives, we specifically developed\nnew techniques to enable LLMs to comprehend complex design documents that\ninclude table data. For evaluation, we conducted experiments using GPT to\nassess the consistency of design items and descriptions across different design\ndocuments in the design process used in actual business operations. Our results\nconfirmed that LLMs can be utilized to identify inconsistencies in software\ndesign documents during the review process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we explored an approach to automate the review process of\nsoftware design documents by using LLM. We first analyzed the review methods of\ndesign documents and organized 11 review perspectives. Additionally, we\nanalyzed the issues of utilizing LLMs for these 11 review perspectives and\ndetermined which perspectives can be reviewed by current general-purpose LLMs\ninstead of humans. For the reviewable perspectives, we specifically developed\nnew techniques to enable LLMs to comprehend complex design documents that\ninclude table data. For evaluation, we conducted experiments using GPT to\nassess the consistency of design items and descriptions across different design\ndocuments in the design process used in actual business operations. Our results\nconfirmed that LLMs can be utilized to identify inconsistencies in software\ndesign documents during the review process."
                },
                "authors": [
                    {
                        "name": "Takasaburo Fukuda"
                    },
                    {
                        "name": "Takao Nakagawa"
                    },
                    {
                        "name": "Keisuke Miyazaki"
                    },
                    {
                        "name": "Susumu Tokumoto"
                    }
                ],
                "author_detail": {
                    "name": "Susumu Tokumoto"
                },
                "author": "Susumu Tokumoto",
                "arxiv_comment": "SANER 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09972v1",
                "updated": "2025-09-12T05:16:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    5,
                    16,
                    56,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T05:16:56Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    5,
                    16,
                    56,
                    4,
                    255,
                    0
                ],
                "title": "Drone-Based Multispectral Imaging and Deep Learning for Timely Detection\n  of Branched Broomrape in Tomato Farms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drone-Based Multispectral Imaging and Deep Learning for Timely Detection\n  of Branched Broomrape in Tomato Farms"
                },
                "summary": "This study addresses the escalating threat of branched broomrape (Phelipanche\nramosa) to California's tomato industry, which supplies over 90 percent of U.S.\nprocessing tomatoes. The parasite's largely underground life cycle makes early\ndetection difficult, while conventional chemical controls are costly,\nenvironmentally harmful, and often ineffective. To address this, we combined\ndrone-based multispectral imagery with Long Short-Term Memory (LSTM) deep\nlearning networks, using the Synthetic Minority Over-sampling Technique (SMOTE)\nto handle class imbalance. Research was conducted on a known broomrape-infested\ntomato farm in Woodland, Yolo County, CA, across five key growth stages\ndetermined by growing degree days (GDD). Multispectral images were processed to\nisolate tomato canopy reflectance. At 897 GDD, broomrape could be detected with\n79.09 percent overall accuracy and 70.36 percent recall without integrating\nlater stages. Incorporating sequential growth stages with LSTM improved\ndetection substantially. The best-performing scenario, which integrated all\ngrowth stages with SMOTE augmentation, achieved 88.37 percent overall accuracy\nand 95.37 percent recall. These results demonstrate the strong potential of\ntemporal multispectral analysis and LSTM networks for early broomrape\ndetection. While further real-world data collection is needed for practical\ndeployment, this study shows that UAV-based multispectral sensing coupled with\ndeep learning could provide a powerful precision agriculture tool to reduce\nlosses and improve sustainability in tomato production.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study addresses the escalating threat of branched broomrape (Phelipanche\nramosa) to California's tomato industry, which supplies over 90 percent of U.S.\nprocessing tomatoes. The parasite's largely underground life cycle makes early\ndetection difficult, while conventional chemical controls are costly,\nenvironmentally harmful, and often ineffective. To address this, we combined\ndrone-based multispectral imagery with Long Short-Term Memory (LSTM) deep\nlearning networks, using the Synthetic Minority Over-sampling Technique (SMOTE)\nto handle class imbalance. Research was conducted on a known broomrape-infested\ntomato farm in Woodland, Yolo County, CA, across five key growth stages\ndetermined by growing degree days (GDD). Multispectral images were processed to\nisolate tomato canopy reflectance. At 897 GDD, broomrape could be detected with\n79.09 percent overall accuracy and 70.36 percent recall without integrating\nlater stages. Incorporating sequential growth stages with LSTM improved\ndetection substantially. The best-performing scenario, which integrated all\ngrowth stages with SMOTE augmentation, achieved 88.37 percent overall accuracy\nand 95.37 percent recall. These results demonstrate the strong potential of\ntemporal multispectral analysis and LSTM networks for early broomrape\ndetection. While further real-world data collection is needed for practical\ndeployment, this study shows that UAV-based multispectral sensing coupled with\ndeep learning could provide a powerful precision agriculture tool to reduce\nlosses and improve sustainability in tomato production."
                },
                "authors": [
                    {
                        "name": "Mohammadreza Narimani"
                    },
                    {
                        "name": "Alireza Pourreza"
                    },
                    {
                        "name": "Ali Moghimi"
                    },
                    {
                        "name": "Mohsen Mesgaran"
                    },
                    {
                        "name": "Parastoo Farajpoor"
                    },
                    {
                        "name": "Hamid Jafarbiglu"
                    }
                ],
                "author_detail": {
                    "name": "Hamid Jafarbiglu"
                },
                "author": "Hamid Jafarbiglu",
                "arxiv_doi": "10.1117/12.3021219.",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1117/12.3021219.",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.09972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Author-accepted version (no publisher header/footer). 10 pages +\n  presentation. Published in Proceedings of SPIE Defense + Commercial Sensing\n  2024, Vol. 13053, Paper 1305304. Event: National Harbor, Maryland, USA.\n  Official version: https://doi.org/10.1117/12.3021219",
                "arxiv_journal_ref": "Proc. SPIE 13053, Autonomous Air and Ground Sensing Systems for\n  Agricultural Optimization and Phenotyping IX, 1305304 (7 June 2024)",
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09970v1",
                "updated": "2025-09-12T05:15:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    5,
                    15,
                    35,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T05:15:35Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    5,
                    15,
                    35,
                    4,
                    255,
                    0
                ],
                "title": "Securing LLM-Generated Embedded Firmware through AI Agent-Driven\n  Validation and Patching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Securing LLM-Generated Embedded Firmware through AI Agent-Driven\n  Validation and Patching"
                },
                "summary": "Large Language Models (LLMs) show promise in generating firmware for embedded\nsystems, but often introduce security flaws and fail to meet real-time\nperformance constraints. This paper proposes a three-phase methodology that\ncombines LLM-based firmware generation with automated security validation and\niterative refinement in a virtualized environment. Using structured prompts,\nmodels like GPT-4 generate firmware for networking and control tasks, deployed\non FreeRTOS via QEMU. These implementations are tested using fuzzing, static\nanalysis, and runtime monitoring to detect vulnerabilities such as buffer\noverflows (CWE-120), race conditions (CWE-362), and denial-of-service threats\n(CWE-400). Specialized AI agents for Threat Detection, Performance\nOptimization, and Compliance Verification collaborate to improve detection and\nremediation. Identified issues are categorized using CWE, then used to prompt\ntargeted LLM-generated patches in an iterative loop. Experiments show a 92.4\\%\nVulnerability Remediation Rate (37.3\\% improvement), 95.8\\% Threat Model\nCompliance, and 0.87 Security Coverage Index. Real-time metrics include 8.6ms\nworst-case execution time and 195{\\mu}s jitter. This process enhances firmware\nsecurity and performance while contributing an open-source dataset for future\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show promise in generating firmware for embedded\nsystems, but often introduce security flaws and fail to meet real-time\nperformance constraints. This paper proposes a three-phase methodology that\ncombines LLM-based firmware generation with automated security validation and\niterative refinement in a virtualized environment. Using structured prompts,\nmodels like GPT-4 generate firmware for networking and control tasks, deployed\non FreeRTOS via QEMU. These implementations are tested using fuzzing, static\nanalysis, and runtime monitoring to detect vulnerabilities such as buffer\noverflows (CWE-120), race conditions (CWE-362), and denial-of-service threats\n(CWE-400). Specialized AI agents for Threat Detection, Performance\nOptimization, and Compliance Verification collaborate to improve detection and\nremediation. Identified issues are categorized using CWE, then used to prompt\ntargeted LLM-generated patches in an iterative loop. Experiments show a 92.4\\%\nVulnerability Remediation Rate (37.3\\% improvement), 95.8\\% Threat Model\nCompliance, and 0.87 Security Coverage Index. Real-time metrics include 8.6ms\nworst-case execution time and 195{\\mu}s jitter. This process enhances firmware\nsecurity and performance while contributing an open-source dataset for future\nresearch."
                },
                "authors": [
                    {
                        "name": "Seyed Moein Abtahi"
                    },
                    {
                        "name": "Akramul Azim"
                    }
                ],
                "author_detail": {
                    "name": "Akramul Azim"
                },
                "author": "Akramul Azim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09969v1",
                "updated": "2025-09-12T05:08:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    5,
                    8,
                    11,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T05:08:11Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    5,
                    8,
                    11,
                    4,
                    255,
                    0
                ],
                "title": "Large Language Models Meet Legal Artificial Intelligence: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Meet Legal Artificial Intelligence: A Survey"
                },
                "summary": "Large Language Models (LLMs) have significantly advanced the development of\nLegal Artificial Intelligence (Legal AI) in recent years, enhancing the\nefficiency and accuracy of legal tasks. To advance research and applications of\nLLM-based approaches in legal domain, this paper provides a comprehensive\nreview of 16 legal LLMs series and 47 LLM-based frameworks for legal tasks, and\nalso gather 15 benchmarks and 29 datasets to evaluate different legal\ncapabilities. Additionally, we analyse the challenges and discuss future\ndirections for LLM-based approaches in the legal domain. We hope this paper\nprovides a systematic introduction for beginners and encourages future research\nin this field. Resources are available at\nhttps://github.com/ZhitianHou/LLMs4LegalAI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have significantly advanced the development of\nLegal Artificial Intelligence (Legal AI) in recent years, enhancing the\nefficiency and accuracy of legal tasks. To advance research and applications of\nLLM-based approaches in legal domain, this paper provides a comprehensive\nreview of 16 legal LLMs series and 47 LLM-based frameworks for legal tasks, and\nalso gather 15 benchmarks and 29 datasets to evaluate different legal\ncapabilities. Additionally, we analyse the challenges and discuss future\ndirections for LLM-based approaches in the legal domain. We hope this paper\nprovides a systematic introduction for beginners and encourages future research\nin this field. Resources are available at\nhttps://github.com/ZhitianHou/LLMs4LegalAI."
                },
                "authors": [
                    {
                        "name": "Zhitian Hou"
                    },
                    {
                        "name": "Zihan Ye"
                    },
                    {
                        "name": "Nanli Zeng"
                    },
                    {
                        "name": "Tianyong Hao"
                    },
                    {
                        "name": "Kun Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Kun Zeng"
                },
                "author": "Kun Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05674v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05674v2",
                "updated": "2025-09-12T04:53:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    4,
                    53,
                    53,
                    4,
                    255,
                    0
                ],
                "published": "2025-07-08T04:57:23Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    4,
                    57,
                    23,
                    1,
                    189,
                    0
                ],
                "title": "Integrating Diffusion-based Multi-task Learning with Online\n  Reinforcement Learning for Robust Quadruped Robot Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Diffusion-based Multi-task Learning with Online\n  Reinforcement Learning for Robust Quadruped Robot Control"
                },
                "summary": "Recent research has highlighted the powerful capabilities of imitation\nlearning in robotics. Leveraging generative models, particularly diffusion\nmodels, these approaches offer notable advantages such as strong multi-task\ngeneralization, effective language conditioning, and high sample efficiency.\nWhile their application has been successful in manipulation tasks, their use in\nlegged locomotion remains relatively underexplored, mainly due to compounding\nerrors that affect stability and difficulties in task transition under limited\ndata. Online reinforcement learning (RL) has demonstrated promising results in\nlegged robot control in the past years, providing valuable insights to address\nthese challenges. In this work, we propose DMLoco, a diffusion-based framework\nfor quadruped robots that integrates multi-task pretraining with online PPO\nfinetuning to enable language-conditioned control and robust task transitions.\nOur approach first pretrains the policy on a diverse multi-task dataset using\ndiffusion models, enabling language-guided execution of various skills. Then,\nit finetunes the policy in simulation to ensure robustness and stable task\ntransition during real-world deployment. By utilizing Denoising Diffusion\nImplicit Models (DDIM) for efficient sampling and TensorRT for optimized\ndeployment, our policy runs onboard at 50Hz, offering a scalable and efficient\nsolution for adaptive, language-guided locomotion on resource-constrained\nrobotic platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has highlighted the powerful capabilities of imitation\nlearning in robotics. Leveraging generative models, particularly diffusion\nmodels, these approaches offer notable advantages such as strong multi-task\ngeneralization, effective language conditioning, and high sample efficiency.\nWhile their application has been successful in manipulation tasks, their use in\nlegged locomotion remains relatively underexplored, mainly due to compounding\nerrors that affect stability and difficulties in task transition under limited\ndata. Online reinforcement learning (RL) has demonstrated promising results in\nlegged robot control in the past years, providing valuable insights to address\nthese challenges. In this work, we propose DMLoco, a diffusion-based framework\nfor quadruped robots that integrates multi-task pretraining with online PPO\nfinetuning to enable language-conditioned control and robust task transitions.\nOur approach first pretrains the policy on a diverse multi-task dataset using\ndiffusion models, enabling language-guided execution of various skills. Then,\nit finetunes the policy in simulation to ensure robustness and stable task\ntransition during real-world deployment. By utilizing Denoising Diffusion\nImplicit Models (DDIM) for efficient sampling and TensorRT for optimized\ndeployment, our policy runs onboard at 50Hz, offering a scalable and efficient\nsolution for adaptive, language-guided locomotion on resource-constrained\nrobotic platforms."
                },
                "authors": [
                    {
                        "name": "Xinyao Qin"
                    },
                    {
                        "name": "Xiaoteng Ma"
                    },
                    {
                        "name": "Yang Qi"
                    },
                    {
                        "name": "Qihan Liu"
                    },
                    {
                        "name": "Chuanyi Xue"
                    },
                    {
                        "name": "Ning Gui"
                    },
                    {
                        "name": "Qinyu Dong"
                    },
                    {
                        "name": "Jun Yang"
                    },
                    {
                        "name": "Bin Liang"
                    }
                ],
                "author_detail": {
                    "name": "Bin Liang"
                },
                "author": "Bin Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05674v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05674v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11829v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11829v4",
                "updated": "2025-09-12T04:48:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    4,
                    48,
                    46,
                    4,
                    255,
                    0
                ],
                "published": "2025-04-16T07:38:19Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    38,
                    19,
                    2,
                    106,
                    0
                ],
                "title": "Déjà Vu: Multilingual LLM Evaluation through the Lens of Machine\n  Translation Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Déjà Vu: Multilingual LLM Evaluation through the Lens of Machine\n  Translation Evaluation"
                },
                "summary": "Generation capabilities and language coverage of multilingual large language\nmodels (mLLMs) are advancing rapidly. However, evaluation practices for\ngenerative abilities of mLLMs are still lacking comprehensiveness, scientific\nrigor, and consistent adoption across research labs, which undermines their\npotential to meaningfully guide mLLM development. We draw parallels with\nmachine translation (MT) evaluation, a field that faced similar challenges and\nhas, over decades, developed transparent reporting standards and reliable\nevaluations for multilingual generative models. Through targeted experiments\nacross key stages of the generative evaluation pipeline, we demonstrate how\nbest practices from MT evaluation can deepen the understanding of quality\ndifferences between models. Additionally, we identify essential components for\nrobust meta-evaluation of mLLMs, ensuring the evaluation methods themselves are\nrigorously assessed. We distill these insights into a checklist of actionable\nrecommendations for mLLM research and development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generation capabilities and language coverage of multilingual large language\nmodels (mLLMs) are advancing rapidly. However, evaluation practices for\ngenerative abilities of mLLMs are still lacking comprehensiveness, scientific\nrigor, and consistent adoption across research labs, which undermines their\npotential to meaningfully guide mLLM development. We draw parallels with\nmachine translation (MT) evaluation, a field that faced similar challenges and\nhas, over decades, developed transparent reporting standards and reliable\nevaluations for multilingual generative models. Through targeted experiments\nacross key stages of the generative evaluation pipeline, we demonstrate how\nbest practices from MT evaluation can deepen the understanding of quality\ndifferences between models. Additionally, we identify essential components for\nrobust meta-evaluation of mLLMs, ensuring the evaluation methods themselves are\nrigorously assessed. We distill these insights into a checklist of actionable\nrecommendations for mLLM research and development."
                },
                "authors": [
                    {
                        "name": "Julia Kreutzer"
                    },
                    {
                        "name": "Eleftheria Briakou"
                    },
                    {
                        "name": "Sweta Agrawal"
                    },
                    {
                        "name": "Marzieh Fadaee"
                    },
                    {
                        "name": "Kocmi Tom"
                    }
                ],
                "author_detail": {
                    "name": "Kocmi Tom"
                },
                "author": "Kocmi Tom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11829v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11829v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11007v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11007v3",
                "updated": "2025-09-12T04:41:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    4,
                    41,
                    28,
                    4,
                    255,
                    0
                ],
                "published": "2025-02-16T06:18:28Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    6,
                    18,
                    28,
                    6,
                    47,
                    0
                ],
                "title": "Local-Cloud Inference Offloading for LLMs in Multi-Modal, Multi-Task,\n  Multi-Dialogue Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local-Cloud Inference Offloading for LLMs in Multi-Modal, Multi-Task,\n  Multi-Dialogue Settings"
                },
                "summary": "Compared to traditional machine learning models, recent large language models\n(LLMs) can exhibit multi-task-solving capabilities through multiple dialogues\nand multi-modal data sources. These unique characteristics of LLMs, together\nwith their large model size, make their deployment more challenging.\nSpecifically, (i) deploying LLMs on local devices faces computational, memory,\nand energy resource issues, while (ii) deploying them in the cloud cannot\nguarantee real-time service and incurs communication/usage costs. In this\npaper, we design TMO, a local-cloud LLM inference system with Three-M\nOffloading: Multi-modal, Multi-task, and Multi-dialogue. TMO incorporates (i) a\nlightweight local LLM that can process simple tasks at high speed and (ii) a\nlarge-scale cloud LLM that can handle multi-modal data sources. We develop a\nresource-constrained reinforcement learning (RCRL) strategy for TMO that\noptimizes the inference location (i.e., local vs. cloud) and multi-modal data\nsources to use for each task/dialogue, aiming to maximize the long-term reward\n(response quality, latency, and usage cost) while adhering to resource\nconstraints. We also contribute M4A1, a new dataset we curated that contains\nreward and cost metrics across multiple modality, task, dialogue, and LLM\nconfigurations, enabling evaluation of offloading decisions. We demonstrate the\neffectiveness of TMO compared to several exploration-decision and LLM-as-Agent\nbaselines, showing significant improvements in latency, cost, and response\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compared to traditional machine learning models, recent large language models\n(LLMs) can exhibit multi-task-solving capabilities through multiple dialogues\nand multi-modal data sources. These unique characteristics of LLMs, together\nwith their large model size, make their deployment more challenging.\nSpecifically, (i) deploying LLMs on local devices faces computational, memory,\nand energy resource issues, while (ii) deploying them in the cloud cannot\nguarantee real-time service and incurs communication/usage costs. In this\npaper, we design TMO, a local-cloud LLM inference system with Three-M\nOffloading: Multi-modal, Multi-task, and Multi-dialogue. TMO incorporates (i) a\nlightweight local LLM that can process simple tasks at high speed and (ii) a\nlarge-scale cloud LLM that can handle multi-modal data sources. We develop a\nresource-constrained reinforcement learning (RCRL) strategy for TMO that\noptimizes the inference location (i.e., local vs. cloud) and multi-modal data\nsources to use for each task/dialogue, aiming to maximize the long-term reward\n(response quality, latency, and usage cost) while adhering to resource\nconstraints. We also contribute M4A1, a new dataset we curated that contains\nreward and cost metrics across multiple modality, task, dialogue, and LLM\nconfigurations, enabling evaluation of offloading decisions. We demonstrate the\neffectiveness of TMO compared to several exploration-decision and LLM-as-Agent\nbaselines, showing significant improvements in latency, cost, and response\nquality."
                },
                "authors": [
                    {
                        "name": "Liangqi Yuan"
                    },
                    {
                        "name": "Dong-Jun Han"
                    },
                    {
                        "name": "Shiqiang Wang"
                    },
                    {
                        "name": "Christopher G. Brinton"
                    }
                ],
                "author_detail": {
                    "name": "Christopher G. Brinton"
                },
                "author": "Christopher G. Brinton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11007v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11007v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09960v1",
                "updated": "2025-09-12T04:34:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    4,
                    34,
                    46,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T04:34:46Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    4,
                    34,
                    46,
                    4,
                    255,
                    0
                ],
                "title": "Limited Reference, Reliable Generation: A Two-Component Framework for\n  Tabular Data Generation in Low-Data Regimes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limited Reference, Reliable Generation: A Two-Component Framework for\n  Tabular Data Generation in Low-Data Regimes"
                },
                "summary": "Synthetic tabular data generation is increasingly essential in data\nmanagement, supporting downstream applications when real-world and high-quality\ntabular data is insufficient. Existing tabular generation approaches, such as\ngenerative adversarial networks (GANs), diffusion models, and fine-tuned Large\nLanguage Models (LLMs), typically require sufficient reference data, limiting\ntheir effectiveness in domain-specific databases with scarce records. While\nprompt-based LLMs offer flexibility without parameter tuning, they often fail\nto capture dataset-specific feature-label dependencies and generate redundant\ndata, leading to degradation in downstream task performance. To overcome these\nissues, we propose ReFine, a framework that (i) derives symbolic \"if-then\"\nrules from interpretable models and embeds them into prompts to explicitly\nguide generation toward domain-specific feature distribution, and (ii) applies\na dual-granularity filtering strategy that suppresses over-sampling patterns\nand selectively refines rare but informative samples to reduce distributional\nimbalance. Extensive experiments on various regression and classification\nbenchmarks demonstrate that ReFine consistently outperforms state-of-the-art\nmethods, achieving up to 0.44 absolute improvement in R-squared for regression\nand 10.0 percent relative improvement in F1 score for classification tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic tabular data generation is increasingly essential in data\nmanagement, supporting downstream applications when real-world and high-quality\ntabular data is insufficient. Existing tabular generation approaches, such as\ngenerative adversarial networks (GANs), diffusion models, and fine-tuned Large\nLanguage Models (LLMs), typically require sufficient reference data, limiting\ntheir effectiveness in domain-specific databases with scarce records. While\nprompt-based LLMs offer flexibility without parameter tuning, they often fail\nto capture dataset-specific feature-label dependencies and generate redundant\ndata, leading to degradation in downstream task performance. To overcome these\nissues, we propose ReFine, a framework that (i) derives symbolic \"if-then\"\nrules from interpretable models and embeds them into prompts to explicitly\nguide generation toward domain-specific feature distribution, and (ii) applies\na dual-granularity filtering strategy that suppresses over-sampling patterns\nand selectively refines rare but informative samples to reduce distributional\nimbalance. Extensive experiments on various regression and classification\nbenchmarks demonstrate that ReFine consistently outperforms state-of-the-art\nmethods, achieving up to 0.44 absolute improvement in R-squared for regression\nand 10.0 percent relative improvement in F1 score for classification tasks."
                },
                "authors": [
                    {
                        "name": "Mingxuan Jiang"
                    },
                    {
                        "name": "Yongxin Wang"
                    },
                    {
                        "name": "Ziyue Dai"
                    },
                    {
                        "name": "Yicun Liu"
                    },
                    {
                        "name": "Hongyi Nie"
                    },
                    {
                        "name": "Sen Liu"
                    },
                    {
                        "name": "Hongfeng Chai"
                    }
                ],
                "author_detail": {
                    "name": "Hongfeng Chai"
                },
                "author": "Hongfeng Chai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01909v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01909v4",
                "updated": "2025-09-12T04:23:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    4,
                    23,
                    22,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-02T03:04:27Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    3,
                    4,
                    27,
                    1,
                    245,
                    0
                ],
                "title": "Oyster-I: Beyond Refusal -- Constructive Safety Alignment for\n  Responsible Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oyster-I: Beyond Refusal -- Constructive Safety Alignment for\n  Responsible Language Models"
                },
                "summary": "Large language models (LLMs) typically deploy safety mechanisms to prevent\nharmful content generation. Most current approaches focus narrowly on risks\nposed by malicious actors, often framing risks as adversarial events and\nrelying on defensive refusals. However, in real-world settings, risks also come\nfrom non-malicious users seeking help while under psychological distress (e.g.,\nself-harm intentions). In such cases, the model's response can strongly\ninfluence the user's next actions. Simple refusals may lead them to repeat,\nescalate, or move to unsafe platforms, creating worse outcomes. We introduce\nConstructive Safety Alignment (CSA), a human-centric paradigm that protects\nagainst malicious misuse while actively guiding vulnerable users toward safe\nand helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic\nanticipation of user reactions, fine-grained risk boundary discovery, and\ninterpretable reasoning control, turning safety into a trust-building process.\nOy1 achieves state-of-the-art safety among open models while retaining high\ngeneral capabilities. On our Constructive Benchmark, it shows strong\nconstructive engagement, close to GPT-5, and unmatched robustness on the\nStrata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from\nrefusal-first to guidance-first safety, CSA redefines the model-user\nrelationship, aiming for systems that are not just safe, but meaningfully\nhelpful. We release Oy1, code, and the benchmark to support responsible,\nuser-centered AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) typically deploy safety mechanisms to prevent\nharmful content generation. Most current approaches focus narrowly on risks\nposed by malicious actors, often framing risks as adversarial events and\nrelying on defensive refusals. However, in real-world settings, risks also come\nfrom non-malicious users seeking help while under psychological distress (e.g.,\nself-harm intentions). In such cases, the model's response can strongly\ninfluence the user's next actions. Simple refusals may lead them to repeat,\nescalate, or move to unsafe platforms, creating worse outcomes. We introduce\nConstructive Safety Alignment (CSA), a human-centric paradigm that protects\nagainst malicious misuse while actively guiding vulnerable users toward safe\nand helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic\nanticipation of user reactions, fine-grained risk boundary discovery, and\ninterpretable reasoning control, turning safety into a trust-building process.\nOy1 achieves state-of-the-art safety among open models while retaining high\ngeneral capabilities. On our Constructive Benchmark, it shows strong\nconstructive engagement, close to GPT-5, and unmatched robustness on the\nStrata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from\nrefusal-first to guidance-first safety, CSA redefines the model-user\nrelationship, aiming for systems that are not just safe, but meaningfully\nhelpful. We release Oy1, code, and the benchmark to support responsible,\nuser-centered AI."
                },
                "authors": [
                    {
                        "name": "Ranjie Duan"
                    },
                    {
                        "name": "Jiexi Liu"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Shiji Zhao"
                    },
                    {
                        "name": "Ruoxi Cheng"
                    },
                    {
                        "name": "Fengxiang Wang"
                    },
                    {
                        "name": "Cheng Wei"
                    },
                    {
                        "name": "Yong Xie"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Defeng Li"
                    },
                    {
                        "name": "Yinpeng Dong"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Yuefeng Chen"
                    },
                    {
                        "name": "Chongwen Wang"
                    },
                    {
                        "name": "Xingjun Ma"
                    },
                    {
                        "name": "Xingxing Wei"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Jun Zhu"
                    },
                    {
                        "name": "Xinfeng Li"
                    },
                    {
                        "name": "Yitong Sun"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Jinzhao Hu"
                    },
                    {
                        "name": "Sha Xu"
                    },
                    {
                        "name": "Yitong Yang"
                    },
                    {
                        "name": "Jialing Tao"
                    },
                    {
                        "name": "Hui Xue"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xue"
                },
                "author": "Hui Xue",
                "arxiv_comment": "Technical Report Code & Model weights available:\n  https://github.com/Alibaba-AAIG/Oyster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01909v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01909v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09955v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09955v1",
                "updated": "2025-09-12T04:11:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    4,
                    11,
                    59,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T04:11:59Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    4,
                    11,
                    59,
                    4,
                    255,
                    0
                ],
                "title": "Adaptive Token Merging for Efficient Transformer Semantic Communication\n  at the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Token Merging for Efficient Transformer Semantic Communication\n  at the Edge"
                },
                "summary": "Large-scale transformers are central to modern semantic communication, yet\ntheir high computational and communication costs hinder deployment on\nresource-constrained edge devices. This paper introduces a training-free\nframework for adaptive token merging, a novel mechanism that compresses\ntransformer representations at runtime by selectively merging semantically\nredundant tokens under per-layer similarity thresholds. Unlike prior\nfixed-ratio reduction, our approach couples merging directly to input\nredundancy, enabling data-dependent adaptation that balances efficiency and\ntask relevance without retraining. We cast the discovery of merging strategies\nas a multi-objective optimization problem and leverage Bayesian optimization to\nobtain Pareto-optimal trade-offs between accuracy, inference cost, and\ncommunication cost. On ImageNet classification, we match the accuracy of the\nunmodified transformer with 30\\% fewer floating-point operations per second and\nunder 20\\% of the original communication cost, while for visual question\nanswering our method achieves performance competitive with the full LLaVA model\nat less than one-third of the compute and one-tenth of the bandwidth. Finally,\nwe show that our adaptive merging is robust across varying channel conditions\nand provides inherent privacy benefits, substantially degrading the efficacy of\nmodel inversion attacks. Our framework provides a practical and versatile\nsolution for deploying powerful transformer models in resource-limited edge\nintelligence scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale transformers are central to modern semantic communication, yet\ntheir high computational and communication costs hinder deployment on\nresource-constrained edge devices. This paper introduces a training-free\nframework for adaptive token merging, a novel mechanism that compresses\ntransformer representations at runtime by selectively merging semantically\nredundant tokens under per-layer similarity thresholds. Unlike prior\nfixed-ratio reduction, our approach couples merging directly to input\nredundancy, enabling data-dependent adaptation that balances efficiency and\ntask relevance without retraining. We cast the discovery of merging strategies\nas a multi-objective optimization problem and leverage Bayesian optimization to\nobtain Pareto-optimal trade-offs between accuracy, inference cost, and\ncommunication cost. On ImageNet classification, we match the accuracy of the\nunmodified transformer with 30\\% fewer floating-point operations per second and\nunder 20\\% of the original communication cost, while for visual question\nanswering our method achieves performance competitive with the full LLaVA model\nat less than one-third of the compute and one-tenth of the bandwidth. Finally,\nwe show that our adaptive merging is robust across varying channel conditions\nand provides inherent privacy benefits, substantially degrading the efficacy of\nmodel inversion attacks. Our framework provides a practical and versatile\nsolution for deploying powerful transformer models in resource-limited edge\nintelligence scenarios."
                },
                "authors": [
                    {
                        "name": "Omar Erak"
                    },
                    {
                        "name": "Omar Alhussein"
                    },
                    {
                        "name": "Hatem Abou-Zeid"
                    },
                    {
                        "name": "Mehdi Bennis"
                    },
                    {
                        "name": "Sami Muhaidat"
                    }
                ],
                "author_detail": {
                    "name": "Sami Muhaidat"
                },
                "author": "Sami Muhaidat",
                "arxiv_comment": "Submitted to IEEE Journals",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09955v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09955v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14249v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14249v8",
                "updated": "2025-09-12T04:08:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    4,
                    8,
                    53,
                    4,
                    255,
                    0
                ],
                "published": "2025-01-24T05:27:46Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    5,
                    27,
                    46,
                    4,
                    24,
                    0
                ],
                "title": "Humanity's Last Exam",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humanity's Last Exam"
                },
                "summary": "Benchmarks are important tools for tracking the rapid advancements in large\nlanguage model (LLM) capabilities. However, benchmarks are not keeping pace in\ndifficulty: LLMs now achieve over 90\\% accuracy on popular benchmarks like\nMMLU, limiting informed measurement of state-of-the-art LLM capabilities. In\nresponse, we introduce Humanity's Last Exam (HLE), a multi-modal benchmark at\nthe frontier of human knowledge, designed to be the final closed-ended academic\nbenchmark of its kind with broad subject coverage. HLE consists of 2,500\nquestions across dozens of subjects, including mathematics, humanities, and the\nnatural sciences. HLE is developed globally by subject-matter experts and\nconsists of multiple-choice and short-answer questions suitable for automated\ngrading. Each question has a known solution that is unambiguous and easily\nverifiable, but cannot be quickly answered via internet retrieval.\nState-of-the-art LLMs demonstrate low accuracy and calibration on HLE,\nhighlighting a significant gap between current LLM capabilities and the expert\nhuman frontier on closed-ended academic questions. To inform research and\npolicymaking upon a clear understanding of model capabilities, we publicly\nrelease HLE at https://lastexam.ai.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarks are important tools for tracking the rapid advancements in large\nlanguage model (LLM) capabilities. However, benchmarks are not keeping pace in\ndifficulty: LLMs now achieve over 90\\% accuracy on popular benchmarks like\nMMLU, limiting informed measurement of state-of-the-art LLM capabilities. In\nresponse, we introduce Humanity's Last Exam (HLE), a multi-modal benchmark at\nthe frontier of human knowledge, designed to be the final closed-ended academic\nbenchmark of its kind with broad subject coverage. HLE consists of 2,500\nquestions across dozens of subjects, including mathematics, humanities, and the\nnatural sciences. HLE is developed globally by subject-matter experts and\nconsists of multiple-choice and short-answer questions suitable for automated\ngrading. Each question has a known solution that is unambiguous and easily\nverifiable, but cannot be quickly answered via internet retrieval.\nState-of-the-art LLMs demonstrate low accuracy and calibration on HLE,\nhighlighting a significant gap between current LLM capabilities and the expert\nhuman frontier on closed-ended academic questions. To inform research and\npolicymaking upon a clear understanding of model capabilities, we publicly\nrelease HLE at https://lastexam.ai."
                },
                "authors": [
                    {
                        "name": "Long Phan"
                    },
                    {
                        "name": "Alice Gatti"
                    },
                    {
                        "name": "Ziwen Han"
                    },
                    {
                        "name": "Nathaniel Li"
                    },
                    {
                        "name": "Josephina Hu"
                    },
                    {
                        "name": "Hugh Zhang"
                    },
                    {
                        "name": "Chen Bo Calvin Zhang"
                    },
                    {
                        "name": "Mohamed Shaaban"
                    },
                    {
                        "name": "John Ling"
                    },
                    {
                        "name": "Sean Shi"
                    },
                    {
                        "name": "Michael Choi"
                    },
                    {
                        "name": "Anish Agrawal"
                    },
                    {
                        "name": "Arnav Chopra"
                    },
                    {
                        "name": "Adam Khoja"
                    },
                    {
                        "name": "Ryan Kim"
                    },
                    {
                        "name": "Richard Ren"
                    },
                    {
                        "name": "Jason Hausenloy"
                    },
                    {
                        "name": "Oliver Zhang"
                    },
                    {
                        "name": "Mantas Mazeika"
                    },
                    {
                        "name": "Dmitry Dodonov"
                    },
                    {
                        "name": "Tung Nguyen"
                    },
                    {
                        "name": "Jaeho Lee"
                    },
                    {
                        "name": "Daron Anderson"
                    },
                    {
                        "name": "Mikhail Doroshenko"
                    },
                    {
                        "name": "Alun Cennyth Stokes"
                    },
                    {
                        "name": "Mobeen Mahmood"
                    },
                    {
                        "name": "Oleksandr Pokutnyi"
                    },
                    {
                        "name": "Oleg Iskra"
                    },
                    {
                        "name": "Jessica P. Wang"
                    },
                    {
                        "name": "John-Clark Levin"
                    },
                    {
                        "name": "Mstyslav Kazakov"
                    },
                    {
                        "name": "Fiona Feng"
                    },
                    {
                        "name": "Steven Y. Feng"
                    },
                    {
                        "name": "Haoran Zhao"
                    },
                    {
                        "name": "Michael Yu"
                    },
                    {
                        "name": "Varun Gangal"
                    },
                    {
                        "name": "Chelsea Zou"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Serguei Popov"
                    },
                    {
                        "name": "Robert Gerbicz"
                    },
                    {
                        "name": "Geoff Galgon"
                    },
                    {
                        "name": "Johannes Schmitt"
                    },
                    {
                        "name": "Will Yeadon"
                    },
                    {
                        "name": "Yongki Lee"
                    },
                    {
                        "name": "Scott Sauers"
                    },
                    {
                        "name": "Alvaro Sanchez"
                    },
                    {
                        "name": "Fabian Giska"
                    },
                    {
                        "name": "Marc Roth"
                    },
                    {
                        "name": "Søren Riis"
                    },
                    {
                        "name": "Saiteja Utpala"
                    },
                    {
                        "name": "Noah Burns"
                    },
                    {
                        "name": "Gashaw M. Goshu"
                    },
                    {
                        "name": "Mohinder Maheshbhai Naiya"
                    },
                    {
                        "name": "Chidozie Agu"
                    },
                    {
                        "name": "Zachary Giboney"
                    },
                    {
                        "name": "Antrell Cheatom"
                    },
                    {
                        "name": "Francesco Fournier-Facio"
                    },
                    {
                        "name": "Sarah-Jane Crowson"
                    },
                    {
                        "name": "Lennart Finke"
                    },
                    {
                        "name": "Zerui Cheng"
                    },
                    {
                        "name": "Jennifer Zampese"
                    },
                    {
                        "name": "Ryan G. Hoerr"
                    },
                    {
                        "name": "Mark Nandor"
                    },
                    {
                        "name": "Hyunwoo Park"
                    },
                    {
                        "name": "Tim Gehrunger"
                    },
                    {
                        "name": "Jiaqi Cai"
                    },
                    {
                        "name": "Ben McCarty"
                    },
                    {
                        "name": "Alexis C Garretson"
                    },
                    {
                        "name": "Edwin Taylor"
                    },
                    {
                        "name": "Damien Sileo"
                    },
                    {
                        "name": "Qiuyu Ren"
                    },
                    {
                        "name": "Usman Qazi"
                    },
                    {
                        "name": "Lianghui Li"
                    },
                    {
                        "name": "Jungbae Nam"
                    },
                    {
                        "name": "John B. Wydallis"
                    },
                    {
                        "name": "Pavel Arkhipov"
                    },
                    {
                        "name": "Jack Wei Lun Shi"
                    },
                    {
                        "name": "Aras Bacho"
                    },
                    {
                        "name": "Chris G. Willcocks"
                    },
                    {
                        "name": "Hangrui Cao"
                    },
                    {
                        "name": "Sumeet Motwani"
                    },
                    {
                        "name": "Emily de Oliveira Santos"
                    },
                    {
                        "name": "Johannes Veith"
                    },
                    {
                        "name": "Edward Vendrow"
                    },
                    {
                        "name": "Doru Cojoc"
                    },
                    {
                        "name": "Kengo Zenitani"
                    },
                    {
                        "name": "Joshua Robinson"
                    },
                    {
                        "name": "Longke Tang"
                    },
                    {
                        "name": "Yuqi Li"
                    },
                    {
                        "name": "Joshua Vendrow"
                    },
                    {
                        "name": "Natanael Wildner Fraga"
                    },
                    {
                        "name": "Vladyslav Kuchkin"
                    },
                    {
                        "name": "Andrey Pupasov Maksimov"
                    },
                    {
                        "name": "Pierre Marion"
                    },
                    {
                        "name": "Denis Efremov"
                    },
                    {
                        "name": "Jayson Lynch"
                    },
                    {
                        "name": "Kaiqu Liang"
                    },
                    {
                        "name": "Aleksandar Mikov"
                    },
                    {
                        "name": "Andrew Gritsevskiy"
                    },
                    {
                        "name": "Julien Guillod"
                    },
                    {
                        "name": "Gözdenur Demir"
                    },
                    {
                        "name": "Dakotah Martinez"
                    },
                    {
                        "name": "Ben Pageler"
                    },
                    {
                        "name": "Kevin Zhou"
                    },
                    {
                        "name": "Saeed Soori"
                    },
                    {
                        "name": "Ori Press"
                    },
                    {
                        "name": "Henry Tang"
                    },
                    {
                        "name": "Paolo Rissone"
                    },
                    {
                        "name": "Sean R. Green"
                    },
                    {
                        "name": "Lina Brüssel"
                    },
                    {
                        "name": "Moon Twayana"
                    },
                    {
                        "name": "Aymeric Dieuleveut"
                    },
                    {
                        "name": "Joseph Marvin Imperial"
                    },
                    {
                        "name": "Ameya Prabhu"
                    },
                    {
                        "name": "Jinzhou Yang"
                    },
                    {
                        "name": "Nick Crispino"
                    },
                    {
                        "name": "Arun Rao"
                    },
                    {
                        "name": "Dimitri Zvonkine"
                    },
                    {
                        "name": "Gabriel Loiseau"
                    },
                    {
                        "name": "Mikhail Kalinin"
                    },
                    {
                        "name": "Marco Lukas"
                    },
                    {
                        "name": "Ciprian Manolescu"
                    },
                    {
                        "name": "Nate Stambaugh"
                    },
                    {
                        "name": "Subrata Mishra"
                    },
                    {
                        "name": "Tad Hogg"
                    },
                    {
                        "name": "Carlo Bosio"
                    },
                    {
                        "name": "Brian P Coppola"
                    },
                    {
                        "name": "Julian Salazar"
                    },
                    {
                        "name": "Jaehyeok Jin"
                    },
                    {
                        "name": "Rafael Sayous"
                    },
                    {
                        "name": "Stefan Ivanov"
                    },
                    {
                        "name": "Philippe Schwaller"
                    },
                    {
                        "name": "Shaipranesh Senthilkuma"
                    },
                    {
                        "name": "Andres M Bran"
                    },
                    {
                        "name": "Andres Algaba"
                    },
                    {
                        "name": "Kelsey Van den Houte"
                    },
                    {
                        "name": "Lynn Van Der Sypt"
                    },
                    {
                        "name": "Brecht Verbeken"
                    },
                    {
                        "name": "David Noever"
                    },
                    {
                        "name": "Alexei Kopylov"
                    },
                    {
                        "name": "Benjamin Myklebust"
                    },
                    {
                        "name": "Bikun Li"
                    },
                    {
                        "name": "Lisa Schut"
                    },
                    {
                        "name": "Evgenii Zheltonozhskii"
                    },
                    {
                        "name": "Qiaochu Yuan"
                    },
                    {
                        "name": "Derek Lim"
                    },
                    {
                        "name": "Richard Stanley"
                    },
                    {
                        "name": "Tong Yang"
                    },
                    {
                        "name": "John Maar"
                    },
                    {
                        "name": "Julian Wykowski"
                    },
                    {
                        "name": "Martí Oller"
                    },
                    {
                        "name": "Anmol Sahu"
                    },
                    {
                        "name": "Cesare Giulio Ardito"
                    },
                    {
                        "name": "Yuzheng Hu"
                    },
                    {
                        "name": "Ariel Ghislain Kemogne Kamdoum"
                    },
                    {
                        "name": "Alvin Jin"
                    },
                    {
                        "name": "Tobias Garcia Vilchis"
                    },
                    {
                        "name": "Yuexuan Zu"
                    },
                    {
                        "name": "Martin Lackner"
                    },
                    {
                        "name": "James Koppel"
                    },
                    {
                        "name": "Gongbo Sun"
                    },
                    {
                        "name": "Daniil S. Antonenko"
                    },
                    {
                        "name": "Steffi Chern"
                    },
                    {
                        "name": "Bingchen Zhao"
                    },
                    {
                        "name": "Pierrot Arsene"
                    },
                    {
                        "name": "Joseph M Cavanagh"
                    },
                    {
                        "name": "Daofeng Li"
                    },
                    {
                        "name": "Jiawei Shen"
                    },
                    {
                        "name": "Donato Crisostomi"
                    },
                    {
                        "name": "Wenjin Zhang"
                    },
                    {
                        "name": "Ali Dehghan"
                    },
                    {
                        "name": "Sergey Ivanov"
                    },
                    {
                        "name": "David Perrella"
                    },
                    {
                        "name": "Nurdin Kaparov"
                    },
                    {
                        "name": "Allen Zang"
                    },
                    {
                        "name": "Ilia Sucholutsky"
                    },
                    {
                        "name": "Arina Kharlamova"
                    },
                    {
                        "name": "Daniil Orel"
                    },
                    {
                        "name": "Vladislav Poritski"
                    },
                    {
                        "name": "Shalev Ben-David"
                    },
                    {
                        "name": "Zachary Berger"
                    },
                    {
                        "name": "Parker Whitfill"
                    },
                    {
                        "name": "Michael Foster"
                    },
                    {
                        "name": "Daniel Munro"
                    },
                    {
                        "name": "Linh Ho"
                    },
                    {
                        "name": "Shankar Sivarajan"
                    },
                    {
                        "name": "Dan Bar Hava"
                    },
                    {
                        "name": "Aleksey Kuchkin"
                    },
                    {
                        "name": "David Holmes"
                    },
                    {
                        "name": "Alexandra Rodriguez-Romero"
                    },
                    {
                        "name": "Frank Sommerhage"
                    },
                    {
                        "name": "Anji Zhang"
                    },
                    {
                        "name": "Richard Moat"
                    },
                    {
                        "name": "Keith Schneider"
                    },
                    {
                        "name": "Zakayo Kazibwe"
                    },
                    {
                        "name": "Don Clarke"
                    },
                    {
                        "name": "Dae Hyun Kim"
                    },
                    {
                        "name": "Felipe Meneguitti Dias"
                    },
                    {
                        "name": "Sara Fish"
                    },
                    {
                        "name": "Veit Elser"
                    },
                    {
                        "name": "Tobias Kreiman"
                    },
                    {
                        "name": "Victor Efren Guadarrama Vilchis"
                    },
                    {
                        "name": "Immo Klose"
                    },
                    {
                        "name": "Ujjwala Anantheswaran"
                    },
                    {
                        "name": "Adam Zweiger"
                    },
                    {
                        "name": "Kaivalya Rawal"
                    },
                    {
                        "name": "Jeffery Li"
                    },
                    {
                        "name": "Jeremy Nguyen"
                    },
                    {
                        "name": "Nicolas Daans"
                    },
                    {
                        "name": "Haline Heidinger"
                    },
                    {
                        "name": "Maksim Radionov"
                    },
                    {
                        "name": "Václav Rozhoň"
                    },
                    {
                        "name": "Vincent Ginis"
                    },
                    {
                        "name": "Christian Stump"
                    },
                    {
                        "name": "Niv Cohen"
                    },
                    {
                        "name": "Rafał Poświata"
                    },
                    {
                        "name": "Josef Tkadlec"
                    },
                    {
                        "name": "Alan Goldfarb"
                    },
                    {
                        "name": "Chenguang Wang"
                    },
                    {
                        "name": "Piotr Padlewski"
                    },
                    {
                        "name": "Stanislaw Barzowski"
                    },
                    {
                        "name": "Kyle Montgomery"
                    },
                    {
                        "name": "Ryan Stendall"
                    },
                    {
                        "name": "Jamie Tucker-Foltz"
                    },
                    {
                        "name": "Jack Stade"
                    },
                    {
                        "name": "T. Ryan Rogers"
                    },
                    {
                        "name": "Tom Goertzen"
                    },
                    {
                        "name": "Declan Grabb"
                    },
                    {
                        "name": "Abhishek Shukla"
                    },
                    {
                        "name": "Alan Givré"
                    },
                    {
                        "name": "John Arnold Ambay"
                    },
                    {
                        "name": "Archan Sen"
                    },
                    {
                        "name": "Muhammad Fayez Aziz"
                    },
                    {
                        "name": "Mark H Inlow"
                    },
                    {
                        "name": "Hao He"
                    },
                    {
                        "name": "Ling Zhang"
                    },
                    {
                        "name": "Younesse Kaddar"
                    },
                    {
                        "name": "Ivar Ängquist"
                    },
                    {
                        "name": "Yanxu Chen"
                    },
                    {
                        "name": "Harrison K Wang"
                    },
                    {
                        "name": "Kalyan Ramakrishnan"
                    },
                    {
                        "name": "Elliott Thornley"
                    },
                    {
                        "name": "Antonio Terpin"
                    },
                    {
                        "name": "Hailey Schoelkopf"
                    },
                    {
                        "name": "Eric Zheng"
                    },
                    {
                        "name": "Avishy Carmi"
                    },
                    {
                        "name": "Ethan D. L. Brown"
                    },
                    {
                        "name": "Kelin Zhu"
                    },
                    {
                        "name": "Max Bartolo"
                    },
                    {
                        "name": "Richard Wheeler"
                    },
                    {
                        "name": "Martin Stehberger"
                    },
                    {
                        "name": "Peter Bradshaw"
                    },
                    {
                        "name": "JP Heimonen"
                    },
                    {
                        "name": "Kaustubh Sridhar"
                    },
                    {
                        "name": "Ido Akov"
                    },
                    {
                        "name": "Jennifer Sandlin"
                    },
                    {
                        "name": "Yury Makarychev"
                    },
                    {
                        "name": "Joanna Tam"
                    },
                    {
                        "name": "Hieu Hoang"
                    },
                    {
                        "name": "David M. Cunningham"
                    },
                    {
                        "name": "Vladimir Goryachev"
                    },
                    {
                        "name": "Demosthenes Patramanis"
                    },
                    {
                        "name": "Michael Krause"
                    },
                    {
                        "name": "Andrew Redenti"
                    },
                    {
                        "name": "David Aldous"
                    },
                    {
                        "name": "Jesyin Lai"
                    },
                    {
                        "name": "Shannon Coleman"
                    },
                    {
                        "name": "Jiangnan Xu"
                    },
                    {
                        "name": "Sangwon Lee"
                    },
                    {
                        "name": "Ilias Magoulas"
                    },
                    {
                        "name": "Sandy Zhao"
                    },
                    {
                        "name": "Ning Tang"
                    },
                    {
                        "name": "Michael K. Cohen"
                    },
                    {
                        "name": "Orr Paradise"
                    },
                    {
                        "name": "Jan Hendrik Kirchner"
                    },
                    {
                        "name": "Maksym Ovchynnikov"
                    },
                    {
                        "name": "Jason O. Matos"
                    },
                    {
                        "name": "Adithya Shenoy"
                    },
                    {
                        "name": "Michael Wang"
                    },
                    {
                        "name": "Yuzhou Nie"
                    },
                    {
                        "name": "Anna Sztyber-Betley"
                    },
                    {
                        "name": "Paolo Faraboschi"
                    },
                    {
                        "name": "Robin Riblet"
                    },
                    {
                        "name": "Jonathan Crozier"
                    },
                    {
                        "name": "Shiv Halasyamani"
                    },
                    {
                        "name": "Shreyas Verma"
                    },
                    {
                        "name": "Prashant Joshi"
                    },
                    {
                        "name": "Eli Meril"
                    },
                    {
                        "name": "Ziqiao Ma"
                    },
                    {
                        "name": "Jérémy Andréoletti"
                    },
                    {
                        "name": "Raghav Singhal"
                    },
                    {
                        "name": "Jacob Platnick"
                    },
                    {
                        "name": "Volodymyr Nevirkovets"
                    },
                    {
                        "name": "Luke Basler"
                    },
                    {
                        "name": "Alexander Ivanov"
                    },
                    {
                        "name": "Seri Khoury"
                    },
                    {
                        "name": "Nils Gustafsson"
                    },
                    {
                        "name": "Marco Piccardo"
                    },
                    {
                        "name": "Hamid Mostaghimi"
                    },
                    {
                        "name": "Qijia Chen"
                    },
                    {
                        "name": "Virendra Singh"
                    },
                    {
                        "name": "Tran Quoc Khánh"
                    },
                    {
                        "name": "Paul Rosu"
                    },
                    {
                        "name": "Hannah Szlyk"
                    },
                    {
                        "name": "Zachary Brown"
                    },
                    {
                        "name": "Himanshu Narayan"
                    },
                    {
                        "name": "Aline Menezes"
                    },
                    {
                        "name": "Jonathan Roberts"
                    },
                    {
                        "name": "William Alley"
                    },
                    {
                        "name": "Kunyang Sun"
                    },
                    {
                        "name": "Arkil Patel"
                    },
                    {
                        "name": "Max Lamparth"
                    },
                    {
                        "name": "Anka Reuel"
                    },
                    {
                        "name": "Linwei Xin"
                    },
                    {
                        "name": "Hanmeng Xu"
                    },
                    {
                        "name": "Jacob Loader"
                    },
                    {
                        "name": "Freddie Martin"
                    },
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Andrea Achilleos"
                    },
                    {
                        "name": "Thomas Preu"
                    },
                    {
                        "name": "Tomek Korbak"
                    },
                    {
                        "name": "Ida Bosio"
                    },
                    {
                        "name": "Fereshteh Kazemi"
                    },
                    {
                        "name": "Ziye Chen"
                    },
                    {
                        "name": "Biró Bálint"
                    },
                    {
                        "name": "Eve J. Y. Lo"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Maria Inês S. Nunes"
                    },
                    {
                        "name": "Jeremiah Milbauer"
                    },
                    {
                        "name": "M Saiful Bari"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Behzad Ansarinejad"
                    },
                    {
                        "name": "Yewen Sun"
                    },
                    {
                        "name": "Stephane Durand"
                    },
                    {
                        "name": "Hossam Elgnainy"
                    },
                    {
                        "name": "Guillaume Douville"
                    },
                    {
                        "name": "Daniel Tordera"
                    },
                    {
                        "name": "George Balabanian"
                    },
                    {
                        "name": "Hew Wolff"
                    },
                    {
                        "name": "Lynna Kvistad"
                    },
                    {
                        "name": "Hsiaoyun Milliron"
                    },
                    {
                        "name": "Ahmad Sakor"
                    },
                    {
                        "name": "Murat Eron"
                    },
                    {
                        "name": "Andrew Favre D. O."
                    },
                    {
                        "name": "Shailesh Shah"
                    },
                    {
                        "name": "Xiaoxiang Zhou"
                    },
                    {
                        "name": "Firuz Kamalov"
                    },
                    {
                        "name": "Sherwin Abdoli"
                    },
                    {
                        "name": "Tim Santens"
                    },
                    {
                        "name": "Shaul Barkan"
                    },
                    {
                        "name": "Allison Tee"
                    },
                    {
                        "name": "Robin Zhang"
                    },
                    {
                        "name": "Alessandro Tomasiello"
                    },
                    {
                        "name": "G. Bruno De Luca"
                    },
                    {
                        "name": "Shi-Zhuo Looi"
                    },
                    {
                        "name": "Vinh-Kha Le"
                    },
                    {
                        "name": "Noam Kolt"
                    },
                    {
                        "name": "Jiayi Pan"
                    },
                    {
                        "name": "Emma Rodman"
                    },
                    {
                        "name": "Jacob Drori"
                    },
                    {
                        "name": "Carl J Fossum"
                    },
                    {
                        "name": "Niklas Muennighoff"
                    },
                    {
                        "name": "Milind Jagota"
                    },
                    {
                        "name": "Ronak Pradeep"
                    },
                    {
                        "name": "Honglu Fan"
                    },
                    {
                        "name": "Jonathan Eicher"
                    },
                    {
                        "name": "Michael Chen"
                    },
                    {
                        "name": "Kushal Thaman"
                    },
                    {
                        "name": "William Merrill"
                    },
                    {
                        "name": "Moritz Firsching"
                    },
                    {
                        "name": "Carter Harris"
                    },
                    {
                        "name": "Stefan Ciobâcă"
                    },
                    {
                        "name": "Jason Gross"
                    },
                    {
                        "name": "Rohan Pandey"
                    },
                    {
                        "name": "Ilya Gusev"
                    },
                    {
                        "name": "Adam Jones"
                    },
                    {
                        "name": "Shashank Agnihotri"
                    },
                    {
                        "name": "Pavel Zhelnov"
                    },
                    {
                        "name": "Mohammadreza Mofayezi"
                    },
                    {
                        "name": "Alexander Piperski"
                    },
                    {
                        "name": "David K. Zhang"
                    },
                    {
                        "name": "Kostiantyn Dobarskyi"
                    },
                    {
                        "name": "Roman Leventov"
                    },
                    {
                        "name": "Ignat Soroko"
                    },
                    {
                        "name": "Joshua Duersch"
                    },
                    {
                        "name": "Vage Taamazyan"
                    },
                    {
                        "name": "Andrew Ho"
                    },
                    {
                        "name": "Wenjie Ma"
                    },
                    {
                        "name": "William Held"
                    },
                    {
                        "name": "Ruicheng Xian"
                    },
                    {
                        "name": "Armel Randy Zebaze"
                    },
                    {
                        "name": "Mohanad Mohamed"
                    },
                    {
                        "name": "Julian Noah Leser"
                    },
                    {
                        "name": "Michelle X Yuan"
                    },
                    {
                        "name": "Laila Yacar"
                    },
                    {
                        "name": "Johannes Lengler"
                    },
                    {
                        "name": "Katarzyna Olszewska"
                    },
                    {
                        "name": "Claudio Di Fratta"
                    },
                    {
                        "name": "Edson Oliveira"
                    },
                    {
                        "name": "Joseph W. Jackson"
                    },
                    {
                        "name": "Andy Zou"
                    },
                    {
                        "name": "Muthu Chidambaram"
                    },
                    {
                        "name": "Timothy Manik"
                    },
                    {
                        "name": "Hector Haffenden"
                    },
                    {
                        "name": "Dashiell Stander"
                    },
                    {
                        "name": "Ali Dasouqi"
                    },
                    {
                        "name": "Alexander Shen"
                    },
                    {
                        "name": "Bita Golshani"
                    },
                    {
                        "name": "David Stap"
                    },
                    {
                        "name": "Egor Kretov"
                    },
                    {
                        "name": "Mikalai Uzhou"
                    },
                    {
                        "name": "Alina Borisovna Zhidkovskaya"
                    },
                    {
                        "name": "Nick Winter"
                    },
                    {
                        "name": "Miguel Orbegozo Rodriguez"
                    },
                    {
                        "name": "Robert Lauff"
                    },
                    {
                        "name": "Dustin Wehr"
                    },
                    {
                        "name": "Colin Tang"
                    },
                    {
                        "name": "Zaki Hossain"
                    },
                    {
                        "name": "Shaun Phillips"
                    },
                    {
                        "name": "Fortuna Samuele"
                    },
                    {
                        "name": "Fredrik Ekström"
                    },
                    {
                        "name": "Angela Hammon"
                    },
                    {
                        "name": "Oam Patel"
                    },
                    {
                        "name": "Faraz Farhidi"
                    },
                    {
                        "name": "George Medley"
                    },
                    {
                        "name": "Forough Mohammadzadeh"
                    },
                    {
                        "name": "Madellene Peñaflor"
                    },
                    {
                        "name": "Haile Kassahun"
                    },
                    {
                        "name": "Alena Friedrich"
                    },
                    {
                        "name": "Rayner Hernandez Perez"
                    },
                    {
                        "name": "Daniel Pyda"
                    },
                    {
                        "name": "Taom Sakal"
                    },
                    {
                        "name": "Omkar Dhamane"
                    },
                    {
                        "name": "Ali Khajegili Mirabadi"
                    },
                    {
                        "name": "Eric Hallman"
                    },
                    {
                        "name": "Kenchi Okutsu"
                    },
                    {
                        "name": "Mike Battaglia"
                    },
                    {
                        "name": "Mohammad Maghsoudimehrabani"
                    },
                    {
                        "name": "Alon Amit"
                    },
                    {
                        "name": "Dave Hulbert"
                    },
                    {
                        "name": "Roberto Pereira"
                    },
                    {
                        "name": "Simon Weber"
                    },
                    {
                        "name": "Handoko"
                    },
                    {
                        "name": "Anton Peristyy"
                    },
                    {
                        "name": "Stephen Malina"
                    },
                    {
                        "name": "Mustafa Mehkary"
                    },
                    {
                        "name": "Rami Aly"
                    },
                    {
                        "name": "Frank Reidegeld"
                    },
                    {
                        "name": "Anna-Katharina Dick"
                    },
                    {
                        "name": "Cary Friday"
                    },
                    {
                        "name": "Mukhwinder Singh"
                    },
                    {
                        "name": "Hassan Shapourian"
                    },
                    {
                        "name": "Wanyoung Kim"
                    },
                    {
                        "name": "Mariana Costa"
                    },
                    {
                        "name": "Hubeyb Gurdogan"
                    },
                    {
                        "name": "Harsh Kumar"
                    },
                    {
                        "name": "Chiara Ceconello"
                    },
                    {
                        "name": "Chao Zhuang"
                    },
                    {
                        "name": "Haon Park"
                    },
                    {
                        "name": "Micah Carroll"
                    },
                    {
                        "name": "Andrew R. Tawfeek"
                    },
                    {
                        "name": "Stefan Steinerberger"
                    },
                    {
                        "name": "Daattavya Aggarwal"
                    },
                    {
                        "name": "Michael Kirchhof"
                    },
                    {
                        "name": "Linjie Dai"
                    },
                    {
                        "name": "Evan Kim"
                    },
                    {
                        "name": "Johan Ferret"
                    },
                    {
                        "name": "Jainam Shah"
                    },
                    {
                        "name": "Yuzhou Wang"
                    },
                    {
                        "name": "Minghao Yan"
                    },
                    {
                        "name": "Krzysztof Burdzy"
                    },
                    {
                        "name": "Lixin Zhang"
                    },
                    {
                        "name": "Antonio Franca"
                    },
                    {
                        "name": "Diana T. Pham"
                    },
                    {
                        "name": "Kang Yong Loh"
                    },
                    {
                        "name": "Joshua Robinson"
                    },
                    {
                        "name": "Abram Jackson"
                    },
                    {
                        "name": "Paolo Giordano"
                    },
                    {
                        "name": "Philipp Petersen"
                    },
                    {
                        "name": "Adrian Cosma"
                    },
                    {
                        "name": "Jesus Colino"
                    },
                    {
                        "name": "Colin White"
                    },
                    {
                        "name": "Jacob Votava"
                    },
                    {
                        "name": "Vladimir Vinnikov"
                    },
                    {
                        "name": "Ethan Delaney"
                    },
                    {
                        "name": "Petr Spelda"
                    },
                    {
                        "name": "Vit Stritecky"
                    },
                    {
                        "name": "Syed M. Shahid"
                    },
                    {
                        "name": "Jean-Christophe Mourrat"
                    },
                    {
                        "name": "Lavr Vetoshkin"
                    },
                    {
                        "name": "Koen Sponselee"
                    },
                    {
                        "name": "Renas Bacho"
                    },
                    {
                        "name": "Zheng-Xin Yong"
                    },
                    {
                        "name": "Florencia de la Rosa"
                    },
                    {
                        "name": "Nathan Cho"
                    },
                    {
                        "name": "Xiuyu Li"
                    },
                    {
                        "name": "Guillaume Malod"
                    },
                    {
                        "name": "Orion Weller"
                    },
                    {
                        "name": "Guglielmo Albani"
                    },
                    {
                        "name": "Leon Lang"
                    },
                    {
                        "name": "Julien Laurendeau"
                    },
                    {
                        "name": "Dmitry Kazakov"
                    },
                    {
                        "name": "Fatimah Adesanya"
                    },
                    {
                        "name": "Julien Portier"
                    },
                    {
                        "name": "Lawrence Hollom"
                    },
                    {
                        "name": "Victor Souza"
                    },
                    {
                        "name": "Yuchen Anna Zhou"
                    },
                    {
                        "name": "Julien Degorre"
                    },
                    {
                        "name": "Yiğit Yalın"
                    },
                    {
                        "name": "Gbenga Daniel Obikoya"
                    },
                    {
                        "name": "Rai"
                    },
                    {
                        "name": "Filippo Bigi"
                    },
                    {
                        "name": "M. C. Boscá"
                    },
                    {
                        "name": "Oleg Shumar"
                    },
                    {
                        "name": "Kaniuar Bacho"
                    },
                    {
                        "name": "Gabriel Recchia"
                    },
                    {
                        "name": "Mara Popescu"
                    },
                    {
                        "name": "Nikita Shulga"
                    },
                    {
                        "name": "Ngefor Mildred Tanwie"
                    },
                    {
                        "name": "Thomas C. H. Lux"
                    },
                    {
                        "name": "Ben Rank"
                    },
                    {
                        "name": "Colin Ni"
                    },
                    {
                        "name": "Matthew Brooks"
                    },
                    {
                        "name": "Alesia Yakimchyk"
                    },
                    {
                        "name": "Huanxu"
                    },
                    {
                        "name": "Liu"
                    },
                    {
                        "name": "Stefano Cavalleri"
                    },
                    {
                        "name": "Olle Häggström"
                    },
                    {
                        "name": "Emil Verkama"
                    },
                    {
                        "name": "Joshua Newbould"
                    },
                    {
                        "name": "Hans Gundlach"
                    },
                    {
                        "name": "Leonor Brito-Santana"
                    },
                    {
                        "name": "Brian Amaro"
                    },
                    {
                        "name": "Vivek Vajipey"
                    },
                    {
                        "name": "Rynaa Grover"
                    },
                    {
                        "name": "Ting Wang"
                    },
                    {
                        "name": "Yosi Kratish"
                    },
                    {
                        "name": "Wen-Ding Li"
                    },
                    {
                        "name": "Sivakanth Gopi"
                    },
                    {
                        "name": "Andrea Caciolai"
                    },
                    {
                        "name": "Christian Schroeder de Witt"
                    },
                    {
                        "name": "Pablo Hernández-Cámara"
                    },
                    {
                        "name": "Emanuele Rodolà"
                    },
                    {
                        "name": "Jules Robins"
                    },
                    {
                        "name": "Dominic Williamson"
                    },
                    {
                        "name": "Vincent Cheng"
                    },
                    {
                        "name": "Brad Raynor"
                    },
                    {
                        "name": "Hao Qi"
                    },
                    {
                        "name": "Ben Segev"
                    },
                    {
                        "name": "Jingxuan Fan"
                    },
                    {
                        "name": "Sarah Martinson"
                    },
                    {
                        "name": "Erik Y. Wang"
                    },
                    {
                        "name": "Kaylie Hausknecht"
                    },
                    {
                        "name": "Michael P. Brenner"
                    },
                    {
                        "name": "Mao Mao"
                    },
                    {
                        "name": "Christoph Demian"
                    },
                    {
                        "name": "Peyman Kassani"
                    },
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "David Avagian"
                    },
                    {
                        "name": "Eshawn Jessica Scipio"
                    },
                    {
                        "name": "Alon Ragoler"
                    },
                    {
                        "name": "Justin Tan"
                    },
                    {
                        "name": "Blake Sims"
                    },
                    {
                        "name": "Rebeka Plecnik"
                    },
                    {
                        "name": "Aaron Kirtland"
                    },
                    {
                        "name": "Omer Faruk Bodur"
                    },
                    {
                        "name": "D. P. Shinde"
                    },
                    {
                        "name": "Yan Carlos Leyva Labrador"
                    },
                    {
                        "name": "Zahra Adoul"
                    },
                    {
                        "name": "Mohamed Zekry"
                    },
                    {
                        "name": "Ali Karakoc"
                    },
                    {
                        "name": "Tania C. B. Santos"
                    },
                    {
                        "name": "Samir Shamseldeen"
                    },
                    {
                        "name": "Loukmane Karim"
                    },
                    {
                        "name": "Anna Liakhovitskaia"
                    },
                    {
                        "name": "Nate Resman"
                    },
                    {
                        "name": "Nicholas Farina"
                    },
                    {
                        "name": "Juan Carlos Gonzalez"
                    },
                    {
                        "name": "Gabe Maayan"
                    },
                    {
                        "name": "Earth Anderson"
                    },
                    {
                        "name": "Rodrigo De Oliveira Pena"
                    },
                    {
                        "name": "Elizabeth Kelley"
                    },
                    {
                        "name": "Hodjat Mariji"
                    },
                    {
                        "name": "Rasoul Pouriamanesh"
                    },
                    {
                        "name": "Wentao Wu"
                    },
                    {
                        "name": "Ross Finocchio"
                    },
                    {
                        "name": "Ismail Alarab"
                    },
                    {
                        "name": "Joshua Cole"
                    },
                    {
                        "name": "Danyelle Ferreira"
                    },
                    {
                        "name": "Bryan Johnson"
                    },
                    {
                        "name": "Mohammad Safdari"
                    },
                    {
                        "name": "Liangti Dai"
                    },
                    {
                        "name": "Siriphan Arthornthurasuk"
                    },
                    {
                        "name": "Isaac C. McAlister"
                    },
                    {
                        "name": "Alejandro José Moyano"
                    },
                    {
                        "name": "Alexey Pronin"
                    },
                    {
                        "name": "Jing Fan"
                    },
                    {
                        "name": "Angel Ramirez-Trinidad"
                    },
                    {
                        "name": "Yana Malysheva"
                    },
                    {
                        "name": "Daphiny Pottmaier"
                    },
                    {
                        "name": "Omid Taheri"
                    },
                    {
                        "name": "Stanley Stepanic"
                    },
                    {
                        "name": "Samuel Perry"
                    },
                    {
                        "name": "Luke Askew"
                    },
                    {
                        "name": "Raúl Adrián Huerta Rodríguez"
                    },
                    {
                        "name": "Ali M. R. Minissi"
                    },
                    {
                        "name": "Ricardo Lorena"
                    },
                    {
                        "name": "Krishnamurthy Iyer"
                    },
                    {
                        "name": "Arshad Anil Fasiludeen"
                    },
                    {
                        "name": "Ronald Clark"
                    },
                    {
                        "name": "Josh Ducey"
                    },
                    {
                        "name": "Matheus Piza"
                    },
                    {
                        "name": "Maja Somrak"
                    },
                    {
                        "name": "Eric Vergo"
                    },
                    {
                        "name": "Juehang Qin"
                    },
                    {
                        "name": "Benjámin Borbás"
                    },
                    {
                        "name": "Eric Chu"
                    },
                    {
                        "name": "Jack Lindsey"
                    },
                    {
                        "name": "Antoine Jallon"
                    },
                    {
                        "name": "I. M. J. McInnis"
                    },
                    {
                        "name": "Evan Chen"
                    },
                    {
                        "name": "Avi Semler"
                    },
                    {
                        "name": "Luk Gloor"
                    },
                    {
                        "name": "Tej Shah"
                    },
                    {
                        "name": "Marc Carauleanu"
                    },
                    {
                        "name": "Pascal Lauer"
                    },
                    {
                        "name": "Tran Đuc Huy"
                    },
                    {
                        "name": "Hossein Shahrtash"
                    },
                    {
                        "name": "Emilien Duc"
                    },
                    {
                        "name": "Lukas Lewark"
                    },
                    {
                        "name": "Assaf Brown"
                    },
                    {
                        "name": "Samuel Albanie"
                    },
                    {
                        "name": "Brian Weber"
                    },
                    {
                        "name": "Warren S. Vaz"
                    },
                    {
                        "name": "Pierre Clavier"
                    },
                    {
                        "name": "Yiyang Fan"
                    },
                    {
                        "name": "Gabriel Poesia Reis e Silva"
                    },
                    {
                        "name": "Long"
                    },
                    {
                        "name": "Lian"
                    },
                    {
                        "name": "Marcus Abramovitch"
                    },
                    {
                        "name": "Xi Jiang"
                    },
                    {
                        "name": "Sandra Mendoza"
                    },
                    {
                        "name": "Murat Islam"
                    },
                    {
                        "name": "Juan Gonzalez"
                    },
                    {
                        "name": "Vasilios Mavroudis"
                    },
                    {
                        "name": "Justin Xu"
                    },
                    {
                        "name": "Pawan Kumar"
                    },
                    {
                        "name": "Laxman Prasad Goswami"
                    },
                    {
                        "name": "Daniel Bugas"
                    },
                    {
                        "name": "Nasser Heydari"
                    },
                    {
                        "name": "Ferenc Jeanplong"
                    },
                    {
                        "name": "Thorben Jansen"
                    },
                    {
                        "name": "Antonella Pinto"
                    },
                    {
                        "name": "Archimedes Apronti"
                    },
                    {
                        "name": "Abdallah Galal"
                    },
                    {
                        "name": "Ng Ze-An"
                    },
                    {
                        "name": "Ankit Singh"
                    },
                    {
                        "name": "Tong Jiang"
                    },
                    {
                        "name": "Joan of Arc Xavier"
                    },
                    {
                        "name": "Kanu Priya Agarwal"
                    },
                    {
                        "name": "Mohammed Berkani"
                    },
                    {
                        "name": "Gang Zhang"
                    },
                    {
                        "name": "Zhehang Du"
                    },
                    {
                        "name": "Benedito Alves de Oliveira Junior"
                    },
                    {
                        "name": "Dmitry Malishev"
                    },
                    {
                        "name": "Nicolas Remy"
                    },
                    {
                        "name": "Taylor D. Hartman"
                    },
                    {
                        "name": "Tim Tarver"
                    },
                    {
                        "name": "Stephen Mensah"
                    },
                    {
                        "name": "Gautier Abou Loume"
                    },
                    {
                        "name": "Wiktor Morak"
                    },
                    {
                        "name": "Farzad Habibi"
                    },
                    {
                        "name": "Sarah Hoback"
                    },
                    {
                        "name": "Will Cai"
                    },
                    {
                        "name": "Javier Gimenez"
                    },
                    {
                        "name": "Roselynn Grace Montecillo"
                    },
                    {
                        "name": "Jakub Łucki"
                    },
                    {
                        "name": "Russell Campbell"
                    },
                    {
                        "name": "Asankhaya Sharma"
                    },
                    {
                        "name": "Khalida Meer"
                    },
                    {
                        "name": "Shreen Gul"
                    },
                    {
                        "name": "Daniel Espinosa Gonzalez"
                    },
                    {
                        "name": "Xavier Alapont"
                    },
                    {
                        "name": "Alex Hoover"
                    },
                    {
                        "name": "Gunjan Chhablani"
                    },
                    {
                        "name": "Freddie Vargus"
                    },
                    {
                        "name": "Arunim Agarwal"
                    },
                    {
                        "name": "Yibo Jiang"
                    },
                    {
                        "name": "Deepakkumar Patil"
                    },
                    {
                        "name": "David Outevsky"
                    },
                    {
                        "name": "Kevin Joseph Scaria"
                    },
                    {
                        "name": "Rajat Maheshwari"
                    },
                    {
                        "name": "Abdelkader Dendane"
                    },
                    {
                        "name": "Priti Shukla"
                    },
                    {
                        "name": "Ashley Cartwright"
                    },
                    {
                        "name": "Sergei Bogdanov"
                    },
                    {
                        "name": "Niels Mündler"
                    },
                    {
                        "name": "Sören Möller"
                    },
                    {
                        "name": "Luca Arnaboldi"
                    },
                    {
                        "name": "Kunvar Thaman"
                    },
                    {
                        "name": "Muhammad Rehan Siddiqi"
                    },
                    {
                        "name": "Prajvi Saxena"
                    },
                    {
                        "name": "Himanshu Gupta"
                    },
                    {
                        "name": "Tony Fruhauff"
                    },
                    {
                        "name": "Glen Sherman"
                    },
                    {
                        "name": "Mátyás Vincze"
                    },
                    {
                        "name": "Siranut Usawasutsakorn"
                    },
                    {
                        "name": "Dylan Ler"
                    },
                    {
                        "name": "Anil Radhakrishnan"
                    },
                    {
                        "name": "Innocent Enyekwe"
                    },
                    {
                        "name": "Sk Md Salauddin"
                    },
                    {
                        "name": "Jiang Muzhen"
                    },
                    {
                        "name": "Aleksandr Maksapetyan"
                    },
                    {
                        "name": "Vivien Rossbach"
                    },
                    {
                        "name": "Chris Harjadi"
                    },
                    {
                        "name": "Mohsen Bahaloohoreh"
                    },
                    {
                        "name": "Claire Sparrow"
                    },
                    {
                        "name": "Jasdeep Sidhu"
                    },
                    {
                        "name": "Sam Ali"
                    },
                    {
                        "name": "Song Bian"
                    },
                    {
                        "name": "John Lai"
                    },
                    {
                        "name": "Eric Singer"
                    },
                    {
                        "name": "Justine Leon Uro"
                    },
                    {
                        "name": "Greg Bateman"
                    },
                    {
                        "name": "Mohamed Sayed"
                    },
                    {
                        "name": "Ahmed Menshawy"
                    },
                    {
                        "name": "Darling Duclosel"
                    },
                    {
                        "name": "Dario Bezzi"
                    },
                    {
                        "name": "Yashaswini Jain"
                    },
                    {
                        "name": "Ashley Aaron"
                    },
                    {
                        "name": "Murat Tiryakioglu"
                    },
                    {
                        "name": "Sheeshram Siddh"
                    },
                    {
                        "name": "Keith Krenek"
                    },
                    {
                        "name": "Imad Ali Shah"
                    },
                    {
                        "name": "Jun Jin"
                    },
                    {
                        "name": "Scott Creighton"
                    },
                    {
                        "name": "Denis Peskoff"
                    },
                    {
                        "name": "Zienab EL-Wasif"
                    },
                    {
                        "name": "Ragavendran P V"
                    },
                    {
                        "name": "Michael Richmond"
                    },
                    {
                        "name": "Joseph McGowan"
                    },
                    {
                        "name": "Tejal Patwardhan"
                    },
                    {
                        "name": "Hao-Yu Sun"
                    },
                    {
                        "name": "Ting Sun"
                    },
                    {
                        "name": "Nikola Zubić"
                    },
                    {
                        "name": "Samuele Sala"
                    },
                    {
                        "name": "Stephen Ebert"
                    },
                    {
                        "name": "Jean Kaddour"
                    },
                    {
                        "name": "Manuel Schottdorf"
                    },
                    {
                        "name": "Dianzhuo Wang"
                    },
                    {
                        "name": "Gerol Petruzella"
                    },
                    {
                        "name": "Alex Meiburg"
                    },
                    {
                        "name": "Tilen Medved"
                    },
                    {
                        "name": "Ali ElSheikh"
                    },
                    {
                        "name": "S Ashwin Hebbar"
                    },
                    {
                        "name": "Lorenzo Vaquero"
                    },
                    {
                        "name": "Xianjun Yang"
                    },
                    {
                        "name": "Jason Poulos"
                    },
                    {
                        "name": "Vilém Zouhar"
                    },
                    {
                        "name": "Sergey Bogdanik"
                    },
                    {
                        "name": "Mingfang Zhang"
                    },
                    {
                        "name": "Jorge Sanz-Ros"
                    },
                    {
                        "name": "David Anugraha"
                    },
                    {
                        "name": "Yinwei Dai"
                    },
                    {
                        "name": "Anh N. Nhu"
                    },
                    {
                        "name": "Xue Wang"
                    },
                    {
                        "name": "Ali Anil Demircali"
                    },
                    {
                        "name": "Zhibai Jia"
                    },
                    {
                        "name": "Yuyin Zhou"
                    },
                    {
                        "name": "Juncheng Wu"
                    },
                    {
                        "name": "Mike He"
                    },
                    {
                        "name": "Nitin Chandok"
                    },
                    {
                        "name": "Aarush Sinha"
                    },
                    {
                        "name": "Gaoxiang Luo"
                    },
                    {
                        "name": "Long Le"
                    },
                    {
                        "name": "Mickaël Noyé"
                    },
                    {
                        "name": "Michał Perełkiewicz"
                    },
                    {
                        "name": "Ioannis Pantidis"
                    },
                    {
                        "name": "Tianbo Qi"
                    },
                    {
                        "name": "Soham Sachin Purohit"
                    },
                    {
                        "name": "Letitia Parcalabescu"
                    },
                    {
                        "name": "Thai-Hoa Nguyen"
                    },
                    {
                        "name": "Genta Indra Winata"
                    },
                    {
                        "name": "Edoardo M. Ponti"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Kaustubh Dhole"
                    },
                    {
                        "name": "Jongee Park"
                    },
                    {
                        "name": "Dario Abbondanza"
                    },
                    {
                        "name": "Yuanli Wang"
                    },
                    {
                        "name": "Anupam Nayak"
                    },
                    {
                        "name": "Diogo M. Caetano"
                    },
                    {
                        "name": "Antonio A. W. L. Wong"
                    },
                    {
                        "name": "Maria del Rio-Chanona"
                    },
                    {
                        "name": "Dániel Kondor"
                    },
                    {
                        "name": "Pieter Francois"
                    },
                    {
                        "name": "Ed Chalstrey"
                    },
                    {
                        "name": "Jakob Zsambok"
                    },
                    {
                        "name": "Dan Hoyer"
                    },
                    {
                        "name": "Jenny Reddish"
                    },
                    {
                        "name": "Jakob Hauser"
                    },
                    {
                        "name": "Francisco-Javier Rodrigo-Ginés"
                    },
                    {
                        "name": "Suchandra Datta"
                    },
                    {
                        "name": "Maxwell Shepherd"
                    },
                    {
                        "name": "Thom Kamphuis"
                    },
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Hyunjun Kim"
                    },
                    {
                        "name": "Ruiji Sun"
                    },
                    {
                        "name": "Jianzhu Yao"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Satyapriya Krishna"
                    },
                    {
                        "name": "Sina Rismanchian"
                    },
                    {
                        "name": "Bonan Pu"
                    },
                    {
                        "name": "Francesco Pinto"
                    },
                    {
                        "name": "Yingheng Wang"
                    },
                    {
                        "name": "Kumar Shridhar"
                    },
                    {
                        "name": "Kalon J. Overholt"
                    },
                    {
                        "name": "Glib Briia"
                    },
                    {
                        "name": "Hieu Nguyen"
                    },
                    {
                        "name": "David"
                    },
                    {
                        "name": "Soler Bartomeu"
                    },
                    {
                        "name": "Tony CY Pang"
                    },
                    {
                        "name": "Adam Wecker"
                    },
                    {
                        "name": "Yifan Xiong"
                    },
                    {
                        "name": "Fanfei Li"
                    },
                    {
                        "name": "Lukas S. Huber"
                    },
                    {
                        "name": "Joshua Jaeger"
                    },
                    {
                        "name": "Romano De Maddalena"
                    },
                    {
                        "name": "Xing Han Lù"
                    },
                    {
                        "name": "Yuhui Zhang"
                    },
                    {
                        "name": "Claas Beger"
                    },
                    {
                        "name": "Patrick Tser Jern Kon"
                    },
                    {
                        "name": "Sean Li"
                    },
                    {
                        "name": "Vivek Sanker"
                    },
                    {
                        "name": "Ming Yin"
                    },
                    {
                        "name": "Yihao Liang"
                    },
                    {
                        "name": "Xinlu Zhang"
                    },
                    {
                        "name": "Ankit Agrawal"
                    },
                    {
                        "name": "Li S. Yifei"
                    },
                    {
                        "name": "Zechen Zhang"
                    },
                    {
                        "name": "Mu Cai"
                    },
                    {
                        "name": "Yasin Sonmez"
                    },
                    {
                        "name": "Costin Cozianu"
                    },
                    {
                        "name": "Changhao Li"
                    },
                    {
                        "name": "Alex Slen"
                    },
                    {
                        "name": "Shoubin Yu"
                    },
                    {
                        "name": "Hyun Kyu Park"
                    },
                    {
                        "name": "Gabriele Sarti"
                    },
                    {
                        "name": "Marcin Briański"
                    },
                    {
                        "name": "Alessandro Stolfo"
                    },
                    {
                        "name": "Truong An Nguyen"
                    },
                    {
                        "name": "Mike Zhang"
                    },
                    {
                        "name": "Yotam Perlitz"
                    },
                    {
                        "name": "Jose Hernandez-Orallo"
                    },
                    {
                        "name": "Runjia Li"
                    },
                    {
                        "name": "Amin Shabani"
                    },
                    {
                        "name": "Felix Juefei-Xu"
                    },
                    {
                        "name": "Shikhar Dhingra"
                    },
                    {
                        "name": "Orr Zohar"
                    },
                    {
                        "name": "My Chiffon Nguyen"
                    },
                    {
                        "name": "Alexander Pondaven"
                    },
                    {
                        "name": "Abdurrahim Yilmaz"
                    },
                    {
                        "name": "Xuandong Zhao"
                    },
                    {
                        "name": "Chuanyang Jin"
                    },
                    {
                        "name": "Muyan Jiang"
                    },
                    {
                        "name": "Stefan Todoran"
                    },
                    {
                        "name": "Xinyao Han"
                    },
                    {
                        "name": "Jules Kreuer"
                    },
                    {
                        "name": "Brian Rabern"
                    },
                    {
                        "name": "Anna Plassart"
                    },
                    {
                        "name": "Martino Maggetti"
                    },
                    {
                        "name": "Luther Yap"
                    },
                    {
                        "name": "Robert Geirhos"
                    },
                    {
                        "name": "Jonathon Kean"
                    },
                    {
                        "name": "Dingsu Wang"
                    },
                    {
                        "name": "Sina Mollaei"
                    },
                    {
                        "name": "Chenkai Sun"
                    },
                    {
                        "name": "Yifan Yin"
                    },
                    {
                        "name": "Shiqi Wang"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Yaowen Chang"
                    },
                    {
                        "name": "Anjiang Wei"
                    },
                    {
                        "name": "Alice Bizeul"
                    },
                    {
                        "name": "Xiaohan Wang"
                    },
                    {
                        "name": "Alexandre Oliveira Arrais"
                    },
                    {
                        "name": "Kushin Mukherjee"
                    },
                    {
                        "name": "Jorge Chamorro-Padial"
                    },
                    {
                        "name": "Jiachen Liu"
                    },
                    {
                        "name": "Xingyu Qu"
                    },
                    {
                        "name": "Junyi Guan"
                    },
                    {
                        "name": "Adam Bouyamourn"
                    },
                    {
                        "name": "Shuyu Wu"
                    },
                    {
                        "name": "Martyna Plomecka"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "Mengze Tang"
                    },
                    {
                        "name": "Jiaqi Deng"
                    },
                    {
                        "name": "Shreyas Subramanian"
                    },
                    {
                        "name": "Haocheng Xi"
                    },
                    {
                        "name": "Haoxuan Chen"
                    },
                    {
                        "name": "Weizhi Zhang"
                    },
                    {
                        "name": "Yinuo Ren"
                    },
                    {
                        "name": "Haoqin Tu"
                    },
                    {
                        "name": "Sejong Kim"
                    },
                    {
                        "name": "Yushun Chen"
                    },
                    {
                        "name": "Sara Vera Marjanović"
                    },
                    {
                        "name": "Junwoo Ha"
                    },
                    {
                        "name": "Grzegorz Luczyna"
                    },
                    {
                        "name": "Jeff J. Ma"
                    },
                    {
                        "name": "Zewen Shen"
                    },
                    {
                        "name": "Dawn Song"
                    },
                    {
                        "name": "Cedegao E. Zhang"
                    },
                    {
                        "name": "Zhun Wang"
                    },
                    {
                        "name": "Gaël Gendron"
                    },
                    {
                        "name": "Yunze Xiao"
                    },
                    {
                        "name": "Leo Smucker"
                    },
                    {
                        "name": "Erica Weng"
                    },
                    {
                        "name": "Kwok Hao Lee"
                    },
                    {
                        "name": "Zhe Ye"
                    },
                    {
                        "name": "Stefano Ermon"
                    },
                    {
                        "name": "Ignacio D. Lopez-Miguel"
                    },
                    {
                        "name": "Theo Knights"
                    },
                    {
                        "name": "Anthony Gitter"
                    },
                    {
                        "name": "Namkyu Park"
                    },
                    {
                        "name": "Boyi Wei"
                    },
                    {
                        "name": "Hongzheng Chen"
                    },
                    {
                        "name": "Kunal Pai"
                    },
                    {
                        "name": "Ahmed Elkhanany"
                    },
                    {
                        "name": "Han Lin"
                    },
                    {
                        "name": "Philipp D. Siedler"
                    },
                    {
                        "name": "Jichao Fang"
                    },
                    {
                        "name": "Ritwik Mishra"
                    },
                    {
                        "name": "Károly Zsolnai-Fehér"
                    },
                    {
                        "name": "Xilin Jiang"
                    },
                    {
                        "name": "Shadab Khan"
                    },
                    {
                        "name": "Jun Yuan"
                    },
                    {
                        "name": "Rishab Kumar Jain"
                    },
                    {
                        "name": "Xi Lin"
                    },
                    {
                        "name": "Mike Peterson"
                    },
                    {
                        "name": "Zhe Wang"
                    },
                    {
                        "name": "Aditya Malusare"
                    },
                    {
                        "name": "Maosen Tang"
                    },
                    {
                        "name": "Isha Gupta"
                    },
                    {
                        "name": "Ivan Fosin"
                    },
                    {
                        "name": "Timothy Kang"
                    },
                    {
                        "name": "Barbara Dworakowska"
                    },
                    {
                        "name": "Kazuki Matsumoto"
                    },
                    {
                        "name": "Guangyao Zheng"
                    },
                    {
                        "name": "Gerben Sewuster"
                    },
                    {
                        "name": "Jorge Pretel Villanueva"
                    },
                    {
                        "name": "Ivan Rannev"
                    },
                    {
                        "name": "Igor Chernyavsky"
                    },
                    {
                        "name": "Jiale Chen"
                    },
                    {
                        "name": "Deepayan Banik"
                    },
                    {
                        "name": "Ben Racz"
                    },
                    {
                        "name": "Wenchao Dong"
                    },
                    {
                        "name": "Jianxin Wang"
                    },
                    {
                        "name": "Laila Bashmal"
                    },
                    {
                        "name": "Duarte V. Gonçalves"
                    },
                    {
                        "name": "Wei Hu"
                    },
                    {
                        "name": "Kaushik Bar"
                    },
                    {
                        "name": "Ondrej Bohdal"
                    },
                    {
                        "name": "Atharv Singh Patlan"
                    },
                    {
                        "name": "Shehzaad Dhuliawala"
                    },
                    {
                        "name": "Caroline Geirhos"
                    },
                    {
                        "name": "Julien Wist"
                    },
                    {
                        "name": "Yuval Kansal"
                    },
                    {
                        "name": "Bingsen Chen"
                    },
                    {
                        "name": "Kutay Tire"
                    },
                    {
                        "name": "Atak Talay Yücel"
                    },
                    {
                        "name": "Brandon Christof"
                    },
                    {
                        "name": "Veerupaksh Singla"
                    },
                    {
                        "name": "Zijian Song"
                    },
                    {
                        "name": "Sanxing Chen"
                    },
                    {
                        "name": "Jiaxin Ge"
                    },
                    {
                        "name": "Kaustubh Ponkshe"
                    },
                    {
                        "name": "Isaac Park"
                    },
                    {
                        "name": "Tianneng Shi"
                    },
                    {
                        "name": "Martin Q. Ma"
                    },
                    {
                        "name": "Joshua Mak"
                    },
                    {
                        "name": "Sherwin Lai"
                    },
                    {
                        "name": "Antoine Moulin"
                    },
                    {
                        "name": "Zhuo Cheng"
                    },
                    {
                        "name": "Zhanda Zhu"
                    },
                    {
                        "name": "Ziyi Zhang"
                    },
                    {
                        "name": "Vaidehi Patil"
                    },
                    {
                        "name": "Ketan Jha"
                    },
                    {
                        "name": "Qiutong Men"
                    },
                    {
                        "name": "Jiaxuan Wu"
                    },
                    {
                        "name": "Tianchi Zhang"
                    },
                    {
                        "name": "Bruno Hebling Vieira"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    },
                    {
                        "name": "Jae-Won Chung"
                    },
                    {
                        "name": "Mohammed Mahfoud"
                    },
                    {
                        "name": "Ha Thi Hoang"
                    },
                    {
                        "name": "Marc Sperzel"
                    },
                    {
                        "name": "Wei Hao"
                    },
                    {
                        "name": "Kristof Meding"
                    },
                    {
                        "name": "Sihan Xu"
                    },
                    {
                        "name": "Vassilis Kostakos"
                    },
                    {
                        "name": "Davide Manini"
                    },
                    {
                        "name": "Yueying Liu"
                    },
                    {
                        "name": "Christopher Toukmaji"
                    },
                    {
                        "name": "Jay Paek"
                    },
                    {
                        "name": "Eunmi Yu"
                    },
                    {
                        "name": "Arif Engin Demircali"
                    },
                    {
                        "name": "Zhiyi Sun"
                    },
                    {
                        "name": "Ivan Dewerpe"
                    },
                    {
                        "name": "Hongsen Qin"
                    },
                    {
                        "name": "Roman Pflugfelder"
                    },
                    {
                        "name": "James Bailey"
                    },
                    {
                        "name": "Johnathan Morris"
                    },
                    {
                        "name": "Ville Heilala"
                    },
                    {
                        "name": "Sybille Rosset"
                    },
                    {
                        "name": "Zishun Yu"
                    },
                    {
                        "name": "Peter E. Chen"
                    },
                    {
                        "name": "Woongyeong Yeo"
                    },
                    {
                        "name": "Eeshaan Jain"
                    },
                    {
                        "name": "Ryan Yang"
                    },
                    {
                        "name": "Sreekar Chigurupati"
                    },
                    {
                        "name": "Julia Chernyavsky"
                    },
                    {
                        "name": "Sai Prajwal Reddy"
                    },
                    {
                        "name": "Subhashini Venugopalan"
                    },
                    {
                        "name": "Hunar Batra"
                    },
                    {
                        "name": "Core Francisco Park"
                    },
                    {
                        "name": "Hieu Tran"
                    },
                    {
                        "name": "Guilherme Maximiano"
                    },
                    {
                        "name": "Genghan Zhang"
                    },
                    {
                        "name": "Yizhuo Liang"
                    },
                    {
                        "name": "Hu Shiyu"
                    },
                    {
                        "name": "Rongwu Xu"
                    },
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Siddharth Suresh"
                    },
                    {
                        "name": "Ziqi Liu"
                    },
                    {
                        "name": "Samaksh Gulati"
                    },
                    {
                        "name": "Songyang Zhang"
                    },
                    {
                        "name": "Peter Turchin"
                    },
                    {
                        "name": "Christopher W. Bartlett"
                    },
                    {
                        "name": "Christopher R. Scotese"
                    },
                    {
                        "name": "Phuong M. Cao"
                    },
                    {
                        "name": "Aakaash Nattanmai"
                    },
                    {
                        "name": "Gordon McKellips"
                    },
                    {
                        "name": "Anish Cheraku"
                    },
                    {
                        "name": "Asim Suhail"
                    },
                    {
                        "name": "Ethan Luo"
                    },
                    {
                        "name": "Marvin Deng"
                    },
                    {
                        "name": "Jason Luo"
                    },
                    {
                        "name": "Ashley Zhang"
                    },
                    {
                        "name": "Kavin Jindel"
                    },
                    {
                        "name": "Jay Paek"
                    },
                    {
                        "name": "Kasper Halevy"
                    },
                    {
                        "name": "Allen Baranov"
                    },
                    {
                        "name": "Michael Liu"
                    },
                    {
                        "name": "Advaith Avadhanam"
                    },
                    {
                        "name": "David Zhang"
                    },
                    {
                        "name": "Vincent Cheng"
                    },
                    {
                        "name": "Brad Ma"
                    },
                    {
                        "name": "Evan Fu"
                    },
                    {
                        "name": "Liam Do"
                    },
                    {
                        "name": "Joshua Lass"
                    },
                    {
                        "name": "Hubert Yang"
                    },
                    {
                        "name": "Surya Sunkari"
                    },
                    {
                        "name": "Vishruth Bharath"
                    },
                    {
                        "name": "Violet Ai"
                    },
                    {
                        "name": "James Leung"
                    },
                    {
                        "name": "Rishit Agrawal"
                    },
                    {
                        "name": "Alan Zhou"
                    },
                    {
                        "name": "Kevin Chen"
                    },
                    {
                        "name": "Tejas Kalpathi"
                    },
                    {
                        "name": "Ziqi Xu"
                    },
                    {
                        "name": "Gavin Wang"
                    },
                    {
                        "name": "Tyler Xiao"
                    },
                    {
                        "name": "Erik Maung"
                    },
                    {
                        "name": "Sam Lee"
                    },
                    {
                        "name": "Ryan Yang"
                    },
                    {
                        "name": "Roy Yue"
                    },
                    {
                        "name": "Ben Zhao"
                    },
                    {
                        "name": "Julia Yoon"
                    },
                    {
                        "name": "Sunny Sun"
                    },
                    {
                        "name": "Aryan Singh"
                    },
                    {
                        "name": "Ethan Luo"
                    },
                    {
                        "name": "Clark Peng"
                    },
                    {
                        "name": "Tyler Osbey"
                    },
                    {
                        "name": "Taozhi Wang"
                    },
                    {
                        "name": "Daryl Echeazu"
                    },
                    {
                        "name": "Hubert Yang"
                    },
                    {
                        "name": "Timothy Wu"
                    },
                    {
                        "name": "Spandan Patel"
                    },
                    {
                        "name": "Vidhi Kulkarni"
                    },
                    {
                        "name": "Vijaykaarti Sundarapandiyan"
                    },
                    {
                        "name": "Ashley Zhang"
                    },
                    {
                        "name": "Andrew Le"
                    },
                    {
                        "name": "Zafir Nasim"
                    },
                    {
                        "name": "Srikar Yalam"
                    },
                    {
                        "name": "Ritesh Kasamsetty"
                    },
                    {
                        "name": "Soham Samal"
                    },
                    {
                        "name": "Hubert Yang"
                    },
                    {
                        "name": "David Sun"
                    },
                    {
                        "name": "Nihar Shah"
                    },
                    {
                        "name": "Abhijeet Saha"
                    },
                    {
                        "name": "Alex Zhang"
                    },
                    {
                        "name": "Leon Nguyen"
                    },
                    {
                        "name": "Laasya Nagumalli"
                    },
                    {
                        "name": "Kaixin Wang"
                    },
                    {
                        "name": "Alan Zhou"
                    },
                    {
                        "name": "Aidan Wu"
                    },
                    {
                        "name": "Jason Luo"
                    },
                    {
                        "name": "Anwith Telluri"
                    },
                    {
                        "name": "Summer Yue"
                    },
                    {
                        "name": "Alexandr Wang"
                    },
                    {
                        "name": "Dan Hendrycks"
                    }
                ],
                "author_detail": {
                    "name": "Dan Hendrycks"
                },
                "arxiv_affiliation": "Quod",
                "author": "Dan Hendrycks",
                "arxiv_comment": "29 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14249v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14249v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09947v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09947v1",
                "updated": "2025-09-12T03:38:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    3,
                    38,
                    15,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T03:38:15Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    3,
                    38,
                    15,
                    4,
                    255,
                    0
                ],
                "title": "Toward Green Code: Prompting Small Language Models for Energy-Efficient\n  Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Green Code: Prompting Small Language Models for Energy-Efficient\n  Code Generation"
                },
                "summary": "There is a growing concern about the environmental impact of large language\nmodels (LLMs) in software development, particularly due to their high energy\nuse and carbon footprint. Small Language Models (SLMs) offer a more sustainable\nalternative, requiring fewer computational resources while remaining effective\nfor fundamental programming tasks. In this study, we investigate whether prompt\nengineering can improve the energy efficiency of SLMs in code generation. We\nevaluate four open-source SLMs, StableCode-Instruct-3B,\nQwen2.5-Coder-3B-Instruct, CodeLlama-7B-Instruct, and Phi-3-Mini-4K-Instruct,\nacross 150 Python problems from LeetCode, evenly distributed into easy, medium,\nand hard categories. Each model is tested under four prompting strategies: role\nprompting, zero-shot, few-shot, and chain-of-thought (CoT). For every generated\nsolution, we measure runtime, memory usage, and energy consumption, comparing\nthe results with a human-written baseline. Our findings show that CoT prompting\nprovides consistent energy savings for Qwen2.5-Coder and StableCode-3B, while\nCodeLlama-7B and Phi-3-Mini-4K fail to outperform the baseline under any\nprompting strategy. These results highlight that the benefits of prompting are\nmodel-dependent and that carefully designed prompts can guide SLMs toward\ngreener software development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a growing concern about the environmental impact of large language\nmodels (LLMs) in software development, particularly due to their high energy\nuse and carbon footprint. Small Language Models (SLMs) offer a more sustainable\nalternative, requiring fewer computational resources while remaining effective\nfor fundamental programming tasks. In this study, we investigate whether prompt\nengineering can improve the energy efficiency of SLMs in code generation. We\nevaluate four open-source SLMs, StableCode-Instruct-3B,\nQwen2.5-Coder-3B-Instruct, CodeLlama-7B-Instruct, and Phi-3-Mini-4K-Instruct,\nacross 150 Python problems from LeetCode, evenly distributed into easy, medium,\nand hard categories. Each model is tested under four prompting strategies: role\nprompting, zero-shot, few-shot, and chain-of-thought (CoT). For every generated\nsolution, we measure runtime, memory usage, and energy consumption, comparing\nthe results with a human-written baseline. Our findings show that CoT prompting\nprovides consistent energy savings for Qwen2.5-Coder and StableCode-3B, while\nCodeLlama-7B and Phi-3-Mini-4K fail to outperform the baseline under any\nprompting strategy. These results highlight that the benefits of prompting are\nmodel-dependent and that carefully designed prompts can guide SLMs toward\ngreener software development."
                },
                "authors": [
                    {
                        "name": "Humza Ashraf"
                    },
                    {
                        "name": "Syed Muhammad Danish"
                    },
                    {
                        "name": "Zeeshan Sattar"
                    }
                ],
                "author_detail": {
                    "name": "Zeeshan Sattar"
                },
                "author": "Zeeshan Sattar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09947v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04996v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04996v5",
                "updated": "2025-09-12T03:15:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    3,
                    15,
                    11,
                    4,
                    255,
                    0
                ],
                "published": "2025-07-07T13:34:49Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    34,
                    49,
                    0,
                    188,
                    0
                ],
                "title": "Agentic Vehicles for Human-Centered Mobility Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Vehicles for Human-Centered Mobility Systems"
                },
                "summary": "Autonomy, from the Greek autos (self) and nomos (law), refers to the capacity\nto operate according to internal rules without external control. Autonomous\nvehicles (AuVs) are therefore understood as systems that perceive their\nenvironment and execute pre-programmed tasks independently of external input,\nconsistent with the SAE levels of automated driving. Yet recent research and\nreal-world deployments have begun to showcase vehicles that exhibit behaviors\noutside the scope of this definition. These include natural language\ninteraction with humans, goal adaptation, contextual reasoning, external tool\nuse, and the handling of unforeseen ethical dilemmas, enabled in part by\nmultimodal large language models (LLMs). These developments highlight not only\na gap between technical autonomy and the broader cognitive and social\ncapacities required for human-centered mobility, but also the emergence of a\nform of vehicle intelligence that currently lacks a clear designation. To\naddress this gap, the paper introduces the concept of agentic vehicles (AgVs):\nvehicles that integrate agentic AI systems to reason, adapt, and interact\nwithin complex environments. It synthesizes recent advances in agentic systems\nand suggests how AgVs can complement and even reshape conventional autonomy to\nensure mobility services are aligned with user and societal needs. The paper\nconcludes by outlining key challenges in the development and governance of AgVs\nand their potential role in shaping future agentic transportation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomy, from the Greek autos (self) and nomos (law), refers to the capacity\nto operate according to internal rules without external control. Autonomous\nvehicles (AuVs) are therefore understood as systems that perceive their\nenvironment and execute pre-programmed tasks independently of external input,\nconsistent with the SAE levels of automated driving. Yet recent research and\nreal-world deployments have begun to showcase vehicles that exhibit behaviors\noutside the scope of this definition. These include natural language\ninteraction with humans, goal adaptation, contextual reasoning, external tool\nuse, and the handling of unforeseen ethical dilemmas, enabled in part by\nmultimodal large language models (LLMs). These developments highlight not only\na gap between technical autonomy and the broader cognitive and social\ncapacities required for human-centered mobility, but also the emergence of a\nform of vehicle intelligence that currently lacks a clear designation. To\naddress this gap, the paper introduces the concept of agentic vehicles (AgVs):\nvehicles that integrate agentic AI systems to reason, adapt, and interact\nwithin complex environments. It synthesizes recent advances in agentic systems\nand suggests how AgVs can complement and even reshape conventional autonomy to\nensure mobility services are aligned with user and societal needs. The paper\nconcludes by outlining key challenges in the development and governance of AgVs\nand their potential role in shaping future agentic transportation systems."
                },
                "authors": [
                    {
                        "name": "Jiangbo Yu"
                    }
                ],
                "author_detail": {
                    "name": "Jiangbo Yu"
                },
                "author": "Jiangbo Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04996v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04996v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09942v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09942v1",
                "updated": "2025-09-12T03:14:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    3,
                    14,
                    50,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T03:14:50Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    3,
                    14,
                    50,
                    4,
                    255,
                    0
                ],
                "title": "SmartCoder-R1: Towards Secure and Explainable Smart Contract Generation\n  with Security-Aware Group Relative Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmartCoder-R1: Towards Secure and Explainable Smart Contract Generation\n  with Security-Aware Group Relative Policy Optimization"
                },
                "summary": "Smart contracts automate the management of high-value assets, where\nvulnerabilities can lead to catastrophic financial losses. This challenge is\namplified in Large Language Models (LLMs) by two interconnected failures: they\noperate as unauditable \"black boxes\" lacking a transparent reasoning process,\nand consequently, generate code riddled with critical security vulnerabilities.\nTo address both issues, we propose SmartCoder-R1 (based on Qwen2.5-Coder-7B), a\nnovel framework for secure and explainable smart contract generation. It begins\nwith Continual Pre-training (CPT) to specialize the model. We then apply Long\nChain-of-Thought Supervised Fine-Tuning (L-CoT SFT) on 7,998 expert-validated\nreasoning-and-code samples to train the model to emulate human security\nanalysis. Finally, to directly mitigate vulnerabilities, we employ\nSecurity-Aware Group Relative Policy Optimization (S-GRPO), a reinforcement\nlearning phase that refines the generation policy by optimizing a weighted\nreward signal for compilation success, security compliance, and format\ncorrectness. Evaluated against 17 baselines on a benchmark of 756 real-world\nfunctions, SmartCoder-R1 establishes a new state of the art, achieving top\nperformance across five key metrics: a ComPass of 87.70%, a VulRate of 8.60%, a\nSafeAval of 80.16%, a FuncRate of 53.84%, and a FullRate of 50.53%. This\nFullRate marks a 45.79% relative improvement over the strongest baseline,\nDeepSeek-R1. Crucially, its generated reasoning also excels in human\nevaluations, achieving high-quality ratings for Functionality (82.7%), Security\n(85.3%), and Clarity (90.7%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart contracts automate the management of high-value assets, where\nvulnerabilities can lead to catastrophic financial losses. This challenge is\namplified in Large Language Models (LLMs) by two interconnected failures: they\noperate as unauditable \"black boxes\" lacking a transparent reasoning process,\nand consequently, generate code riddled with critical security vulnerabilities.\nTo address both issues, we propose SmartCoder-R1 (based on Qwen2.5-Coder-7B), a\nnovel framework for secure and explainable smart contract generation. It begins\nwith Continual Pre-training (CPT) to specialize the model. We then apply Long\nChain-of-Thought Supervised Fine-Tuning (L-CoT SFT) on 7,998 expert-validated\nreasoning-and-code samples to train the model to emulate human security\nanalysis. Finally, to directly mitigate vulnerabilities, we employ\nSecurity-Aware Group Relative Policy Optimization (S-GRPO), a reinforcement\nlearning phase that refines the generation policy by optimizing a weighted\nreward signal for compilation success, security compliance, and format\ncorrectness. Evaluated against 17 baselines on a benchmark of 756 real-world\nfunctions, SmartCoder-R1 establishes a new state of the art, achieving top\nperformance across five key metrics: a ComPass of 87.70%, a VulRate of 8.60%, a\nSafeAval of 80.16%, a FuncRate of 53.84%, and a FullRate of 50.53%. This\nFullRate marks a 45.79% relative improvement over the strongest baseline,\nDeepSeek-R1. Crucially, its generated reasoning also excels in human\nevaluations, achieving high-quality ratings for Functionality (82.7%), Security\n(85.3%), and Clarity (90.7%)."
                },
                "authors": [
                    {
                        "name": "Lei Yu"
                    },
                    {
                        "name": "Jingyuan Zhang"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Jiajia Ma"
                    },
                    {
                        "name": "Li Yang"
                    },
                    {
                        "name": "Fengjun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Fengjun Zhang"
                },
                "author": "Fengjun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09942v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09942v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15178v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15178v2",
                "updated": "2025-09-12T03:03:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    3,
                    3,
                    36,
                    4,
                    255,
                    0
                ],
                "published": "2025-02-21T03:16:23Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    3,
                    16,
                    23,
                    4,
                    52,
                    0
                ],
                "title": "Enhancing Speech Large Language Models with Prompt-Aware Mixture of\n  Audio Encoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Speech Large Language Models with Prompt-Aware Mixture of\n  Audio Encoders"
                },
                "summary": "Connecting audio encoders with large language models (LLMs) allows the LLM to\nperform various audio understanding tasks, such as automatic speech recognition\n(ASR) and audio captioning (AC). Most research focuses on training an adapter\nlayer to generate a unified audio feature for the LLM. However, different tasks\nmay require distinct features that emphasize either semantic or acoustic\naspects, making task-specific audio features more desirable. In this paper, we\npropose Prompt-aware Mixture (PaM) to enhance the Speech LLM that uses multiple\naudio encoders. Our approach involves using different experts to extract\ndifferent features based on the prompt that indicates different tasks.\nExperiments demonstrate that with PaM, only one Speech LLM surpasses the best\nperformances achieved by all single-encoder Speech LLMs on ASR, Speaker Number\nVerification, and AC tasks. PaM also outperforms other feature fusion\nbaselines, such as concatenation and averaging. Our code would be available at:\nhttps://github.com/shanweiqiao/PaM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Connecting audio encoders with large language models (LLMs) allows the LLM to\nperform various audio understanding tasks, such as automatic speech recognition\n(ASR) and audio captioning (AC). Most research focuses on training an adapter\nlayer to generate a unified audio feature for the LLM. However, different tasks\nmay require distinct features that emphasize either semantic or acoustic\naspects, making task-specific audio features more desirable. In this paper, we\npropose Prompt-aware Mixture (PaM) to enhance the Speech LLM that uses multiple\naudio encoders. Our approach involves using different experts to extract\ndifferent features based on the prompt that indicates different tasks.\nExperiments demonstrate that with PaM, only one Speech LLM surpasses the best\nperformances achieved by all single-encoder Speech LLMs on ASR, Speaker Number\nVerification, and AC tasks. PaM also outperforms other feature fusion\nbaselines, such as concatenation and averaging. Our code would be available at:\nhttps://github.com/shanweiqiao/PaM"
                },
                "authors": [
                    {
                        "name": "Weiqiao Shan"
                    },
                    {
                        "name": "Yuang Li"
                    },
                    {
                        "name": "Yuhao Zhang"
                    },
                    {
                        "name": "Yingfeng Luo"
                    },
                    {
                        "name": "Chen Xu"
                    },
                    {
                        "name": "Xiaofeng Zhao"
                    },
                    {
                        "name": "Long Meng"
                    },
                    {
                        "name": "Yunfei Lu"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Hao Yang"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "16 pages,4 figures, 16 tables, to be published in EMNLP 2025 main\n  conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15178v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15178v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]